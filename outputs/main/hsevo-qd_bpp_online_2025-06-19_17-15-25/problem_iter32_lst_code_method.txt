{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by fit, fullness, adaptive scaling, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor, prioritize almost full bins\n        fullness = bins_remain_cap[fit_indices] / bins_remain_cap.max()\n        fullness_priority = 1.0 - fullness # smaller remaining -> higher priority\n\n        # Remaining capacity factor, prioritize small remaining capacity\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)\n\n        # Combined priority: weighted sum of fullness and remaining capacity.\n        # Adapt weights based on item size; larger items favor fullness.\n        weight_fullness = min(item, 0.75)  # Weight increases with item size, up to 0.75\n        weight_remaining = 1.0 - weight_fullness\n        combined_priority = weight_fullness * fullness_priority + weight_remaining * remaining_cap_priority\n\n        # Adaptive scaling based on average remaining capacity and item size\n        scale = np.mean(bins_remain_cap) + item\n        priorities[fit_indices] = combined_priority / (scale + 1e-9)\n\n        # Exploration strategy:\n        exploration_strength = min(0.05, item / 5)  # Exploration scales with item size\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n    else:\n        # If no bin fits, give a negative priority to discourage this action.\n        priorities = np.full_like(priorities, -1e9)\n        return priorities\n\n    # Penalize bins where the item doesn't fit by setting priority to a very low value\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases.\n    # Apply only if there's a positive priority.\n    if np.any(priorities > 0):\n        sum_priorities = np.sum(priorities[priorities > 0])\n        if sum_priorities > 0:\n            priorities = priorities / sum_priorities\n        else:\n            priorities = np.zeros_like(priorities)  # all priorities are 0\n    elif np.any(priorities < 0) and np.all(priorities <= 0): #all negative\n         priorities = np.zeros_like(priorities)  # all priorities are 0\n    else:\n         priorities = np.zeros_like(priorities)\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on multiple factors, dynamically adjusting parameters based on context.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero\n\n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        # Data-driven adaptive scaling based on item and bin characteristics\n        bin_capacity_mean = np.mean(bins_remain_cap[fit_indices])\n        item_scale = min(1.0, item / bin_capacity_mean) #Scale relative to the average bin size. Cap at 1 to avoid excessive scaling for large item\n\n        # Contextual Scaling: Dynamically adjust the scale based on item size relative to bin capacity\n        scale = bin_capacity_mean * (1 - 0.5 * item_scale)  # Reduce scale for larger items to encourage filling smaller bins first\n        priorities[fit_indices] = combined_priority / (scale + 1e-9)  # Adding small value to avoid potential division by zero\n\n        # Calibrated Randomness: Item-size aware and decaying randomness with capacity awareness\n        exploration_strength = min(0.1, item / bin_capacity_mean)  # Smaller items relative to bin capacity get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Robust Handling and Normalization\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_number: float = 5.103658190440242e-09, exploration_rate: float = 0.005800481282238711, max_exploration: float = 0.18151908939431077, penalty: float = -3710931775.991233) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + small_number)\n\n        # Dynamic exploration factor\n        exploration_factor = min(max_exploration, exploration_rate * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = penalty\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_number: float = 5.103658190440242e-09, exploration_rate: float = 0.005800481282238711, max_exploration: float = 0.18151908939431077, penalty: float = -3710931775.991233) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + small_number)\n\n        # Dynamic exploration factor\n        exploration_factor = min(max_exploration, exploration_rate * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = penalty\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_number: float = 5.103658190440242e-09, exploration_rate: float = 0.005800481282238711, max_exploration: float = 0.18151908939431077, penalty: float = -3710931775.991233) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + small_number)\n\n        # Dynamic exploration factor\n        exploration_factor = min(max_exploration, exploration_rate * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = penalty\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive scaling, fit, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive scaling, fit, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive scaling, fit, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                randomness_strength: float = 0.09055507501984036,\n                no_fit_priority: float = -1668213914.7516727,\n                epsilon: float = 4.1062069920696874e-09) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + epsilon)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * randomness_strength\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                randomness_strength: float = 0.09055507501984036,\n                no_fit_priority: float = -1668213914.7516727,\n                epsilon: float = 4.1062069920696874e-09) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + epsilon)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * randomness_strength\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on a combination of factors, including best fit,\n    fullness, and adaptive scaling, with a focus on balancing exploration and exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit component: Prioritize bins where the remaining capacity is smallest.\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness component: Prioritize bins that are already relatively full.\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive Scaling: Scale based on item size.  Larger items favor tighter fits.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority  # Scale best-fit\n\n        # Combine priorities. Use a weighted average to balance fit and fullness.\n        priorities[fit_indices] = 0.6 * adaptive_priority + 0.4 * fullness_priority\n\n        # Introduce more strategic randomness (exploration) proportional to remaining space.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Bins where the item doesn't fit get a very low priority.\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize only if positive priorities exist\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n    #Else do nothing: all zeroes or all negative\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_random_component_weight: float = 0.034626156087, not_fit_priority: float = -4265404883.739313, division_eps: float = 2.4217411725186963e-09, fit_priority_increase_factor: float = 0.9670808628354719) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins that are already relatively full, to reduce fragmentation.\n    It also includes a small random component to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_random_component_weight: Weight of the small random component. Default is 0.01.\n        not_fit_priority: Priority given to bins where the item doesn't fit. Default is -1e9.\n        division_eps: Epsilon value to avoid division by zero. Default is 1e-9.\n        fit_priority_increase_factor: Factor to increase priority of bins where item fits. Default is 1.0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        # Prioritize bins that are already relatively full.  We want to minimize fragmentation.\n        # The smaller the remaining capacity after adding the item, the higher the priority.\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices] + fit_priority_increase_factor / (remaining_capacity[fit_indices] + division_eps) # (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices]   # Avoid division by zero\n        \n        # Add a small random component to encourage exploration and escape local optima\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * small_random_component_weight\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = not_fit_priority  # Large negative value\n    \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fit, fullness, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Incorporate some randomness for exploration\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and item-aware exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit priority\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness priority\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive scaling based on item size and bin capacity.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority\n\n        # Combine priorities with adaptive weights.\n        weight_fit = 0.5 + 0.5 * (item / np.max(bins_remain_cap)) # Larger items favor fit more\n        priorities[fit_indices] = weight_fit * adaptive_priority + (1 - weight_fit) * fullness_priority\n\n        # Strategic exploration based on remaining capacity and item size.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * (item / np.max(bins_remain_cap)) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Very low priority for bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on remaining capacity, fit, item size, and bin occupancy, with adaptive scaling and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # 1. Prioritize bins with tighter fit, encouraging better space utilization\n        fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Prioritize bins with higher occupancy (more items already packed), aiming to consolidate packings\n        occupancy_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+ 1e-9)\n\n        # 3. Adaptive scaling based on the item size relative to average remaining capacity.  Larger items favor almost full bins\n        scale = np.mean(bins_remain_cap[fit_indices])\n        item_size_priority = (item / (scale + 1e-9)) \n\n        #4. Combine priorities with weights. Dynamically adjust weights based on performance characteristics (omitted for simplicity but crucial in a real-world scenario).\n        alpha, beta, gamma = 0.6, 0.3, 0.1 # Weights can be dynamically tuned. Experiment with different values to improve performance.\n        priorities[fit_indices] = alpha * fit_priority + beta * occupancy_priority + gamma * item_size_priority\n        \n        # 5. Constrained Random Exploration: Add small noise only to promising bins\n        exploration_noise = np.random.rand(np.sum(fit_indices)) * 0.01\n        priorities[fit_indices] += exploration_noise\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n    \n    # Normalize priorities to a probability distribution\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n          priorities = priorities - np.min(priorities)\n          priorities = priorities / np.sum(priorities)\n\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 4.80808521021539,\n                no_fit_priority: float = -4449241166.928589,\n                avoid_zero_division: float = 9.480380501898912e-09) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}