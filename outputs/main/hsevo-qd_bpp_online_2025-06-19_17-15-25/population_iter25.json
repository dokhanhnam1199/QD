[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = 1 / (remaining_capacity[fit_indices] + 1e-9)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 6.0,
    "halstead": 439.44362512259653,
    "mi": 78.54110725816716,
    "token_count": 200.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fit and remaining capacity, with normalization.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    \n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = 1 / (remaining_capacity[fit_indices] + 1e-9)\n        \n    priorities[remaining_capacity < 0] = -1e9\n\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 142.7018117963935,
    "mi": 83.9064421464444,
    "token_count": 158.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins that are already relatively full, to reduce fragmentation.\n    It also includes a small random component to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        # Prioritize bins that are already relatively full.  We want to minimize fragmentation.\n        # The smaller the remaining capacity after adding the item, the higher the priority.\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices] + 1 / (remaining_capacity[fit_indices] + 1e-9) # (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices]   # Avoid division by zero\n        \n        # Add a small random component to encourage exploration and escape local optima\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9  # Large negative value\n    \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    \n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 289.5158000807695,
    "mi": 78.10593116293605,
    "token_count": 224.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 4.80808521021539,\n                no_fit_priority: float = -4449241166.928589,\n                avoid_zero_division: float = 9.480380501898912e-09) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 129.65784284662087,
    "mi": 86.12855289617025,
    "token_count": 185.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices]) # Dynamic scale \n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Incorporate some randomness for exploration\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 3.9788591942560925,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 211.52361657053456,
    "mi": 84.12654317825582,
    "token_count": 183.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on remaining capacity, fit, item size, and bin occupancy, with adaptive scaling and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # 1. Prioritize bins with tighter fit, encouraging better space utilization\n        fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Prioritize bins with higher occupancy (more items already packed), aiming to consolidate packings\n        occupancy_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+ 1e-9)\n\n        # 3. Adaptive scaling based on the item size relative to average remaining capacity.  Larger items favor almost full bins\n        scale = np.mean(bins_remain_cap[fit_indices])\n        item_size_priority = (item / (scale + 1e-9)) \n\n        #4. Combine priorities with weights. Dynamically adjust weights based on performance characteristics (omitted for simplicity but crucial in a real-world scenario).\n        alpha, beta, gamma = 0.6, 0.3, 0.1 # Weights can be dynamically tuned. Experiment with different values to improve performance.\n        priorities[fit_indices] = alpha * fit_priority + beta * occupancy_priority + gamma * item_size_priority\n        \n        # 5. Constrained Random Exploration: Add small noise only to promising bins\n        exploration_noise = np.random.rand(np.sum(fit_indices)) * 0.01\n        priorities[fit_indices] += exploration_noise\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n    \n    # Normalize priorities to a probability distribution\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n          priorities = priorities - np.min(priorities)\n          priorities = priorities / np.sum(priorities)\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 286.51993510959227,
    "mi": 78.93427044953592,
    "token_count": 322.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter19_response1.txt_stdout.txt",
    "code_path": "problem_iter19_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, small_random_component_weight: float = 0.034626156087, not_fit_priority: float = -4265404883.739313, division_eps: float = 2.4217411725186963e-09, fit_priority_increase_factor: float = 0.9670808628354719) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version prioritizes bins that are already relatively full, to reduce fragmentation.\n    It also includes a small random component to encourage exploration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_random_component_weight: Weight of the small random component. Default is 0.01.\n        not_fit_priority: Priority given to bins where the item doesn't fit. Default is -1e9.\n        division_eps: Epsilon value to avoid division by zero. Default is 1e-9.\n        fit_priority_increase_factor: Factor to increase priority of bins where item fits. Default is 1.0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        # Prioritize bins that are already relatively full.  We want to minimize fragmentation.\n        # The smaller the remaining capacity after adding the item, the higher the priority.\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices] + fit_priority_increase_factor / (remaining_capacity[fit_indices] + division_eps) # (bins_remain_cap[fit_indices] - remaining_capacity[fit_indices]) / bins_remain_cap[fit_indices]   # Avoid division by zero\n        \n        # Add a small random component to encourage exploration and escape local optima\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * small_random_component_weight\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = not_fit_priority  # Large negative value\n    \n    # Normalize the priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    \n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.038691663342641,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 222.9388339674094,
    "mi": 72.50077767752744,
    "token_count": 232.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 6,
    "tryHS": true,
    "obj": 3.948942959712818,
    "SLOC": 13.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 180.68572508221183,
    "mi": 79.89045254875423,
    "token_count": 172.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on a combination of factors, focusing on best fit,\n    penalizing near misses, and adaptively scaling based on item size.  Includes a\n    dynamically adjusted exploration factor.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n    n_bins = len(bins_remain_cap)\n\n    if np.any(fit_indices):\n        # 1. Best Fit Emphasis: Prioritize bins where the item fits best (smallest waste).\n        priorities[fit_indices] = 1.0 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Penalize Near Misses (Bins that are just too small): Soft constraint handling.\n        near_miss_indices = (remaining_capacity < 0) & (bins_remain_cap >= (item - 0.1 * item)) # within 10% of fitting\n        priorities[near_miss_indices] = -0.5 # Small negative priority; consider if other options exist\n\n        # 3. Adaptive Scaling Based on Item Size: Adjust priority scaling based on the item's relative size.\n        item_size_ratio = item / np.mean(bins_remain_cap[fit_indices]) if np.any(fit_indices) else 0.5  #Ratio of the current item size to average bin capacity.\n        scale_factor = max(0.1, min(1.0, item_size_ratio))  # Clamp to [0.1, 1] to avoid extreme scaling\n        priorities[fit_indices] *= scale_factor\n\n        # 4. Dynamic Exploration:  Introduce randomness that *decreases* as more bins become suitable.\n        #    Fewer suitable bins = more exploration needed.\n        exploration_factor = 0.1 * (1 - (np.sum(fit_indices) / n_bins))  # Scale randomness based on fit ratio\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    else:\n        # If NO bins fit, give a very small non-zero chance to bins which are closest to the required size.\n        closest_bin_index = np.argmin(bins_remain_cap)\n        priorities[closest_bin_index] = 0.0001\n\n    # 5. Bins where the item doesn't fit at all get a very negative priority (unless we're *forced* to use them).\n    priorities[remaining_capacity < 0] = np.where(priorities[remaining_capacity < 0] != -0.5, -1e9, priorities[remaining_capacity < 0])\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    priority_sum = np.sum(priorities)\n    if priority_sum > 0:\n        priorities = priorities / priority_sum\n    elif priority_sum < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.full_like(priorities, 1/len(priorities))\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 400.90527603206624,
    "mi": 79.76664761002536,
    "token_count": 328.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter22_response0.txt_stdout.txt",
    "code_path": "problem_iter22_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 3.13991587165158,\n                no_fit_priority: float = -5972937806.369464,\n                avoid_zero_division: float = 4.3625326870224956e-09,\n                priority_initial_value: float = 0.3331244850783358,\n                priority_normalization_threshold_positive: float = 0.5351562608975046,\n                priority_normalization_threshold_negative: float = -0.7319646314026829) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n        priority_initial_value: Initial value for the priority array.\n        priority_normalization_threshold_positive: Threshold for positive priority normalization.\n        priority_normalization_threshold_negative: Threshold for negative priority normalization.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, priority_initial_value, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > priority_normalization_threshold_positive:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < priority_normalization_threshold_negative:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 133.78294855911892,
    "mi": 85.91520747863008,
    "token_count": 209.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fit, fullness, and adaptive scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Incorporate some randomness for exploration\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.038691663342641,
    "SLOC": 10.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 115.94522208456974,
    "mi": 90.8149361149426,
    "token_count": 136.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on a combination of factors, including best fit,\n    fullness, and adaptive scaling, with a focus on balancing exploration and exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit component: Prioritize bins where the remaining capacity is smallest.\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness component: Prioritize bins that are already relatively full.\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive Scaling: Scale based on item size.  Larger items favor tighter fits.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority  # Scale best-fit\n\n        # Combine priorities. Use a weighted average to balance fit and fullness.\n        priorities[fit_indices] = 0.6 * adaptive_priority + 0.4 * fullness_priority\n\n        # Introduce more strategic randomness (exploration) proportional to remaining space.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Bins where the item doesn't fit get a very low priority.\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize only if positive priorities exist\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n    #Else do nothing: all zeroes or all negative\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.008775428799367,
    "SLOC": 27.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 399.0314892856283,
    "mi": 78.30374497603326,
    "token_count": 332.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using adaptive scaling, fit, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 3.181092939768657,
    "SLOC": 25.0,
    "cyclomatic_complexity": 6.0,
    "halstead": 474.9705508214449,
    "mi": 75.5960755714673,
    "token_count": 347.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response3.txt_stdout.txt",
    "code_path": "problem_iter15_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using a combination of remaining capacity, item fit,\n    normalized fullness, and adaptive exploration. Aims to improve upon v1.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Calculate fullness (normalized remaining capacity)\n        fullness = 1 - (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices])\n        fullness = np.clip(fullness, 0, 1)  # Ensure fullness is within [0, 1]\n\n        # Priority based on fullness, favoring bins that are already somewhat full\n        priorities[fit_indices] += fullness\n\n        # Fit score: Higher if the item fits well (less remaining space)\n        fit_score = (item / bins_remain_cap[fit_indices])\n        priorities[fit_indices] += fit_score\n\n        # Adaptive scaling based on the average item size and bin capacity\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        adaptive_scale = item / scale\n        priorities[fit_indices] += adaptive_scale\n\n        # Exploration factor, adaptively scaled based on the number of available bins\n        exploration_factor = min(0.1, 0.05 / (np.sum(fit_indices) + 1e-9))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where the item doesn't fit harshly\n    priorities[remaining_capacity < 0] = -1e9\n\n    # If no bins can accept the item return equal priority\n    if np.all(remaining_capacity < 0):\n        priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    else:\n        # Normalize priorities to ensure they sum to 1 (if possible)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n        elif np.sum(priorities) < 0:\n            priorities = priorities - np.min(priorities)\n            priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 307.35774857210805,
    "mi": 80.99267634401636,
    "token_count": 299.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter16_hs2.txt_stdout.txt",
    "code_path": "problem_iter16_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                randomness_strength: float = 0.09055507501984036,\n                no_fit_priority: float = -1668213914.7516727,\n                epsilon: float = 4.1062069920696874e-09) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + epsilon)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * randomness_strength\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 3.6796968488233035,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 185.4406125843753,
    "mi": 81.10992999842686,
    "token_count": 223.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response3.txt_stdout.txt",
    "code_path": "problem_iter17_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins with adaptive scaling, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity and item size\n        scale = np.mean(bins_remain_cap[fit_indices]) + item\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Dynamic exploration factor\n        exploration_factor = min(0.1, 0.01 * np.sum(fit_indices))\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    # Penalize bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.230953330674122,
    "SLOC": 16.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 235.0,
    "mi": 81.17644675778453,
    "token_count": 219.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response1.txt_stdout.txt",
    "code_path": "problem_iter21_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on multiple factors including fit, fullness, remaining capacity, and a refined exploration strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero. small remaining get high priority\n        \n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        #Adaptive scaling based on item size\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = combined_priority / scale\n\n\n        # Refined exploration strategy: Item-size aware and decaying randomness\n        exploration_strength = min(0.1, item)  # Smaller items get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities, handling edge cases\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.263661747108102,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 322.4095353505972,
    "mi": 82.7567328453389,
    "token_count": 242.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response1.txt_stdout.txt",
    "code_path": "problem_iter20_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on adaptive scaling, fit, fullness, and exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on remaining capacity\n        scale = np.mean(bins_remain_cap[fit_indices]) \n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + 1e-9) # Fullness / remaining, scaled\n\n        # Introduce randomness, scaled by item size to prevent excessive exploration for large items\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * 0.01 * (item + 0.1) # Scale explore\n\n    # Very low priority to bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.812126047068214,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 199.1772208002305,
    "mi": 82.74542555784232,
    "token_count": 197.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter23_response3.txt_stdout.txt",
    "code_path": "problem_iter23_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and item-aware exploration.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Best-Fit priority\n        best_fit_priority = 1 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # Fullness priority\n        fullness_priority = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max()+1e-9)\n\n        # Adaptive scaling based on item size and bin capacity.\n        scale = item / bins_remain_cap[fit_indices].mean() if bins_remain_cap[fit_indices].mean() > 0 else item\n        adaptive_priority = scale * best_fit_priority\n\n        # Combine priorities with adaptive weights.\n        weight_fit = 0.5 + 0.5 * (item / np.max(bins_remain_cap)) # Larger items favor fit more\n        priorities[fit_indices] = weight_fit * adaptive_priority + (1 - weight_fit) * fullness_priority\n\n        # Strategic exploration based on remaining capacity and item size.\n        exploration_factor = np.random.rand(np.sum(fit_indices)) * (remaining_capacity[fit_indices] / bins_remain_cap[fit_indices].max()) * (item / np.max(bins_remain_cap)) * 0.1\n        priorities[fit_indices] += exploration_factor\n\n    # Very low priority for bins where item doesn't fit\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Normalize priorities\n    if np.any(priorities > 0):\n        priorities[priorities > 0] /= np.sum(priorities[priorities > 0])\n    elif np.any(priorities < 0) and np.all(priorities <=0):\n        priorities = priorities - np.min(priorities)\n        if np.sum(priorities) > 0:\n            priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 269.63553058626803,
    "mi": 75.15617534635977,
    "token_count": 216.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter24_response0.txt_stdout.txt",
    "code_path": "problem_iter24_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins based on multiple factors, dynamically adjusting parameters based on context.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Fullness factor: Prioritize bins that are already relatively full\n        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / bins_remain_cap[fit_indices].max()\n        fullness_priority = fullness\n\n        # Remaining capacity factor: Prefer bins with tighter fits but avoid nearly full bins\n        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero\n\n        # Combine fullness and remaining capacity.\n        combined_priority = fullness_priority + remaining_cap_priority\n\n        # Data-driven adaptive scaling based on item and bin characteristics\n        bin_capacity_mean = np.mean(bins_remain_cap[fit_indices])\n        item_scale = min(1.0, item / bin_capacity_mean) #Scale relative to the average bin size. Cap at 1 to avoid excessive scaling for large item\n\n        # Contextual Scaling: Dynamically adjust the scale based on item size relative to bin capacity\n        scale = bin_capacity_mean * (1 - 0.5 * item_scale)  # Reduce scale for larger items to encourage filling smaller bins first\n        priorities[fit_indices] = combined_priority / (scale + 1e-9)  # Adding small value to avoid potential division by zero\n\n        # Calibrated Randomness: Item-size aware and decaying randomness with capacity awareness\n        exploration_strength = min(0.1, item / bin_capacity_mean)  # Smaller items relative to bin capacity get more exploration\n        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength\n        priorities[fit_indices] += exploration_bonus\n\n    # Penalize bins where the item doesn't fit heavily\n    priorities[remaining_capacity < 0] = -1e9\n\n    # Robust Handling and Normalization\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        min_priority = np.min(priorities)\n        priorities = priorities - min_priority\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.zeros_like(priorities) # Handle case where all priorities are very negative after shift\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 2.343438372556841,
    "SLOC": 23.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 412.53150435559246,
    "mi": 77.98781381096404,
    "token_count": 349.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter25_response0.txt_stdout.txt",
    "code_path": "problem_iter25_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority_scale: float = 4.456751880260094,\n                no_fit_priority: float = -6273020216.245163,\n                avoid_zero_division: float = 9.372769307564907e-09,\n                priority_initial_value: float = 0.7634367741311676,\n                priority_normalization_threshold_positive: float = 0.8300987155255392,\n                priority_normalization_threshold_negative: float = -0.7175872942225637) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority_scale: Scaling factor for the priority of bins where the item fits.\n        no_fit_priority: Priority given to bins where the item doesn't fit.\n        avoid_zero_division: Small value to avoid division by zero.\n        priority_initial_value: Initial value for the priority array.\n        priority_normalization_threshold_positive: Threshold for positive priority normalization.\n        priority_normalization_threshold_negative: Threshold for negative priority normalization.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, priority_initial_value, dtype=float)\n    \n    # Calculate remaining capacity after adding the item.\n    remaining_capacity = bins_remain_cap - item\n    \n    # Give high priority to bins where the item fits and leaves minimal waste.\n    fit_indices = remaining_capacity >= 0\n    if np.any(fit_indices):\n        priorities[fit_indices] = fit_priority_scale / (remaining_capacity[fit_indices] + avoid_zero_division)  # Avoid division by zero\n    \n    # Give a very low priority (or negative) to bins where the item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority  # Large negative value\n        \n    # Normalize the priorities\n    if np.sum(priorities) > priority_normalization_threshold_positive:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < priority_normalization_threshold_negative:\n      priorities = priorities - np.min(priorities)\n      priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 133.78294855911892,
    "mi": 85.91520747863008,
    "token_count": 209.0,
    "exec_success": true
  }
]