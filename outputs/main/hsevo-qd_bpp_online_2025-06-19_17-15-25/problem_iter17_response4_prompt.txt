{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                randomness_strength: float = 0.09055507501984036,\n                no_fit_priority: float = -1668213914.7516727,\n                epsilon: float = 4.1062069920696874e-09) -> np.ndarray:\n    \"\"\"Prioritizes bins based on fullness, fit, adaptive scaling, and randomness.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n\n    if np.any(fit_indices):\n        # Adaptive scaling based on average remaining capacity of bins where the item fits.\n        scale = np.mean(bins_remain_cap[fit_indices])\n        priorities[fit_indices] = (bins_remain_cap[fit_indices] / scale) / (remaining_capacity[fit_indices] + epsilon)\n\n        # Introduce randomness for exploration.\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * randomness_strength\n\n    # Very low priority to bins where item doesn't fit.\n    priorities[remaining_capacity < 0] = no_fit_priority\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    if np.sum(priorities) > 0:\n        priorities = priorities / np.sum(priorities)\n    elif np.sum(priorities) < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on a combination of factors, focusing on best fit,\n    penalizing near misses, and adaptively scaling based on item size.  Includes a\n    dynamically adjusted exploration factor.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n    fit_indices = remaining_capacity >= 0\n    n_bins = len(bins_remain_cap)\n\n    if np.any(fit_indices):\n        # 1. Best Fit Emphasis: Prioritize bins where the item fits best (smallest waste).\n        priorities[fit_indices] = 1.0 / (remaining_capacity[fit_indices] + 1e-9)\n\n        # 2. Penalize Near Misses (Bins that are just too small): Soft constraint handling.\n        near_miss_indices = (remaining_capacity < 0) & (bins_remain_cap >= (item - 0.1 * item)) # within 10% of fitting\n        priorities[near_miss_indices] = -0.5 # Small negative priority; consider if other options exist\n\n        # 3. Adaptive Scaling Based on Item Size: Adjust priority scaling based on the item's relative size.\n        item_size_ratio = item / np.mean(bins_remain_cap[fit_indices]) if np.any(fit_indices) else 0.5  #Ratio of the current item size to average bin capacity.\n        scale_factor = max(0.1, min(1.0, item_size_ratio))  # Clamp to [0.1, 1] to avoid extreme scaling\n        priorities[fit_indices] *= scale_factor\n\n        # 4. Dynamic Exploration:  Introduce randomness that *decreases* as more bins become suitable.\n        #    Fewer suitable bins = more exploration needed.\n        exploration_factor = 0.1 * (1 - (np.sum(fit_indices) / n_bins))  # Scale randomness based on fit ratio\n        priorities[fit_indices] += np.random.rand(np.sum(fit_indices)) * exploration_factor\n\n    else:\n        # If NO bins fit, give a very small non-zero chance to bins which are closest to the required size.\n        closest_bin_index = np.argmin(bins_remain_cap)\n        priorities[closest_bin_index] = 0.0001\n\n    # 5. Bins where the item doesn't fit at all get a very negative priority (unless we're *forced* to use them).\n    priorities[remaining_capacity < 0] = np.where(priorities[remaining_capacity < 0] != -0.5, -1e9, priorities[remaining_capacity < 0])\n\n    # Normalize priorities to ensure they sum to 1 (or handle negative priorities).\n    priority_sum = np.sum(priorities)\n    if priority_sum > 0:\n        priorities = priorities / priority_sum\n    elif priority_sum < 0:\n        priorities = priorities - np.min(priorities)\n        priorities = priorities / np.sum(priorities) if np.sum(priorities) > 0 else np.full_like(priorities, 1/len(priorities))\n\n    return priorities\n\n### Analyze & experience\n- *   Comparing (1st) vs (20th), we see that the better heuristic incorporates remaining capacity, fit indices, and normalization, while the worse heuristic uses only item/bin ratios and log ratios.\n*   (2nd best) vs (second worst) shows that the second best also incorporates randomness, normalization, and fit indices. In contrast, the second worst uses a simple division to calculate remaining capacities without considering the item's fit.\n*   Comparing (1st) vs (2nd), both are identical, suggesting that their performance is the same, and likely near optimal given the information available.\n*   (3rd) vs (4th), we see the introduction of configurable parameters (randomness strength, no fit priority, epsilon) in the 4th heuristic. This allows for fine-tuning but doesn't necessarily guarantee better performance without proper parameter setting.\n*   Comparing (second worst) vs (worst), the second worst prioritizes based on remaining capacity and penalizes bins where items don't fit, whereas the worst simply calculates ratios. The normalization in the second worst is also important.\n*   Overall: The superior heuristics integrate fit, fullness (remaining capacity), adaptive scaling, exploration (randomness), and proper normalization of priorities. Penalizing bins where items don't fit is a consistent theme. Adaptive scaling considers the relationship between item size and bin capacity. Simpler heuristics focusing solely on ratios or without normalization tend to perform worse. The best approaches use adaptive scaling based on average bin capacities and a small amount of randomness for exploration.\n- \nOkay, let's redefine \"Current Self-Reflection\" for better heuristic design, focusing on actionable insights and avoiding common pitfalls.\n\nHere's a breakdown to guide the process:\n\n*   **Keywords:** Adaptive scaling, normalization, multi-factor consideration, exploration (randomness), edge cases, configurable parameters, robustness, infeasibility penalties.\n\n*   **Advice:** Focus on dynamically adjusting heuristic priorities based on item sizes, bin capacities, and remaining space. Normalization of priorities makes selection consistent. Rigorously test and tune configurable parameters to avoid overfitting.\n\n*   **Avoid:** Oversimplified calculations, neglecting edge cases and constraints, overly complex dynamically adjusted weights without validation, ignoring infeasible solutions.\n\n*   **Explanation:** The goal is to create heuristics that are both efficient and robust. Adaptive scaling improves adaptability to different problem instances. Normalization ensures fair comparison of priorities. Exploration helps escape local optima. Configurable parameters enable fine-tuning. Proper handling of edge cases ensures reliability.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}