```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Prioritizes bins based on multiple factors including fit, fullness,
    remaining capacity, adaptive scaling, and calibrated randomness."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    remaining_capacity = bins_remain_cap - item
    fit_indices = remaining_capacity >= 0

    if np.any(fit_indices):
        # Fullness factor: Prioritize bins that are already relatively full
        fullness = (bins_remain_cap[fit_indices].max() - bins_remain_cap[fit_indices]) / (bins_remain_cap[fit_indices].max() + 1e-9)
        fullness_priority = fullness

        # Remaining capacity factor: Prefer bins with tighter fits
        remaining_cap_priority = 1.0 / (remaining_capacity[fit_indices] + 0.01)  # Avoid division by zero
        
        # Combine fullness and remaining capacity.
        combined_priority = fullness_priority + remaining_cap_priority

        # Adaptive scaling based on item size and bin capacity distribution
        capacity_mean = np.mean(bins_remain_cap[fit_indices])
        capacity_std = np.std(bins_remain_cap[fit_indices]) + 1e-9 #avoid division by zero

        # Scale combined priority based on item size relative to bin capacity distribution
        scale = (item / capacity_mean) + (1/(1+capacity_std))
        priorities[fit_indices] = combined_priority / (scale + 0.1)


        # Calibrated randomness: Adjust randomness based on fit count and item size
        num_fits = np.sum(fit_indices)
        exploration_strength = min(0.1, item / (1 + num_fits))  # Fewer fits -> more exploration; Larger item -> less exploration
        exploration_bonus = np.random.rand(np.sum(fit_indices)) * exploration_strength
        priorities[fit_indices] += exploration_bonus

    # Penalize bins where the item doesn't fit heavily
    priorities[remaining_capacity < 0] = -1e9

    # Robust handling of priorities to prevent instability
    if np.any(priorities > 1e8):
        priorities[priorities > 1e8] = 1e8  # Cap large positive values

    if np.any(priorities < -1e8):
         priorities[priorities < -1e8] = -1e8 # Cap large negative values
        

    # Normalize priorities, handling edge cases
    total_priority = np.sum(priorities)
    if total_priority > 0:
        priorities = priorities / total_priority
    elif total_priority < 0:
        min_priority = np.min(priorities)
        priorities = priorities - min_priority
        normalized_sum = np.sum(priorities)
        if normalized_sum > 0:
            priorities = priorities / normalized_sum
        else:
            priorities = np.zeros_like(priorities)  # All priorities are essentially zero
    else:
        priorities = np.ones_like(priorities) / len(priorities) if len(priorities)>0 else priorities #If all zero, make them equally likely

    return priorities
```
