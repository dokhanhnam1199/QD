[
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation considers both the remaining capacity and aims to balance\n    between filling bins almost to capacity and maintaining some underused capacity\n    to avoid fragmentation. It penalizes bins that are overly packed and favors\n    bins that are underused but can still fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Initialize priority scores with a high value for feasible bins\n    # The closer a bin is to being full, the higher its priority\n    priority_scores = np.where(new_remain_cap >= 0, 1 / (1 + new_remain_cap), -np.inf)\n    \n    # Penalize bins that are already more than half full to avoid them taking more small items\n    max_bin_cap = np.max(bins_remain_cap + item)\n    priority_scores -= np.where(bins_remain_cap < 0.5 * max_bin_cap, 0, priority_scores)\n    \n    # Favor bins that are underused but can still fit the item\n    priority_scores -= np.where(new_remain_cap > 0.5 * max_bin_cap, 0.01, 0)\n    \n    return priority_scores",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on filling bins that are closer to being full,\n    penalizes bins that cannot hold the item, and slightly favors earlier bins\n    to minimize fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Initialize priority scores with a high value (small negative) for feasible bins\n    # The closer a bin is to being full, the higher its priority\n    priority_scores = np.where(new_remain_cap >= 0, 1 / (1 + new_remain_cap), -np.inf)\n    \n    # Favor earlier bins by adding a slight bonus for earlier bins\n    bin_indices = np.arange(len(bins_remain_cap))\n    priority_scores -= 0.01 * bin_indices\n    \n    return priority_scores",
    "response_id": 1,
    "obj": 3.9988033506182825,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on filling bins that are closer to being full,\n    penalizes bins that cannot hold the item, and slightly favors earlier bins\n    to minimize fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Initialize priority scores with a high value (small negative) for feasible bins\n    # The closer a bin is to being full, the higher its priority\n    priority_scores = np.where(new_remain_cap >= 0, 1 / (1 + new_remain_cap), -np.inf)\n    \n    # Favor earlier bins by adding a slight bonus for earlier bins\n    bin_indices = np.arange(len(bins_remain_cap))\n    priority_scores -= 0.001 * bin_indices\n    \n    return priority_scores",
    "response_id": 2,
    "obj": 4.038691663342641,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prioritizes feasible bins, penalizes small remaining capacities harshly,\n    and slightly favors earlier bins to reduce fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Penalize infeasible bins harshly\n    feasibility_penalty = np.where(new_remain_cap < 0, -np.inf, 0)\n    \n    # Penalize bins with small remaining capacities harshly\n    small_capacity_penalty = np.where(new_remain_cap >= 0, 1 / (1 + new_remain_cap), 0)\n    \n    # Favor slightly earlier bins by adding a slight bonus for earlier bins\n    num_bins = len(bins_remain_cap)\n    bin_index_bonus = np.linspace(0, 0.1, num_bins)\n    \n    # Calculate priority scores\n    priority_scores = feasibility_penalty - small_capacity_penalty + bin_index_bonus\n    \n    return priority_scores",
    "response_id": 3,
    "obj": 63.153171120861586,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on the remaining capacity and penalizes bins that are over half full.\n    It aims to reduce fragmentation by preferring bins that are almost filled.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Calculate the priority score for feasible bins\n    priority_scores = np.where(new_remain_cap >= 0, -new_remain_cap, -np.inf)\n    \n    # Penalize bins that are more than half full\n    half_full_threshold = 0.5 * (np.max(bins_remain_cap) + item)\n    priority_scores = np.where(bins_remain_cap < half_full_threshold, priority_scores, -np.inf)\n    \n    return priority_scores",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response5.txt_stdout.txt",
    "code_path": "problem_iter12_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on remaining capacity, harshly penalizes overfills,\n    favors earlier bins, minimizes fragmentation, and adjusts penalties for bins that are more than half full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Penalize heavily for overfills\n    overfill_penalty = np.where(new_remain_cap < 0, -np.inf, 0)\n    \n    # Calculate the priority score: higher score if the remaining capacity is small (i.e., almost full)\n    priority_scores = -np.where(new_remain_cap >= 0, new_remain_cap, 0) + overfill_penalty\n    \n    # Penalize bins that are already more than half full to avoid them taking more small items\n    half_full_threshold = 0.5 * (np.max(bins_remain_cap) + item)\n    priority_scores = np.where(bins_remain_cap < half_full_threshold, priority_scores, priority_scores - 5)\n    \n    # Favor earlier bins with a slight bias\n    num_bins = len(bins_remain_cap)\n    bin_index_penalty = np.linspace(0, 0.5, num_bins)\n    priority_scores -= bin_index_penalty\n    \n    return priority_scores",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response6.txt_stdout.txt",
    "code_path": "problem_iter12_code6.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on the remaining capacity and penalizes bins that are over half full.\n    It aims to reduce fragmentation by preferring bins that are almost filled.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Calculate the priority score for feasible bins\n    priority_scores = np.where(new_remain_cap >= 0, -new_remain_cap, -np.inf)\n    \n    # Penalize bins that are more than half full\n    half_full_threshold = 0.5 * (np.max(bins_remain_cap) + item)\n    priority_scores = np.where(bins_remain_cap < half_full_threshold, priority_scores, -np.inf)\n    \n    return priority_scores",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response7.txt_stdout.txt",
    "code_path": "problem_iter12_code7.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation focuses on filling bins that are closer to being full,\n    penalizes bins that cannot hold the item, and slightly favors earlier bins\n    to minimize fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Initialize priority scores with a high value (small negative) for feasible bins\n    # The closer a bin is to being full, the higher its priority\n    priority_scores = np.where(new_remain_cap >= 0, 1 / (1 + new_remain_cap), -np.inf)\n    \n    # Favor earlier bins by adding a slight bonus for earlier bins\n    bin_indices = np.arange(len(bins_remain_cap))\n    priority_scores -= 0.01 * bin_indices\n    \n    return priority_scores",
    "response_id": 7,
    "obj": 3.9988033506182825,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response8.txt_stdout.txt",
    "code_path": "problem_iter12_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation harshly penalizes overfills, adjusts half-full thresholds,\n    and uses linear biases for bin positions to minimize fragmentation and favor earlier bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Harshly penalize overfills\n    overfill_penalty = np.where(new_remain_cap < 0, -np.inf, 0)\n    \n    # Calculate the priority score: higher score if the remaining capacity is small (i.e., almost full)\n    priority_scores = -np.where(new_remain_cap >= 0, new_remain_cap, 0) + overfill_penalty\n    \n    # Adjust penalty for bins that are already more than 70% full to avoid them taking more small items\n    half_full_threshold = 0.7 * (np.max(bins_remain_cap) + item)\n    priority_scores[new_remain_cap < half_full_threshold] -= 10\n    \n    # Favor earlier bins with a linear bias\n    num_bins = len(bins_remain_cap)\n    bin_index_penalty = np.linspace(0, 1, num_bins)\n    priority_scores -= bin_index_penalty\n    \n    return priority_scores",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response9.txt_stdout.txt",
    "code_path": "problem_iter12_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation prioritizes earlier bins, harshly penalizes overfills,\n    and dynamically adjusts half-full penalties to minimize fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n    \n    # Penalize heavily for overfills\n    overfill_penalty = np.where(new_remain_cap < 0, -np.inf, 0)\n    \n    # Calculate the priority score: higher score if the remaining capacity is small (i.e., almost full)\n    priority_scores = -np.where(new_remain_cap >= 0, new_remain_cap, 0) + overfill_penalty\n    \n    # Dynamically penalize bins that are more than half full to avoid them taking more small items\n    half_full_threshold = 0.5 * (bins_remain_cap + item)\n    half_full_penalty = np.where(half_full_threshold > 0, np.where(new_remain_cap > half_full_threshold, -0.1, 0), 0)\n    \n    # Favor earlier bins slightly by adding a small bonus that decreases with index\n    index_bonus = np.arange(len(bins_remain_cap)) * -0.01\n    \n    # Combine all adjustments into the priority scores\n    priority_scores += half_full_penalty + index_bonus\n    \n    return priority_scores",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]