import numpy as np

def heuristics_v2(distance_matrix):
    """{This algorithm combines aspects of reinforcement learning and Monte Carlo sampling to build a heuristic matrix by rewarding edges used in low-cost paths generated through random walks guided by learned Q-values.}"""
    n = distance_matrix.shape[0]
    q_table = np.zeros((n, n))
    learning_rate = 0.1
    discount_factor = 0.9
    exploration_rate = 0.2
    num_episodes = 50

    heuristics_matrix = np.zeros((n, n))

    for episode in range(num_episodes):
        start_node = np.random.randint(n)
        current_node = start_node
        path = [start_node]
        unvisited = list(range(n))
        unvisited.remove(start_node)
        total_reward = 0
        episode_path = []

        while unvisited:
            if np.random.rand() < exploration_rate:
                next_node = np.random.choice(unvisited)
            else:
                q_values = q_table[current_node, :]
                valid_actions = [node for node in unvisited]
                valid_q_values = q_values[valid_actions]
                if np.all(valid_q_values == valid_q_values[0]):
                     next_node = np.random.choice(unvisited)
                else:
                    next_node = valid_actions[np.argmax(valid_q_values)]

            reward = -distance_matrix[current_node, next_node]
            total_reward += reward
            episode_path.append((current_node, next_node))
            path.append(next_node)
            unvisited.remove(next_node)

            current_node = next_node

        # Return to start
        reward = -distance_matrix[current_node, start_node]
        total_reward += reward
        episode_path.append((current_node, start_node))

        path_length = -total_reward

        #Update Q-table
        for i in range(len(episode_path)):
            state, action = episode_path[i]
            next_state = episode_path[(i+1)%len(episode_path)][1] if i < len(episode_path) - 1 else episode_path[0][0]
            old_value = q_table[state, action]
            next_max = np.max(q_table[action,:]) if i < len(episode_path)-1 else np.max(q_table[episode_path[-1][1],:])
            new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_max)
            q_table[state, action] = new_value

        # Update heuristics matrix
        for i in range(len(path) - 1):
            heuristics_matrix[path[i], path[i+1]] += 1.0 / path_length
            heuristics_matrix[path[i+1], path[i]] += 1.0 / path_length
        heuristics_matrix[path[-1], path[0]] += 1.0/path_length
        heuristics_matrix[path[0], path[-1]] += 1.0/path_length
        
    return heuristics_matrix
