[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors inspired by physics.\n    It considers the 'energy' required to place the item (smaller remaining capacity = higher energy),\n    a 'potential well' effect (bins with capacities slightly larger than the item are favored),\n    and a 'repulsion' effect (bins with capacities much larger than the item are discouraged).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 'Energy' term: Higher priority for bins that are almost full\n    energy = 1.0 / (bins_remain_cap + 1e-9)  # Avoid division by zero\n    priorities += energy\n\n    # 'Potential well' term: Favor bins with capacities slightly larger than the item\n    diff = bins_remain_cap - item\n    potential_well = np.exp(-(diff**2) / (2 * (item/3)**2)) #Gaussian centered at item. Larger variance when item size is bigger\n    priorities += potential_well\n\n    # 'Repulsion' term: Discourage bins with capacities much larger than the item\n    repulsion = np.exp(-bins_remain_cap / (item*5))  # Exponential decay with bin capacity. Higher decay if item is small\n    priorities -= 0.5 * repulsion # We don't want it to be the dominating factor\n\n    # Consider bins where item doesn't fit as non viable\n    priorities[bins_remain_cap < item] = -np.inf\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.9190267251695206,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 74.23092131656186,
    "mi": 72.08234061130545,
    "token_count": 97.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity & wasted space with physics-inspired scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        # Hybrid approach: capacity + inverse waste\n        priorities[valid_bins] = bins_remain_cap[valid_bins] / (1 + waste)\n    else:\n        priorities = np.full_like(bins_remain_cap, -np.inf)\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.948942959712818,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 375.2635575392197,
    "mi": 84.75565328016305,
    "token_count": 210.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, focusing on\n    bin utilization and fragmentation reduction. It adaptively adjusts parameters\n    based on item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization Term: Favor bins that will be well-utilized after adding the item.\n    #    This encourages filling bins more completely.\n    utilization = (item / (np.max(bins_remain_cap) + 1e-9)) # Normalized item size relative to max bin size\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + 1e-9) # %age of bin remaining after packing\n\n    # Penalize bins where item doesn't fit:\n    priorities[bins_remain_cap < item] = -np.inf\n\n    # If the item fits, then it adds priority\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation Term: Discourage creating very small remaining spaces.\n    #    Bins with remaining capacity close to a small threshold are penalized.\n    frag_threshold = item / 4.0  # Adjust as needed; smaller items shouldn't create very small fragments\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + 1e-9)) #frag threshold is deviation penalty\n\n    #Only apply penalty for bins that will contain the item:\n    priorities[fit_mask] -= 0.2 * frag_penalty[fit_mask] # Lower weight than utilization\n\n    # 3. Best Fit term - gives some extra credit if this is the best fit option:\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 0.1, 0)\n    priorities += best_fit_bonus\n\n\n    # 4. Empty Bin Consideration: Prefer using empty bins if the item is relatively large.\n    empty_bin_mask = bins_remain_cap == np.max(bins_remain_cap) # Check for bins with the original capacity\n    large_item_threshold = 0.75 * np.max(bins_remain_cap)\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 0.3 # Prefer to put large item in empty bin\n\n    return priorities",
    "response_id": 2,
    "tryHS": true,
    "obj": 3.8891104906262464,
    "SLOC": 24.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 407.1739523509744,
    "mi": 78.85347340979983,
    "token_count": 247.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_hs3.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, frag_threshold_ratio: float = 8.28198787550797, frag_penalty_weight: float = 0.4122918453619374, best_fit_bonus_value: float = 0.4437888393504407, large_item_threshold_ratio: float = 0.5341026657399712, empty_bin_bonus: float = 0.37683202977082947, small_number: float = 9.345067154505308e-07) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, focusing on\n    bin utilization and fragmentation reduction. It adaptively adjusts parameters\n    based on item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        frag_threshold_ratio:  Adjust as needed; smaller items shouldn't create very small fragments\n        frag_penalty_weight: Lower weight than utilization\n        best_fit_bonus_value: Bonus given to the best fit option.\n        large_item_threshold_ratio: Threshold to determine if an item is large relative to bin capacity.\n        empty_bin_bonus: Priority bonus for placing large items in empty bins.\n        small_number: A small number to prevent division by zero.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization Term: Favor bins that will be well-utilized after adding the item.\n    #    This encourages filling bins more completely.\n    utilization = (item / (np.max(bins_remain_cap) + small_number)) # Normalized item size relative to max bin size\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + small_number) # %age of bin remaining after packing\n\n    # Penalize bins where item doesn't fit:\n    priorities[bins_remain_cap < item] = -np.inf\n\n    # If the item fits, then it adds priority\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation Term: Discourage creating very small remaining spaces.\n    #    Bins with remaining capacity close to a small threshold are penalized.\n    frag_threshold = item / frag_threshold_ratio  # Adjust as needed; smaller items shouldn't create very small fragments\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + small_number)) #frag threshold is deviation penalty\n\n    #Only apply penalty for bins that will contain the item:\n    priorities[fit_mask] -= frag_penalty_weight * frag_penalty[fit_mask] # Lower weight than utilization\n\n    # 3. Best Fit term - gives some extra credit if this is the best fit option:\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, best_fit_bonus_value, 0)\n    priorities += best_fit_bonus\n\n\n    # 4. Empty Bin Consideration: Prefer using empty bins if the item is relatively large.\n    empty_bin_mask = bins_remain_cap == np.max(bins_remain_cap) # Check for bins with the original capacity\n    large_item_threshold = large_item_threshold_ratio * np.max(bins_remain_cap)\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += empty_bin_bonus # Prefer to put large item in empty bin\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 3.9888312724371757,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 474.72181256129835,
    "mi": 66.28799507312716,
    "token_count": 331.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, and best-fit for bin priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization\n    utilization = item / (np.max(bins_remain_cap) + 1e-9)\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + 1e-9)\n\n    # Infeasible bin penalty\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation penalty\n    frag_threshold = item / 4.0\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + 1e-9))\n    priorities[fit_mask] -= 0.2 * frag_penalty[fit_mask]\n\n    # 3. Best Fit Bonus\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 0.1, 0)\n    priorities += best_fit_bonus\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.8891104906262464,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 199.68581616031315,
    "mi": 87.96078599459103,
    "token_count": 126.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, now including a simulated \"pressure\"\n    analogy and adaptive adjustments for large and small items, and explicitly avoids creating extremely small\n    fragments by rejecting bins that would lead to near-zero remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit.  Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # 1. Utilization Term:  Favor better utilization, but scale based on item size.\n    # Larger items get a bigger utilization bonus.\n    utilization = item / max_cap  # Item size relative to maximum capacity\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance:  Strongly penalize bins that will result in tiny fragments.\n    # Also, penalize moderately if it creates a slightly larger, but still small fragment.\n    tiny_fragment_threshold = 0.05 * max_cap  # Significantly smaller than before, more aggressive\n    small_fragment_threshold = 0.2 * max_cap   #Adjusted threshold\n    \n    tiny_fragment_penalty = -10.0 #Very strong penalty\n    small_fragment_penalty = -2.0\n\n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n    \n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n    \n    # 3. \"Pressure\" Term:  Simulate bins as containers with \"pressure.\"  Bins with higher remaining\n    # capacity exert more \"pressure\" to accept the item, but scale down with item size to prevent overfilling.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus:  Reward the bin that provides the absolute best fit (smallest waste).\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.5, 0)\n    priorities += best_fit_bonus\n    \n    # 5. Empty Bin Preference (Adaptive): If the item is a significant fraction of bin size, strongly prefer\n    # an empty bin.  The threshold is now adaptive and more stringent.\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 3.0 # Strong preference\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 1.4459513362584764,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 446.79700005769257,
    "mi": 73.80183474144629,
    "token_count": 321.0,
    "exec_success": true
  }
]