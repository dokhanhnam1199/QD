[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors inspired by physics.\n    It considers the 'energy' required to place the item (smaller remaining capacity = higher energy),\n    a 'potential well' effect (bins with capacities slightly larger than the item are favored),\n    and a 'repulsion' effect (bins with capacities much larger than the item are discouraged).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 'Energy' term: Higher priority for bins that are almost full\n    energy = 1.0 / (bins_remain_cap + 1e-9)  # Avoid division by zero\n    priorities += energy\n\n    # 'Potential well' term: Favor bins with capacities slightly larger than the item\n    diff = bins_remain_cap - item\n    potential_well = np.exp(-(diff**2) / (2 * (item/3)**2)) #Gaussian centered at item. Larger variance when item size is bigger\n    priorities += potential_well\n\n    # 'Repulsion' term: Discourage bins with capacities much larger than the item\n    repulsion = np.exp(-bins_remain_cap / (item*5))  # Exponential decay with bin capacity. Higher decay if item is small\n    priorities -= 0.5 * repulsion # We don't want it to be the dominating factor\n\n    # Consider bins where item doesn't fit as non viable\n    priorities[bins_remain_cap < item] = -np.inf\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.9190267251695206,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 74.23092131656186,
    "mi": 72.08234061130545,
    "token_count": 97.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity & wasted space with physics-inspired scaling.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        # Hybrid approach: capacity + inverse waste\n        priorities[valid_bins] = bins_remain_cap[valid_bins] / (1 + waste)\n    else:\n        priorities = np.full_like(bins_remain_cap, -np.inf)\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.948942959712818,
    "SLOC": 14.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 375.2635575392197,
    "mi": 84.75565328016305,
    "token_count": 210.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, focusing on\n    bin utilization and fragmentation reduction. It adaptively adjusts parameters\n    based on item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization Term: Favor bins that will be well-utilized after adding the item.\n    #    This encourages filling bins more completely.\n    utilization = (item / (np.max(bins_remain_cap) + 1e-9)) # Normalized item size relative to max bin size\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + 1e-9) # %age of bin remaining after packing\n\n    # Penalize bins where item doesn't fit:\n    priorities[bins_remain_cap < item] = -np.inf\n\n    # If the item fits, then it adds priority\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation Term: Discourage creating very small remaining spaces.\n    #    Bins with remaining capacity close to a small threshold are penalized.\n    frag_threshold = item / 4.0  # Adjust as needed; smaller items shouldn't create very small fragments\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + 1e-9)) #frag threshold is deviation penalty\n\n    #Only apply penalty for bins that will contain the item:\n    priorities[fit_mask] -= 0.2 * frag_penalty[fit_mask] # Lower weight than utilization\n\n    # 3. Best Fit term - gives some extra credit if this is the best fit option:\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 0.1, 0)\n    priorities += best_fit_bonus\n\n\n    # 4. Empty Bin Consideration: Prefer using empty bins if the item is relatively large.\n    empty_bin_mask = bins_remain_cap == np.max(bins_remain_cap) # Check for bins with the original capacity\n    large_item_threshold = 0.75 * np.max(bins_remain_cap)\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 0.3 # Prefer to put large item in empty bin\n\n    return priorities",
    "response_id": 2,
    "tryHS": true,
    "obj": 3.8891104906262464,
    "SLOC": 24.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 407.1739523509744,
    "mi": 78.85347340979983,
    "token_count": 247.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response1.txt_stdout.txt",
    "code_path": "problem_iter13_code1.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, frag_threshold_ratio: float = 4.184028982477466, frag_penalty_weight: float = 0.11782538481957816, best_fit_bonus_value: float = 0.6850624935180503, large_item_threshold_ratio: float = 0.37628776647690587, empty_bin_bonus: float = 0.9559112977961204, small_number: float = 4.790358539872454e-06, utilization_weight: float = 1.5832661929024672) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, focusing on\n    bin utilization and fragmentation reduction. It adaptively adjusts parameters\n    based on item size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        frag_threshold_ratio:  Adjust as needed; smaller items shouldn't create very small fragments\n        frag_penalty_weight: Lower weight than utilization\n        best_fit_bonus_value: Bonus given to the best fit option.\n        large_item_threshold_ratio: Threshold to determine if an item is large relative to bin capacity.\n        empty_bin_bonus: Priority bonus for placing large items in empty bins.\n        small_number: A small number to prevent division by zero.\n        utilization_weight: Weight of the utilization term.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization Term: Favor bins that will be well-utilized after adding the item.\n    #    This encourages filling bins more completely.\n    utilization = (item / (np.max(bins_remain_cap) + small_number)) # Normalized item size relative to max bin size\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + small_number) # %age of bin remaining after packing\n\n    # Penalize bins where item doesn't fit:\n    priorities[bins_remain_cap < item] = -np.inf\n\n    # If the item fits, then it adds priority\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization * utilization_weight\n\n    # 2. Fragmentation Term: Discourage creating very small remaining spaces.\n    #    Bins with remaining capacity close to a small threshold are penalized.\n    frag_threshold = item / frag_threshold_ratio  # Adjust as needed; smaller items shouldn't create very small fragments\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + small_number)) #frag threshold is deviation penalty\n\n    #Only apply penalty for bins that will contain the item:\n    priorities[fit_mask] -= frag_penalty_weight * frag_penalty[fit_mask] # Lower weight than utilization\n\n    # 3. Best Fit term - gives some extra credit if this is the best fit option:\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, best_fit_bonus_value, 0)\n    priorities += best_fit_bonus\n\n\n    # 4. Empty Bin Consideration: Prefer using empty bins if the item is relatively large.\n    empty_bin_mask = bins_remain_cap == np.max(bins_remain_cap) # Check for bins with the original capacity\n    large_item_threshold = large_item_threshold_ratio * np.max(bins_remain_cap)\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += empty_bin_bonus # Prefer to put large item in empty bin\n\n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 2.2836059034702925,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "halstead": 496.6593447001757,
    "mi": 65.10806616572313,
    "token_count": 339.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, and best-fit for bin priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Bin Utilization\n    utilization = item / (np.max(bins_remain_cap) + 1e-9)\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + 1e-9)\n\n    # Infeasible bin penalty\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation penalty\n    frag_threshold = item / 4.0\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + 1e-9))\n    priorities[fit_mask] -= 0.2 * frag_penalty[fit_mask]\n\n    # 3. Best Fit Bonus\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 0.1, 0)\n    priorities += best_fit_bonus\n\n    return priorities",
    "response_id": 3,
    "tryHS": true,
    "obj": 3.8891104906262464,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 199.68581616031315,
    "mi": 87.96078599459103,
    "token_count": 126.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, now including a simulated \"pressure\"\n    analogy and adaptive adjustments for large and small items, and explicitly avoids creating extremely small\n    fragments by rejecting bins that would lead to near-zero remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit.  Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # 1. Utilization Term:  Favor better utilization, but scale based on item size.\n    # Larger items get a bigger utilization bonus.\n    utilization = item / max_cap  # Item size relative to maximum capacity\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance:  Strongly penalize bins that will result in tiny fragments.\n    # Also, penalize moderately if it creates a slightly larger, but still small fragment.\n    tiny_fragment_threshold = 0.05 * max_cap  # Significantly smaller than before, more aggressive\n    small_fragment_threshold = 0.2 * max_cap   #Adjusted threshold\n    \n    tiny_fragment_penalty = -10.0 #Very strong penalty\n    small_fragment_penalty = -2.0\n\n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n    \n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n    \n    # 3. \"Pressure\" Term:  Simulate bins as containers with \"pressure.\"  Bins with higher remaining\n    # capacity exert more \"pressure\" to accept the item, but scale down with item size to prevent overfilling.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus:  Reward the bin that provides the absolute best fit (smallest waste).\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.5, 0)\n    priorities += best_fit_bonus\n    \n    # 5. Empty Bin Preference (Adaptive): If the item is a significant fraction of bin size, strongly prefer\n    # an empty bin.  The threshold is now adaptive and more stringent.\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 3.0 # Strong preference\n\n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 1.4459513362584764,
    "SLOC": 25.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 446.79700005769257,
    "mi": 73.80183474144629,
    "token_count": 321.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, best fit, and empty bin.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    small_number = 1e-9\n    frag_threshold_ratio = 4.0\n    frag_penalty_weight = 0.2\n    best_fit_bonus_value = 0.1\n    large_item_threshold_ratio = 0.75\n    empty_bin_bonus = 0.3\n\n    # 1. Bin Utilization\n    utilization = (item / (np.max(bins_remain_cap) + small_number))\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + small_number)\n\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation\n    frag_threshold = item / frag_threshold_ratio\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + small_number))\n    if np.any(fit_mask):\n        priorities[fit_mask] -= frag_penalty_weight * frag_penalty[fit_mask]\n\n    # 3. Best Fit\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, best_fit_bonus_value, 0)\n    priorities += best_fit_bonus\n\n    # 4. Empty Bin\n    empty_bin_mask = bins_remain_cap == np.max(bins_remain_cap)\n    large_item_threshold = large_item_threshold_ratio * np.max(bins_remain_cap)\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += empty_bin_bonus\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.8891104906262464,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 640.5196324567918,
    "mi": 77.87493226697536,
    "token_count": 305.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response4.txt_stdout.txt",
    "code_path": "problem_iter9_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, including utilization,\n    fragmentation avoidance, pressure, best fit, and empty bin preference, with adaptive adjustments\n    based on item size and bin characteristics. It aims to improve upon priority_v1 by dynamically\n    adjusting penalties and bonuses to better handle diverse item sizes and bin configurations,\n    and adding a bin diversity term.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit. Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # 1. Utilization Term: Favor better utilization, but scale based on item size.\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance: Penalize bins that will result in tiny fragments.\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n\n    # Adaptive penalty based on item size: Larger items impose a heavier penalty for tiny fragments\n    tiny_fragment_penalty = -5.0 - 5.0 * utilization  # Higher penalty for larger items\n    small_fragment_penalty = -1.0 - 1.0 * utilization\n    \n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. \"Pressure\" Term: Bins with higher remaining capacity exert more \"pressure\".  Adjust scaling dynamically.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus: Reward the bin that provides the absolute best fit (smallest waste).\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.0 + 0.5 * (1 - utilization), 0)  # Adaptive bonus\n    priorities += best_fit_bonus\n\n    # 5. Empty Bin Preference (Adaptive): If the item is a significant fraction of bin size, strongly prefer an empty bin.\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 2.0 + 1.0 * utilization  # Stronger adaptive preference\n    \n    # 6. Bin Diversity Bonus: Encourage spreading items across different bins.\n    # This penalizes using bins with similar remaining capacities if there are other options.\n    if num_bins > 1:\n        std_dev = np.std(bins_remain_cap[fit_mask])  # Standard deviation of remaining capacities\n        diversity_bonus = std_dev / max_cap # Normalize\n        priorities[fit_mask] += 0.25 * diversity_bonus # Moderate diversity bonus\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 1.2165935380933433,
    "SLOC": 35.0,
    "cyclomatic_complexity": 6.0,
    "halstead": 891.8445706896129,
    "mi": 72.66641763580655,
    "token_count": 391.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter10_response0.txt_stdout.txt",
    "code_path": "problem_iter10_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, *,\n                 tiny_fragment_threshold_fraction: float = 0.0965737114130352,\n                 small_fragment_threshold_fraction: float = 0.2976749170583864,\n                 tiny_fragment_penalty: float = -19.035781294528093,\n                 small_fragment_penalty: float = -2.9898952738151054,\n                 pressure_weight: float = 0.5918140088284347,\n                 best_fit_bonus_value: float = 1.3079400824759972,\n                 empty_bin_threshold_fraction: float = 0.46099185424258277,\n                 empty_bin_preference: float = 3.136501304787648) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, now including a simulated \"pressure\"\n    analogy and adaptive adjustments for large and small items, and explicitly avoids creating extremely small\n    fragments by rejecting bins that would lead to near-zero remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        tiny_fragment_threshold_fraction: Fraction of max_cap to define tiny fragments.\n        small_fragment_threshold_fraction: Fraction of max_cap to define small fragments.\n        tiny_fragment_penalty: Penalty for creating a tiny fragment.\n        small_fragment_penalty: Penalty for creating a small fragment.\n        pressure_weight: Weight of the pressure term.\n        best_fit_bonus_value: Bonus for the best fit bin.\n        empty_bin_threshold_fraction: Fraction of max_cap to consider an item large for empty bin preference.\n        empty_bin_preference: Preference for empty bins when item is large.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit.  Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 30.0,
    "mi": 89.59694567160192,
    "token_count": 146.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, and a potential well to prioritize bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # 1. Utilization term\n    utilization = item / (np.max(bins_remain_cap) + 1e-9)\n    remaining_percentage = (bins_remain_cap - item) / (np.max(bins_remain_cap) + 1e-9)\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation term\n    frag_threshold = item / 4.0\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + 1e-9))\n    priorities[fit_mask] -= 0.2 * frag_penalty[fit_mask]\n\n    # 3. Potential well term (from v1)\n    diff = bins_remain_cap - item\n    potential_well = np.exp(-(diff**2) / (2 * (item/3)**2))\n    priorities += 0.1 * potential_well # Lower weight\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 3.7594734742720433,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 423.2486508667636,
    "mi": 78.45360198190255,
    "token_count": 208.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function refines the priority calculation by incorporating a refined fragmentation penalty,\n    a capacity-aware best-fit bonus, a bin-emptiness gradient, and an advanced diversity measure\n    that considers both remaining capacity and the number of items already packed.  Adaptive scaling\n    is used throughout to adjust to varying item and bin sizes.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit. Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # 1. Utilization Term: Favor better utilization, but scale based on item size.\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Refined Fragmentation Avoidance: Penalize bins that will result in tiny or small fragments,\n    # with adaptive penalties based on the item's relative size compared to the bin.\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n\n    # Adaptive penalty based on item size: Larger items impose a heavier penalty for tiny fragments\n    tiny_fragment_penalty = -5.0 - 7.0 * utilization  # Enhanced penalty for larger items causing tiny fragments\n    small_fragment_penalty = -1.0 - 2.0 * utilization  # Enhanced penalty for larger items causing small fragments\n\n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. \"Pressure\" Term: Bins with higher remaining capacity exert more \"pressure\". Adjust scaling dynamically.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Capacity-Aware Best Fit Bonus: Reward the bin that provides the absolute best fit (smallest waste),\n    # with a bonus that increases as the bin's capacity increases.\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, (1.0 + 0.5 * (1 - utilization)) * (bins_remain_cap / max_cap), 0)  # Adaptive bonus\n    priorities += best_fit_bonus\n\n    # 5. Empty Bin Preference (Adaptive Gradient): If the item is a significant fraction of bin size, strongly prefer an empty bin.\n    # Add a gradient based on how close the bin is to empty.\n    empty_bin_mask = bins_remain_cap == max_cap\n    nearly_empty_mask = (bins_remain_cap > 0.9 * max_cap) & (bins_remain_cap < max_cap)\n    empty_bin_threshold = 0.6 * max_cap\n\n    if item >= empty_bin_threshold:\n        if np.any(empty_bin_mask):\n            priorities[empty_bin_mask] += 2.5 + 1.5 * utilization  # Stronger adaptive preference\n\n        if np.any(nearly_empty_mask):\n            priorities[nearly_empty_mask] += 1.0 + 0.5 * utilization\n\n    # 6. Advanced Bin Diversity Bonus: Encourage spreading items across different bins, considering both\n    # remaining capacity and the number of items already packed in each bin.  This encourages more balanced loading.\n    if num_bins > 1:\n        # Estimate 'fullness' based on how close to the original max_cap each bin is.\n        bin_fullness = (max_cap - bins_remain_cap) / max_cap\n        diversity_metric = np.std(bin_fullness[fit_mask])  # Std dev of fullness\n        diversity_bonus = diversity_metric  # Normalize\n        priorities[fit_mask] += 0.3 * diversity_bonus  # Moderate diversity bonus\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 1.5257279617072266,
    "SLOC": 49.0,
    "cyclomatic_complexity": 8.0,
    "halstead": 1584.2447418042693,
    "mi": 68.45183796952362,
    "token_count": 595.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, best fit, and bin diversity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    small_number = 1e-9\n    frag_threshold_ratio = 4.0\n    frag_penalty_weight = 0.2\n    best_fit_bonus_value = 0.1\n    large_item_threshold_ratio = 0.75\n    empty_bin_bonus = 0.3\n    bin_diversity_weight = 0.05\n\n    # 1. Bin Utilization\n    max_cap = np.max(bins_remain_cap)\n    utilization = (item / (max_cap + small_number))\n    remaining_percentage = (bins_remain_cap - item) / (max_cap + small_number)\n\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n        priorities[fit_mask] += (1 - remaining_percentage[fit_mask]) * utilization\n\n    # 2. Fragmentation\n    frag_threshold = item / frag_threshold_ratio\n    frag_penalty = np.exp(-np.abs(bins_remain_cap - item - frag_threshold) / (frag_threshold + small_number))\n    if np.any(fit_mask):\n        priorities[fit_mask] -= frag_penalty_weight * frag_penalty[fit_mask]\n\n    # 3. Best Fit\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, best_fit_bonus_value, 0)\n    priorities += best_fit_bonus\n\n    # 4. Empty Bin\n    empty_bin_mask = bins_remain_cap == max_cap\n    large_item_threshold = large_item_threshold_ratio * max_cap\n    if item > large_item_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += empty_bin_bonus\n\n    # 5. Bin Diversity (Encourage using bins with diverse fill levels)\n    bin_diversity = np.std(bins_remain_cap[fit_mask]) if np.sum(fit_mask) > 1 else 0.0\n    if np.any(fit_mask):\n        priorities[fit_mask] += bin_diversity_weight * bin_diversity\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 3.8891104906262464,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 423.2486508667636,
    "mi": 79.6827274813862,
    "token_count": 236.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response1.txt_stdout.txt",
    "code_path": "problem_iter15_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, including utilization,\n    fragmentation avoidance, pressure, best fit, and empty bin preference, with adaptive adjustments\n    based on item size and bin characteristics. It aims to improve upon priority_v1 by dynamically\n    adjusting penalties and bonuses to better handle diverse item sizes and bin configurations,\n    and adding a bin diversity term, and considering the number of available bins. It also adjusts fragmentation penalties based on item size relative to bin size.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit. Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # 1. Utilization Term: Favor better utilization, but scale based on item size.\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance: Penalize bins that will result in tiny fragments.\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n\n    # Adaptive penalty based on item size: Larger items impose a heavier penalty for tiny fragments\n    # Adjust penalty based on how large the item is relative to the bin.\n    item_size_ratio = item / max_cap\n    tiny_fragment_penalty = -5.0 - 5.0 * utilization * (1 + item_size_ratio)\n    small_fragment_penalty = -1.0 - 1.0 * utilization * (1 + item_size_ratio)\n    \n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. \"Pressure\" Term: Bins with higher remaining capacity exert more \"pressure\".  Adjust scaling dynamically.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus: Reward the bin that provides the absolute best fit (smallest waste).\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.0 + 0.5 * (1 - utilization), 0)  # Adaptive bonus\n    priorities += best_fit_bonus\n\n    # 5. Empty Bin Preference (Adaptive): If the item is a significant fraction of bin size, strongly prefer an empty bin.\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 2.0 + 1.0 * utilization  # Stronger adaptive preference\n    \n    # 6. Bin Diversity Bonus: Encourage spreading items across different bins.\n    # This penalizes using bins with similar remaining capacities if there are other options.\n    if num_bins > 1:\n        std_dev = np.std(bins_remain_cap[fit_mask])  # Standard deviation of remaining capacities\n        diversity_bonus = std_dev / max_cap # Normalize\n        diversity_weight = min(0.25, 0.1 * num_bins)  # Reduce diversity bonus if there are few bins\n        priorities[fit_mask] += diversity_weight * diversity_bonus\n\n    # 7. Number of bins term: If we have a lot of bins, be more aggressive in filling them up.\n    # If we have few bins, try to conserve space better.\n\n    bin_quantity_scaling = 1.0 + 0.1 * num_bins\n    priorities[fit_mask] *= bin_quantity_scaling\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 1.2265656162744383,
    "SLOC": 44.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 1327.4492386549305,
    "mi": 70.1667572762325,
    "token_count": 500.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter17_response5.txt_stdout.txt",
    "code_path": "problem_iter17_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, pressure, best fit, and diversity.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # Hard constraint: Item must fit.\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if not np.any(fit_mask):\n        return priorities\n\n    # 1. Utilization\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n    tiny_fragment_penalty = -5.0 - 5.0 * utilization\n    small_fragment_penalty = -1.0 - 1.0 * utilization    \n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. Pressure\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.0 + 0.5 * (1 - utilization), 0)\n    priorities += best_fit_bonus\n    \n    # 5. Diversity\n    if num_bins > 1:\n        std_dev = np.std(bins_remain_cap[fit_mask])\n        diversity_bonus = std_dev / max_cap\n        priorities[fit_mask] += 0.25 * diversity_bonus\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 1.3661747108097373,
    "SLOC": 29.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 635.9495684877126,
    "mi": 72.4956043481395,
    "token_count": 311.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response0.txt_stdout.txt",
    "code_path": "problem_iter18_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function prioritizes bins based on a combination of factors, including utilization,\n    fragmentation avoidance, pressure, best fit, and empty bin preference, with adaptive adjustments\n    based on item size and bin characteristics. It aims to improve upon priority_v1 by dynamically\n    adjusting penalties and bonuses to better handle diverse item sizes and bin configurations,\n    and adding a bin diversity term. It also incorporates a level-based bin selection mechanism.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # 0. Hard constraint: Item must fit. Bins that can't fit get -inf priority.\n    priorities[bins_remain_cap < item] = -np.inf\n\n    fit_mask = bins_remain_cap >= item\n\n    if not np.any(fit_mask):\n        return priorities  # No bin can fit the item\n\n    # --- Level-Based Bin Selection ---\n    # Divide bins into levels based on remaining capacity.\n    level1_threshold = 0.25 * max_cap  # Lowest 25%\n    level2_threshold = 0.50 * max_cap  # 25% to 50%\n    level3_threshold = 0.75 * max_cap  # 50% to 75%\n\n    level1_mask = (bins_remain_cap >= item) & (bins_remain_cap <= level1_threshold)\n    level2_mask = (bins_remain_cap > level1_threshold) & (bins_remain_cap <= level2_threshold)\n    level3_mask = (bins_remain_cap > level2_threshold) & (bins_remain_cap <= level3_threshold)\n    level4_mask = (bins_remain_cap > level3_threshold)\n\n    # Adjust priorities based on levels: Favor levels that provide a tighter fit.\n    level_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    level_bonus[level1_mask] += 0.5\n    level_bonus[level2_mask] += 0.3\n    level_bonus[level3_mask] += 0.1\n    priorities += level_bonus\n    # --- End Level-Based Bin Selection ---\n\n    # 1. Utilization Term: Favor better utilization, but scale based on item size.\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance: Penalize bins that will result in tiny fragments.\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n\n    # Adaptive penalty based on item size: Larger items impose a heavier penalty for tiny fragments\n    tiny_fragment_penalty = -5.0 - 5.0 * utilization  # Higher penalty for larger items\n    small_fragment_penalty = -1.0 - 1.0 * utilization\n    \n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. \"Pressure\" Term: Bins with higher remaining capacity exert more \"pressure\".  Adjust scaling dynamically.\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus: Reward the bin that provides the absolute best fit (smallest waste).\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.0 + 0.5 * (1 - utilization), 0)  # Adaptive bonus\n    priorities += best_fit_bonus\n\n    # 5. Empty Bin Preference (Adaptive): If the item is a significant fraction of bin size, strongly prefer an empty bin.\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 2.0 + 1.0 * utilization  # Stronger adaptive preference\n    \n    # 6. Bin Diversity Bonus: Encourage spreading items across different bins.\n    # This penalizes using bins with similar remaining capacities if there are other options.\n    if num_bins > 1:\n        std_dev = np.std(bins_remain_cap[fit_mask])  # Standard deviation of remaining capacities\n        diversity_bonus = std_dev / max_cap # Normalize\n        priorities[fit_mask] += 0.25 * diversity_bonus # Moderate diversity bonus\n\n    # 7. Item Size Awareness: Adjust bonus/penalty depending on item size.\n    if item > 0.7 * max_cap:  # Large Item\n        priorities[fit_mask] -= 0.2  # Slight penalty to discourage filling almost completely\n    elif item < 0.3 * max_cap: # Small Item\n        priorities[fit_mask] += 0.1 # Encourage packing smaller items together\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 1.8847227762265748,
    "SLOC": 40.0,
    "cyclomatic_complexity": 7.0,
    "halstead": 1324.2830770795736,
    "mi": 70.3235788440878,
    "token_count": 505.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response5.txt_stdout.txt",
    "code_path": "problem_iter20_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, fragmentation, pressure, & diversity with adaptive penalties.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    max_cap = np.max(bins_remain_cap)\n    num_bins = len(bins_remain_cap)\n\n    # 0. Hard constraint\n    priorities[bins_remain_cap < item] = -np.inf\n    fit_mask = bins_remain_cap >= item\n    if not np.any(fit_mask):\n        return priorities\n\n    # 1. Utilization Term\n    utilization = item / max_cap\n    remaining_capacity_after_fit = bins_remain_cap - item\n    utilization_score = (1 - remaining_capacity_after_fit / max_cap) * utilization\n    priorities[fit_mask] += utilization_score[fit_mask]\n\n    # 2. Fragmentation Avoidance (Adaptive Penalty)\n    tiny_fragment_threshold = 0.05 * max_cap\n    small_fragment_threshold = 0.2 * max_cap\n    tiny_fragment_penalty = -5.0 - 5.0 * utilization\n    small_fragment_penalty = -1.0 - 1.0 * utilization\n\n    tiny_fragment_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit <= tiny_fragment_threshold)\n    small_fragment_mask = (remaining_capacity_after_fit > tiny_fragment_threshold) & (remaining_capacity_after_fit <= small_fragment_threshold)\n\n    priorities[tiny_fragment_mask] += tiny_fragment_penalty\n    priorities[small_fragment_mask] += small_fragment_penalty\n\n    # 3. Pressure Term\n    pressure = bins_remain_cap / max_cap * (1 - utilization)\n    priorities[fit_mask] += 0.5 * pressure[fit_mask]\n\n    # 4. Best Fit Bonus (Adaptive)\n    diffs = bins_remain_cap - item\n    min_diff = np.min(diffs[diffs >= 0]) if np.any(diffs >= 0) else np.inf\n    best_fit_bonus = np.where(diffs == min_diff, 1.0 + 0.5 * (1 - utilization), 0)\n    priorities += best_fit_bonus\n\n    # 5. Empty Bin Preference (Adaptive)\n    empty_bin_mask = bins_remain_cap == max_cap\n    empty_bin_threshold = 0.6 * max_cap\n    if item >= empty_bin_threshold and np.any(empty_bin_mask):\n        priorities[empty_bin_mask] += 2.0 + 1.0 * utilization\n\n    # 6. Bin Diversity Bonus\n    if num_bins > 1:\n        std_dev = np.std(bins_remain_cap[fit_mask])\n        diversity_bonus = std_dev / max_cap\n        priorities[fit_mask] += 0.25 * diversity_bonus\n\n    # 7. Item Size Awareness\n    if item > 0.7 * max_cap:\n        priorities[fit_mask] -= 0.2\n    elif item < 0.3 * max_cap:\n        priorities[fit_mask] += 0.1\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 1.2165935380933433,
    "SLOC": 25.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 750.1751056422819,
    "mi": 74.60971298780568,
    "token_count": 297.0,
    "exec_success": true
  }
]