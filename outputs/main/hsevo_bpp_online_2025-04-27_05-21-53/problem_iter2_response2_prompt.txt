{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin is viable only if it has enough capacity for the item.\n    viable_bins = bins_remain_cap >= item\n    if not np.any(viable_bins):\n      return priorities # If no bin can fit, return all zeros\n\n    # Calculate the remaining capacity after adding the item to each viable bin.\n    remaining_capacities = bins_remain_cap - item\n    remaining_capacities[remaining_capacities < 0 ] = 0 # setting non-viable bins cap to zero, needed for division\n    \n    # Calculate a \"fullness\" score for each viable bin.  Bins that are fuller (closer to full) are penalized.\n    # Avoid division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):  # Handle potential divide by zero. Set invalid to ignore will give us nan, which are handled below\n      fullness_scores = np.where(bins_remain_cap > 0, (bins_remain_cap - item) / bins_remain_cap, 0) # remaining capacity / initial capacity\n    fullness_scores = np.nan_to_num(fullness_scores, nan=1.0, posinf = 1.0, neginf = 1.0)\n    # Bins that are close to full (smaller relative wasted space) should be prioritized.\n\n    priorities = np.where(viable_bins, 1.0 - fullness_scores, -np.inf) # prioritize viable bins, and discourage (but allow) packing on almost full bins\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version incorporates a few Einstein-inspired heuristics:\n\n    1. **Space-Time Curvature Analogy:** Smaller remaining capacity bins \"attract\" smaller items more strongly.  This is modeled by a higher priority for bins where the item fits relatively snugly (but still fits).\n\n    2. **Principle of Least Action (Energy Minimization):** We prefer filling bins as completely as possible to minimize wasted space (potential energy). This is implemented by rewarding bins with remaining capacity close to the item size.\n\n    3. **Relativistic Effect (Size Dilation):** As the item size approaches the bin capacity, the relative \"difficulty\" of packing increases dramatically. We model this by a term that penalizes near-overflows, especially if many near-full bins exist. This also emulates length contraction when an item is placed into bins and a contraction term will be calculated to avoid packing it into those bins.\n\n    4. **Avoid Extreme Packing Densities:** If the bins are nearly full, apply a scaling factor to priorities based on the average fullness ratio. We prioritize dispersing the load across bins.\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    # First, handle cases where item doesn't fit.\n    fits = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[~fits] = -np.inf  # Very low priority for bins too small.\n    if not np.any(fits):\n        return priorities\n\n    # Calculate a \"snug fit\" score: Reward bins where the item fits relatively well.\n    snug_fit_score = (bins_remain_cap[fits] - item) / bins_remain_cap[fits]\n    snug_fit_score = 1.0 - snug_fit_score  # Higher score for better fit (closer to item size)\n    snug_fit_score = np.clip(snug_fit_score, 0.0, 1.0)  # Ensure values are within bounds\n    priorities[fits] += snug_fit_score\n\n    # Energy minimization: Prefer bins that get filled up nicely\n    remaining_space = bins_remain_cap[fits] - item\n    energy_score = np.exp(-np.abs(remaining_space) / (item + 1e-6))\n    priorities[fits] += energy_score\n\n    # Relativistic penalty for near-overflows.\n    overflow_margin = 0.05 * item  # Define a small margin near overflow\n\n    near_overflow = (bins_remain_cap[fits] - item) < overflow_margin\n    if np.any(near_overflow):\n        overflow_penalty = np.exp(5 * (bins_remain_cap[fits][near_overflow] - item - overflow_margin) / overflow_margin)\n        priorities[fits][near_overflow] -= overflow_penalty\n\n    #Contraction term to avoid packing item to a nearly full bin\n    contraction_term = np.exp(5 * (item-bins_remain_cap[fits]))\n    priorities[fits] -= contraction_term\n\n    # Avoid Extreme Densities scaling factor\n    avg_fullness = np.mean((np.sum(bins_remain_cap) - bins_remain_cap) / np.sum(bins_remain_cap))\n    density_scale = 1.0 - np.clip(avg_fullness, 0.0, 0.9)  # Apply the scaling based on the range, can be between 0.1 to 1\n\n    priorities[fits] *= density_scale # Scale priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see 1st uses a heliocentric model analogy, prioritizing bins closer in capacity to the item, while 20th prioritizes minimizing wasted space directly and uses a random factor. 1st normalizes differences and scales priority based on percentage filled, while 20th squares the remaining capacity.\n\nComparing (2nd) vs (19th), 2nd prioritizes bins with the smallest remaining capacity after packing and uses Newton's law of cooling analogy, while 19th prioritizes bins that can fit the item snugly by squaring the remaining space and adds a random factor. 2nd adds a bias to partially filled bins, and sets infeasible bins to negative infinity while 19th returns the bin capacities directly\n\nComparing (3rd) vs (18th), 3rd calculates fullness scores and prioritizes viable bins, discouraging almost-full bins, while 18th prioritizes bins that fit snugly, squaring the wasted space and adding a random factor. 3rd uses np.where to assign priorities and handles potential division by zero, while 18th returns bin capacities if no bins fit.\n\nComparing (4th) vs (17th), 4th incorporates feasibility checks, capacity utilization with exponential preference, and fragmentation avoidance, while 17th considers waste, fill level, and avoidance of near-full bins, normalizing waste and using exponentials for fill level priority. 4th penalizes fragmentation only if enough feasible bins exist. 17th uses a combination of normalized waste, fill level priority and fullness penalty, with weighting.\n\nComparing (5th) vs (16th), 5th duplicates the code from the 4th heuristic, which may be problematic since duplicate functions are redundant. 16th prioritizes bins that are \"almost full\" after adding item but penalize near-perfect fits and penalize bins that would have too little remaining space.\n\nComparing (6th) vs (15th), 6th calculates wasted space and adds a bonus for relatively full bins using a logarithm while 15th prioritizes bins large enough to accomodate item, prioritize \"almost full\" bins and penalize near-perfect fits and bins that would have too little remaining space.\n\nComparing (7th) vs (14th), 7th and 14th both include relativistic near-overflow penalization, however 7th has a perfect fit huge boost, while 14th has Space-Time Curvature Analogy, Principle of Least Action and Avoid Extreme Packing Densities\n\nComparing (8th) vs (13th), 8th prioritizes bins with smaller wasted space and gives higher priority to almost full bins after adding item while 13th also includes relativistic near-overflow penalization and Avoid Extreme Packing Densities\n\nComparing (9th) vs (12th), 9th duplicates the code from the 5th heuristic, which may be problematic since duplicate functions are redundant. 12th has Remaining capacity after packing relative to original capacity\n\nComparing (10th) vs (11th), 10th includes a perfect fit huge boost, while 11th has Remaining capacity after packing relative to original capacity\n\nComparing (second worst) vs (worst), we see 11th has Remaining capacity after packing relative to original capacity whereas 12th duplicates the code from the 11th heuristic, which may be problematic since duplicate functions are redundant.\n\nOverall: The better heuristics incorporate feasibility checks, capacity utilization, and fragmentation avoidance, often using mathematical functions like exponentials and logarithms to prioritize near-full bins while penalizing excessive waste or near-overflows. The best approaches also incorporate scaling factors or biases to encourage complete filling and avoid creating bins that can only hold very small items. Poorer performing heuristics have less sophisticated logic that do not account for edge cases and duplicate code.\n- \nOkay, let's redefine \"Current Self-Reflection\" to make it more effective for heuristic design, focusing on avoiding pitfalls and leading to better heuristics.\n\n*   **Keywords:** Adaptive Learning, Outcome Analysis, Solution Diversity, Algorithmic Bias.\n*   **Advice:** Analyze heuristic *performance* across diverse problem instances and parameter settings. Quantify solution *quality*, not just feasibility. Explicitly track *algorithmic bias* and design heuristics to mitigate it. Explore diverse solution pathways and avoid premature convergence.\n*   **Avoid:** Focusing solely on mathematical models without empirical validation. Ignoring the impact of parameter tuning on heuristic performance. Neglecting the exploration of alternative solution generation strategies.\n*   **Explanation:** Shift from static rules to adaptive learning from results. Validate and prevent bias. Aim for more diverse and better solutions.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}