{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function considers:\n    1. The wasted space if the item is added to the bin (smaller wasted space is better).\n    2. A preference for bins that are already somewhat full. This encourages filling bins more completely.\n    3. A large penalty for bins that are too small to hold the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    for i, cap in enumerate(bins_remain_cap):\n        if cap < item:\n            # Large negative priority if item doesn't fit.  Think of this as an infinite cost to overflowing a bin.\n            priorities[i] = -np.inf\n        else:\n            wasted_space = cap - item\n            # Favor bins with less wasted space (negative because lower wasted space means HIGHER priority)\n            priorities[i] -= wasted_space\n\n            # Add a bonus for bins that are already relatively full\n            #  The fuller, the better, but with diminishing returns, hence the log.\n            # Avoid log(0) if item is exactly the bin capacity.\n            if wasted_space > 0:\n                 priorities[i] += np.log(item / cap) # Use item/cap for more resolution.\n            else:\n                priorities[i] += 1.0 # Max bonus if perfect fit (cap=item, wasted space is exactly 0).\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This priority function considers:\n    1. Waste: The smaller the waste, the higher the priority.\n    2. Fill Level: Prefer bins that are already somewhat full.\n    3. Avoidance of Near-Full Bins: Avoid bins that will be nearly full after packing (risk of fragmentation).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n\n    # Calculate waste if the item is placed in each bin.\n    waste = bins_remain_cap - item\n    waste[waste < 0] = np.inf  # Mark infeasible bins as infinitely bad.\n\n    # Normalize waste (lower waste is better).  Small values of waste should contribute heavily\n    normalized_waste = np.exp(-waste)\n\n    # Encourage filling partially filled bins.  The closer to halfway, the better.\n    fill_level_priority = np.exp(-np.abs(bins_remain_cap - item - np.mean(bins_remain_cap))/np.std(bins_remain_cap)) if np.std(bins_remain_cap) > 0 else np.ones_like(bins_remain_cap)\n\n    # Penalize bins that become very full after packing (e.g., > 90% full).\n    fullness_after = (bins_remain_cap - item) / np.max(bins_remain_cap) # max capacity\n    fullness_penalty = np.where(fullness_after < 0.1, -100 * (0.1-fullness_after), 0)\n\n    # Combine the factors. Weighting is important.\n    priorities = (normalized_waste * 0.6 +\n                 fill_level_priority * 0.3 +\n                 fullness_penalty * 0.1)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see 1st uses a heliocentric model analogy, prioritizing bins closer in capacity to the item, while 20th prioritizes minimizing wasted space directly and uses a random factor. 1st normalizes differences and scales priority based on percentage filled, while 20th squares the remaining capacity.\n\nComparing (2nd) vs (19th), 2nd prioritizes bins with the smallest remaining capacity after packing and uses Newton's law of cooling analogy, while 19th prioritizes bins that can fit the item snugly by squaring the remaining space and adds a random factor. 2nd adds a bias to partially filled bins, and sets infeasible bins to negative infinity while 19th returns the bin capacities directly\n\nComparing (3rd) vs (18th), 3rd calculates fullness scores and prioritizes viable bins, discouraging almost-full bins, while 18th prioritizes bins that fit snugly, squaring the wasted space and adding a random factor. 3rd uses np.where to assign priorities and handles potential division by zero, while 18th returns bin capacities if no bins fit.\n\nComparing (4th) vs (17th), 4th incorporates feasibility checks, capacity utilization with exponential preference, and fragmentation avoidance, while 17th considers waste, fill level, and avoidance of near-full bins, normalizing waste and using exponentials for fill level priority. 4th penalizes fragmentation only if enough feasible bins exist. 17th uses a combination of normalized waste, fill level priority and fullness penalty, with weighting.\n\nComparing (5th) vs (16th), 5th duplicates the code from the 4th heuristic, which may be problematic since duplicate functions are redundant. 16th prioritizes bins that are \"almost full\" after adding item but penalize near-perfect fits and penalize bins that would have too little remaining space.\n\nComparing (6th) vs (15th), 6th calculates wasted space and adds a bonus for relatively full bins using a logarithm while 15th prioritizes bins large enough to accomodate item, prioritize \"almost full\" bins and penalize near-perfect fits and bins that would have too little remaining space.\n\nComparing (7th) vs (14th), 7th and 14th both include relativistic near-overflow penalization, however 7th has a perfect fit huge boost, while 14th has Space-Time Curvature Analogy, Principle of Least Action and Avoid Extreme Packing Densities\n\nComparing (8th) vs (13th), 8th prioritizes bins with smaller wasted space and gives higher priority to almost full bins after adding item while 13th also includes relativistic near-overflow penalization and Avoid Extreme Packing Densities\n\nComparing (9th) vs (12th), 9th duplicates the code from the 5th heuristic, which may be problematic since duplicate functions are redundant. 12th has Remaining capacity after packing relative to original capacity\n\nComparing (10th) vs (11th), 10th includes a perfect fit huge boost, while 11th has Remaining capacity after packing relative to original capacity\n\nComparing (second worst) vs (worst), we see 11th has Remaining capacity after packing relative to original capacity whereas 12th duplicates the code from the 11th heuristic, which may be problematic since duplicate functions are redundant.\n\nOverall: The better heuristics incorporate feasibility checks, capacity utilization, and fragmentation avoidance, often using mathematical functions like exponentials and logarithms to prioritize near-full bins while penalizing excessive waste or near-overflows. The best approaches also incorporate scaling factors or biases to encourage complete filling and avoid creating bins that can only hold very small items. Poorer performing heuristics have less sophisticated logic that do not account for edge cases and duplicate code.\n- \nOkay, let's redefine \"Current Self-Reflection\" to make it more effective for heuristic design, focusing on avoiding pitfalls and leading to better heuristics.\n\n*   **Keywords:** Adaptive Learning, Outcome Analysis, Solution Diversity, Algorithmic Bias.\n*   **Advice:** Analyze heuristic *performance* across diverse problem instances and parameter settings. Quantify solution *quality*, not just feasibility. Explicitly track *algorithmic bias* and design heuristics to mitigate it. Explore diverse solution pathways and avoid premature convergence.\n*   **Avoid:** Focusing solely on mathematical models without empirical validation. Ignoring the impact of parameter tuning on heuristic performance. Neglecting the exploration of alternative solution generation strategies.\n*   **Explanation:** Shift from static rules to adaptive learning from results. Validate and prevent bias. Aim for more diverse and better solutions.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}