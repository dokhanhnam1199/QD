{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins based on a combination of factors, including fullness,\n    fragmentation avoidance, and a dynamic adjustment based on item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n\n    # Infeasible bins get lowest priority\n    priorities[remaining_capacity < 0] = -np.inf\n    feasible_bins = remaining_capacity >= 0\n\n    if np.any(feasible_bins):\n        # 1. Prioritize bins that will be nearly full after packing.\n        almost_full_threshold = 0.15  # Slightly wider range\n        almost_full = feasible_bins & (remaining_capacity <= almost_full_threshold * bins_remain_cap)\n        priorities[almost_full] += 15  # Increased priority\n\n        # 2. Discourage very small remainders (fragmentation).  Slightly more aggressive.\n        small_remainder_threshold = 0.25  # Increased threshold\n        small_remainder = feasible_bins & (remaining_capacity > 0) & (remaining_capacity <= small_remainder_threshold * bins_remain_cap)\n        priorities[small_remainder] -= 5\n\n        # 3. Penalize near-perfect fits, but less severely if the item is large.\n        near_perfect_fit_threshold = 0.01\n        near_perfect_fit = feasible_bins & (remaining_capacity > (1 - near_perfect_fit_threshold) * bins_remain_cap)\n        priorities[near_perfect_fit] -= 3  # Slightly reduced penalty\n\n        # 4. Base priority on utilization, scaled by item size.  Larger items get more influence.\n        utilization = item / bins_remain_cap\n        priorities[feasible_bins] += utilization[feasible_bins] * (0.5 + item)  # Scale by item size\n\n        # 5. Adaptive adjustment:  Favor bins whose remaining capacity is close to the item size.\n        capacity_difference = np.abs(bins_remain_cap - item)\n        priority_boost = np.exp(-capacity_difference / np.mean(bins_remain_cap[feasible_bins]))  # Gaussian-like boost\n        priorities[feasible_bins] += priority_boost[feasible_bins] * 2\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins considering multiple factors:\n    - Remaining capacity relative to item size.\n    - Potential for creating balanced bin utilization.\n    - Penalties for creating very small remainders or near-perfect fits.\n    - Encourages packing into bins that are already somewhat filled.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    remaining_capacity = bins_remain_cap - item\n\n    # Infeasible bins get lowest priority\n    priorities[remaining_capacity < 0] = -np.inf\n\n    feasible_bins = remaining_capacity >= 0\n\n    if np.any(feasible_bins):\n        # 1. Prioritize bins where the item fills a significant portion of the bin.\n        utilization = item / bins_remain_cap\n        priorities[feasible_bins] += 2 * utilization[feasible_bins]  # Increased weight\n\n        # 2. Encourage balanced bin utilization (avoiding very empty or very full bins after packing).\n        #   - Aim for a target utilization around 70-80%.\n        target_utilization = 0.75\n        expected_new_capacity = bins_remain_cap[feasible_bins] - item\n        expected_utilization = (bins_remain_cap[feasible_bins] - expected_new_capacity) / bins_remain_cap[feasible_bins]\n        \n        # give a higher priority to bins which are closer to target utilization after packing\n        balanced_utilization_score = np.exp(-((expected_utilization - target_utilization)**2) / 0.02) # Gaussian-like weighting\n        priorities[feasible_bins] += 3 * balanced_utilization_score # Increased weight\n\n        # 3. Penalize Near-Perfect Fit (leaving very little unused space).\n        near_perfect_fit_threshold = 0.05  # Increased threshold slightly\n        near_perfect_fit = feasible_bins & (remaining_capacity <= near_perfect_fit_threshold * bins_remain_cap)\n        priorities[near_perfect_fit] -= 5\n\n        # 4. Penalize small remainders to avoid fragmentation. Adjusted Threshold\n        small_remainder_threshold = 0.15 # Decreased threshold\n        small_remainder = feasible_bins & (remaining_capacity > 0) & (remaining_capacity <= small_remainder_threshold * bins_remain_cap)\n        priorities[small_remainder] -= 3\n\n        # 5. Give a slight bonus to bins that are already partially filled (but not too full). This encourages\n        #    using existing bins instead of always starting new ones.\n        already_filled_threshold_low = 0.2\n        already_filled_threshold_high = 0.9\n        already_filled = feasible_bins & (bins_remain_cap > already_filled_threshold_low * np.max(bins_remain_cap)) & (bins_remain_cap < already_filled_threshold_high * np.max(bins_remain_cap))\n        priorities[already_filled] += 1\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the 1st heuristic has a lot of tunable hyper parameters that could be optimized with enough experimentation to get better packing results. Comparing (2nd) vs (19th) we see the inverse, the 2nd heuristic is too rigid and has too little parameters. Comparing (1st) vs (2nd), we see that the first heuristic has more parameters, which gives it more flexibility and adaptability, where the second one has hard coded parameters. Comparing (3rd) vs (4th), we see that 3rd is much more interpretable than the 4th heuristic. Comparing (2nd worst) vs (worst), we see the importance of including perfect fit bonus. Overall: more parameters with adaptive thresholds, bonus and penalties leads to better heuristics, also making the heuristic more interpretable is important.\n- \nHere's a refined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Adaptability, Exploration, Transparency, Multi-objective.\n*   **Advice:** Prioritize flexible designs with adaptive parameters and stochastic elements for robust exploration. Ensure interpretability.\n*   **Avoid:** Rigid, hardcoded parameters; redundant code; focusing solely on single objectives.\n*   **Explanation:** Design heuristics that dynamically adjust to problem characteristics, actively explore the solution space, and are easily understood and maintained.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}