{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines inverse waste with exponential weighting for better bin fit.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        priorities[valid_bins] = 1.0 / (1 + np.exp(-5 * waste))\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priorities based on the remaining capacity of bins and the item size.\n\n    Args:\n        item (float): The size of the item.\n        bins_remain_cap (np.ndarray): The remaining capacity of each bin.\n        small_value (float, optional): A small value added to the denominator to prevent division by zero. Defaults to 1e-6.\n\n    Returns:\n        np.ndarray: The calculated priorities for each bin.\n    \"\"\"\n    temp = bins_remain_cap - item\n    priorities = np.where(temp >= 0, np.exp(temp / (item + small_value)), 0)\n    priorities = priorities / np.sum(priorities)\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), both use inverse waste with exponential weighting, but the 1st normalizes *after* exponentiation on all possible bins, while the 2nd normalizes only on feasible bins and uses a scaling factor of 5.  (1st) is preferable due to broader consideration.\n\nComparing (3rd) vs (4th), (3rd) simply returns zeros \u2013 a baseline, while (4th) computes a relative waste and uses an exponential weighting, offering a proper prioritization. (4th) is significantly better.\n\nComparing (5th) vs (6th), both calculate inverse waste, but (6th) adds `min_waste` to the denominator which can smooth the priority landscape but potentially over-corrects. (5th) is cleaner.\n\nComparing (7th) vs (8th), both are fundamentally inverse waste, (7th) directly uses the distance (waste) in the exponential, while (8th) calculates distances explicitly and then applies the inverse. (7th) is concise and likely performs similarly.\n\nComparing (9th) vs (10th), both use exponential weighting based on waste, but (10th) calculates *relative* waste (item/capacity), offering a better normalization. (10th) is superior.\n\nComparing (11th) vs (12th/13th), (11th) returns only zeros. (12th/13th) are identical and introduce a threshold and weight, resulting in a meaningful prioritization. (12th/13th) are far better.\n\nComparing (14th) vs (15th/16th), (14th), (15th) and (16th) are identical. All are good as they use an exponential function of waste, but the constant `5` may require tuning.\n\nComparing (17th) vs (18th), (17th) tries to find the *worst* fit and prioritize that bin, which is a very unusual approach and likely suboptimal. (18th) computes an exponential of `item / capacity`, which is a more standard practice. (18th) is better.\n\nComparing (19th) vs (20th), (19th) is using a small value to avoid dividing by zero and normalizing by the sum of priorities, but (20th) uses the worst fit capacities which can lead to poor packing. (19th) is better.\n\nOverall: Prioritizing based on inverse waste combined with exponential weighting appears to be the strongest pattern. Normalizing appropriately (after exponentiation, or using relative waste) is crucial. Adding thresholds or fixed weights requires careful tuning.  Directly focusing on the 'worst' fit appears to be a flawed strategy.  Simplicity and clear mathematical basis (like inverse waste) generally outperform complex, ad-hoc approaches.\n- \nOkay, let's distill those reflections into actionable heuristic design guidance. Here's a breakdown aiming for that $999K reward!\n\n* **Keywords:** Waste minimization, exponential weighting, normalization, relative prioritization, robustness.\n* **Advice:** Focus on quantifying *relative* fit (waste/item size). Use inverse waste with exponential weighting *before* normalization to a probability distribution. Prioritize simplicity and clear logic.\n* **Avoid:** Identifying \"worst\" fits, direct use of remaining capacity alone, unnecessary complexity, negative prioritization, redundant calculations.\n* **Explanation:**  Strong heuristics aren\u2019t about finding bad options, but consistently favoring *better* ones. Normalization creates a stable, comparable scoring system, while exponential weighting refines sensitivity to fit quality. Robustness handles all scenarios gracefully.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}