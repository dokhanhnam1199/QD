```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns a dynamic, context-sensitive priority score for each bin.

    This heuristic transcends the simple 'Best Fit' by introducing emergent
    complexity through multi-factor scoring with adaptive, non-linear weights.
    It considers not only the immediate fit but also the long-term utility of
    the remaining bin space and the overall state of the packing process.

    Inspired by principles of resource distribution and system equilibrium,
    this function prioritizes bins based on a synthesized "desirability score".
    It aims to balance immediate space efficiency with a strategic allocation
    that might lead to better overall packing density and fewer new bins,
    adapting its preferences based on the item's size and the current
    fragmentation state of the available bins.

    Args:
        item: Size of item to be added to the bin (assumed to be normalized, e.g., 0.0 to 1.0).
        bins_remain_cap: Array of current remaining capacities for each bin
                         (values should be consistent with `BIN_CAPACITY`).

    Return:
        Array of same size as bins_remain_cap with a complex priority score for each bin.
        Bins where the item does not fit receive a score of -infinity to ensure
        they are never chosen. The bin with the highest (least negative) score
        is deemed the most 'desirable' for the item.
    """
    # Assume a standard bin capacity for normalization. This is a crucial
    # underlying assumption for calculating fill levels and relative item sizes.
    # In typical BPP, bins are normalized to 1.0.
    BIN_CAPACITY = 1.0

    # Initialize scores for all bins to negative infinity.
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # If no bin can fit the item, return early.
    if not np.any(can_fit_mask):
        return scores

    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # --- Adaptive Contextual Metrics (Simulating System State for "Adaptive Scoring") ---
    # These metrics are computed from the current `bins_remain_cap` array and
    # dynamically adjust heuristic parameters. This is a proxy for "observed performance"
    # or "system equilibrium" within a stateless function.

    # Standard deviation of remaining capacities: A measure of the diversity/fragmentation
    # of available space. Higher STD suggests more unevenly filled bins.
    # Add a small epsilon to prevent division by zero if all capacities are identical.
    std_dev_remain_cap = np.std(bins_remain_cap) + 1e-6

    # Average fill level across all bins: Indicates overall system fullness.
    # Helps to decide whether to consolidate or open new bins implicitly.
    num_total_bins = len(bins_remain_cap)
    total_capacity = BIN_CAPACITY * num_total_bins
    total_used_capacity = total_capacity - np.sum(bins_remain_cap)
    # Add epsilon to denominator to prevent division by zero for total_capacity in edge case
    avg_fill_level = total_used_capacity / (total_capacity + 1e-6)

    # --- Component 1: Core Fit Tendency (Mutated Best Fit) ---
    # This is the base term, rewarding bins that lead to a smaller remaining capacity.
    # It acts as a primary drive towards space efficiency.
    r_after_placement = valid_bins_remain_cap - item
    score_fit = -r_after_placement  # Maximize (minimize negative remaining capacity)

    # --- Component 2: Fragment Optimization (Non-monotonic, Context-Sensitive) ---
    # This term penalizes both extremely small (potentially unusable) and
    # excessively large (potentially wasted) remaining capacities. It introduces a
    # "sweet spot" for `r_after_placement`, which dynamically adapts.
    
    # 'Ideal' fragment size: A base value plus a small fraction of the item size.
    # Adapted by `avg_fill_level`: If system is full, we prefer smaller fragments.
    fragment_target = (0.02 + 0.1 * item) * (1.0 - avg_fill_level / 2.0)
    fragment_target = np.maximum(fragment_target, 0.01) # Ensure target is not zero or negative

    # 'Tolerance' for fragment size: Controls how strictly we adhere to the ideal.
    # Adapted by `std_dev_remain_cap`: More fragmented system means more tolerant.
    fragment_tolerance = (0.05 + 0.1 * item) * (1.0 + std_dev_remain_cap * 0.5)
    fragment_tolerance = np.maximum(fragment_tolerance, 1e-6) # Prevent division by zero

    # Gaussian-like reward: Peaks when `r_after_placement` is near `fragment_target`.
    score_fragment_utility = np.exp(-(r_after_placement - fragment_target)**2 / (2 * fragment_tolerance**2))

    # --- Component 3: Bin Utilization Incentive (Adaptive Fullness) ---
    # Rewards using bins that are already relatively full. This encourages consolidation
    # of items into existing bins, potentially reducing the total number of bins used.
    # The emphasis of this term changes based on the item size and overall system fullness.

    initial_fill_level = (BIN_CAPACITY - valid_bins_remain_cap) / BIN_CAPACITY
    # Square the fill level to heavily penalize choosing very empty bins.
    # Scale inversely with item size (smaller items have more choice, so this matters more).
    # Scale by `utilization_emphasis`: If system is mostly empty, prioritize filling existing bins.
    utilization_emphasis = 1.0 + (1.0 - avg_fill_level)**2 * 1.5 # Higher when bins are generally empty

    score_utilization = (initial_fill_level**2) * (1.0 / (item + 1e-6)) * utilization_emphasis

    # --- Dynamic and Non-Linear Weighting of Components ---
    # The influence of each component adapts based on the `item` size and the
    # current `std_dev_remain_cap` and `avg_fill_level` to reflect
    # "high-dimensional tuning" and "adaptive scoring."

    # Weight for Core Fit: Increases with item size (large items demand precise fits).
    W_fit = 1.0 + (item / BIN_CAPACITY)**2
    # If bins are very fragmented, emphasize exact fit slightly more.
    W_fit *= (1.0 + std_dev_remain_cap * 0.1)

    # Weight for Fragment Optimization: More important for medium/small items, less for very large ones.
    W_fragment = 0.5 * (1.0 - (item / BIN_CAPACITY))
    # If bins are uniformly filled, perhaps focus more on ideal fragments.
    W_fragment *= (1.0 + (1.0 - std_dev_remain_cap) * 0.2)

    # Weight for Utilization: Heavily weighted for smaller items to consolidate, less for larger.
    W_utilization = 0.8 * (1.0 - (item / BIN_CAPACITY))**2
    # If system is generally empty, strongly encourage using existing bins.
    W_utilization *= (1.0 + (1.0 - avg_fill_level)**2 * 0.5)

    # Ensure all weights are non-negative.
    W_fit = np.maximum(W_fit, 0.0)
    W_fragment = np.maximum(W_fragment, 0.0)
    W_utilization = np.maximum(W_utilization, 0.0)

    # --- Combine the Scores (Emergent Complexity) ---
    # The final score is a weighted sum, where weights are non-linearly adapted.
    # The interaction between these terms with dynamic weights fosters emergent behavior.
    combined_scores = (W_fit * score_fit +
                       W_fragment * score_fragment_utility +
                       W_utilization * score_utilization)

    # Assign the calculated scores back to the masked positions.
    scores[can_fit_mask] = combined_scores

    return scores
```
