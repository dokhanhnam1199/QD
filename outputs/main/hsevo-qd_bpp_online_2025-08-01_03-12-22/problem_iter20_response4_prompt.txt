{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    base_fit_priority: float = 50.0,\n    rem_cap_penalty_factor: float = 3.5,\n    rem_cap_penalty_exponent: float = 1.7,\n    relative_fill_bonus_factor: float = 8.0,\n    perfect_fit_threshold: float = 0.01,\n    perfect_fit_bonus: float = 180.0,\n    min_allowed_priority: float = 0.5,\n    non_fitting_priority: float = -10.0\n) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function refines the \"Best Fit\" strategy, enhancing \"priority_v1\"\n    with a strong emphasis on \"Intelligent Bin Closure\". It introduces a\n    significant, dedicated bonus for \"near-perfect fits\", aggressively\n    prioritizing bins that can be almost completely filled, thereby minimizing\n    fragmentation and promoting efficient bin closure.\n\n    Args:\n        item: Size of item to be added to the bin. Must be greater than 0.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n        base_fit_priority: A base score given to any bin where the item can fit.\n                           This sets the initial priority before any adjustments.\n        rem_cap_penalty_factor: A multiplier for the non-linear penalty applied\n                                based on the remaining capacity after item placement.\n                                Higher values penalize larger remaining capacities more.\n        rem_cap_penalty_exponent: An exponent (typically > 1) applied to the\n                                  remaining capacity when calculating the penalty.\n                                  A value greater than 1 makes the penalty for\n                                  larger remaining capacities disproportionately\n                                  more severe, thereby strongly favoring very tight fits\n                                  (i.e., less remaining space).\n        relative_fill_bonus_factor: A multiplier for a bonus that rewards placing\n                                    an item into a bin where it occupies a significant\n                                    proportion of the bin's *current* available capacity.\n                                    This encourages \"filling up\" bins that are already\n                                    partially utilized or finding relatively large items\n                                    for smaller remaining spaces.\n        perfect_fit_threshold: If the remaining capacity after item placement is\n                               less than or equal to this threshold, a substantial\n                               'perfect_fit_bonus' is added. This strategically targets\n                               scenarios where a bin can be considered 'closed' or\n                               left with negligible unusable space.\n        perfect_fit_bonus: A large, fixed bonus applied to bins that achieve a\n                           near-perfect fit. This value should be high enough to\n                           make such bins overwhelmingly attractive, reflecting a\n                           strategic priority for bin closure.\n        min_allowed_priority: The minimum priority score a bin can receive if\n                              the item fits. This acts as a floor, preventing\n                              valid placement priorities from becoming too low.\n        non_fitting_priority: The priority score assigned to bins where the\n                              item does not fit. Typically a low or negative value.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities with the score for non-fitting bins\n    priorities = np.full_like(bins_remain_cap, fill_value=non_fitting_priority, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only process if there are bins where the item can fit\n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        remaining_after_placement = fitting_bins_remain_cap - item\n\n        # 1. Non-linear Penalty for Remaining Capacity (Enhanced Best Fit)\n        # This term strongly favors bins with minimal remaining space after placement.\n        rem_cap_penalty = rem_cap_penalty_factor * (remaining_after_placement ** rem_cap_penalty_exponent)\n\n        # 2. Relative Fill Bonus\n        # This term rewards maximizing the utilization of the bin's current available space.\n        # Division by zero is avoided as fitting_bins_remain_cap >= item > 0.\n        relative_fill_ratio = item / fitting_bins_remain_cap\n        relative_fill_bonus = relative_fill_bonus_factor * relative_fill_ratio\n\n        # 3. Strategic Near-Perfect Fit Bonus (Intelligent Bin Closure)\n        # This is a new, aggressive component designed to strongly incentivize\n        # \"closing\" a bin by leaving a very small or zero remaining capacity.\n        # It ensures that bins nearing full capacity are given top preference.\n        near_perfect_fit_mask = remaining_after_placement <= perfect_fit_threshold\n        perfect_fit_bonus_term = np.where(near_perfect_fit_mask, perfect_fit_bonus, 0.0)\n\n        # Combine the base priority, non-linear penalty, relative fill bonus,\n        # and the new perfect fit bonus.\n        calculated_priorities = (\n            base_fit_priority\n            - rem_cap_penalty\n            + relative_fill_bonus\n            + perfect_fit_bonus_term  # Add the high bonus for near-perfect fits\n        )\n\n        # Ensure that the calculated priority does not fall below a minimum allowed value\n        calculated_priorities = np.maximum(calculated_priorities, min_allowed_priority)\n\n        # Assign the calculated priorities to the corresponding bins\n        priorities[can_fit_mask] = calculated_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                bins_remain_cap: np.ndarray,\n                base_fit_priority: float = 40.67957013415283,\n                remaining_capacity_penalty_factor: float = 2.527225052325968,\n                min_allowed_priority: float = 0.5896372460377476,\n                non_fitting_priority: float = -8.783827459234484,\n                perfect_fit_bonus: float = 10.0,\n                perfect_fit_epsilon: float = 1e-9) -> np.ndarray:\n    \"\"\"Combines Best Fit with a strong bonus for perfect item placement.\n    Prioritizes minimizing remaining bin capacity while significantly rewarding bins\n    that are completely filled by the item, improving space utilization.\n    \"\"\"\n    # Initialize all priorities with the value for non-fitting bins\n    priorities = np.full_like(bins_remain_cap, fill_value=non_fitting_priority, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only process if there are bins where the item can fit\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity for bins where the item could be placed\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Calculate the base priority using a \"Best Fit\" approach\n        # This penalizes larger remaining capacities, encouraging a tight fit.\n        calculated_priorities = base_fit_priority - (remaining_capacity_penalty_factor * remaining_after_placement)\n\n        # Apply a significant bonus for \"perfect fits\" (remaining capacity is nearly zero).\n        # This incentivizes closing bins efficiently.\n        perfect_fit_mask_local = np.isclose(remaining_after_placement, 0.0, atol=perfect_fit_epsilon)\n        calculated_priorities[perfect_fit_mask_local] += perfect_fit_bonus\n\n        # Ensure that the calculated priority does not fall below a minimum allowed value\n        calculated_priorities = np.maximum(calculated_priorities, min_allowed_priority)\n\n        # Assign the calculated priorities to the corresponding bins\n        priorities[can_fit_mask] = calculated_priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that Heuristic 1st employs a sophisticated multi-factor scoring system, incorporating a non-linear penalty for remaining capacity and a bonus for relative fill, aiming to optimize bin utilization. In stark contrast, Heuristic 20th simply returns an array of zeros, effectively providing no intelligent guidance for bin selection and leading to arbitrary placement. This fundamental difference in strategy explains why 1st is ranked best and 20th is worst.\n\nComparing (2nd) vs (19th), Heuristic 2nd is a highly refined strategy similar to 1st, adding an explicit bonus for perfect fits. Heuristic 19th, like 20th, returns all zeros, offering no strategic value. The performance difference mirrors that of 1st vs 20th, showcasing the superiority of intelligent heuristics over no strategy.\n\nComparing (1st) vs (2nd), Heuristic 1st omits the explicit `perfect_fit_bonus` found in Heuristic 2nd. The fact that 1st performs better suggests that its combination of a non-linear penalty (where `rem_cap_penalty_exponent > 1`) and a relative fill bonus (rewarding `item / fitting_bins_remain_cap`) implicitly handles perfect and tight fits so effectively that an additional, large, explicit bonus becomes redundant or even detrimental. An overly aggressive, discrete bonus might lead to local optima that are not globally efficient, perhaps by prioritizing a perfect fit in a bin when another less \"perfect\" fit could lead to better overall packing for future items.\n\nComparing (3rd) vs (4th), both implement a `perfect_fit_bonus`. However, Heuristic 4th applies a higher bonus (`180.0` vs `150.0`) and uses a `perfect_fit_threshold` to define \"near-perfect fits\" more broadly (i.e., `remaining_after_placement <= threshold`) than 3rd's stricter `np.abs(remaining_after_placement) < perfect_fit_tolerance`. Heuristic 3rd being better than 4th indicates that making the \"perfect fit\" bonus too large or applying it too broadly (to a wider threshold) can degrade performance. This reinforces the idea that over-incentivizing specific conditions might lead to less optimal global solutions in complex packing scenarios.\n\nComparing (19th) vs (20th), both heuristics are identical, returning `np.zeros_like(bins_remain_cap)`. This implies they offer no intelligence in bin selection and are thus equally poor, ranking at the very bottom.\n\nOverall: The best-performing heuristics (1st-6th) leverage multi-component scoring. A non-linear penalty for remaining capacity is highly effective for promoting tight fits. The \"relative fill bonus\" (prioritizing items that occupy a large proportion of *current* available bin space) appears to be a crucial component for efficient bin utilization. Simpler heuristics, such as pure \"Best Fit\" (13th, 14th) or \"Proportional Fill\" (10th, 15th), perform poorly, as do those relying only on linear penalties (7th, 8th). Explicit \"perfect fit\" bonuses, while intuitive, need careful calibration, as the top heuristic (1st) excels without one, suggesting a smoothly weighted function can implicitly capture this goal better than an abrupt, large bonus. Complex heuristics (e.g., 16th), even with good ideas, can perform poorly if parameters are not optimally tuned.\n- \nHere's a redefined 'Current self-reflection' to design better heuristics:\n\n*   **Keywords:** Continuous Scoring, Parameter Calibration, Performance Validation, Objective Alignment.\n*   **Advice:** Design heuristics with continuous, multi-factor scoring functions that non-linearly prioritize efficient resource utilization and bin closure. Meticulously calibrate all parameters to align precisely with the optimization objective, enhancing foundational principles like Best Fit.\n*   **Avoid:** Relying on untuned, overly simplistic, or unvalidated complex strategies. Do not conflate basic functional correctness or code clarity with core heuristic performance design principles. Shun non-discriminating or trivial decision rules.\n*   **Explanation:** Superior heuristics emerge from rigorously tuned, objective-driven scoring functions, where every component is validated to maximize strategic decision-making and overall system performance.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}