{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority score for each bin. Combines Best Fit with a penalty for very small, non-zero remainders.\n    Prioritizes perfect fits, then larger useful gaps over tiny unusable ones, aiming to minimize wasted fragmented space.\n    \"\"\"\n    # Initialize scores for all bins to negative infinity, ensuring un-fittable bins are never chosen.\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if item were placed. This is the core Best Fit principle.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    # Initial score: negative of remaining capacity. Perfect fit (0 remainder) gets 0.0 (highest).\n    scores[can_fit_mask] = -remaining_after_fit\n\n    # Define thresholds for what constitutes a \"tiny\" and potentially \"unusable\" remainder.\n    # These values are empirical and might need tuning based on typical item/bin size distributions.\n    TINY_REMAINDER_THRESHOLD = 0.05  # e.g., 5% of an assumed normalized bin capacity (e.g., if max capacity is 1.0)\n    PENALTY_FOR_TINY_REMAINDER = 0.001 # A small penalty, ensuring a perfect fit (0 remainder) still receives the highest score.\n\n    # Identify valid bins that would result in a very small, non-zero remainder.\n    # A small epsilon (1e-9) is used to robustly check for non-zero floating-point values.\n    tiny_remainder_cond = (remaining_after_fit > 1e-9) & (remaining_after_fit < TINY_REMAINDER_THRESHOLD)\n\n    # Apply a penalty to the scores of bins that leave a tiny, potentially unusable remainder.\n    # This slightly discourages leaving highly fragmented space, encouraging either a perfect fit\n    # or a more substantial, potentially useful remaining gap for future, larger items.\n    scores[can_fit_mask][tiny_remainder_cond] -= PENALTY_FOR_TINY_REMAINDER\n\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        default_priority_value: The base priority value used to initialize the priority array.\n                                In the original implementation, this was implicitly 0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # The original implementation implicitly returned an array filled with zeros.\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the top-ranked heuristic employs a \"Best Fit\" strategy with meticulously tuned parameters (`base_fit_priority`, `remaining_capacity_penalty_factor`, `min_allowed_priority`), actively seeking to minimize remaining bin capacity after item placement. In contrast, the worst-ranked heuristic (and others in the lowest tier, 15th-20th) effectively assigns uniform or zero priority to all fitting bins, resulting in no strategic choice. This stark performance difference highlights the profound impact of even a simple, well-defined greedy strategy over a non-discriminating approach, demonstrating the immense value of strategic prioritization.\n\nComparing (1st) vs (2nd) and (3rd) vs (4th), the heuristics are identical in their implementation and parameter values. Their consecutive ranking suggests either negligible performance differences (statistical ties) or an extremely robust performance for this specific \"Best Fit\" variant, making it consistently the best choice. Similarly, comparing (19th) vs (20th), these heuristics are functionally identical, both providing no strategic prioritization, confirming their equally poor performance and serving as a baseline for non-optimized bin assignment.\n\nComparing the simpler \"Best Fit\" heuristics (1st-4th) against the more complex, multi-factor adaptive heuristics (5th, 9th, 12th, 14th), the simpler approach surprisingly performed better. The adaptive heuristics incorporate dynamic weighting, non-linear scoring, and probabilistic elements to account for \"Quantum Fit,\" \"Harmonic Fullness,\" and \"Flux Equilibrium.\" Their lower ranking indicates that increased complexity, dynamic adjustments, and stochasticity do not guarantee superior performance and can even hinder it if not perfectly balanced or if they lead to less stable decision-making in the given problem context.\n\nFurthermore, heuristics like 6th, 7th, 8th, and 13th, which augment Best Fit with specific penalties for small fragments or bonuses for high utilization, are ranked in the mid-tier. This suggests that while these additions are conceptually sound, their specific implementation or parameterization might not have yielded sufficient gains to surpass the top-performing, simpler \"Best Fit\" with its optimized fixed parameters. The \"Proportional Fill\" strategy (10th, 11th) which prioritizes items filling a larger *proportion* of the remaining bin space, performed notably worse than Best Fit, indicating that minimizing absolute remaining space is more effective than maximizing relative fill in this scenario.\n\nOverall: The best performing heuristics are simple and consistently apply a 'Best Fit' strategy, indicating that simplicity with strong, fixed parameters can often outperform complex, dynamically-tuned strategies in this context. The worst performing heuristics are those that lack any specific prioritization logic.\n- \nHere's a redefined 'Current self-reflection' to design better heuristics, avoiding the pitfalls of 'Ineffective self-reflection':\n\n### Redefined 'Current self-reflection'\n1.  **Prioritize Core Intuition:** Design around fundamental, domain-specific principles (e.g., minimizing waste) that are inherently effective and interpretable.\n2.  **Controlled Refinement:** Incremental additions to heuristic logic must demonstrate measurable, consistent performance gains beyond simpler baselines.\n3.  **Focused Discrimination:** Heuristics must provide clear, non-trivial guidance, actively differentiating choices based on the primary optimization objective.\n4.  **Parameter Prudence:** Optimize a minimal set of well-understood parameters for robustness; complex, untuned schemes typically underperform.\n\n---\n\n*   **Keywords:** Core Principles, Simplicity, Validation, Focused Guidance, Parameter Tuning.\n*   **Advice:** Design heuristics grounded in clear core principles, rigorously validating any added complexity, and precisely tuning minimal parameters for robust optimization.\n*   **Avoid:** Generic implementation concerns, unvalidated multi-factor scoring, or vague problem data advice that doesn't translate to specific heuristic logic.\n*   **Explanation:** This approach prioritizes effective, interpretable heuristic logic over over-engineered or poorly justified solutions, leading to more reliable and performing systems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}