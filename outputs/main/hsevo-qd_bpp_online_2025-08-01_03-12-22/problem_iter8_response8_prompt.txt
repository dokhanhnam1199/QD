{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for each bin, based on an \"Adaptive Fullness Prioritization\" heuristic.\n\n    This heuristic mutates the standard 'Best Fit' by introducing strategic considerations\n    for space management beyond simple minimization of remaining capacity. It aims to:\n\n    1.  **Strongly Reward Perfect Fits**: A perfect fit (leaving 0 remaining capacity)\n        is the most efficient use of space, effectively \"closing\" a bin. This is\n        given a significant bonus.\n    2.  **Prioritize High Overall Utilization**: Similar to Best Fit, bins that\n        become very full after placing the item are generally preferred.\n    3.  **Penalize Fragmented Space**: A minor penalty is applied to bins that\n        are left with a very small, non-zero remaining capacity. Such 'fragments'\n        are often too small to be useful for subsequent items and can lead to\n        wasted space or increased bin count if many such bins accumulate.\n        This encourages the selection of bins that either achieve a perfect fit,\n        or leave a more 'useful' (larger) amount of remaining space, allowing for\n        greater flexibility for future items.\n\n    The goal is to not just minimize residual space, but to do so in a way\n    that minimizes \"unusable\" small fragments, promoting overall\n    packing efficiency and potentially reducing the number of bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        BIN_CAPACITY: The maximum capacity of a single bin. Default to 1.0,\n                      assuming item and capacities are normalized.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score for each bin.\n        Higher scores indicate a more desirable bin. Bins where the item does not\n        fit receive a score of -infinity.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after hypothetical placement for fitting bins\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring component: Utilization after placing the item.\n    # A higher utilization means less remaining space, similar to Best Fit.\n    # Scores range from 0 (empty bin after placement) to 1 (full bin).\n    utilization_score = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n\n    # Define a small epsilon for floating point comparisons to handle near-zero values.\n    epsilon = 1e-9\n\n    # Strategic Bonus: Strongly reward perfect fits.\n    # Using np.isclose for robust floating point comparison to zero.\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=epsilon)\n    # Adding a substantial bonus (e.g., 1.0) makes perfect fits unequivocally\n    # the highest priority, pushing their score beyond the normal 0-1 range.\n    utilization_score[perfect_fit_mask] += 1.0\n\n    # Strategic Penalty: Slightly penalize very small, non-zero remaining capacities.\n    # These are deemed \"fragmented\" or potentially \"wasted\" space.\n    # The threshold for what constitutes a \"small fragment\" can be tuned,\n    # here set to 5% of the bin capacity.\n    fragment_threshold = 0.05 * BIN_CAPACITY\n    \n    # Identify bins that have a small, non-zero remainder.\n    # Ensure it's greater than epsilon to not penalize perfect fits.\n    fragment_mask = (remaining_after_fit > epsilon) & (remaining_after_fit < fragment_threshold)\n    \n    # Subtract a small penalty (e.g., 0.1) to make these bins slightly less\n    # attractive compared to those leaving a more useful or zero remainder.\n    utilization_score[fragment_mask] -= 0.1\n\n    # Assign the calculated scores to the bins where the item can fit.\n    scores[can_fit_mask] = utilization_score\n\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        default_priority_value: The base priority value used to initialize the priority array.\n                                In the original implementation, this was implicitly 0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # The original implementation implicitly returned an array filled with zeros.\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see that Heuristic 1st implements a classic \"Best Fit\" strategy, prioritizing bins that leave the smallest positive remaining capacity after an item is placed. Its score is simply the negative of the remaining capacity. Heuristic 2nd, while also aiming for \"Best Fit,\" attempts to refine this with an \"Adaptive Fullness Prioritization\" by adding a strong bonus for perfect fits and a minor penalty for creating small, fragmented remaining capacities. The ranking indicates that the simpler, pure Best Fit (1st) is superior to the more complex adaptive version (2nd). This suggests that the additional strategic considerations (perfect fit bonus, fragment penalty) and their specific parameter values (e.g., bonus=1.0, penalty=0.1, fragment_threshold=0.05) either didn't improve performance or actively hindered it in this context, perhaps by introducing undesirable trade-offs or being poorly tuned.\n\nComparing (1st, 4th, 7th, 8th, 10th) with (2nd, 3rd, 5th, 6th, 9th), we observe that the first group consists of identical \"Best Fit\" heuristics, while the second group comprises identical \"Adaptive Fullness Prioritization\" heuristics. The varying ranks within these identical groups (e.g., 1st vs 4th for Best Fit, or 2nd vs 3rd for Adaptive) strongly suggest that the evaluation process might involve elements of randomness (e.g., tie-breaking for equal scores) or multiple test runs, leading to slightly different performance outcomes for functionally identical code. However, the consistent pattern is that the \"Best Fit\" variants generally rank higher than the \"Adaptive Fullness Prioritization\" variants.\n\nComparing (1st) vs (11th), and indeed any of the top-ranked heuristics (Groups A & B) against the bottom-ranked heuristics (Groups C & D, Heuristics 11th-20th), there's a stark contrast. The top-ranked heuristics employ a specific, problem-aware strategy (minimizing remaining capacity, or trying to achieve perfect fits/avoid fragments). In contrast, Heuristics 11th through 20th are functionally identical, simply returning `np.zeros_like(bins_remain_cap)`. This trivial approach provides no intelligent prioritization, effectively making the bin selection arbitrary (e.g., first fit, or random if ties are broken randomly among zeros). The dramatically lower ranking of these \"zero priority\" heuristics clearly demonstrates the paramount importance of incorporating even a basic level of problem-specific optimization logic.\n\nOverall: The best-performing heuristics are those that implement a straightforward, well-understood space-optimization strategy like Best Fit. Attempts to introduce more nuanced, rule-based adjustments (like perfect fit bonuses or fragment penalties) can degrade performance if not precisely tuned or if the underlying assumptions don't perfectly align with the problem's dynamics. Heuristics that offer no strategic guidance perform significantly worse.\n- \nHere's a redefined 'Current self-reflection' focused on designing better heuristics, adhering to your constraints:\n\n*   **Keywords:** Adaptive Strategy, Search Dynamics, Parameter Learning, Pattern Exploitation.\n\n*   **Advice:** Heuristics should incorporate adaptive mechanisms that refine search strategies based on evolving solution landscapes. Prioritize computational efficiency by designing intelligent decision points that effectively prune the search space. Focus on exploiting complex patterns to achieve emergent, superior performance.\n\n*   **Avoid:** Discussions on basic code correctness or clarity, straightforward mathematical expressions, generic documentation advice, or stating that a heuristic simply uses data or differentiates options.\n\n*   **Explanation:** Effective heuristics transcend static rules by dynamically adjusting their approach to the problem's current state, enabling deeper exploration or quicker convergence. This strategic adaptation is key to unlocking advanced performance beyond simple rule sets.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}