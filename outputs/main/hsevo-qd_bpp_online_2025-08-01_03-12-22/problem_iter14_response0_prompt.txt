{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Global constant for bin capacity. In a real system, this might be a parameter\n# passed around or part of a Bin class. For simplicity, assuming a standard capacity.\nBIN_CAPACITY = 1.0 \n\n    \"\"\"\n    Returns a priority score for each bin, guiding the selection of where to place an item.\n    This heuristic embodies multi-factor, context-sensitive adaptive scoring with\n    an element of probabilistic selection to foster emergent packing patterns,\n    going beyond simple 'Best Fit'.\n\n    The priority score for a bin is a complex function considering:\n    1.  **Fundamental Tightness (Quantum Fit)**: A non-linear assessment of how\n        perfectly the item fits, emphasizing minimal remaining space with an\n        exponential decay. It serves as the primary driver for efficient space\n        utilization.\n    2.  **Desired State Affinity (Harmonic Fullness)**: Rewards bins that, after\n        the item is placed, achieve a dynamically adjusted 'ideal' fullness level.\n        This ideal shifts based on the incoming item's size, aiming to create\n        a balanced distribution of bin states rather than just emptying them or\n        always starting new ones.\n    3.  **Boundary Avoidance (Flux Equilibrium)**: Introduces penalties for\n        situations where placing the item would result in leaving either an\n        extremely small, potentially unusable fragment of space, or an\n        excessively large, underutilized space. This encourages more 'useful'\n        bin states.\n\n    The weights for these components are not fixed but are dynamically\n    calculated based on the incoming item's size and the current statistical\n    distribution of remaining capacities across all bins. This aims for a\n    self-adjusting behavior without explicit historical learning within this\n    function call itself. A small, normally distributed noise component is also\n    added to introduce a 'probabilistic selection' element, preventing rigid\n    deterministic ties and encouraging exploration of potentially similar-priority\n    bins, which can lead to more diverse and robust packing solutions over time.\n\n    Args:\n        item: Size of item to be added to the bin (float, assumed normalized to BIN_CAPACITY).\n        bins_remain_cap: NumPy array of remaining capacities for each bin.\n\n    Returns:\n        NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for the corresponding bin. Bins where the item\n        cannot fit receive a score of -np.inf to ensure they are never chosen.\n        The bin with the highest score is the preferred choice.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return early with all scores as -inf.\n    if not np.any(can_fit_mask):\n        return scores\n\n    fitting_bins_idx = np.where(can_fit_mask)[0]\n    current_bin_caps = bins_remain_cap[fitting_bins_idx]\n    remaining_after_fit = current_bin_caps - item\n\n    # --- Adaptive Parameters & Context-Sensitive Weighting ---\n    # These internal parameters dynamically adjust based on the current problem state.\n    # This reflects the \"adaptive scoring\" and \"high-dimensional tuning\" aspects,\n    # fostering emergent and context-sensitive behavior.\n    \n    item_relative_size = item / BIN_CAPACITY # Item size normalized to bin capacity\n    \n    # Calculate properties of the current bin capacity distribution for context\n    # Use clip to prevent division by zero if all capacities are identical\n    std_remain_cap = np.std(bins_remain_cap) \n    std_remain_cap_normalized = std_remain_cap / BIN_CAPACITY if BIN_CAPACITY > 0 else 0\n    avg_remain_cap_normalized = np.mean(bins_remain_cap) / BIN_CAPACITY if BIN_CAPACITY > 0 else 0\n    \n    # --- Dynamic Weights for Scoring Components ---\n    # These weights are functions of the current state, allowing for non-monotonic\n    # and context-sensitive behavior in the overall heuristic.\n\n    # Weight for Quantum Fit: Prioritize tight fit more for smaller items or if bins are generally full.\n    # This encourages finishing bins when items are small or space is scarce.\n    weight_quantum_fit = 1.0 + 0.5 * (1.0 - item_relative_size) * (1.0 - avg_remain_cap_normalized)\n\n    # Weight for Harmonic Fullness: Emphasize achieving a target fullness more for 'medium' items\n    # and when bin capacities are diverse (more options for optimal filling).\n    weight_harmonic_fullness = 0.8 + 0.7 * (1.0 - np.abs(item_relative_size - 0.5) * 2) * std_remain_cap_normalized\n    \n    # Weight for Flux Equilibrium: Penalize extreme remaining capacities more when bins are already polarized\n    # (high std_remain_cap) or when there's a strong need to balance bin states.\n    weight_flux_equilibrium = 0.6 + 0.8 * std_remain_cap_normalized\n\n    # === Scoring Components Calculation ===\n\n    # Component 1: Fundamental Tightness (Quantum Fit)\n    # A non-linear, exponentially decaying reward. A perfect fit (remainder 0) gives a score of 0.\n    # Larger remainders yield increasingly negative (worse) scores. This is a mutated version\n    # of the 'Best Fit' concept from priority_v1, making the penalty for wasted space more severe.\n    score_quantum_fit = -np.expm1(remaining_after_fit / BIN_CAPACITY) # exp(x)-1, gives 0 for x=0, negative for x>0\n\n    # Component 2: Desired State Affinity (Harmonic Fullness)\n    # Rewards bins that achieve a specific target fullness after the item is placed.\n    # The target fullness dynamically shifts: smaller items tend to be used to 'top off'\n    # bins (higher target fullness), while larger items might aim for less full bins.\n    dynamic_target_fullness = 0.65 + 0.3 * (1.0 - item_relative_size) # Ranges from 0.65 (large item) to 0.95 (small item)\n    \n    # New fullness of the bin after placing the item\n    new_fullness = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n    \n    # The variance scale (width of the Gaussian peak) also adapts. Wider for extreme item sizes,\n    # tighter for medium items (desiring more precise placement).\n    fullness_variance_scale = 0.05 + 0.15 * (np.abs(item_relative_size - 0.5) * 2) \n    \n    # Gaussian-like reward: peaks at dynamic_target_fullness, range [0, 1]\n    # Small epsilon in denominator for numerical stability if variance is extremely small.\n    epsilon_denominator = 1e-9 \n    score_harmonic_fullness = np.exp(-((new_fullness - dynamic_target_fullness)**2) / (2.0 * fullness_variance_scale**2 + epsilon_denominator))\n\n    # Component 3: Boundary Avoidance (Flux Equilibrium)\n    # Penalizes leaving very small, potentially unusable fragments, or very large unused spaces.\n    # This guides towards creating 'useful' remaining capacities and balancing bin utilization.\n    \n    # Adaptive thresholds for \"too small\" or \"too large\" remaining capacity\n    min_fragment_threshold = 0.02 * BIN_CAPACITY + 0.05 * item # Penalize leaving tiny unusable bits more\n    max_open_space_threshold = 0.9 * BIN_CAPACITY - 0.05 * item # Penalize wasting a bin for a small item\n    \n    # Sigmoid steepness for smooth, non-linear transitions in penalty\n    sigmoid_steepness = 100.0 / BIN_CAPACITY \n    \n    # Penalty for leaving very small remaining capacity (approaches 1 as remainder -> 0)\n    penalty_low_rem = 1.0 / (1.0 + np.exp((remaining_after_fit - min_fragment_threshold) * sigmoid_steepness))\n    # Penalty for leaving very large remaining capacity (approaches 1 as remainder -> BIN_CAPACITY-item)\n    penalty_high_rem = 1.0 / (1.0 + np.exp(-(remaining_after_fit - max_open_space_threshold) * sigmoid_steepness))\n    \n    # Combined flux penalty. This is a negative contribution to the total score.\n    score_flux_equilibrium = -(penalty_low_rem + penalty_high_rem)\n    \n    # === Combined Score Calculation ===\n    # A weighted sum of the components. The weights are dynamic, and a small\n    # amount of Gaussian noise is added for 'probabilistic selection' and\n    # to encourage exploration of similar-priority bins, fostering emergent behavior.\n    \n    exploration_noise_scale = 1e-4 * BIN_CAPACITY # Small noise relative to bin capacity, ensures exploration without dominating\n    \n    combined_scores_for_fitting_bins = (\n        weight_quantum_fit * score_quantum_fit +\n        weight_harmonic_fullness * score_harmonic_fullness +\n        weight_flux_equilibrium * score_flux_equilibrium +\n        np.random.normal(0, scale=exploration_noise_scale, size=len(fitting_bins_idx))\n    )\n\n    # Assign the calculated scores back to the original scores array,\n    # leaving -np.inf for bins where the item does not fit.\n    scores[fitting_bins_idx] = combined_scores_for_fitting_bins\n\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the top-ranked heuristic employs a \"Best Fit\" strategy with meticulously tuned parameters (`base_fit_priority`, `remaining_capacity_penalty_factor`, `min_allowed_priority`), actively seeking to minimize remaining bin capacity after item placement. In contrast, the worst-ranked heuristic (and others in the lowest tier, 15th-20th) effectively assigns uniform or zero priority to all fitting bins, resulting in no strategic choice. This stark performance difference highlights the profound impact of even a simple, well-defined greedy strategy over a non-discriminating approach, demonstrating the immense value of strategic prioritization.\n\nComparing (1st) vs (2nd) and (3rd) vs (4th), the heuristics are identical in their implementation and parameter values. Their consecutive ranking suggests either negligible performance differences (statistical ties) or an extremely robust performance for this specific \"Best Fit\" variant, making it consistently the best choice. Similarly, comparing (19th) vs (20th), these heuristics are functionally identical, both providing no strategic prioritization, confirming their equally poor performance and serving as a baseline for non-optimized bin assignment.\n\nComparing the simpler \"Best Fit\" heuristics (1st-4th) against the more complex, multi-factor adaptive heuristics (5th, 9th, 12th, 14th), the simpler approach surprisingly performed better. The adaptive heuristics incorporate dynamic weighting, non-linear scoring, and probabilistic elements to account for \"Quantum Fit,\" \"Harmonic Fullness,\" and \"Flux Equilibrium.\" Their lower ranking indicates that increased complexity, dynamic adjustments, and stochasticity do not guarantee superior performance and can even hinder it if not perfectly balanced or if they lead to less stable decision-making in the given problem context.\n\nFurthermore, heuristics like 6th, 7th, 8th, and 13th, which augment Best Fit with specific penalties for small fragments or bonuses for high utilization, are ranked in the mid-tier. This suggests that while these additions are conceptually sound, their specific implementation or parameterization might not have yielded sufficient gains to surpass the top-performing, simpler \"Best Fit\" with its optimized fixed parameters. The \"Proportional Fill\" strategy (10th, 11th) which prioritizes items filling a larger *proportion* of the remaining bin space, performed notably worse than Best Fit, indicating that minimizing absolute remaining space is more effective than maximizing relative fill in this scenario.\n\nOverall: The best performing heuristics are simple and consistently apply a 'Best Fit' strategy, indicating that simplicity with strong, fixed parameters can often outperform complex, dynamically-tuned strategies in this context. The worst performing heuristics are those that lack any specific prioritization logic.\n- \nHere's a redefined 'Current self-reflection' to design better heuristics, avoiding the pitfalls of 'Ineffective self-reflection':\n\n### Redefined 'Current self-reflection'\n1.  **Prioritize Core Intuition:** Design around fundamental, domain-specific principles (e.g., minimizing waste) that are inherently effective and interpretable.\n2.  **Controlled Refinement:** Incremental additions to heuristic logic must demonstrate measurable, consistent performance gains beyond simpler baselines.\n3.  **Focused Discrimination:** Heuristics must provide clear, non-trivial guidance, actively differentiating choices based on the primary optimization objective.\n4.  **Parameter Prudence:** Optimize a minimal set of well-understood parameters for robustness; complex, untuned schemes typically underperform.\n\n---\n\n*   **Keywords:** Core Principles, Simplicity, Validation, Focused Guidance, Parameter Tuning.\n*   **Advice:** Design heuristics grounded in clear core principles, rigorously validating any added complexity, and precisely tuning minimal parameters for robust optimization.\n*   **Avoid:** Generic implementation concerns, unvalidated multi-factor scoring, or vague problem data advice that doesn't translate to specific heuristic logic.\n*   **Explanation:** This approach prioritizes effective, interpretable heuristic logic over over-engineered or poorly justified solutions, leading to more reliable and performing systems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}