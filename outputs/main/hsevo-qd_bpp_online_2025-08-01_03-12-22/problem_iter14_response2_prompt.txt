{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n                bins_remain_cap: np.ndarray,\n                base_fit_priority: float = 40.67957013415283,\n                remaining_capacity_penalty_factor: float = 2.527225052325968,\n                min_allowed_priority: float = 0.5896372460377476,\n                non_fitting_priority: float = -8.783827459234484) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function calculates priority based on a \"Best Fit\" strategy:\n    bins that can accommodate the item and have less remaining capacity after\n    placement are prioritized higher.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        base_fit_priority: A base score given to any bin where the item can fit.\n                           This sets the initial priority before any penalties.\n        remaining_capacity_penalty_factor: A multiplier for the penalty applied\n                                           based on the remaining capacity after\n                                           the item is placed. A higher value\n                                           means larger remaining capacities\n                                           are penalized more heavily,\n                                           encouraging a \"best-fit\" approach.\n        min_allowed_priority: The minimum priority score a bin can receive if\n                              the item fits. This acts as a floor, preventing\n                              priorities from becoming too low or negative\n                              for valid placements.\n        non_fitting_priority: The priority score assigned to bins where the\n                              item does not fit. Typically 0 or a negative value.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities with the value for non-fitting bins\n    priorities = np.full_like(bins_remain_cap, fill_value=non_fitting_priority, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only process if there are bins where the item can fit\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity for bins where the item could be placed\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Calculate the priority for these bins\n        # The formula applies a penalty based on the remaining capacity:\n        # P = base_fit_priority - (penalty_factor * remaining_capacity)\n        # This encourages smaller remaining capacities (best fit).\n        calculated_priorities = base_fit_priority - (remaining_capacity_penalty_factor * remaining_after_placement)\n\n        # Ensure that the calculated priority does not fall below a minimum allowed value\n        calculated_priorities = np.maximum(calculated_priorities, min_allowed_priority)\n\n        # Assign the calculated priorities to the corresponding bins\n        priorities[can_fit_mask] = calculated_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority scores for each bin, based on an \"Adaptive Fullness Prioritization\" heuristic.\n\n    This heuristic mutates the standard 'Best Fit' by introducing strategic considerations\n    for space management beyond simple minimization of remaining capacity. It aims to:\n\n    1.  **Strongly Reward Perfect Fits**: A perfect fit (leaving 0 remaining capacity)\n        is the most efficient use of space, effectively \"closing\" a bin. This is\n        given a significant bonus.\n    2.  **Prioritize High Overall Utilization**: Similar to Best Fit, bins that\n        become very full after placing the item are generally preferred.\n    3.  **Penalize Fragmented Space**: A minor penalty is applied to bins that\n        are left with a very small, non-zero remaining capacity. Such 'fragments'\n        are often too small to be useful for subsequent items and can lead to\n        wasted space or increased bin count if many such bins accumulate.\n        This encourages the selection of bins that either achieve a perfect fit,\n        or leave a more 'useful' (larger) amount of remaining space, allowing for\n        greater flexibility for future items.\n\n    The goal is to not just minimize residual space, but to do so in a way\n    that minimizes \"unusable\" small fragments, promoting overall\n    packing efficiency and potentially reducing the number of bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        BIN_CAPACITY: The maximum capacity of a single bin. Default to 1.0,\n                      assuming item and capacities are normalized.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score for each bin.\n        Higher scores indicate a more desirable bin. Bins where the item does not\n        fit receive a score of -infinity.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after hypothetical placement for fitting bins\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring component: Utilization after placing the item.\n    # A higher utilization means less remaining space, similar to Best Fit.\n    # Scores range from 0 (empty bin after placement) to 1 (full bin).\n    utilization_score = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n\n    # Define a small epsilon for floating point comparisons to handle near-zero values.\n    epsilon = 1e-9\n\n    # Strategic Bonus: Strongly reward perfect fits.\n    # Using np.isclose for robust floating point comparison to zero.\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=epsilon)\n    # Adding a substantial bonus (e.g., 1.0) makes perfect fits unequivocally\n    # the highest priority, pushing their score beyond the normal 0-1 range.\n    utilization_score[perfect_fit_mask] += 1.0\n\n    # Strategic Penalty: Slightly penalize very small, non-zero remaining capacities.\n    # These are deemed \"fragmented\" or potentially \"wasted\" space.\n    # The threshold for what constitutes a \"small fragment\" can be tuned,\n    # here set to 5% of the bin capacity.\n    fragment_threshold = 0.05 * BIN_CAPACITY\n    \n    # Identify bins that have a small, non-zero remainder.\n    # Ensure it's greater than epsilon to not penalize perfect fits.\n    fragment_mask = (remaining_after_fit > epsilon) & (remaining_after_fit < fragment_threshold)\n    \n    # Subtract a small penalty (e.g., 0.1) to make these bins slightly less\n    # attractive compared to those leaving a more useful or zero remainder.\n    utilization_score[fragment_mask] -= 0.1\n\n    # Assign the calculated scores to the bins where the item can fit.\n    scores[can_fit_mask] = utilization_score\n\n    return scores\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the top-ranked heuristic employs a \"Best Fit\" strategy with meticulously tuned parameters (`base_fit_priority`, `remaining_capacity_penalty_factor`, `min_allowed_priority`), actively seeking to minimize remaining bin capacity after item placement. In contrast, the worst-ranked heuristic (and others in the lowest tier, 15th-20th) effectively assigns uniform or zero priority to all fitting bins, resulting in no strategic choice. This stark performance difference highlights the profound impact of even a simple, well-defined greedy strategy over a non-discriminating approach, demonstrating the immense value of strategic prioritization.\n\nComparing (1st) vs (2nd) and (3rd) vs (4th), the heuristics are identical in their implementation and parameter values. Their consecutive ranking suggests either negligible performance differences (statistical ties) or an extremely robust performance for this specific \"Best Fit\" variant, making it consistently the best choice. Similarly, comparing (19th) vs (20th), these heuristics are functionally identical, both providing no strategic prioritization, confirming their equally poor performance and serving as a baseline for non-optimized bin assignment.\n\nComparing the simpler \"Best Fit\" heuristics (1st-4th) against the more complex, multi-factor adaptive heuristics (5th, 9th, 12th, 14th), the simpler approach surprisingly performed better. The adaptive heuristics incorporate dynamic weighting, non-linear scoring, and probabilistic elements to account for \"Quantum Fit,\" \"Harmonic Fullness,\" and \"Flux Equilibrium.\" Their lower ranking indicates that increased complexity, dynamic adjustments, and stochasticity do not guarantee superior performance and can even hinder it if not perfectly balanced or if they lead to less stable decision-making in the given problem context.\n\nFurthermore, heuristics like 6th, 7th, 8th, and 13th, which augment Best Fit with specific penalties for small fragments or bonuses for high utilization, are ranked in the mid-tier. This suggests that while these additions are conceptually sound, their specific implementation or parameterization might not have yielded sufficient gains to surpass the top-performing, simpler \"Best Fit\" with its optimized fixed parameters. The \"Proportional Fill\" strategy (10th, 11th) which prioritizes items filling a larger *proportion* of the remaining bin space, performed notably worse than Best Fit, indicating that minimizing absolute remaining space is more effective than maximizing relative fill in this scenario.\n\nOverall: The best performing heuristics are simple and consistently apply a 'Best Fit' strategy, indicating that simplicity with strong, fixed parameters can often outperform complex, dynamically-tuned strategies in this context. The worst performing heuristics are those that lack any specific prioritization logic.\n- \nHere's a redefined 'Current self-reflection' to design better heuristics, avoiding the pitfalls of 'Ineffective self-reflection':\n\n### Redefined 'Current self-reflection'\n1.  **Prioritize Core Intuition:** Design around fundamental, domain-specific principles (e.g., minimizing waste) that are inherently effective and interpretable.\n2.  **Controlled Refinement:** Incremental additions to heuristic logic must demonstrate measurable, consistent performance gains beyond simpler baselines.\n3.  **Focused Discrimination:** Heuristics must provide clear, non-trivial guidance, actively differentiating choices based on the primary optimization objective.\n4.  **Parameter Prudence:** Optimize a minimal set of well-understood parameters for robustness; complex, untuned schemes typically underperform.\n\n---\n\n*   **Keywords:** Core Principles, Simplicity, Validation, Focused Guidance, Parameter Tuning.\n*   **Advice:** Design heuristics grounded in clear core principles, rigorously validating any added complexity, and precisely tuning minimal parameters for robust optimization.\n*   **Avoid:** Generic implementation concerns, unvalidated multi-factor scoring, or vague problem data advice that doesn't translate to specific heuristic logic.\n*   **Explanation:** This approach prioritizes effective, interpretable heuristic logic over over-engineered or poorly justified solutions, leading to more reliable and performing systems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}