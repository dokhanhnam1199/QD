{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority score for each bin. Combines Best Fit with a penalty for very small, non-zero remainders.\n    Prioritizes perfect fits, then larger useful gaps over tiny unusable ones, aiming to minimize wasted fragmented space.\n    \"\"\"\n    # Initialize scores for all bins to negative infinity, ensuring un-fittable bins are never chosen.\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if item were placed. This is the core Best Fit principle.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    # Initial score: negative of remaining capacity. Perfect fit (0 remainder) gets 0.0 (highest).\n    scores[can_fit_mask] = -remaining_after_fit\n\n    # Define thresholds for what constitutes a \"tiny\" and potentially \"unusable\" remainder.\n    # These values are empirical and might need tuning based on typical item/bin size distributions.\n    TINY_REMAINDER_THRESHOLD = 0.05  # e.g., 5% of an assumed normalized bin capacity (e.g., if max capacity is 1.0)\n    PENALTY_FOR_TINY_REMAINDER = 0.001 # A small penalty, ensuring a perfect fit (0 remainder) still receives the highest score.\n\n    # Identify valid bins that would result in a very small, non-zero remainder.\n    # A small epsilon (1e-9) is used to robustly check for non-zero floating-point values.\n    tiny_remainder_cond = (remaining_after_fit > 1e-9) & (remaining_after_fit < TINY_REMAINDER_THRESHOLD)\n\n    # Apply a penalty to the scores of bins that leave a tiny, potentially unusable remainder.\n    # This slightly discourages leaving highly fragmented space, encouraging either a perfect fit\n    # or a more substantial, potentially useful remaining gap for future, larger items.\n    scores[can_fit_mask][tiny_remainder_cond] -= PENALTY_FOR_TINY_REMAINDER\n\n    return scores\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), (3rd) vs (4th), (5th) vs (9th), (6th) vs (7th) vs (8th), (12th) vs (17th), (13th) vs (15th), and (14th) vs (16th), we observe that the source code for the functions is *identical* within these ranked groups. This implies that their differences in ranking from best to worst must stem entirely from the specific *default parameter values* chosen, highlighting the paramount importance of hyperparameter tuning for heuristic performance, rather than fundamental algorithmic differences between these identically coded functions.\n\nComparing (best - 1st) vs (worst - 20th), the 1st heuristic employs a sophisticated, multi-factor scoring combining a non-linear penalty for remaining capacity (`rem_cap_penalty_exponent`) and a bonus for the item's relative size to the bin's current available space (`relative_fill_bonus`). This aggressively favors tight fits and efficient bin utilization. In stark contrast, the 20th heuristic is trivial, returning `np.zeros_like`, effectively assigning equal priority to all bins and offering no strategic guidance for packing, resulting in arbitrary or First Fit-like behavior, explaining its lowest rank.\n\nComparing (19th - second worst) vs (20th - worst), even though Heuristic 19th is incomplete, its comments and initial structure reveal an intention to combine proportional fill, non-linear high utilization bonuses, and fragmentation penalties. This demonstrates a conceptual attempt at a complex, multi-criteria heuristic, which is inherently more sophisticated than the completely non-discriminatory, zero-priority approach of Heuristic 20th. This difference in design philosophy justifies 19th being ranked above 20th, despite its incompleteness.\n\nComparing (6th/7th/8th) vs (5th/9th), heuristics 6-8 build upon the \"Best Fit\" principle (similar to 5th/9th) by explicitly adding a `perfect_fit_bonus`. This strategic bonus incentivizes \"closing\" bins, which is crucial for efficient packing. Their higher ranking suggests that directly rewarding perfect fits improves performance over a purely linear remaining capacity penalty.\n\nComparing the top-ranked (1st-4th) with other sophisticated variants like (14th/16th) and (18th), the 1st-4th heuristic's specific combination of a non-linear `rem_cap_penalty_exponent` and a `relative_fill_bonus` appears to be a highly effective formulation. The more complex 18th heuristic, despite its advanced multi-factor, context-sensitive adaptive scoring with probabilistic elements, is ranked very low. This suggests that over-engineering, poorly chosen default parameters, or the introduction of noise can significantly hinder performance in greedy heuristics, where simplicity and well-calibrated core principles often prevail.\n\nOverall, the ranking suggests that a blend of \"Best Fit\" (minimizing remaining capacity, especially with non-linear penalties), rewarding high relative utilization, and potentially incentivizing bin closure (either directly or implicitly) are key components for high-performing heuristics. Simple, pure strategies (like 10th's basic Best Fit or 12th/17th's pure fill ratio) tend to perform less well than well-tuned hybrid approaches.\n- \nHere's the redefined self-reflection for designing better heuristics:\n\n*   **Keywords:** Intelligent Bin Closure, Tuned Simplicity, Validated Complexity, Strategic Scoring.\n*   **Advice:** Prioritize aggressive bin closure using a refined Best Fit, with non-linear capacity penalties and utilization bonuses. Embrace simple, well-tuned core logic. Introduce complexity only with rigorous performance validation. Ensure choices are strategically discriminating.\n*   **Avoid:** Generic software quality discussions as primary design drivers; untuned or unvalidated complexity; heuristics offering non-discriminating choices.\n*   **Explanation:** This redefinition provides actionable, performance-centric design principles, focusing on strategic decision-making and disciplined complexity to achieve superior heuristic outcomes.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}