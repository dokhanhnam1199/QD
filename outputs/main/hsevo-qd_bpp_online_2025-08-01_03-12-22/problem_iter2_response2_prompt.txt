{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    As Galileo sought the most elegant and efficient arrangement in the cosmos,\n    so too shall we organize our earthly possessions with wisdom. This heuristic,\n    akin to the \"Best Fit\" strategy, is designed to ensure a harmonious and\n    dense packing of our bins, minimizing wasted space.\n\n    It calculates a \"snugness\" score for each potential bin. Bins that allow for\n    a perfect fit of the item (leaving no capacity unused) are given the highest\n    priority, for they represent the most efficient utilization. Following these,\n    priority is given to bins that, after accommodating the item, would possess\n    the smallest positive remaining capacity. Bins where the item cannot be\n    accommodated at all are wisely relegated to the lowest possible priority,\n    preventing futile attempts.\n\n    This ensures that we always choose the existing vessel that most precisely\n    accommodates our new piece, akin to how each planet finds its precise,\n    efficient orbit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more preferred bin.\n    \"\"\"\n    # Initialize all priorities to a profoundly low value. This ensures that\n    # any bin incapable of holding the item is swiftly ignored, much like\n    # discarding faulty observational data.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate the potential remaining space in each bin should the item be placed there.\n    # We are only interested in those scenarios where this value is non-negative,\n    # signifying a feasible fit.\n    potential_remainders = bins_remain_cap - item\n\n    # Create a celestial mask, indicating only those bins where the item can truly fit.\n    fits_mask = potential_remainders >= 0\n\n    # For the bins within this valid celestial sphere (where the item fits),\n    # we assign a priority score. To achieve the \"best fit\" \u2013 minimizing the\n    # remaining space \u2013 we use the negative of the remaining space as our priority.\n    # This cunningly makes a perfect fit (remainder 0) the highest score (0),\n    # while increasingly larger remainders yield progressively lower (more negative) scores.\n    # Thus, selecting the maximum priority score will naturally choose the\n    # most snugly fitting bin.\n    priorities[fits_mask] = -potential_remainders[fits_mask]\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we see functionally identical \"Best Fit\" heuristics. The distinction lies in their narrative and variable naming. The first heuristic uses \"quantum particle\" analogy and calculates `potential_residual_space` before masking, while the second uses \"Rosalind Franklin\" and masks earlier for `remaining_space_after_fit`. Both correctly assign negative infinity for non-fitting bins and negative of residual space for fitting ones. The subtle ranking difference implies a preference for the \"quantum\" analogy or slight structural choice.\n\nComparing (2nd) vs (3rd), both are Best Fit. Heuristic 3rd introduces a \"Galileo\" analogy and variable `potential_remainders`. The code logic remains effectively identical to the best fit strategy, emphasizing that the narrative and stylistic choices in explanations play a significant role in their perceived quality.\n\nComparing (3rd) vs (4th), both implement Best Fit but use slightly different mathematical forms for the priority score: 3rd uses `-potential_remainders` (i.e., `-(bins_remain_cap - item)`) while 4th uses `item - bins_remain_cap`. These are mathematically equivalent for ranking. The 4th heuristic employs a \"Nikola Tesla\" analogy. The ranking suggests a slight preference for the `-(capacity - item)` expression for conceptual clarity or the \"Galileo\" analogy over \"Tesla\".\n\nComparing (4th) vs (5th), (5th) vs (6th), (6th) vs (7th), and (7th) vs (8th), we observe that Heuristics 1st, 5th, and 7th are identical, as are 4th, 6th, and 9th. Heuristic 8th is also functionally identical to 1st, 2nd, 3rd, 5th, 7th, and 10th. The differences are purely in the docstrings and comments' analogies (quantum, Rosalind Franklin, Galileo, Tesla, Einstein-like). The ranking among these functionally identical heuristics demonstrates subjective preference for certain narrative styles or scientific metaphors.\n\nComparing (9th) vs (10th), Heuristic 9th is a duplicate of the \"Tesla\" analogy (which uses `item - bins_remain_cap`), while 10th uses a straightforward \"Best Fit\" explanation and the `-(potential_remaining)` calculation. Heuristic 10th's higher rank suggests that a clear, concise, and direct explanation can be preferred over a more elaborate or less universally appreciated analogy, or that the `-(C-I)` form is subtly clearer.\n\nComparing (10th) vs (11th), this marks a significant functional difference. Heuristic 10th correctly implements the \"Best Fit\" logic. In stark contrast, Heuristic 11th returns `np.zeros_like(bins_remain_cap)`, effectively providing no meaningful prioritization and is thus a very poor heuristic for \"Best Fit\". It would either pick the first available bin or arbitrarily based on other tie-breaking rules, leading to suboptimal packing.\n\nComparing (11th) to (20th), all are identical and exhibit the same critical flaw of returning zeros, making them equally ineffective as \"Best Fit\" heuristics. Additionally, they include unused imports (`random`, `math`, `scipy`, `torch`), indicating poor code hygiene. This consistent functional failure and code clutter explain their lowest ranking.\n\nOverall: The ranking primarily distinguishes between functionally correct \"Best Fit\" heuristics (1-10) and functionally broken/meaningless ones (11-20). Among the correct ones, the ranking is determined by the quality, clarity, and perhaps subjective appeal of the docstrings, comments, and chosen analogies, with a slight preference for the `-(capacity - item)` calculation form.\n- \nHere's a redefined self-reflection for designing better heuristics:\n\n### Redefined Current Self-reflection\n\n*   **Keywords:** Empirical, Iterative, Aligned, Transparent.\n*   **Advice:** Rigorously test heuristics' empirical performance on diverse problem sets and edge cases. Ensure all design choices and mathematical logic directly align with the optimization objective. Document design rationale, benchmark results, and trade-offs to facilitate future improvements. Adopt an iterative design process, continuously refining based on performance analysis.\n*   **Avoid:** Designing heuristics without empirical validation or diverse test data; employing complex, opaque logic that obscures its alignment with the objective; neglecting to document design decisions, performance metrics, or identified limitations; treating heuristic design as a one-off, static process instead of iterative.\n*   **Explanation:** This self-reflection promotes a data-driven, scientific approach to heuristic design, ensuring not just correctness but also practical effectiveness, continuous improvement, and deep understanding across varied problem landscapes.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}