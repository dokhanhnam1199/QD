[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true,
    "tryHS": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Inspired by the principles of efficient space-time utilization, this\n    heuristic aims to find the 'tightest fit' for the item.\n    A tighter fit means less wasted space within a bin, optimizing the\n    overall density of packing and minimizing the necessity for new bins.\n\n    From my perspective, 'Best Fit' is analogous to minimizing the\n    residual 'field distortion' (unused capacity) in a local region\n    (a bin). We prioritize bins that, upon accommodating the item, leave\n    the smallest possible, yet positive, remaining capacity. A perfect fit,\n    leaving zero residual capacity, is the most efficient use of space\n    and is therefore given the highest priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are calculated as the negative of the remaining capacity after\n        the item is placed. Bins where the item does not fit receive a score\n        of -infinity to ensure they are never chosen.\n        The bin with the largest (least negative) score is the 'best fit'.\n    \"\"\"\n    # Initialize scores for all bins to negative infinity.\n    # This ensures that bins where the item cannot fit will never be selected.\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Determine which bins have sufficient remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For the bins where the item fits, calculate the remaining capacity\n    # if the item were to be placed there.\n    # We want to minimize this remaining capacity to achieve a 'tight fit'.\n    # Hence, we take the negative of this value:\n    # A smaller positive remainder (e.g., 0.1) becomes a larger negative score (-0.1).\n    # A perfect fit (0.0 remainder) becomes the highest score (0.0).\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    scores[can_fit_mask] = -remaining_after_fit\n\n    return scores",
    "response_id": 1,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority score for each bin. Combines Best Fit with a penalty for very small, non-zero remainders.\n    Prioritizes perfect fits, then larger useful gaps over tiny unusable ones, aiming to minimize wasted fragmented space.\n    \"\"\"\n    # Initialize scores for all bins to negative infinity, ensuring un-fittable bins are never chosen.\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if item were placed. This is the core Best Fit principle.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    # Initial score: negative of remaining capacity. Perfect fit (0 remainder) gets 0.0 (highest).\n    scores[can_fit_mask] = -remaining_after_fit\n\n    # Define thresholds for what constitutes a \"tiny\" and potentially \"unusable\" remainder.\n    # These values are empirical and might need tuning based on typical item/bin size distributions.\n    TINY_REMAINDER_THRESHOLD = 0.05  # e.g., 5% of an assumed normalized bin capacity (e.g., if max capacity is 1.0)\n    PENALTY_FOR_TINY_REMAINDER = 0.001 # A small penalty, ensuring a perfect fit (0 remainder) still receives the highest score.\n\n    # Identify valid bins that would result in a very small, non-zero remainder.\n    # A small epsilon (1e-9) is used to robustly check for non-zero floating-point values.\n    tiny_remainder_cond = (remaining_after_fit > 1e-9) & (remaining_after_fit < TINY_REMAINDER_THRESHOLD)\n\n    # Apply a penalty to the scores of bins that leave a tiny, potentially unusable remainder.\n    # This slightly discourages leaving highly fragmented space, encouraging either a perfect fit\n    # or a more substantial, potentially useful remaining gap for future, larger items.\n    scores[can_fit_mask][tiny_remainder_cond] -= PENALTY_FOR_TINY_REMAINDER\n\n    return scores",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, BIN_CAPACITY: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Returns priority scores for each bin, based on an \"Adaptive Fullness Prioritization\" heuristic.\n\n    This heuristic mutates the standard 'Best Fit' by introducing strategic considerations\n    for space management beyond simple minimization of remaining capacity. It aims to:\n\n    1.  **Strongly Reward Perfect Fits**: A perfect fit (leaving 0 remaining capacity)\n        is the most efficient use of space, effectively \"closing\" a bin. This is\n        given a significant bonus.\n    2.  **Prioritize High Overall Utilization**: Similar to Best Fit, bins that\n        become very full after placing the item are generally preferred.\n    3.  **Penalize Fragmented Space**: A minor penalty is applied to bins that\n        are left with a very small, non-zero remaining capacity. Such 'fragments'\n        are often too small to be useful for subsequent items and can lead to\n        wasted space or increased bin count if many such bins accumulate.\n        This encourages the selection of bins that either achieve a perfect fit,\n        or leave a more 'useful' (larger) amount of remaining space, allowing for\n        greater flexibility for future items.\n\n    The goal is to not just minimize residual space, but to do so in a way\n    that minimizes \"unusable\" small fragments, promoting overall\n    packing efficiency and potentially reducing the number of bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        BIN_CAPACITY: The maximum capacity of a single bin. Default to 1.0,\n                      assuming item and capacities are normalized.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score for each bin.\n        Higher scores indicate a more desirable bin. Bins where the item does not\n        fit receive a score of -infinity.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity after hypothetical placement for fitting bins\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Primary scoring component: Utilization after placing the item.\n    # A higher utilization means less remaining space, similar to Best Fit.\n    # Scores range from 0 (empty bin after placement) to 1 (full bin).\n    utilization_score = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n\n    # Define a small epsilon for floating point comparisons to handle near-zero values.\n    epsilon = 1e-9\n\n    # Strategic Bonus: Strongly reward perfect fits.\n    # Using np.isclose for robust floating point comparison to zero.\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=epsilon)\n    # Adding a substantial bonus (e.g., 1.0) makes perfect fits unequivocally\n    # the highest priority, pushing their score beyond the normal 0-1 range.\n    utilization_score[perfect_fit_mask] += 1.0\n\n    # Strategic Penalty: Slightly penalize very small, non-zero remaining capacities.\n    # These are deemed \"fragmented\" or potentially \"wasted\" space.\n    # The threshold for what constitutes a \"small fragment\" can be tuned,\n    # here set to 5% of the bin capacity.\n    fragment_threshold = 0.05 * BIN_CAPACITY\n    \n    # Identify bins that have a small, non-zero remainder.\n    # Ensure it's greater than epsilon to not penalize perfect fits.\n    fragment_mask = (remaining_after_fit > epsilon) & (remaining_after_fit < fragment_threshold)\n    \n    # Subtract a small penalty (e.g., 0.1) to make these bins slightly less\n    # attractive compared to those leaving a more useful or zero remainder.\n    utilization_score[fragment_mask] -= 0.1\n\n    # Assign the calculated scores to the bins where the item can fit.\n    scores[can_fit_mask] = utilization_score\n\n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, default_priority_value: float = 0.6879606910044531) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        default_priority_value: The base priority value used to initialize the priority array.\n                                In the original implementation, this was implicitly 0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # The original implementation implicitly returned an array filled with zeros.",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 1.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Focuses on maximizing bin utilization by prioritizing bins that offer the highest proportional fill.\n\n    This heuristic adaptively weighs each bin's suitability based on the item's\n    relative size to the bin's current capacity, exploiting the pattern\n    of high-density packing for efficient bin closure.\n    \"\"\"\n    # Initialize scores for all bins to negative infinity, ensuring bins where\n    # the item cannot fit are never selected.\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins with sufficient remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For valid bins, calculate the 'fill ratio': the proportion of the bin's\n    # current remaining capacity that the item would occupy.\n    # This adaptively prioritizes bins that, upon accommodating the item, achieve\n    # the highest relative utilization, leading to a more compact packing strategy.\n    # A higher ratio indicates a tighter fit relative to the available space,\n    # exploiting the pattern of efficient, high-density placement.\n    # Note: Assumes item > 0. If item is 0, division would be 0/X (score 0), which is handled.\n    # If item > 0 and bins_remain_cap is 0, can_fit_mask would be False.\n    scores[can_fit_mask] = item / bins_remain_cap[can_fit_mask]\n\n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response2.txt_stdout.txt",
    "code_path": "problem_iter9_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, BIN_CAPACITY: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Returns priority with which to add an item to each bin, evolving beyond simple\n    'Best Fit' by incorporating a strategic incentive for achieving high bin utilization.\n\n    This heuristic implements an 'Adaptive Strategy' by dynamically weighting\n    the decision based on the resulting fullness of a bin. It refines the\n    'Search Dynamics' by guiding the placement towards configurations that\n    efficiently consolidate items, thus leading to potentially fewer bins.\n    The function 'exploits patterns' by recognizing and significantly rewarding\n    scenarios where placing an item results in a very highly utilized bin,\n    thereby aiming to 'close' bins effectively. While direct 'Parameter Learning'\n    is outside this function's scope, the strategic parameters (e.g., UTIL_POWER,\n    BONUS_SCALING_FACTOR) are designed to be tunable to adapt to different\n    problem characteristics or distributions, allowing for emergent, superior performance.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        BIN_CAPACITY: The total capacity of a single bin. This is a crucial\n                      parameter for calculating utilization and is assumed to be uniform.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are a composite of the 'Best Fit' principle (minimizing remaining\n        capacity) and a non-linear bonus for bins that become highly utilized.\n        Bins where the item does not fit receive a score of -infinity.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the base Best Fit score for fitting bins.\n    # This is the negative of the remaining capacity after placing the item.\n    # A perfect fit (0 remaining) yields a base score of 0, which is the highest.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    best_fit_scores = -remaining_after_fit\n\n    # Calculate the new utilization of the bin if the item were placed there.\n    # Utilization is the filled portion of the bin relative to its total capacity.\n    new_utilization = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n\n    # --- Strategic Utilization Bonus ---\n    # This component drives the 'adaptive strategy' and 'pattern exploitation'.\n    # We apply a non-linear bonus to highly utilized bins.\n    # The higher the new_utilization, the disproportionately larger the bonus becomes.\n    # This strongly incentivizes configurations that make bins very full,\n    # thereby contributing to overall bin reduction.\n\n    # POWER determines how aggressively the bonus increases with utilization.\n    # A higher power (e.g., 3.0, 4.0) heavily penalizes lower utilization\n    # and significantly rewards extremely high utilization.\n    UTIL_POWER = 4.0\n\n    # SCALING_FACTOR controls the overall magnitude of the bonus relative to the base score.\n    # This can be tuned to balance the 'tight fit' vs. 'full bin' objectives.\n    BONUS_SCALING_FACTOR = 5.0\n\n    # CLIP_MIN_UTIL sets a threshold below which the utilization bonus starts to apply.\n    # This prevents giving a bonus for bins that are still mostly empty after placement,\n    # focusing the incentive on bins genuinely moving towards completion.\n    CLIP_MIN_UTIL = 0.5\n\n    # Calculate an 'effective utilization' for bonus calculation.\n    # This ensures that only the portion of utilization above CLIP_MIN_UTIL contributes,\n    # and no bonus is applied if utilization is below the threshold.\n    effective_utilization = np.maximum(0.0, new_utilization - CLIP_MIN_UTIL)\n\n    # The bonus is scaled by the item size to reflect that larger items\n    # contributing to a full bin have a more significant impact.\n    # The exponential application of `UTIL_POWER` makes this bonus highly\n    # sensitive to the final utilization, prioritizing near-full bins.\n    utilization_bonus = (effective_utilization**UTIL_POWER) * item * BONUS_SCALING_FACTOR\n\n    # Combine the Best Fit score with the strategic utilization bonus.\n    # The final score guides the 'search dynamics' towards a more globally\n    # optimal packing arrangement by balancing local tightness with overall\n    # bin consolidation.\n    scores[can_fit_mask] = best_fit_scores + utilization_bonus\n\n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines proportional fill, non-linear high utilization bonus, and fragmentation penalty.\n\n    Prioritizes bins by relative fill, rewards perfect/near-perfect fits,\n    and penalizes creating very small, often unusable, remaining space.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can actually fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return scores with -inf for all\n    if not np.any(can_fit_mask):\n        return scores\n\n    # Get relevant remaining capacities for eligible bins\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Components ---\n\n    # 1. Proportional Fill (from 'priority_v0'): Base score for how much of the bin's current\n    #    remaining capacity the item will occupy. Maximize this for a tighter fit.\n    base_fill_scores = item / valid_bins_remain_cap\n\n    # Calculate remaining capacity after placing the item\n    remaining_after_item = valid_bins_remain_cap - item\n\n    # Constants for tuning the bonus/penalty\n    K_UTIL_BONUS = 2.0  # Weight for the utilization bonus\n    K_FRAGMENT_PENALTY = 0.75 # Weight for the fragmentation penalty\n    EXP_SHARPNESS = 10.0 # Controls how sharply the utilization bonus drops with remaining space\n    EPSILON = 1e-6      # Small value to detect near-perfect fits or distinguish from zero\n    FRAGMENT_THRESHOLD = 0.1 # Max remaining capacity after item that is considered a 'fragment'\n\n    # 2. Non-linear Bonus for High Utilization / Near-Perfect Fits (from 3rd & 7th heuristics)\n    # This bonus heavily rewards bins that become very full or achieve a perfect fit.\n    # The exponential term ensures a strong, non-linear incentive as remaining_after_item approaches zero.\n    utilization_bonus = K_UTIL_BONUS * np.exp(-EXP_SHARPNESS * remaining_after_item)\n\n    # 3. Penalty for Fragmentation (from 6th & 7th heuristics)\n    # Penalizes leaving very small, non-zero amounts of space that might be unusable.\n    # This applies if the remaining space is positive but below a certain threshold.\n    fragment_penalty_mask = (remaining_after_item > EPSILON) & (remaining_after_item <= FRAGMENT_THRESHOLD)\n    fragment_penalties = np.zeros_like(base_fill_scores)\n    fragment_penalties[fragment_penalty_mask] = K_FRAGMENT_PENALTY\n\n    # Combine all components for the valid bins\n    total_valid_scores = base_fill_scores + utilization_bonus - fragment_penalties\n\n    # Assign calculated scores back to the original scores array\n    scores[can_fit_mask] = total_valid_scores\n\n    return scores",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\n# Global constant for bin capacity. In a real system, this might be a parameter\n# passed around or part of a Bin class. For simplicity, assuming a standard capacity.\nBIN_CAPACITY = 1.0 \n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns a priority score for each bin, guiding the selection of where to place an item.\n    This heuristic embodies multi-factor, context-sensitive adaptive scoring with\n    an element of probabilistic selection to foster emergent packing patterns,\n    going beyond simple 'Best Fit'.\n\n    The priority score for a bin is a complex function considering:\n    1.  **Fundamental Tightness (Quantum Fit)**: A non-linear assessment of how\n        perfectly the item fits, emphasizing minimal remaining space with an\n        exponential decay. It serves as the primary driver for efficient space\n        utilization.\n    2.  **Desired State Affinity (Harmonic Fullness)**: Rewards bins that, after\n        the item is placed, achieve a dynamically adjusted 'ideal' fullness level.\n        This ideal shifts based on the incoming item's size, aiming to create\n        a balanced distribution of bin states rather than just emptying them or\n        always starting new ones.\n    3.  **Boundary Avoidance (Flux Equilibrium)**: Introduces penalties for\n        situations where placing the item would result in leaving either an\n        extremely small, potentially unusable fragment of space, or an\n        excessively large, underutilized space. This encourages more 'useful'\n        bin states.\n\n    The weights for these components are not fixed but are dynamically\n    calculated based on the incoming item's size and the current statistical\n    distribution of remaining capacities across all bins. This aims for a\n    self-adjusting behavior without explicit historical learning within this\n    function call itself. A small, normally distributed noise component is also\n    added to introduce a 'probabilistic selection' element, preventing rigid\n    deterministic ties and encouraging exploration of potentially similar-priority\n    bins, which can lead to more diverse and robust packing solutions over time.\n\n    Args:\n        item: Size of item to be added to the bin (float, assumed normalized to BIN_CAPACITY).\n        bins_remain_cap: NumPy array of remaining capacities for each bin.\n\n    Returns:\n        NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for the corresponding bin. Bins where the item\n        cannot fit receive a score of -np.inf to ensure they are never chosen.\n        The bin with the highest score is the preferred choice.\n    \"\"\"\n    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bin can fit the item, return early with all scores as -inf.\n    if not np.any(can_fit_mask):\n        return scores\n\n    fitting_bins_idx = np.where(can_fit_mask)[0]\n    current_bin_caps = bins_remain_cap[fitting_bins_idx]\n    remaining_after_fit = current_bin_caps - item\n\n    # --- Adaptive Parameters & Context-Sensitive Weighting ---\n    # These internal parameters dynamically adjust based on the current problem state.\n    # This reflects the \"adaptive scoring\" and \"high-dimensional tuning\" aspects,\n    # fostering emergent and context-sensitive behavior.\n    \n    item_relative_size = item / BIN_CAPACITY # Item size normalized to bin capacity\n    \n    # Calculate properties of the current bin capacity distribution for context\n    # Use clip to prevent division by zero if all capacities are identical\n    std_remain_cap = np.std(bins_remain_cap) \n    std_remain_cap_normalized = std_remain_cap / BIN_CAPACITY if BIN_CAPACITY > 0 else 0\n    avg_remain_cap_normalized = np.mean(bins_remain_cap) / BIN_CAPACITY if BIN_CAPACITY > 0 else 0\n    \n    # --- Dynamic Weights for Scoring Components ---\n    # These weights are functions of the current state, allowing for non-monotonic\n    # and context-sensitive behavior in the overall heuristic.\n\n    # Weight for Quantum Fit: Prioritize tight fit more for smaller items or if bins are generally full.\n    # This encourages finishing bins when items are small or space is scarce.\n    weight_quantum_fit = 1.0 + 0.5 * (1.0 - item_relative_size) * (1.0 - avg_remain_cap_normalized)\n\n    # Weight for Harmonic Fullness: Emphasize achieving a target fullness more for 'medium' items\n    # and when bin capacities are diverse (more options for optimal filling).\n    weight_harmonic_fullness = 0.8 + 0.7 * (1.0 - np.abs(item_relative_size - 0.5) * 2) * std_remain_cap_normalized\n    \n    # Weight for Flux Equilibrium: Penalize extreme remaining capacities more when bins are already polarized\n    # (high std_remain_cap) or when there's a strong need to balance bin states.\n    weight_flux_equilibrium = 0.6 + 0.8 * std_remain_cap_normalized\n\n    # === Scoring Components Calculation ===\n\n    # Component 1: Fundamental Tightness (Quantum Fit)\n    # A non-linear, exponentially decaying reward. A perfect fit (remainder 0) gives a score of 0.\n    # Larger remainders yield increasingly negative (worse) scores. This is a mutated version\n    # of the 'Best Fit' concept from priority_v1, making the penalty for wasted space more severe.\n    score_quantum_fit = -np.expm1(remaining_after_fit / BIN_CAPACITY) # exp(x)-1, gives 0 for x=0, negative for x>0\n\n    # Component 2: Desired State Affinity (Harmonic Fullness)\n    # Rewards bins that achieve a specific target fullness after the item is placed.\n    # The target fullness dynamically shifts: smaller items tend to be used to 'top off'\n    # bins (higher target fullness), while larger items might aim for less full bins.\n    dynamic_target_fullness = 0.65 + 0.3 * (1.0 - item_relative_size) # Ranges from 0.65 (large item) to 0.95 (small item)\n    \n    # New fullness of the bin after placing the item\n    new_fullness = (BIN_CAPACITY - remaining_after_fit) / BIN_CAPACITY\n    \n    # The variance scale (width of the Gaussian peak) also adapts. Wider for extreme item sizes,\n    # tighter for medium items (desiring more precise placement).\n    fullness_variance_scale = 0.05 + 0.15 * (np.abs(item_relative_size - 0.5) * 2) \n    \n    # Gaussian-like reward: peaks at dynamic_target_fullness, range [0, 1]\n    # Small epsilon in denominator for numerical stability if variance is extremely small.\n    epsilon_denominator = 1e-9 \n    score_harmonic_fullness = np.exp(-((new_fullness - dynamic_target_fullness)**2) / (2.0 * fullness_variance_scale**2 + epsilon_denominator))\n\n    # Component 3: Boundary Avoidance (Flux Equilibrium)\n    # Penalizes leaving very small, potentially unusable fragments, or very large unused spaces.\n    # This guides towards creating 'useful' remaining capacities and balancing bin utilization.\n    \n    # Adaptive thresholds for \"too small\" or \"too large\" remaining capacity\n    min_fragment_threshold = 0.02 * BIN_CAPACITY + 0.05 * item # Penalize leaving tiny unusable bits more\n    max_open_space_threshold = 0.9 * BIN_CAPACITY - 0.05 * item # Penalize wasting a bin for a small item\n    \n    # Sigmoid steepness for smooth, non-linear transitions in penalty\n    sigmoid_steepness = 100.0 / BIN_CAPACITY \n    \n    # Penalty for leaving very small remaining capacity (approaches 1 as remainder -> 0)\n    penalty_low_rem = 1.0 / (1.0 + np.exp((remaining_after_fit - min_fragment_threshold) * sigmoid_steepness))\n    # Penalty for leaving very large remaining capacity (approaches 1 as remainder -> BIN_CAPACITY-item)\n    penalty_high_rem = 1.0 / (1.0 + np.exp(-(remaining_after_fit - max_open_space_threshold) * sigmoid_steepness))\n    \n    # Combined flux penalty. This is a negative contribution to the total score.\n    score_flux_equilibrium = -(penalty_low_rem + penalty_high_rem)\n    \n    # === Combined Score Calculation ===\n    # A weighted sum of the components. The weights are dynamic, and a small\n    # amount of Gaussian noise is added for 'probabilistic selection' and\n    # to encourage exploration of similar-priority bins, fostering emergent behavior.\n    \n    exploration_noise_scale = 1e-4 * BIN_CAPACITY # Small noise relative to bin capacity, ensures exploration without dominating\n    \n    combined_scores_for_fitting_bins = (\n        weight_quantum_fit * score_quantum_fit +\n        weight_harmonic_fullness * score_harmonic_fullness +\n        weight_flux_equilibrium * score_flux_equilibrium +\n        np.random.normal(0, scale=exploration_noise_scale, size=len(fitting_bins_idx))\n    )\n\n    # Assign the calculated scores back to the original scores array,\n    # leaving -np.inf for bins where the item does not fit.\n    scores[fitting_bins_idx] = combined_scores_for_fitting_bins\n\n    return scores",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_hs2.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                base_fit_priority: float = 40.67957013415283,\n                remaining_capacity_penalty_factor: float = 2.527225052325968,\n                min_allowed_priority: float = 0.5896372460377476,\n                non_fitting_priority: float = -8.783827459234484) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This function calculates priority based on a \"Best Fit\" strategy:\n    bins that can accommodate the item and have less remaining capacity after\n    placement are prioritized higher.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        base_fit_priority: A base score given to any bin where the item can fit.\n                           This sets the initial priority before any penalties.\n        remaining_capacity_penalty_factor: A multiplier for the penalty applied\n                                           based on the remaining capacity after\n                                           the item is placed. A higher value\n                                           means larger remaining capacities\n                                           are penalized more heavily,\n                                           encouraging a \"best-fit\" approach.\n        min_allowed_priority: The minimum priority score a bin can receive if\n                              the item fits. This acts as a floor, preventing\n                              priorities from becoming too low or negative\n                              for valid placements.\n        non_fitting_priority: The priority score assigned to bins where the\n                              item does not fit. Typically 0 or a negative value.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities with the value for non-fitting bins\n    priorities = np.full_like(bins_remain_cap, fill_value=non_fitting_priority, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only process if there are bins where the item can fit\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity for bins where the item could be placed\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Calculate the priority for these bins\n        # The formula applies a penalty based on the remaining capacity:\n        # P = base_fit_priority - (penalty_factor * remaining_capacity)\n        # This encourages smaller remaining capacities (best fit).\n        calculated_priorities = base_fit_priority - (remaining_capacity_penalty_factor * remaining_after_placement)\n\n        # Ensure that the calculated priority does not fall below a minimum allowed value\n        calculated_priorities = np.maximum(calculated_priorities, min_allowed_priority)\n\n        # Assign the calculated priorities to the corresponding bins\n        priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.028719585161557,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                base_fit_priority: float = 40.67957013415283,\n                remaining_capacity_penalty_factor: float = 2.527225052325968,\n                min_allowed_priority: float = 0.5896372460377476,\n                non_fitting_priority: float = -8.783827459234484,\n                perfect_fit_bonus: float = 10.0,\n                perfect_fit_epsilon: float = 1e-9) -> np.ndarray:\n    \"\"\"Combines Best Fit with a strong bonus for perfect item placement.\n    Prioritizes minimizing remaining bin capacity while significantly rewarding bins\n    that are completely filled by the item, improving space utilization.\n    \"\"\"\n    # Initialize all priorities with the value for non-fitting bins\n    priorities = np.full_like(bins_remain_cap, fill_value=non_fitting_priority, dtype=float)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only process if there are bins where the item can fit\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity for bins where the item could be placed\n        remaining_after_placement = bins_remain_cap[can_fit_mask] - item\n\n        # Calculate the base priority using a \"Best Fit\" approach\n        # This penalizes larger remaining capacities, encouraging a tight fit.\n        calculated_priorities = base_fit_priority - (remaining_capacity_penalty_factor * remaining_after_placement)\n\n        # Apply a significant bonus for \"perfect fits\" (remaining capacity is nearly zero).\n        # This incentivizes closing bins efficiently.\n        perfect_fit_mask_local = np.isclose(remaining_after_placement, 0.0, atol=perfect_fit_epsilon)\n        calculated_priorities[perfect_fit_mask_local] += perfect_fit_bonus\n\n        # Ensure that the calculated priority does not fall below a minimum allowed value\n        calculated_priorities = np.maximum(calculated_priorities, min_allowed_priority)\n\n        # Assign the calculated priorities to the corresponding bins\n        priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.028719585161557,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]