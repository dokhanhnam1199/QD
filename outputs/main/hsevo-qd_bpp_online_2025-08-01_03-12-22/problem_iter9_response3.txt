```python
import numpy as np

# A conceptual BIN_CAPACITY for normalization.
# In typical online Bin Packing Problem (BPP) literature, item sizes
# and bin capacities are often normalized, with BIN_CAPACITY being 1.0.
# If your problem context uses a different, fixed bin capacity (e.g., 100),
# this value should be adjusted accordingly. For this heuristic,
# it's crucial for understanding the 'relative size' of an item.
BIN_CAPACITY = 1.0 

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which to add an item to each bin, using an adaptive blending heuristic.

    This heuristic transcends a static 'Best Fit' by dynamically blending 'Best Fit'
    (minimizing post-placement remainder) and a 'Worst Fit' (maximizing post-placement
    remainder) principle. The blending ratio is adaptively determined by the item's size,
    thereby refining the search dynamics for suitable bins.

    For **larger items**, the heuristic aggressively favors 'Best Fit'. This strategic
    choice minimizes bin fragmentation and delays the necessity of opening new bins,
    as large items are critical for efficient overall packing density. This exploits
    the pattern that large items require precise placement to prevent significant
    unusable gaps.

    For **smaller items**, the heuristic shifts its emphasis, incorporating more of a
    'Worst Fit' bias (or, more precisely, a reduced emphasis on the immediate tightest fit).
    This adaptive strategy aims to preserve larger contiguous spaces in other bins for
    subsequent, potentially larger items. Rather than filling the smallest possible
    gap with a small item and risking the creation of many fragmented, hard-to-utilize
    capacities, this approach encourages placing small items where they least
    disrupt the availability of large continuous spaces. This represents an
    exploitation of the pattern that small items can easily fragment bins.

    The 'adaptive' mechanism is implemented through a dynamically adjusted weighting factor,
    which fluidly alters the strategic objective of the bin selection process based on
    the instantaneous item characteristics, thereby enhancing the overall search dynamics.

    Args:
        item: Size of item to be added to the bin. Assumed to be normalized
              relative to `BIN_CAPACITY` (e.g., in [0, 1] if BIN_CAPACITY=1.0).
        bins_remain_cap: Array of remaining capacities for each bin.
                         Assumed to be in the same scale as `item` and `BIN_CAPACITY`.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores are calculated as a weighted sum of Best Fit and Worst Fit components.
        Bins where the item does not fit receive a score of -infinity to ensure
        they are never chosen.
    """
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)
    can_fit_mask = bins_remain_cap >= item

    # Calculate remaining capacity after placement for bins where the item fits.
    remaining_after_fit = bins_remain_cap[can_fit_mask] - item

    # Normalize item size for adaptive weighting. This allows the heuristic to
    # gauge an item's 'largeness' or 'smallness' relative to a bin's capacity.
    # Clip item at a small positive value to prevent division by zero for an
    # item size of 0, though item sizes are typically positive in BPP.
    item_norm = item / BIN_CAPACITY
    
    # Adaptive Weighting based on item_norm:
    # As `item_norm` increases (item is larger), the weight for Best Fit increases,
    # and the weight for Worst Fit decreases. This creates a smooth transition
    # in the heuristic's preference.
    # The range [0.2, 1.0] for w_best_fit ensures that:
    # - Even for the smallest items (item_norm=0), Best Fit still has 20% influence,
    #   preventing chaotic behavior while allowing a strong Worst Fit bias.
    # - For items filling the entire bin (item_norm=1), it becomes pure Best Fit.
    w_best_fit = (item_norm * 0.8) + 0.2  # Scales from 0.2 (for item_norm=0) to 1.0 (for item_norm=1)
    w_worst_fit = 1.0 - w_best_fit         # Scales from 0.8 (for item_norm=0) to 0.0 (for item_norm=1)

    # Calculate Best Fit component score:
    # A smaller 'remaining_after_fit' results in a higher (less negative) score.
    score_best_fit = -remaining_after_fit

    # Calculate Worst Fit component score:
    # A larger 'remaining_after_fit' results in a higher (more positive) score.
    # This promotes keeping large gaps available for future large items.
    score_worst_fit = remaining_after_fit

    # Combine scores using the adaptive weights.
    # The sum `w_best_fit + w_worst_fit` always equals 1.0, ensuring a normalized influence.
    scores[can_fit_mask] = w_best_fit * score_best_fit + w_worst_fit * score_worst_fit
    
    return scores
```
