```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray, bin_size: float = 1.0) -> np.ndarray:
    """Combines adaptive fill ratio, best fit, frag. penalty & reuse, exploration with dynamic adjustments and lookahead."""
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    num_bins = len(bins_remain_cap)

    # --- 1. Adaptive Weights (Item Size & Bin Availability) ---
    item_size_factor = min(0.75, item)  # Capping item size factor
    bin_availability_factor = min(1.27, np.sum(bins_remain_cap >= item) / (num_bins + 1e-6))  # Smooth availability factor

    fill_ratio_weight = 0.07 + 0.1 * item_size_factor
    best_fit_weight = 2.8 + 0.7 * bin_availability_factor
    fragmentation_penalty_weight = 1.5 + 0.8 * item_size_factor
    reuse_weight = 0.8 + 0.5 * bin_availability_factor

    # --- 2. Fill Ratio (Eligible Bins) ---
    eligible_bins = bins_remain_cap >= item
    if np.any(eligible_bins):
        fill_ratio = item / (bins_remain_cap[eligible_bins] + 0.001)  # Avoid division by zero
        priorities[eligible_bins] += fill_ratio_weight * fill_ratio

        # Best Fit - Exponential decay
        remaining_space = bins_remain_cap[eligible_bins] - item
        priorities[eligible_bins] += best_fit_weight * np.exp(-2.7 * remaining_space)

    # --- 3. Fragmentation Penalty (Dynamic Threshold) ---
    remaining_space_all = bins_remain_cap - item
    fragmentation_threshold = item * (0.45 + 0.04 * item_size_factor) * (0.2 / np.sqrt(num_bins + 1e-6))
    fragmented_bins = (remaining_space_all > 0) & (remaining_space_all <= fragmentation_threshold)

    # Scale penalty based on fill level and item size
    bin_fill_level = (bin_size - bins_remain_cap[fragmented_bins]) / bin_size
    fragmentation_penalty_scaling = bin_fill_level * item_size_factor
    priorities[fragmented_bins] -= fragmentation_penalty_weight * fragmentation_penalty_scaling

    # --- 4. Almost Empty Reuse ---
    average_fill = np.mean(1 - bins_remain_cap / bin_size)
    almost_empty_threshold = 0.93 - (0.2 * average_fill)
    almost_empty = bins_remain_cap > almost_empty_threshold
    if np.any(almost_empty):
        almost_empty_bonus = 0.09 + average_fill * 0.6
        priorities[almost_empty] += reuse_weight * almost_empty_bonus

    # --- 5. Too Full Penalty ---
    too_full_bins = (bins_remain_cap < item) & (bins_remain_cap > (item - 0.42))
    priorities[too_full_bins] -= 0.08

    # --- 6. Exploration (Scaled Randomness) ---
    exploration_factor = 0.026 * item
    priorities += np.random.uniform(-exploration_factor, exploration_factor, size=num_bins)

    # --- 7. Encourage Even Utilization (with stronger weighting) ---
    bin_utilization = 1 - bins_remain_cap / bin_size
    utilization_std = np.std(bin_utilization)
    priorities += (1 - utilization_std) * 0.2  # Doubled the bonus

    # --- 8. Lookahead (Simulated Placement) ---
    # Penalize bins that, after placing the current item, would become difficult to fill later.
    simulated_remaining_caps = bins_remain_cap - item
    simulated_remaining_caps[simulated_remaining_caps < 0] = -1 # Mark that the item can't be added

    small_space_penalty_threshold = 0.15 #bins with remaining capacity less than this gets penalized
    difficult_to_fill = (simulated_remaining_caps > 0) & (simulated_remaining_caps <= small_space_penalty_threshold)

    # Scale penalty by how much space is wasted and the size of the item
    waste_amount = small_space_penalty_threshold - simulated_remaining_caps[difficult_to_fill]
    waste_penalty = waste_amount/ small_space_penalty_threshold

    priorities[difficult_to_fill] -= 0.05 * waste_penalty * item_size_factor

    return priorities
```
