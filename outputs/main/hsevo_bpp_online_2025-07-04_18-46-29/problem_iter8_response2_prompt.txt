{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # 1. Capacity Utilization: Encourage filling bins effectively\n    potential_bins = bins_remain_cap >= item\n    if np.any(potential_bins):\n        utilization_ratio = (bins_remain_cap[potential_bins] - item) / bins_remain_cap[potential_bins]\n        priorities[potential_bins] += (1 - utilization_ratio)**2  # Higher priority for better utilization\n\n    # 2. Smallest Remaining Capacity: First-Fit Decreasing concept\n    eligible_bins = bins_remain_cap >= item\n    if np.any(eligible_bins):\n        min_remaining = np.min(bins_remain_cap[eligible_bins])\n        priorities[bins_remain_cap == min_remaining] += 2  # High priority\n\n    # 3. Fragmentation Penalty: Penalize bins that might lead to fragmentation\n    fragmentation_risk = (bins_remain_cap - item)  # Remaining space after item placement\n    priorities[potential_bins] -= np.exp(-5 * fragmentation_risk[potential_bins])  # Exponential penalty\n    \n    # 4. Balancing Factor:  Prioritize bins that are neither too full nor too empty\n    bin_fullness = 1 - (bins_remain_cap / np.max(bins_remain_cap)) #scale it\n    priorities[potential_bins] += np.exp(-10 * np.abs(bin_fullness[potential_bins] - 0.5)) #middle range priority\n    \n    # 5. Item Size Consideration: Adapt based on item size\n    priorities[potential_bins] += item / np.max(bins_remain_cap)   # Larger items get more priority\n    \n    # 6. Sigmoid Smoothing:  Smooth transitions for a more gradual effect\n    priorities = 1 / (1 + np.exp(-priorities))\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Objective function: combination of fill-level, capacity, and adaptive penalty\n\n    # 1. Fill-Level Preference (Sigmoid to smooth the preference curve)\n    potential_bins = bins_remain_cap >= item\n    if np.any(potential_bins):\n        fill_ratio = (bins_remain_cap[potential_bins] - item) / bins_remain_cap[potential_bins]\n        priorities[potential_bins] += 1 / (1 + np.exp(-10 * (fill_ratio - 0.5))) # Sigmoid centered around 0.5 fill ratio\n\n    # 2. Capacity Consideration (Prioritize bins that are not too full or too empty after insertion)\n    eligible_bins = bins_remain_cap >= item\n    if np.any(eligible_bins):\n        remaining_after_fit = bins_remain_cap[eligible_bins] - item\n        capacity_score = np.exp(-0.5 * (remaining_after_fit - np.mean(remaining_after_fit))**2 / np.std(remaining_after_fit)**2) if np.std(remaining_after_fit) > 0 else np.ones_like(remaining_after_fit)  # Gaussian-like preference\n        priorities[eligible_bins] += capacity_score\n        \n\n    # 3. Adaptive Penalty (Dynamically adjust penalty based on how full the bins are becoming)\n    bin_utilization = 1 - bins_remain_cap / np.max(bins_remain_cap)  # or 1 - bins_remain_cap if bins have a fixed size\n    penalty = np.exp(5 * (bin_utilization - 0.9)) # High penalty as bins become >90% full\n    priorities -= penalty\n    \n    # 4. Encourage bins with space slightly greater than item\n    near_fit = (bins_remain_cap >= item) & (bins_remain_cap <= item * 1.2)\n    if np.any(near_fit):\n        priorities[near_fit] += 0.75\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a simple combination of filling bins closer to full, prioritizing bins with the smallest remaining capacity and adding a non-linearity component and penalizing near-full bins. While the worst balances waste, fullness, and avoids near-full bins by using a smoothed waste penalty and adaptive bonus for good fit.\nComparing (2nd) vs (19th), we see (2nd) considers capacity utilization, smallest remaining capacity, fragmentation penalty, bin balancing, and item size considerations and applies a sigmoid smoothing. In contrast, (19th) balances waste, fullness and avoids near-full bins by using a smoothed waste penalty and adaptive bonus for good fit.\nComparing (1st) vs (2nd), we see the best heuristic is simpler and more directly targets filling bins, while the second heuristic attempts a more complex balancing act with more features.\nComparing (3rd) vs (4th), we see (3rd) combines best fit, target fullness and adaptive penalty with a small randomness, while (4th) uses tuned parameters to balances waste, fullness, and fit.\nComparing (19th) vs (20th), we see they are identical. Overall: Simplicity and direct targeting of fill optimization might be more effective than complex, multi-faceted approaches. Adaptive penalties and bonuses seem useful. Explicit randomization may help.\n- \nOkay, let's redefine \"Current self-reflection\" into something more effective for designing better heuristics, especially in bin packing or similar optimization problems.\n\nHere's the breakdown:\n\n*   **Keywords:** Incremental refinement, objective function alignment, adaptive mechanisms, exploration-exploitation balance.\n\n*   **Advice:** Start with a simple heuristic aligned with the primary objective. Systematically introduce complexity, rigorously evaluating the impact of each addition on the *overall* objective function. Focus on designing adaptive bonuses/penalties *linked directly* to improvements (or deteriorations) in the objective function's value.\n\n*   **Avoid:** Single-factor focus, arbitrary complexity, untuned parameters, and isolated evaluations.\n\n*   **Explanation:** Effective self-reflection for heuristic design means consciously connecting new features to the overarching goal. Don't just add penalties or complexity; understand *why* they improve performance relative to the defined objective function.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}