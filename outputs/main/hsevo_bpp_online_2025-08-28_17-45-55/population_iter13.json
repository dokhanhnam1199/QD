[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization with a best-bin boost for stable priority.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n\n    if std_waste > 0:\n        normalized_waste = (waste - mean_waste) / std_waste\n    else:\n        normalized_waste = np.zeros_like(waste)\n\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1 / (1 + np.exp(-normalized_waste))\n\n    best_bin_index = np.argmin(waste)\n    priorities[valid_bins[best_bin_index]] = np.max(priorities[valid_bins])\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 108.04820237218406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with sigmoid scaling and a best-bin boost, normalizing waste.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if np.sum(possible_bins) == 0:\n        return np.zeros_like(bins_remain_cap)\n    waste = bins_remain_cap[possible_bins] - item\n    priorities = 1.0 / (waste + 1e-9)\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n    if std_waste > 0:\n        normalized_waste = (waste - mean_waste) / std_waste\n    else:\n        normalized_waste = np.zeros_like(waste)\n    sigmoid_scaled_waste = 1.0 / (1.0 + np.exp(-normalized_waste * 2.0))\n    priorities = sigmoid_scaled_waste\n    priorities[~possible_bins] = 0.0\n    best_bin_index = np.argmin(waste)\n    priorities[possible_bins][best_bin_index] += 1.0\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 192.29419688230416,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization, sigmoid scaling, and a best-bin boost,\n    with robust handling of edge cases and numerical stability.\"\"\"\n    sigmoid_scale = 5.0\n    best_bin_boost = 1.0\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        mean_waste = np.mean(waste)\n        std_waste = np.std(waste)\n        if std_waste == 0:\n            normalized_waste = np.zeros_like(waste)\n        else:\n            normalized_waste = (waste - mean_waste) / std_waste\n        sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n        priorities[valid_bins] = sigmoid_scaled_waste\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    return priorities",
    "response_id": 2,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 131.18329672565338,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with sigmoid scaling and best-fit boost.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if not np.any(possible_bins):\n        return np.zeros_like(bins_remain_cap)\n    waste = bins_remain_cap[possible_bins] - item\n    relative_waste = waste / (np.max(waste) + 1e-6)\n    sigmoid_scaled_waste = 1 / (1 + np.exp(-5.0 * relative_waste))\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[possible_bins] = sigmoid_scaled_waste\n    best_bin_index = np.argmin(waste)\n    priorities[possible_bins[best_bin_index]] += 2.0\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 14, in priority_v2\n    normalized_waste = (waste - mean_waste) / std_waste\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n12\n2\n126.65973476959637\n"
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit with sigmoid scaling and normalization to enhance stability.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n    waste = bins_remain_cap[valid_bins] - item\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n    if std_waste == 0:\n        normalized_waste = np.zeros_like(waste)\n    else:\n        normalized_waste = (waste - mean_waste) / std_waste\n    priorities = np.zeros_like(bins_remain_cap)\n    sigmoid_scale = 1.0\n    best_bin_boost = 2.0\n    priorities[valid_bins] = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n    best_bin_index = np.argmin(waste)\n    priorities[np.where(valid_bins)[0][best_bin_index]] = best_bin_boost\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 126.65973476959637,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with sigmoid scaling and normalization for stability.\"\"\"\n    sigmoid_scale = 1.0\n    best_bin_boost = 10.0\n    possible_bins = bins_remain_cap >= item\n    if np.sum(possible_bins) == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[possible_bins] - item\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n    if std_waste == 0:\n      std_waste = 1e-6\n    normalized_waste = (waste - mean_waste) / std_waste\n    sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[possible_bins] = sigmoid_scaled_waste\n    best_bin_index = np.argmin(waste)\n    priorities[possible_bins[best_bin_index]] += best_bin_boost\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 21, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n18\n3\n146.71880002307702\n"
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit with sigmoid scaling and normalization using mean and std.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if not np.any(possible_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[possible_bins] - item\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n    if std_waste == 0:\n        std_waste = 1e-6\n    normalized_waste = (waste - mean_waste) / std_waste\n    sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * 2))\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[possible_bins] = sigmoid_scaled_waste\n    best_bin_index = np.argmin(waste)\n    priorities[possible_bins[best_bin_index]] += 1.0\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 19, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n16\n3\n142.13383752235586\n"
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with sigmoid scaling and a best-bin boost,\n    prioritizing numerical stability and robust bin selection.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if not np.any(possible_bins):\n        return np.zeros_like(bins_remain_cap)\n    waste = bins_remain_cap[possible_bins] - item\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[possible_bins] = 1.0 / (1.0 + np.exp(-waste / (np.std(waste) + 1e-6)))\n    best_bin_index = np.argmin(waste)\n    boost_factor = 2.0\n    priorities[possible_bins[best_bin_index]] += boost_factor\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 14, in priority_v2\n    normalized_waste = np.zeros_like(waste)\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n11\n2\n108.04820237218406\n"
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best-fit with sigmoid scaling and normalization for robustness.\"\"\"\n    sigmoid_scale = 1.0\n    best_bin_boost = 10.0\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        mean_waste = np.mean(waste)\n        std_waste = np.std(waste)\n        if std_waste == 0:\n            normalized_waste = np.zeros_like(waste)\n        else:\n            normalized_waste = (waste - mean_waste) / std_waste\n        sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n        priorities[valid_bins] = sigmoid_scaled_waste\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 131.18329672565338,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization with a best-bin boost for stable priority.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    mean_waste = np.mean(waste)\n    std_waste = np.std(waste)\n\n    if std_waste > 0:\n        normalized_waste = (waste - mean_waste) / std_waste\n    else:\n        normalized_waste = np.zeros_like(waste)\n\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1 / (1 + np.exp(-normalized_waste))\n\n    best_bin_index = np.argmin(waste)\n    priorities[valid_bins[best_bin_index]] = np.max(priorities[valid_bins])\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 108.04820237218406,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines best-fit, worst-fit, and a capacity-based score with adaptive weighting.\n    Uses a statistical approach to bin selection, aiming for robustness.\n    \"\"\"\n    sigmoid_scale = 5.0\n    best_bin_boost = 1.0\n    worst_fit_penalty = 0.5\n\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    best_bin_index = np.argmin(waste)\n    worst_bin_index = np.argmax(waste)\n\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if np.any(valid_bins):\n        capacities = bins_remain_cap[valid_bins]\n        normalized_capacities = capacities / np.max(capacities)\n        capacity_score = normalized_capacities\n\n        best_fit_score = np.zeros_like(capacity_score)\n        best_fit_score[np.argmin(waste)] = 1.0\n\n        worst_fit_score = np.zeros_like(capacity_score)\n        worst_fit_score[np.argmax(waste)] = -worst_fit_penalty\n\n        sigmoid_scaled_waste = 1 / (1 + np.exp(-waste * sigmoid_scale))\n\n        combined_score = 0.5 * sigmoid_scaled_waste + 0.3 * capacity_score + 0.2 * (best_fit_score + worst_fit_score)\n        priorities[valid_bins] = combined_score\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 228.9358278562653,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, alpha=1.0, beta=0.5, gamma=2.0) -> np.ndarray:\n    \"\"\"\n    A mutated priority function for online Bin Packing Problem (BPP).\n    Combines best-fit with a dynamically adjusted priority based on bin utilization and item size.\n    Uses parameters alpha, beta, and gamma to control the weighting of different factors.\n    \"\"\"\n    valid_bins = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap)\n\n    if np.any(valid_bins):\n        bin_utilization = bins_remain_cap[valid_bins] / np.max(bins_remain_cap)\n        waste = bins_remain_cap[valid_bins] - item\n        \n        priority_component1 = np.exp(-alpha * waste / np.max(bins_remain_cap))\n        priority_component2 = bin_utilization**beta\n        \n        combined_priority = priority_component1 * priority_component2\n        priorities[valid_bins] = combined_priority\n\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += gamma\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 117.61261085748234,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A priority function for online Bin Packing Problem (BPP) that combines\n    best-fit decreasing with a dynamic penalty for bins that are too small\n    and a bonus for bins with high remaining capacity. It uses a\n    learned weighting scheme between these factors.\n    \"\"\"\n    sigmoid_scale = 5.0\n    best_bin_boost = 10.0\n    small_bin_penalty_factor = 2.0\n    high_cap_bonus_factor = 1.0\n\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    \n    priorities = np.zeros_like(bins_remain_cap)\n\n    if np.any(valid_bins):\n        \n        waste_scores = 1 / (1 + np.exp(-waste * sigmoid_scale))\n        \n        penalty = np.where(bins_remain_cap < item * small_bin_penalty_factor,\n                           -1.0, 0.0)\n        \n        bonus = np.where(bins_remain_cap > item * (1 + high_cap_bonus_factor),\n                         1.0, 0.0)\n        \n        \n        combined_scores = 0.5 * waste_scores + 0.2 * bonus - 0.3 * penalty\n        priorities[valid_bins] = combined_scores\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 279.17595007788486,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\nsigmoid_scale = 5.0\nbest_bin_boost = 1.0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A priority function for online bin packing, combining best-fit,\n    worst-fit (to promote diversity), and sigmoid scaling with a dynamic\n    adjustment based on bin utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities\n\n    waste = bins_remain_cap[valid_bins] - item\n    best_bin_index = np.argmin(waste)\n    worst_bin_index = np.argmax(waste)\n\n    bin_utilization = bins_remain_cap / np.max(bins_remain_cap)\n    utilization_factor = 1 + bin_utilization\n\n    sigmoid_scaled_waste = 1 / (1 + np.exp(-waste * sigmoid_scale))\n    priorities[valid_bins] = sigmoid_scaled_waste * utilization_factor[valid_bins]\n\n    priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    priorities[np.where(valid_bins)[0][worst_bin_index]] += 0.5\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 32.18986836856801,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 157.89111045234063,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines best-fit with a dynamic scaling factor based on bin utilization\n    and a penalty for fragmentation.\n    \"\"\"\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities\n\n    waste = bins_remain_cap[valid_bins] - item\n    \n    bin_utilization = 1 - (bins_remain_cap / np.max(bins_remain_cap))\n    utilization_factor = bin_utilization[valid_bins]\n    \n    fragmentation_penalty = 1.0 / (1 + np.exp(-10 * (1 - bins_remain_cap / np.max(bins_remain_cap))))\n    fragmentation_penalty = fragmentation_penalty[valid_bins]\n\n    priorities[valid_bins] = (1.0 - waste / np.max(waste)) * utilization_factor + fragmentation_penalty * 0.1\n    \n    best_bin_index = np.argmax(priorities[valid_bins])\n    priorities[np.where(valid_bins)[0][best_bin_index]] += 0.5\n    \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 245.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_hs0.txt_stdout.txt",
    "code_path": "problem_iter13_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, sigmoid_scale: float = 9.73767708582636, best_bin_boost: float = 3.788704854914895) -> np.ndarray:\n    \"\"\"Combines waste minimization, sigmoid scaling, and a best-bin boost, with robust handling of edge cases and numerical stability.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        mean_waste = np.mean(waste)\n        std_waste = np.std(waste)\n        if std_waste == 0:\n            normalized_waste = np.zeros_like(waste)\n        else:\n            normalized_waste = (waste - mean_waste) / std_waste\n        sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n        priorities[valid_bins] = sigmoid_scaled_waste\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 131.18329672565338,
    "exec_success": true
  }
]