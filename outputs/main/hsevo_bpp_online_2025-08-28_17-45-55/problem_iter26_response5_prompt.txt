{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A priority function for online bin packing that dynamically adjusts exploration vs. exploitation.\n    It combines best-fit with a time-dependent exploration bonus.\n    \"\"\"\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins)\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit with a mild worst-fit penalty and handles invalid bins.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if np.sum(possible_bins) == 0:\n        return np.zeros_like(bins_remain_cap)\n    waste = bins_remain_cap[possible_bins] - item\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[possible_bins] = 1 / (1 + np.exp(-waste * 2.0))\n    best_bin_index = np.argmin(waste)\n    worst_bin_index = np.argmax(waste)\n    priorities[possible_bins][best_bin_index] += 1.0\n    priorities[possible_bins][worst_bin_index] -= 0.2\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we observe identical code. This is unusual for a ranked list.  (1st/2nd) vs (3rd), the key difference is the 3rd heuristic introduces dynamic scaling with `std_waste`, exploration with `exploration_boost`, and relies on more complex statistical calculations (mean, std). (3rd) vs (4th), the 4th is simpler, using a time-dependent exploration bonus instead of standard deviation based exploration. (4th) vs (5th), the 5th adds worst-fit consideration and a more complex exploration factor dependent on average remaining capacity. (5th) vs (6th/7th), all three are identical, again suggesting an issue with the ranking. (6th/7th) vs (8th/9th) again identical and those use a simpler best-fit with a worst-fit penalty.  (8th/9th) vs (10th), the 10th uses a combination of best-fit, diversity (bin usage), and exploration, which is more complex. (10th) vs (11th), they are identical. (11th) vs (12th), the 12th simplifies back to a time-dependent exploration bonus. (12th) vs (13th/14th), all three are identical again. (13th/14th) vs (15th/16th), identical. (15th/16th) vs (17th/18th), identical.  (17th/18th) vs (19th), identical. (19th) vs (20th), the 20th incorporates both best-fit, worst-fit, and exploration, with a random component to the exploration boost.\n\nOverall: The initial rankings are questionable due to numerous code duplicates. Generally, the better heuristics (1st-4th) focus on a stable best-fit approach potentially combined with dynamic adjustments. Heuristics (5th-20th) introduce increasing complexity with worst-fit penalties, diversity components and exploration, often with little demonstrated improvement. The use of `np.exp` with a scaling factor is consistently beneficial. Exploration, when used, seems more effective when dynamically adjusted based on bin usage/waste.\n- \nOkay, let's distill this extensive self-reflection into actionable advice for designing better heuristics, aiming for that $999K! Here's a breakdown:\n\n* **Keywords:** Stability, Simplicity, Tunability, Diversity.\n* **Advice:** Prioritize a robust, simple 'best-fit' core. Introduce *controlled* diversity via small, tunable penalties/rewards to non-best bins. Dynamically adjust exploration *based on bin state*, not just time/waste.\n* **Avoid:** Complex scaling (beyond sigmoid), aggressive dynamic adjustments, direct division, and code duplication. Over-reliance on normalization *as a primary driver* instead of underlying logic.\n* **Explanation:** The repeated emphasis across the \"ineffective\" reflections points to instability and over-fitting. A solid, predictable core (best-fit + sigmoid) provides a foundation. Tunable parameters allow adaptation without sacrificing robustness.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}