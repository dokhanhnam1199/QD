{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit waste with sigmoid scaling and a decaying best-bin boost.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1 / (1 + np.exp(-5 * (1 - (item / bins_remain_cap[valid_bins]))))\n    \n    best_bin_index = np.argmin(waste)\n    priorities[np.where(valid_bins)[0][best_bin_index]] += 2.0\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste minimization with sigmoid scaling and a decaying best-bin boost.\"\"\"\n    possible_bins = bins_remain_cap >= item\n    if np.sum(possible_bins) == 0:\n        return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing heuristics 1st vs 2nd, the 1st introduces pre-defined `sigmoid_scale` and `best_bin_boost` constants, likely tuned for better performance across various scenarios, while the 2nd calculates these values dynamically. The dynamic approach in 2nd seems less stable. Comparing 1st vs 3rd, the 3rd adds a decaying boost based on `iteration`, intending to focus on best-fit early on and explore more later. However, this introduces a dependency on iteration number, making the heuristic stateful, and it's not immediately clear if the decay rate is optimal.  Comparing 3rd vs 4th, they are identical. Comparing 5th vs 6th, 5th normalizes the waste using standard deviation which can improve stability. 6th is similar to 2nd without normalization. Comparing 7th vs 8th, again identical. Comparing 9th vs 10th, 9th uses a relative waste and scaling factor within the sigmoid, while 10th appears to apply exponential functions in a less clear manner, possibly leading to instability. Comparing 10th vs 11th & 12th, they are identical.  Comparing 13th vs 14th, 15th and 16th, they are identical, providing std normalization and a constant boost. Comparing 17th vs 18th, they are identical. Comparing 19th vs 20th, they are identical.  The primary difference between the top heuristics lies in the stability induced by pre-defined scaling factors (1st) and normalization (5th, 13th-16th), and how the best-bin boost is implemented. The latter heuristics progressively become more convoluted without demonstrable improvement.  Overall: The best heuristics (1st, 5th, 13th-16th) focus on robust scaling and stabilization, while later versions introduce complexity without clear benefit, and some suffer from being stateful (iteration dependency) or unstable.\n- \nOkay, let's craft a redefined 'Current Self-Reflection' for superior heuristic design, aiming for that $999K! Here's a breakdown, stepping through the analysis of the provided texts:\n\n**Step 1: Identify Core Conflict:** The \"Current\" & \"Ineffective\" reflections *appear* to want the same things (stability, sigmoid scaling, normalization). The difference lies in *where* to apply them. \"Ineffective\" focuses on the *objective function* directly, while \"Current\" prioritizes stability *in the heuristic itself*. This is crucial \u2013 stable heuristic behavior allows for better exploration.\n\n**Step 2: Extract Principles:** The \"Current\" reflection's success suggests robustness comes from predictable behavior, even if it means a slight compromise on immediate objective optimization. \"Ineffective\"\u2019s pitfalls stem from sensitivity.\n\n**Step 3: Redefine for Heuristic Design:**\n\n*   **Keywords:** Robustness, Predictability, Controlled Exploration, Scalability.\n*   **Advice:** Prioritize *heuristic mechanism* stability. Employ sigmoid scaling & normalization not to directly *optimize* waste *within* a step, but to *regulate* the heuristic's choices *between* steps.\n*   **Avoid:** Directly encoding objective function values into heuristic selection *without* strong smoothing and stability controls. Overly aggressive/dynamic adjustments based on immediate results.\n*   **Explanation:** A stable heuristic offers consistent search behavior. Sigmoids provide controlled selection pressure. This facilitates broader exploration and avoids getting stuck in locally optimal, yet fragile, solutions.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}