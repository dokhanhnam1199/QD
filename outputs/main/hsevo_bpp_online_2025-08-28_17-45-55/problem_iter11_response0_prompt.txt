{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\nsigmoid_scale = 1.0\nbest_bin_boost = 10.0\n\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        mean_waste = np.mean(waste)\n        std_waste = np.std(waste)\n        if std_waste > 0:\n            normalized_waste = (waste - mean_waste) / std_waste\n        else:\n            normalized_waste = np.zeros_like(waste)\n        sigmoid_scaled_waste = 1 / (1 + np.exp(-normalized_waste * sigmoid_scale))\n        priorities[valid_bins] = sigmoid_scaled_waste\n        best_bin_index = np.argmin(waste)\n        priorities[np.where(valid_bins)[0][best_bin_index]] += best_bin_boost\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit with sigmoid scaling and a stable best bin boost.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    waste = bins_remain_cap[valid_bins] - item\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1 / (1 + np.exp(-waste / (np.std(bins_remain_cap[valid_bins]) + 1e-6)))\n\n    best_bin_index = np.argmin(waste)\n    priorities[valid_bins[best_bin_index]] = np.max(priorities[valid_bins])\n    return priorities\n\n### Analyze & experience\n- Comparing the top-ranked heuristics (1st-3rd) with the bottom-ranked (18th-20th), the primary difference lies in the thoroughness of handling edge cases and the stability of the calculations. The best heuristics consistently check `np.any(valid_bins)` to avoid errors when no bins are suitable for the current item.  They also include `small_value` or `1e-6` to prevent division by zero, crucial for numerical stability.  The best consistently use the `sigmoid_scale` and `best_bin_boost` to tune bin preference.\n\nComparing (1st) vs (2nd) and (1st) vs (3rd), we find the code is literally identical.\nComparing (2nd) vs (4th) similarly shows identical code. This suggests the ranking is somewhat arbitrary.\nComparing (5th) vs (6th), (5th) normalizes the waste using mean and standard deviation before applying the sigmoid, improving bin selection, whereas (6th) uses a simple reciprocal of waste and a fixed sigmoid scaling. (5th) is better.\nComparing (7th) vs (8th), (7th) normalizes waste using the standard deviation while (8th) uses relative waste which can be unstable if `np.max(waste)` is close to zero. (7th) is better.\nComparing (9th) vs (10th), (9th) normalizes with mean and standard deviation and uses a boost factor while (10th) uses a simple reciprocal, then applies sigmoid. (9th) is better.\nComparing (11th) vs (12th), they\u2019re nearly identical, differing only in minor whitespace and potential execution order.\nComparing (13th) vs (16th), they\u2019re identical.\nComparing (14th) vs (15th) and (17th) vs (18th), there's no code inside function, so they are the worst.\nComparing (19th) vs (20th), they\u2019re identical.\n\nOverall: The most effective heuristics prioritize numerical stability by avoiding division by zero, using normalization to prevent extreme values from dominating, and incorporating adjustable parameters (sigmoid scale, boost factor) to fine-tune performance. The redundant code suggests the initial ranking might not fully reflect underlying differences.\n- \nOkay, let's refine this self-reflection for superior heuristic design, aiming for that $999K! Here's a breakdown, focusing on actionable improvements:\n\n* **Keywords:** Meta-optimization, Robustness, Parameter Sensitivity, Statistical Control.\n* **Advice:** Focus on *systematically* exploring parameter spaces (adjustable parameters are good, but need structured testing \u2013 Design of Experiments!). Prioritize methods quantifying heuristic performance *variation* across datasets, not just averages.\n* **Avoid:** Prematurely focusing on specific heuristics (best-fit/sigmoid). The goal isn't *that* solution, it\u2019s *understanding why* some approaches are more adaptable. Avoid over-reliance on single normalization techniques.\n* **Explanation:** The prior reflection drifted into solution-specific tuning. Truly effective self-reflection identifies *principles* of robust design. Numerical stability is vital, but it's a symptom of good meta-optimization\u2014understanding parameter interactions and data sensitivity\u2014not the sole focus.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}