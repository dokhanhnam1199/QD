{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines best-fit (waste minimization) with sigmoid scaling for stability.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    valid_bins = bins_remain_cap >= item\n    if np.any(valid_bins):\n        waste = bins_remain_cap[valid_bins] - item\n        best_bin_index = np.argmin(waste)\n        \n        sigmoid_scaled_waste = 1 / (1 + np.exp(-waste))\n        priorities[valid_bins] = sigmoid_scaled_waste\n        priorities[np.where(valid_bins)[0][best_bin_index]] = 2.0\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins based on fit and minimizes waste using a sigmoid function.\"\"\"\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return np.zeros_like(bins_remain_cap)\n\n    fit_levels = item / bins_remain_cap[valid_bins]\n    priorities = np.zeros_like(bins_remain_cap)\n    priorities[valid_bins] = 1 / (1 + np.exp(-5 * (1 - fit_levels)))\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), (3rd), (4th): These are identical functions. They all combine best-fit with sigmoid scaling and a 'best bin boost'. This approach is generally sound, prioritizing both minimal waste and some exploration to avoid getting stuck in local optima. The constants (2.5369418572825637 and 1.704707830932726) likely result from parameter tuning.\n\nComparing (5th) vs (6th), (7th), (8th):  (5th) and (7th) are identical, and (6th) and (8th) are identical. The primary difference is the 'best bin boost'. (5th)/(7th) do not boost any bin and simply scale waste, while (6th)/(8th) boost the best bin with a value of 2.0. The sigmoid scaling in (5th) and (7th) is applied directly to the priority, but uses a basic `1e-9` to prevent division by zero.\n\nComparing (9th) vs (10th), (11th), (12th):  (9th) is similar to (5th) and (7th) but more concise. (10th) and (12th) are simpler, only using a sigmoid on the waste. (11th) and (13th) and (15th) are identical, standardizing the waste using `np.std`, which can improve scaling. (11th), (13th) and (15th) also include a 'best bin boost' of 2.0. \n\nComparing (14th) vs (16th): (14th) uses a different sigmoid input (based on `1 - fit_levels`), while (16th) normalizes fit scores before applying the sigmoid. Normalizing via `max_fit` in (16th) is a good practice for ensuring stable sigmoid input values.\n\nComparing (17th) vs (18th), (19th), (20th): (17th) and (12th) are identical. (18th), (19th), and (20th) are identical, using fit ratios and exponentiation, with a 'best bin boost'.\n\nOverall: The best heuristics consistently combine best-fit (waste minimization) with a sigmoid scaling function for stability and the standardisation of waste using the standard deviation of the bins, with the addition of a best bin boost to avoid early sub-optimisation. The most problematic heuristics are those that lack sigmoid scaling, rely on potentially unstable division, or don\u2019t normalise/standardise the data.  The constants seem tuned, hinting that finding appropriate parameters is crucial.\n- \nOkay, let's refine this heuristic design approach! Here\u2019s a breakdown focusing on maximizing effectiveness and avoiding pitfalls, aiming for that $999K tip!\n\n* **Keywords:** Sigmoid Scaling, Stability, Exploration, Parameter Tuning.\n* **Advice:** Focus on *relative* heuristic fitness (sigmoid output) \u2013 not absolute objective function improvement. Experiment with multiple sigmoid parameters (k, x0) tailored to problem scale. Implement a decaying 'best bin boost' over iterations.\n* **Avoid:** Directly optimizing for the bin packing objective *within* the heuristic priority. Over-reliance on normalization without outlier handling.\n* **Explanation:** Sigmoids provide stable, differentiable prioritization.  The boost prevents stagnation, while tuning adapts to problem characteristics. Prioritizing heuristic quality *guides* search, rather than directly solving, leading to better exploration.\n\n\n\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}