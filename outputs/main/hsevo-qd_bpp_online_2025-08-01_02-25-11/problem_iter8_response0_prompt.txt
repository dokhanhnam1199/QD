{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation extends \"Best Fit\" with a \"Consolidation Bonus\".\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity (Best Fit). Additionally, it gives a linear bonus\n    to bins that are already partially filled (i.e., not completely empty),\n    to encourage consolidation and avoid opening new bins prematurely.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         This array typically includes capacities of active bins\n                         and potentially one or more 'empty' bins representing\n                         new bins that can be opened.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # A small positive constant bonus to encourage consolidation into existing bins.\n    # This value should be carefully chosen. It should be small enough to\n    # generally not override the primary \"Best Fit\" principle, but large enough\n    # to consistently break ties or influence choices when potential remaining\n    # capacities are very close.\n    CONSOLIDATION_BONUS = 0.01\n\n    # Initialize priorities for all bins to negative infinity.\n    # This ensures that bins where the item cannot fit will not be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Handle the edge case where there are no bins to consider.\n    if len(bins_remain_cap) == 0:\n        return priorities\n\n    # Infer the maximum capacity of an 'empty' bin from the input array.\n    # This assumes that if a new bin is to be opened, its capacity\n    # (or equivalent to a fresh bin's capacity) is present in `bins_remain_cap`.\n    # If all bins are already partially filled, this will consider the least\n    # filled bin as the 'reference empty' for bonus purposes, promoting consolidation\n    # into the most-filled (least remaining capacity) bins among the current set.\n    BIN_CAPACITY = np.max(bins_remain_cap)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the priorities array with -np.inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item were placed in each fitting bin.\n    # This is the core of the \"Best Fit\" strategy.\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Base score: The goal is to minimize the remaining capacity, which means\n    # maximizing the negative of the remaining capacity. This creates a monotonic\n    # scoring where a smaller positive remainder yields a higher priority score.\n    base_scores = -potential_remaining_cap\n\n    # Determine which of the fitting bins are \"partially filled\" (not completely empty).\n    # A bin is considered partially filled if its current remaining capacity\n    # is less than the inferred full bin capacity (BIN_CAPACITY).\n    is_partially_filled_mask = (bins_remain_cap[can_fit_mask] < BIN_CAPACITY)\n\n    # Initialize bonuses for fitting bins to zero.\n    consolidation_bonuses = np.zeros_like(base_scores)\n\n    # Apply the consolidation bonus only to the fitting bins that are partially filled.\n    # This encourages using existing, non-empty bins before opening new ones.\n    consolidation_bonuses[is_partially_filled_mask] = CONSOLIDATION_BONUS\n\n    # The total priority for fitting bins is the sum of the base Best Fit score\n    # and the consolidation bonus.\n    priorities[can_fit_mask] = base_scores + consolidation_bonuses\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- *   **Comparing (1st/2nd) vs (3rd/9th):** Both employ a \"Best Fit\" strategy augmented with a \"Consolidation Bonus\" to favor partially filled bins. The primary difference lies in the `CONSOLIDATION_BONUS` magnitude (0.01 in 1st/2nd vs. 1e-6 in 3rd/9th) and the method of determining `BIN_CAPACITY` (inferred from `np.max` in 1st/2nd vs. explicit argument with `np.isclose` for robustness in 3rd/9th). The higher ranking of 1st/2nd suggests that a more substantial bonus (0.01) is more effective than a very small one (1e-6) at encouraging beneficial consolidation without unduly disrupting the Best Fit principle.\n*   **Comparing (3rd/9th) vs (4th):** Heuristics 3rd and 9th are identical, adding a `1e-6` consolidation bonus to a Best Fit strategy. Heuristic 4th is a pure Best Fit implementation, maximizing the negative of the potential remaining capacity (`-potential_remaining_cap`). The higher ranking of 3rd/9th over 4th indicates that even a small consolidation bonus is generally beneficial compared to a strict Best Fit, as it helps reduce the total number of bins by prioritizing existing ones.\n*   **Comparing (4th) vs (5th/6th/8th):** Heuristic 4th calculates Best Fit using `-potential_remaining_cap`. Heuristics 5th, 6th, and 8th are identical, also implementing Best Fit but via `2 * item - bins_remain_cap`. Both methods correctly rank bins by tightness of fit. The higher ranking of 4th suggests that its specific scoring range (0 for perfect fit, decreasing for worse fits) or numerical behavior is marginally more effective or stable than the alternative scoring, which uses `item` for perfect fit and decreases from there.\n*   **Comparing (5th/6th/8th) vs (7th/10th):** Heuristics 5th, 6th, and 8th represent a standard Best Fit approach. Heuristics 7th and 10th are identical and represent a parameterized Best Fit strategy, where the weighting of `potential_remaining_cap` and the `priority_no_fit` constant are explicitly passed with seemingly \"tuned\" floating-point values (e.g., `weight_remaining_cap = -0.493...`). The lower ranking of 7th/10th indicates that these specific tuned parameters perform worse than the simpler, standard Best Fit approaches. This suggests either the tuning process was suboptimal, or the specific values don't generalize well, or that deviating from a simple `-1.0` weight for `potential_remaining_cap` is detrimental.\n*   **Comparing (7th/10th) vs (11th-20th):** Heuristics 7th and 10th represent a sub-optimal but still functional Best Fit variant. Heuristics 11th through 20th are all identical, returning a zero-filled array, meaning all bins are considered equally preferable (or effectively random selection among fitting bins). This demonstrates that any form of intelligent packing, even if sub-optimally tuned, is significantly better than a completely unprioritized or arbitrary selection of bins.\n*   **Overall:** The best heuristics strategically combine Best Fit with a consolidation bonus, with the magnitude of the bonus being crucial for performance. Pure Best Fit strategies are strong baselines, and their precise scoring formula can have subtle impacts. Parameterized heuristics, while theoretically flexible, can perform poorly if the parameters are not optimally chosen or if the tuning objective doesn't perfectly align with the problem's goal. Completely unprioritized bin selection is the worst approach.\n- \nHere's a redefinition of self-reflection, focusing on principles for better heuristics:\n\n*   **Keywords:** Adaptive Priorities, Non-linear Functions, Exploration, Multi-objective.\n*   **Advice:** Prioritize dynamic, non-linear scoring systems that adapt to solution progression. Integrate controlled exploration, even if it means temporary non-greedy choices. Embrace complex numerical transformations and intricate parameter tuning. Consider multi-objective optimization beyond immediate \"waste\" minimization.\n*   **Avoid:** Static or strictly monotonic priority relationships. Sole reliance on simple greedy strategies. Ignoring the potential of non-linear \"tricks\" or complex interactions. A singular, myopic objective like minimal remaining space.\n*   **Explanation:** Sophisticated, adaptive decision models, coupled with strategic exploration and nuanced objective functions, can reveal richer solution landscapes and overcome limitations of simpler, fixed heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}