{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Hybrid Fit\" strategy, building upon Best Fit\n    by incorporating multi-objective considerations and non-linear scoring.\n    It prioritizes:\n    1.  **Perfect or Near-Perfect Fits:** Maximizing the chance of completely filling a bin\n        to reduce the total number of active bins and eliminate fragmentation.\n    2.  **Best Fit:** After perfect fits, it reverts to the standard Best Fit strategy\n        of minimizing the remaining capacity.\n    3.  **Fragment Avoidance:** It penalizes bins that, after placement, would be left\n        with a very small, \"awkward\" amount of space that is unlikely to be useful\n        for subsequent items (e.g., too small for most items, but not zero).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         Assumes capacities and item sizes are normalized or\n                         consistent in their units.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return -inf for all (or new bin logic handled externally)\n        return priorities\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # --- Tunable Parameters for the Hybrid Fit Heuristic ---\n    # These parameters are crucial for balancing the different objectives.\n    # They might require tuning based on the specific distribution of item sizes\n    # and bin capacity in your problem instance.\n\n    # EPSILON_PERFECT_FIT: A small tolerance for floating point comparisons to\n    # consider a bin \"perfectly\" filled. If remaining capacity is within this,\n    # it's considered a perfect fit.\n    EPSILON_PERFECT_FIT = 1e-9\n\n    # TINY_REMAINDER_THRESHOLD: The upper bound for what is considered a \"tiny\"\n    # or \"awkward\" remaining capacity. If a bin's remaining capacity falls\n    # between EPSILON_PERFECT_FIT and this threshold, it incurs a penalty.\n    # This value typically corresponds to a small fraction of the bin's total capacity.\n    # For example, if bin capacity is 1.0, 0.05 means 5% of capacity.\n    TINY_REMAINDER_THRESHOLD = 0.05\n\n    # PERFECT_FIT_BONUS: A large positive bonus applied to bins that achieve\n    # a perfect or near-perfect fit. This should be high enough to make perfect\n    # fits almost always the top priority.\n    PERFECT_FIT_BONUS = 1000.0\n\n    # TINY_REMAINDER_PENALTY: A significant negative penalty applied to bins\n    # that are left with a tiny, potentially unusable remainder. This pushes\n    # the algorithm away from creating such fragmented bins.\n    TINY_REMAINDER_PENALTY = 500.0\n\n    # --- Apply the scoring logic ---\n    # 1. Base Score: Best Fit (minimize remaining capacity)\n    #    A smaller positive remaining capacity results in a larger (less negative) base score.\n    current_priorities = -potential_remaining_cap\n\n    # 2. Perfect/Near-Perfect Fit Bonus\n    #    Identify bins where remaining capacity is very close to zero.\n    perfect_fit_mask = potential_remaining_cap <= EPSILON_PERFECT_FIT\n    current_priorities[perfect_fit_mask] += PERFECT_FIT_BONUS\n\n    # 3. Tiny Remainder Penalty\n    #    Identify bins where remaining capacity is small but not zero (a \"dead zone\").\n    #    Ensure these are not already marked as perfect fits.\n    tiny_remainder_mask = (potential_remaining_cap > EPSILON_PERFECT_FIT) & \\\n                          (potential_remaining_cap < TINY_REMAINDER_THRESHOLD)\n    current_priorities[tiny_remainder_mask] -= TINY_REMAINDER_PENALTY\n\n    # Assign the calculated priorities back to the main priorities array\n    priorities[can_fit_mask] = current_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), Heuristic 1st implements Best Fit with a small, constant consolidation bonus for used bins, explicitly using `bin_capacity`. Heuristic 2nd is a Best Fit with a numerically tuned `weight_remaining_cap` and `priority_no_fit`. The higher ranking of 1st suggests that a simple, robust consolidation bonus often outperforms a pure Best Fit, even if the latter's weight is supposedly \"tuned.\"\n\nComparing (2nd) vs (3rd), Heuristic 3rd is identical in code to Heuristic 1st. This presents a strong inconsistency in the provided ranking, as 1st is ranked higher than 2nd, but 2nd is ranked higher than 3rd. Based on the code, 1st and 3rd should perform identically and thus be ranked equally. This implies external factors or an arbitrary aspect to the ranking for these identical heuristics.\n\nComparing (3rd) vs (4th), both apply a consolidation bonus to Best Fit. Heuristic 3rd explicitly takes `bin_capacity` as a parameter, which is reliable for identifying 'used' bins. Heuristic 4th *infers* `BIN_CAPACITY` from `np.max(bins_remain_cap)`. This inference is less robust, as it might not represent the true bin capacity if no empty bin is present. Additionally, 4th uses a significantly larger `CONSOLIDATION_BONUS` (0.01 vs 1e-6), which could overly bias towards consolidation, potentially sacrificing tighter fits. The explicit parameter in 3rd is generally more stable.\n\nComparing (6th) vs (8th), Heuristic 6th (Hybrid Fit with perfect fit bonus and tiny remainder penalty) is ranked lower than Heuristic 8th (Pure Best Fit). This is counter-intuitive, as 6th aims for more sophisticated optimization. This suggests that the fixed, aggressive bonus and penalty parameters (`1000.0`, `500.0`) in 6th might not be optimally tuned or are too disruptive to the core Best Fit principle, leading to worse performance than a simpler, consistent Best Fit approach.\n\nComparing (8th) vs (11th), Heuristic 8th (Pure Best Fit) performs significantly better than Heuristic 11th, which assigns zero priority to all bins, effectively making the choice random among fitting bins. This demonstrates that any intelligent prioritization (even simple Best Fit) provides substantial gains over no specific strategy.\n\nOverall: The presence of multiple identical heuristic functions ranked differently (e.g., 1st, 3rd, 7th; 4th, 5th; 6th, 9th, 10th; 11th-20th) strongly suggests an inconsistency in the ranking method or the influence of external factors not reflected in the code. A small, well-controlled consolidation bonus (as in 1st/3rd/7th) appears to be a robust improvement over pure Best Fit. More complex multi-objective strategies, while conceptually appealing, are highly sensitive to parameter tuning; without it, they can underperform simpler, more robust heuristics.\n- \nHere's a redefinition of 'Current self-reflection' for designing better heuristics:\n\n*   **Keywords:** Adaptive, Holistic, Robustness, Exploration\n*   **Advice:** Design heuristics that dynamically adapt to problem characteristics. Focus on leveraging deep domain insights to guide search, promoting diverse solution exploration beyond immediate greedy choices. Prioritize robustness and generalizability across varied instances.\n*   **Avoid:** Relying solely on static, fixed-rule scoring or simple consolidation bonuses. Avoid discussions of specific \"Best Fit\" enhancements, monotonic priorities, or numerical \"tricks.\" Do not base design solely on local optimality without considering broader solution space.\n*   **Explanation:** Effective heuristics transcend rigid rules by understanding problem dynamics and exploring a wider solution landscape, leading to more resilient and adaptable performance.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}