```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Hybrid Adaptive Fit: Combines Best Fit with non-linear bonuses for bin completion
    and large remaining space, plus an adaptive penalty for fragmentation,
    optimizing bin utility and minimizing open bins.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Tolerance for floating point comparisons to avoid issues with near-zero values
    TOLERANCE_EPS = 1e-9

    # Mask for bins where the item can fit (capacity >= item size, with a small tolerance)
    can_fit_mask = bins_remain_cap >= item - TOLERANCE_EPS

    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # If no bin can fit the item, return priorities initialized to -inf
    if fitting_bins_remain_cap.size == 0:
        return priorities

    # Calculate potential remaining capacity if the item were placed
    potential_remaining_cap = fitting_bins_remain_cap - item
    # Ensure no negative remainders due to floating point inaccuracies when item == capacity
    potential_remaining_cap[potential_remaining_cap < 0] = 0.0

    # --- Core Best Fit Principle (Foundation, similar to priority_v1) ---
    # Bins with smaller remaining capacity after placement get a base higher priority.
    # This maximizes the effective filled capacity, a core idea from Best Fit heuristics.
    calculated_priorities = -potential_remaining_cap

    # --- Advanced Non-linear & Adaptive Components (from priority_v0's sophistication) ---

    # 1. Exact Fit / Bin Completion Bonus:
    # Applies a very strong, rapidly decaying exponential bonus for near-perfect fits.
    # This aggressively incentivizes closing bins efficiently, preventing small, unusable gaps.
    EXACT_FIT_BONUS_MAGNITUDE = 5000.0  # High magnitude to make exact fits dominant
    EXACT_FIT_DECAY_RATE = 50.0         # High decay rate for a very sharp peak at 0
    
    exact_fit_bonus = EXACT_FIT_BONUS_MAGNITUDE * np.exp(-EXACT_FIT_DECAY_RATE * potential_remaining_cap)
    calculated_priorities += exact_fit_bonus

    # 2. Fragmentation Penalty ("Valley of Despair"):
    # Introduces a significant, non-linear penalty for bins that, after placing the item,
    # would be left with a non-zero, "awkward" amount of remaining capacity. This penalty
    # is shaped like an inverted Gaussian curve, being harshest for mid-range remainders
    # (e.g., 30-50% of the item's size), discouraging fragmented spaces.
    
    if item > TOLERANCE_EPS: # Only apply if item size is meaningful for relative calculations
        # Focus fragmentation penalty on remainders that are not too large.
        fragment_consideration_mask = (potential_remaining_cap > TOLERANCE_EPS) & \
                                      (potential_remaining_cap <= 1.5 * item)

        if np.any(fragment_consideration_mask):
            # Normalize the remaining capacity by the item's size for adaptive scaling.
            normalized_fragment_rem = potential_remaining_cap[fragment_consideration_mask] / item

            # Parameters for the Gaussian penalty curve.
            FRAGMENT_PENALTY_PEAK_RATIO = 0.4 # Peak penalty when remaining capacity is 40% of item size
            FRAGMENT_PENALTY_STD_DEV = 0.2    # Standard deviation: controls the width of the penalty zone
            PENALTY_MAGNITUDE = 100.0         # Maximum strength of the fragmentation penalty

            # Calculate the Gaussian penalty. It's negative to subtract from priority.
            penalty = -PENALTY_MAGNITUDE * np.exp(
                -((normalized_fragment_rem - FRAGMENT_PENALTY_PEAK_RATIO)**2) / (2 * FRAGMENT_PENALTY_STD_DEV**2)
            )
            calculated_priorities[fragment_consideration_mask] += penalty

    # 3. Quality of Large Remaining Space Bonus:
    # Provides a moderate, logarithmically increasing bonus for bins that are left with a
    # substantial amount of free capacity (e.g., more than double the current item's size).
    # This incentivizes keeping bins with genuinely useful large spaces available for future
    # large items, promoting overall bin utility.
    
    if item > TOLERANCE_EPS:
        # Define "large enough" remaining capacity relative to the item size.
        LARGE_REM_THRESHOLD_MULTIPLE = 2.0
        
        large_rem_mask = potential_remaining_cap > (LARGE_REM_THRESHOLD_MULTIPLE * item)

        if np.any(large_rem_mask):
            LARGE_REM_BONUS_FACTOR = 20.0 # Moderate bonus strength

            # Scale the argument for log1p to ensure positive values and manage sensitivity.
            scaled_log_arg = potential_remaining_cap[large_rem_mask] / (item * LARGE_REM_THRESHOLD_MULTIPLE)
            log_bonus_amount = LARGE_REM_BONUS_FACTOR * np.log1p(np.minimum(scaled_log_arg, 100.0)) # Cap for stability
            
            calculated_priorities[large_rem_mask] += log_bonus_amount

    # Assign the calculated priorities back to the original array for fitting bins
    priorities[can_fit_mask] = calculated_priorities

    return priorities
```
