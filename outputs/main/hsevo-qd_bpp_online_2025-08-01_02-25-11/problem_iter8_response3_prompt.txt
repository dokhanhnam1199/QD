{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Best Fit\" strategy:\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity. This helps to 'tightly pack' items into existing bins,\n    leaving larger capacities open for larger items or reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # For fitting bins, we want to maximize the \"tightness\".\n    # A smaller remaining capacity means a tighter fit.\n    # To achieve this with argmax (which finds the maximum priority score),\n    # we can use the negative of the potential_remaining_cap.\n    # E.g., if remainders are [0.1, 0.5, 0.8], their negatives are [-0.1, -0.5, -0.8].\n    # The max of negatives is -0.1, which corresponds to the smallest positive remainder 0.1.\n    priorities[can_fit_mask] = -potential_remaining_cap\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- *   **Comparing (1st/2nd) vs (3rd/9th):** Both employ a \"Best Fit\" strategy augmented with a \"Consolidation Bonus\" to favor partially filled bins. The primary difference lies in the `CONSOLIDATION_BONUS` magnitude (0.01 in 1st/2nd vs. 1e-6 in 3rd/9th) and the method of determining `BIN_CAPACITY` (inferred from `np.max` in 1st/2nd vs. explicit argument with `np.isclose` for robustness in 3rd/9th). The higher ranking of 1st/2nd suggests that a more substantial bonus (0.01) is more effective than a very small one (1e-6) at encouraging beneficial consolidation without unduly disrupting the Best Fit principle.\n*   **Comparing (3rd/9th) vs (4th):** Heuristics 3rd and 9th are identical, adding a `1e-6` consolidation bonus to a Best Fit strategy. Heuristic 4th is a pure Best Fit implementation, maximizing the negative of the potential remaining capacity (`-potential_remaining_cap`). The higher ranking of 3rd/9th over 4th indicates that even a small consolidation bonus is generally beneficial compared to a strict Best Fit, as it helps reduce the total number of bins by prioritizing existing ones.\n*   **Comparing (4th) vs (5th/6th/8th):** Heuristic 4th calculates Best Fit using `-potential_remaining_cap`. Heuristics 5th, 6th, and 8th are identical, also implementing Best Fit but via `2 * item - bins_remain_cap`. Both methods correctly rank bins by tightness of fit. The higher ranking of 4th suggests that its specific scoring range (0 for perfect fit, decreasing for worse fits) or numerical behavior is marginally more effective or stable than the alternative scoring, which uses `item` for perfect fit and decreases from there.\n*   **Comparing (5th/6th/8th) vs (7th/10th):** Heuristics 5th, 6th, and 8th represent a standard Best Fit approach. Heuristics 7th and 10th are identical and represent a parameterized Best Fit strategy, where the weighting of `potential_remaining_cap` and the `priority_no_fit` constant are explicitly passed with seemingly \"tuned\" floating-point values (e.g., `weight_remaining_cap = -0.493...`). The lower ranking of 7th/10th indicates that these specific tuned parameters perform worse than the simpler, standard Best Fit approaches. This suggests either the tuning process was suboptimal, or the specific values don't generalize well, or that deviating from a simple `-1.0` weight for `potential_remaining_cap` is detrimental.\n*   **Comparing (7th/10th) vs (11th-20th):** Heuristics 7th and 10th represent a sub-optimal but still functional Best Fit variant. Heuristics 11th through 20th are all identical, returning a zero-filled array, meaning all bins are considered equally preferable (or effectively random selection among fitting bins). This demonstrates that any form of intelligent packing, even if sub-optimally tuned, is significantly better than a completely unprioritized or arbitrary selection of bins.\n*   **Overall:** The best heuristics strategically combine Best Fit with a consolidation bonus, with the magnitude of the bonus being crucial for performance. Pure Best Fit strategies are strong baselines, and their precise scoring formula can have subtle impacts. Parameterized heuristics, while theoretically flexible, can perform poorly if the parameters are not optimally chosen or if the tuning objective doesn't perfectly align with the problem's goal. Completely unprioritized bin selection is the worst approach.\n- \nHere's a redefinition of self-reflection, focusing on principles for better heuristics:\n\n*   **Keywords:** Adaptive Priorities, Non-linear Functions, Exploration, Multi-objective.\n*   **Advice:** Prioritize dynamic, non-linear scoring systems that adapt to solution progression. Integrate controlled exploration, even if it means temporary non-greedy choices. Embrace complex numerical transformations and intricate parameter tuning. Consider multi-objective optimization beyond immediate \"waste\" minimization.\n*   **Avoid:** Static or strictly monotonic priority relationships. Sole reliance on simple greedy strategies. Ignoring the potential of non-linear \"tricks\" or complex interactions. A singular, myopic objective like minimal remaining space.\n*   **Explanation:** Sophisticated, adaptive decision models, coupled with strategic exploration and nuanced objective functions, can reveal richer solution landscapes and overcome limitations of simpler, fixed heuristics.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}