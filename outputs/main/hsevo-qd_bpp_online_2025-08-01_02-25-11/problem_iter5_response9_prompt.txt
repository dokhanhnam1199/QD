{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Best Fit\" strategy:\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity. This helps to 'tightly pack' items into existing bins,\n    leaving larger capacities open for larger items or reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # For fitting bins, we want to maximize the \"tightness\".\n    # A smaller remaining capacity means a tighter fit.\n    # To achieve this with argmax (which finds the maximum priority score),\n    # we can use the negative of the potential_remaining_cap.\n    # E.g., if remainders are [0.1, 0.5, 0.8], their negatives are [-0.1, -0.5, -0.8].\n    # The max of negatives is -0.1, which corresponds to the smallest positive remainder 0.1.\n    priorities[can_fit_mask] = -potential_remaining_cap\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)\n\n### Analyze & experience\n- Comparing (1st, 4th, 9th) vs (2nd, 3rd, 6th, 7th), we observe that the top-ranked heuristics (1st, 4th, 9th) implement a \"Best Fit\" strategy augmented with a `used_bin_bonus`. This bonus (`1e-6`) is added to the priority of bins that are already partially filled (i.e., not entirely empty/fresh). In contrast, the second group of heuristics (2nd, 3rd, 6th, 7th) uses a parameterized Best Fit, multiplying the `potential_remaining_cap` by a `weight_remaining_cap` (defaulting to -0.49...). The `used_bin_bonus` explicitly biases the selection towards consolidating items into existing bins, potentially delaying the opening of new bins. This targeted consolidation heuristic appears to be more effective than a simple weighted Best Fit in minimizing total bins.\n\nComparing (2nd, 3rd, 6th, 7th) vs (5th, 8th) vs (10th), all these heuristics fundamentally implement a \"Best Fit\" strategy by prioritizing bins that result in the smallest remaining capacity.\n(2nd, 3rd, 6th, 7th) use `weight_remaining_cap * potential_remaining_cap`.\n(5th, 8th) use `2 * item - bins_remain_cap[fits_mask]`, which simplifies to `-(bins_remain_cap[fits_mask] - item) + item`, or `-potential_remaining_cap + item`. Since `item` is constant for a given call, this is equivalent to `-potential_remaining_cap` for `argmax` selection.\n(10th) directly uses `-potential_remaining_cap`.\nThe slight difference in ranking among these Best Fit variants suggests that the specific `weight_remaining_cap` value in the 2nd group might be marginally better tuned for the problem, or the differences are negligible and reflect noise in the evaluation process. However, all these Best Fit variants demonstrate superior performance compared to the naive approaches.\n\nComparing (5th, 8th, 10th) vs (11th-20th), the \"Best Fit\" heuristics (5th, 8th, 10th) consistently outperform the lowest-ranked heuristics (11th-20th). The low-ranked heuristics simply return `np.zeros_like(bins_remain_cap)`, effectively giving equal priority to all bins where the item fits. This leads to arbitrary bin selection (e.g., first fit by index), which is generally suboptimal for bin packing as it doesn't try to minimize wasted space or consolidate items efficiently. The \"Best Fit\" strategy, by minimizing remaining capacity, actively works towards better packing density and thus fewer bins.\n\nOverall: The most effective heuristics combine the core \"Best Fit\" principle with a specific mechanism to encourage the reuse and consolidation of existing bins over opening new ones. Simple Best Fit is generally good, and subtle parameter tuning might offer minor gains. Naive heuristics that don't prioritize intelligent bin selection perform significantly worse.\n- \n### Redefined Current Self-reflection\n\n*   **Keywords:** Monotonic Scoring, Consolidation Priority, Feasibility Handling, Linear Simplicity.\n*   **Advice:** Redefine Best Fit scores monotonically (e.g., `1 / remaining_space` or `-remaining_space`). Add a *simple, linear* bonus for existing partially-filled bins to strongly encourage consolidation. Explicitly assign negative infinity priority to infeasible bins.\n*   **Avoid:** Complex non-linear transformations or numerical `epsilon` for scoring. Arbitrary weighting. Implicitly handling infeasible choices; make it an explicit rule.\n*   **Explanation:** A clear monotonic scoring, combined with a straightforward linear consolidation bonus, provides a robust, intelligent greedy strategy. Explicitly managing infeasibility ensures correctness and efficiency, significantly outperforming ad-hoc designs.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}