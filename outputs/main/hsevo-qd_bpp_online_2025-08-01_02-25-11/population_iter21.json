[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    return np.zeros_like(bins_remain_cap)",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 2.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Best Fit\" strategy:\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity. This helps to 'tightly pack' items into existing bins,\n    leaving larger capacities open for larger items or reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # For fitting bins, we want to maximize the \"tightness\".\n    # A smaller remaining capacity means a tighter fit.\n    # To achieve this with argmax (which finds the maximum priority score),\n    # we can use the negative of the potential_remaining_cap.\n    # E.g., if remainders are [0.1, 0.5, 0.8], their negatives are [-0.1, -0.5, -0.8].\n    # The max of negatives is -0.1, which corresponds to the smallest positive remainder 0.1.\n    priorities[can_fit_mask] = -potential_remaining_cap\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority for 'Best Fit' by maximizing the effective filled capacity.\n\n    Prioritizes bins that achieve the highest fill level after placing the item,\n    yielding positive scores for valid fits and penalizing impossible ones.\n    \"\"\"\n    # Initialize all priorities to an extremely low value, ensuring bins that cannot\n    # accommodate the item are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify which bins possess sufficient remaining capacity for the item.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins where the item demonstrably fits, calculate a priority score.\n    # This score, 2 * item - bins_remain_cap[fits_mask], maximizes the resulting\n    # effective filled capacity relative to the item's size. A perfect fit\n    # (where the bin's remaining capacity becomes zero) yields the highest positive\n    # score (equal to `item`), while less efficient fits yield lower positive scores.\n    # This combines the efficiency of masking first with a positive-scaled Best Fit score.\n    priorities[fits_mask] = 2 * item - bins_remain_cap[fits_mask]\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation builds on the \"Best Fit\" strategy but introduces a slight\n    preference for existing, partially filled bins over entirely new (empty) bins.\n    The goal is to encourage filling up bins already in use, potentially delaying\n    the opening of new bins, which can lead to fewer total bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        bin_capacity: The maximum capacity of a single bin. Default to 1.0,\n                      assuming item sizes are normalized relative to this capacity.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only consider bins where the item fits\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # 2. Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # 3. Base priority: Best Fit strategy (minimize remaining capacity)\n    # A smaller potential_remaining_cap (closer to 0) means a tighter fit,\n    # which is preferred. By taking the negative, we convert minimization\n    # to maximization for `np.argmax`.\n    # A perfect fit (potential_remaining_cap == 0) will result in a priority of 0,\n    # which is the highest possible score from this component.\n    base_priorities = -potential_remaining_cap\n\n    # 4. Mutation: Add a small bonus for choosing an already used bin over a fresh one.\n    # This slightly biases towards consolidating items into existing bins.\n    # The bonus value (e.g., 1e-6) should be carefully chosen. It must be\n    # small enough not to override a significantly better Best Fit (i.e., a\n    # much smaller potential_remaining_cap difference), but large enough\n    # to break ties or influence decisions when Best Fit scores are very close.\n    # For floating-point comparisons, a tolerance (e.g., np.finfo(float).eps * 10)\n    # is often used, but for simplicity and common BPP scenarios where `bin_capacity`\n    # is exactly 1.0 for fresh bins, direct comparison or `x < capacity` is often sufficient.\n    used_bin_bonus = 1e-6  # A small constant bonus\n\n    # Identify bins that are not \"fresh\" (i.e., not entirely empty/unused).\n    # A bin is considered 'used' if its remaining capacity is strictly less than the full capacity.\n    # Using np.isclose for robustness against floating-point inaccuracies when comparing to bin_capacity.\n    is_used_bin_mask = ~np.isclose(fitting_bins_remain_cap, bin_capacity)\n\n    # Apply the bonus only to bins that can fit the item AND are already used.\n    # This applies the bonus to the elements within the 'can_fit_mask' subset.\n    priorities[can_fit_mask] = base_priorities\n    priorities[can_fit_mask][is_used_bin_mask] += used_bin_bonus\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_hs1.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                priority_no_fit: float = -7469923285.667422,\n                weight_remaining_cap: float = -0.493607220196141) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Best Fit\" strategy:\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity. This helps to 'tightly pack' items into existing bins,\n    leaving larger capacities open for larger items or reducing fragmentation.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        priority_no_fit: The priority assigned to bins where the item cannot fit.\n                         Default is -np.inf, effectively excluding them.\n        weight_remaining_cap: The weight applied to the potential remaining\n                              capacity. A negative value (e.g., -1.0 for Best Fit)\n                              prioritizes smaller remaining capacities. A positive\n                              value would prioritize larger remaining capacities\n                              (Worst Fit).\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, priority_no_fit, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Apply the weight to the potential remaining capacity.\n    # For Best Fit, weight_remaining_cap should be negative.\n    priorities[can_fit_mask] = weight_remaining_cap * potential_remaining_cap\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Returns priority for bin selection, combining Best Fit with a consolidation bonus.\n\n    Prioritizes bins that offer the tightest fit, adding a small bonus for\n    partially-filled bins to encourage consolidation and reduce new bin openings.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Get remaining capacities for only the bins that can fit the item\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size == 0:\n        return priorities # No bin can fit the item, return all -inf priorities\n\n    # 2. Calculate potential remaining capacity if item were placed in fitting bins\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # 3. Base priority: Best Fit strategy (minimize remaining capacity)\n    # A smaller potential_remaining_cap (closer to 0) means a tighter fit,\n    # which is preferred. By taking the negative, we convert minimization\n    # to maximization for `np.argmax`. A perfect fit (0 remaining) gets a 0 score here.\n    base_priorities_for_fitting_bins = -potential_remaining_cap\n\n    # 4. Consolidation Bonus: Add a small bonus for choosing an already used bin.\n    # This encourages filling existing bins before opening new ones, potentially saving bins.\n    # The value (e.g., 1e-6) should be small enough not to override a significantly\n    # better \"Best Fit\" (i.e., a much smaller potential_remaining_cap difference),\n    # but large enough to break ties or influence decisions when Best Fit scores are very close.\n    used_bin_bonus = 1e-6\n\n    # Identify bins that are 'used' (i.e., not entirely empty/fresh).\n    # A bin is considered 'used' if its remaining capacity is strictly less than the full bin_capacity.\n    # Using np.isclose for robustness against floating-point inaccuracies when comparing to bin_capacity.\n    is_used_bin_mask = ~np.isclose(fitting_bins_remain_cap, bin_capacity)\n\n    # Apply the bonus only to bins that can fit the item AND are already used.\n    # This modifies the base priorities for the subset of fitting bins.\n    base_priorities_for_fitting_bins[is_used_bin_mask] += used_bin_bonus\n\n    # Assign the calculated priorities back to the main priorities array for the fitting bins\n    priorities[can_fit_mask] = base_priorities_for_fitting_bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation extends \"Best Fit\" with a \"Consolidation Bonus\".\n    It prioritizes bins that, after the item is placed, will have the smallest\n    remaining capacity (Best Fit). Additionally, it gives a linear bonus\n    to bins that are already partially filled (i.e., not completely empty),\n    to encourage consolidation and avoid opening new bins prematurely.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         This array typically includes capacities of active bins\n                         and potentially one or more 'empty' bins representing\n                         new bins that can be opened.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # A small positive constant bonus to encourage consolidation into existing bins.\n    # This value should be carefully chosen. It should be small enough to\n    # generally not override the primary \"Best Fit\" principle, but large enough\n    # to consistently break ties or influence choices when potential remaining\n    # capacities are very close.\n    CONSOLIDATION_BONUS = 0.01\n\n    # Initialize priorities for all bins to negative infinity.\n    # This ensures that bins where the item cannot fit will not be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Handle the edge case where there are no bins to consider.\n    if len(bins_remain_cap) == 0:\n        return priorities\n\n    # Infer the maximum capacity of an 'empty' bin from the input array.\n    # This assumes that if a new bin is to be opened, its capacity\n    # (or equivalent to a fresh bin's capacity) is present in `bins_remain_cap`.\n    # If all bins are already partially filled, this will consider the least\n    # filled bin as the 'reference empty' for bonus purposes, promoting consolidation\n    # into the most-filled (least remaining capacity) bins among the current set.\n    BIN_CAPACITY = np.max(bins_remain_cap)\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the priorities array with -np.inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item were placed in each fitting bin.\n    # This is the core of the \"Best Fit\" strategy.\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Base score: The goal is to minimize the remaining capacity, which means\n    # maximizing the negative of the remaining capacity. This creates a monotonic\n    # scoring where a smaller positive remainder yields a higher priority score.\n    base_scores = -potential_remaining_cap\n\n    # Determine which of the fitting bins are \"partially filled\" (not completely empty).\n    # A bin is considered partially filled if its current remaining capacity\n    # is less than the inferred full bin capacity (BIN_CAPACITY).\n    is_partially_filled_mask = (bins_remain_cap[can_fit_mask] < BIN_CAPACITY)\n\n    # Initialize bonuses for fitting bins to zero.\n    consolidation_bonuses = np.zeros_like(base_scores)\n\n    # Apply the consolidation bonus only to the fitting bins that are partially filled.\n    # This encourages using existing, non-empty bins before opening new ones.\n    consolidation_bonuses[is_partially_filled_mask] = CONSOLIDATION_BONUS\n\n    # The total priority for fitting bins is the sum of the base Best Fit score\n    # and the consolidation bonus.\n    priorities[can_fit_mask] = base_scores + consolidation_bonuses\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a dynamic consolidation bonus.\n    The bonus scales linearly with the bin's current fill level,\n    prioritizing fuller bins to enhance utilization and reduce total bin count.\n    \"\"\"\n    # A positive constant scaler for the dynamic consolidation bonus.\n    # This value determines the maximum potential bonus for a fully used bin.\n    # It should be small enough not to consistently override the primary Best Fit principle,\n    # but large enough to effectively encourage consolidation when Best Fit scores are close.\n    DYNAMIC_BONUS_SCALER = 0.01\n\n    # Initialize priorities for all bins to negative infinity.\n    # This ensures that bins where the item cannot fit will not be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Handle the edge case where there are no bins to consider.\n    if len(bins_remain_cap) == 0:\n        return priorities\n\n    # Infer the maximum capacity of a new, empty bin from the input array.\n    # This assumes that if a new bin can be opened, its full capacity\n    # is represented as the maximum value in `bins_remain_cap`.\n    BIN_CAPACITY = np.max(bins_remain_cap)\n    \n    # If BIN_CAPACITY is zero or negative (e.g., all bins are full or invalid),\n    # no item (with positive size) can fit, or there's no valid reference\n    # for calculating fill ratios. Priorities will remain -inf.\n    if BIN_CAPACITY <= 0:\n        return priorities\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return the priorities array with -np.inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity if the item were placed in each fitting bin.\n    # This is the core of the \"Best Fit\" strategy.\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Base score: The goal is to minimize the remaining capacity (maximize negative remaining capacity).\n    base_scores = -potential_remaining_cap\n\n    # Calculate the current filled amount of the bin before placing the item.\n    current_filled_amount = BIN_CAPACITY - bins_remain_cap[can_fit_mask]\n\n    # Clip current_filled_amount to ensure it's within valid bounds [0, BIN_CAPACITY].\n    # This guards against potential floating point errors or unusual input data.\n    current_filled_amount = np.clip(current_filled_amount, 0, BIN_CAPACITY)\n\n    # Calculate the fill ratio: how full the bin is currently (0 for empty, 1 for full).\n    fill_ratio = current_filled_amount / BIN_CAPACITY\n\n    # Apply a dynamic consolidation bonus: it's higher for bins that are already fuller.\n    # Empty bins (fill_ratio=0) will receive no bonus, naturally favoring Best Fit without bias.\n    consolidation_bonuses = DYNAMIC_BONUS_SCALER * fill_ratio\n\n    # The total priority for fitting bins is the sum of the base Best Fit score\n    # and the dynamic consolidation bonus.\n    priorities[can_fit_mask] = base_scores + consolidation_bonuses\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This implementation aims for a \"Hybrid Fit\" strategy, building upon Best Fit\n    by incorporating multi-objective considerations and non-linear scoring.\n    It prioritizes:\n    1.  **Perfect or Near-Perfect Fits:** Maximizing the chance of completely filling a bin\n        to reduce the total number of active bins and eliminate fragmentation.\n    2.  **Best Fit:** After perfect fits, it reverts to the standard Best Fit strategy\n        of minimizing the remaining capacity.\n    3.  **Fragment Avoidance:** It penalizes bins that, after placement, would be left\n        with a very small, \"awkward\" amount of space that is unlikely to be useful\n        for subsequent items (e.g., too small for most items, but not zero).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         Assumes capacities and item sizes are normalized or\n                         consistent in their units.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item, return -inf for all (or new bin logic handled externally)\n        return priorities\n\n    # Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # --- Tunable Parameters for the Hybrid Fit Heuristic ---\n    # These parameters are crucial for balancing the different objectives.\n    # They might require tuning based on the specific distribution of item sizes\n    # and bin capacity in your problem instance.\n\n    # EPSILON_PERFECT_FIT: A small tolerance for floating point comparisons to\n    # consider a bin \"perfectly\" filled. If remaining capacity is within this,\n    # it's considered a perfect fit.\n    EPSILON_PERFECT_FIT = 1e-9\n\n    # TINY_REMAINDER_THRESHOLD: The upper bound for what is considered a \"tiny\"\n    # or \"awkward\" remaining capacity. If a bin's remaining capacity falls\n    # between EPSILON_PERFECT_FIT and this threshold, it incurs a penalty.\n    # This value typically corresponds to a small fraction of the bin's total capacity.\n    # For example, if bin capacity is 1.0, 0.05 means 5% of capacity.\n    TINY_REMAINDER_THRESHOLD = 0.05\n\n    # PERFECT_FIT_BONUS: A large positive bonus applied to bins that achieve\n    # a perfect or near-perfect fit. This should be high enough to make perfect\n    # fits almost always the top priority.\n    PERFECT_FIT_BONUS = 1000.0\n\n    # TINY_REMAINDER_PENALTY: A significant negative penalty applied to bins\n    # that are left with a tiny, potentially unusable remainder. This pushes\n    # the algorithm away from creating such fragmented bins.\n    TINY_REMAINDER_PENALTY = 500.0\n\n    # --- Apply the scoring logic ---\n    # 1. Base Score: Best Fit (minimize remaining capacity)\n    #    A smaller positive remaining capacity results in a larger (less negative) base score.\n    current_priorities = -potential_remaining_cap\n\n    # 2. Perfect/Near-Perfect Fit Bonus\n    #    Identify bins where remaining capacity is very close to zero.\n    perfect_fit_mask = potential_remaining_cap <= EPSILON_PERFECT_FIT\n    current_priorities[perfect_fit_mask] += PERFECT_FIT_BONUS\n\n    # 3. Tiny Remainder Penalty\n    #    Identify bins where remaining capacity is small but not zero (a \"dead zone\").\n    #    Ensure these are not already marked as perfect fits.\n    tiny_remainder_mask = (potential_remaining_cap > EPSILON_PERFECT_FIT) & \\\n                          (potential_remaining_cap < TINY_REMAINDER_THRESHOLD)\n    current_priorities[tiny_remainder_mask] -= TINY_REMAINDER_PENALTY\n\n    # Assign the calculated priorities back to the main priorities array\n    priorities[can_fit_mask] = current_priorities\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Combines Best Fit with a progressive consolidation bonus.\n    Prioritizes tighter fits but slightly favors bins that are already more filled,\n    encouraging consolidation and balanced bin usage.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # 2. Base priority: Best Fit (minimize remaining capacity)\n    # A smaller potential_remaining_cap (closer to 0) means a tighter fit, which is preferred.\n    # By taking the negative, we convert minimization to maximization.\n    potential_remaining_cap = fitting_bins_remain_cap - item\n    base_priorities_fitting = -potential_remaining_cap\n\n    # 3. Consolidation Bonus: Encourage filling already used bins, scaled by their current fullness.\n    # This adapts the bonus based on how much a bin is already committed, promoting consolidation.\n    current_fullness_fitting = (bin_capacity - fitting_bins_remain_cap) / bin_capacity\n\n    # A small, controlled weight ensures this bonus doesn't override significantly better Best Fits.\n    consolidation_bonus_weight = 1e-4 # This parameter can be tuned for specific problem instances.\n\n    # The bonus is proportional to the bin's current fullness. Empty bins (fullness 0) get 0 bonus.\n    consolidation_bonus_fitting = current_fullness_fitting * consolidation_bonus_weight\n\n    # 4. Combine base Best Fit priority with the adaptive consolidation bonus\n    combined_priorities_fitting = base_priorities_fitting + consolidation_bonus_fitting\n\n    # 5. Assign calculated priorities back to the main priorities array\n    priorities[can_fit_mask] = combined_priorities_fitting\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which to add an item to each bin.\n\n    This implementation aims for a \"Target Remainder Fit\" strategy. It seeks to\n    assign an item to a bin such that the bin's remaining capacity after placement\n    is close to a predefined 'ideal' target value. This approach is more adaptive\n    and holistic than a simple Best Fit, as it manages the distribution of\n    remaining bin capacities, potentially leaving more \"useful\" space for\n    future items and promoting a more diverse set of bin states.\n\n    The 'ideal_remainder_ratio' parameter determines the target remaining capacity\n    as a fraction of the total bin capacity. A value of 0.0 would revert to a\n    pure Best Fit (minimizing remaining capacity). A value closer to 1.0 would\n    lean towards Worst Fit (maximizing remaining capacity). A moderate value\n    (e.g., 0.25) attempts to maintain a balanced bin state.\n\n    Assumptions:\n    1. The problem implies a fixed `BIN_CAPACITY` for all bins. Since it's not\n       an explicit argument, we infer it from `bins_remain_cap`. In online BPP,\n       new bins are typically added at full capacity. Therefore, `np.max(bins_remain_cap)`\n       is used as a reasonable proxy for `BIN_CAPACITY`, assuming at least one\n       bin is either empty (full capacity) or has the largest possible remaining capacity.\n       For extreme edge cases (e.g., all bins are almost full and no new empty bin has been opened yet),\n       this inference might be inaccurate, but it's a common practical approach.\n    2. `item` and `bins_remain_cap` values are in consistent units.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Handle case where no bins are available or bins_remain_cap is empty\n    if bins_remain_cap.size == 0:\n        return priorities\n\n    # Infer BIN_CAPACITY from the maximum remaining capacity among current bins.\n    # This assumes that at least one \"fresh\" bin (with full capacity) is or was available,\n    # or that the maximum value represents the standard bin capacity.\n    BIN_CAPACITY = np.max(bins_remain_cap)\n    if BIN_CAPACITY == 0: # Avoid division by zero if all bins are exactly 0 capacity\n        return priorities # No item can fit anywhere\n\n    # Define the ideal remaining capacity ratio after placing an item.\n    # This is a key heuristic parameter for tuning:\n    # 0.0 for Best Fit, ~0.2-0.3 for a more balanced \"middle-fit\".\n    ideal_remainder_ratio = 0.25\n    TARGET_REMAINDER = BIN_CAPACITY * ideal_remainder_ratio\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate priority based on the absolute difference from the TARGET_REMAINDER.\n    # We want to minimize this absolute difference, so we take its negative.\n    # This means bins whose `potential_remaining_cap` is closest to `TARGET_REMAINDER`\n    # will receive the highest priority.\n    priorities[can_fit_mask] = -np.abs(potential_remaining_cap - TARGET_REMAINDER)\n\n    return priorities",
    "response_id": 4,
    "tryHS": true,
    "obj": 13.901076984443566,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Combines Best Fit with an adaptive bonus for consolidating items into\n    already partially filled bins, encouraging finishing off existing bins.\n    \"\"\"\n    # This design prioritizes tight fits (Best Fit) and enhances consolidation.\n    # It introduces an adaptive bonus for already used bins, scaled by their current\n    # fill level, aiming to 'top off' existing bins and reduce the total bin count.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Only consider bins where the item fits\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_remain_cap.size == 0:\n        return priorities # No bins can fit the item\n\n    # 2. Calculate remaining capacity if the item were placed in fitting bins\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # 3. Base priority: Best Fit strategy (minimize remaining capacity)\n    # A smaller potential_remaining_cap (closer to 0) means a tighter fit,\n    # which is preferred. By taking the negative, we convert minimization to maximization.\n    base_priorities = -potential_remaining_cap\n\n    # 4. Adaptive Consolidation Bonus:\n    # Bonus for already partially filled bins, proportional to their current fill level.\n    # This encourages 'topping off' bins that are already more full.\n    consolidation_scaling_factor = 0.001 # A small constant, tuneable\n\n    # Identify bins that are not \"fresh\" (i.e., not entirely empty/unused).\n    is_used_bin_mask = ~np.isclose(fitting_bins_remain_cap, bin_capacity, atol=1e-9)\n\n    # Calculate the current \"fill level\" for fitting bins\n    current_fill_level = bin_capacity - fitting_bins_remain_cap\n\n    # Apply the adaptive bonus only to bins that can fit and are already used.\n    adaptive_bonus = np.zeros_like(base_priorities)\n    adaptive_bonus[is_used_bin_mask] = consolidation_scaling_factor * current_fill_level[is_used_bin_mask]\n\n    # Combine base priority (Best Fit) and adaptive consolidation bonus\n    priorities[can_fit_mask] = base_priorities + adaptive_bonus\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter15_response4.txt_stdout.txt",
    "code_path": "problem_iter15_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic combines \"Best Fit\" with a \"Bin Consolidation\" and\n    \"Fragmentation Avoidance\" strategy. It aims to achieve global efficacy\n    through local, non-linear interactions, adapting to the current item's size.\n\n    The priority calculation is composed of:\n    1.  **Best Fit Core:** Prioritizes bins that result in the smallest\n        remaining capacity after the item is placed. This is the foundational\n        linear component.\n    2.  **Exact Fit Bonus (Non-linear):** Provides a significant, discrete\n        bonus for bins where the item fits perfectly, leading to zero\n        remaining capacity. This encourages complete bin utilization and closure.\n    3.  **Fragmentation Penalty (Non-linear & Adaptive):** Applies a penalty\n        to bins that, after placing the item, would be left with a small,\n        non-zero remaining capacity. This penalty is particularly harsh if the\n        remaining capacity is less than or equal to the current `item`'s size,\n        discouraging the creation of fragmented space that might be difficult\n        to fill with future items of similar scale. The penalty scales with\n        how close the remainder is to the `item`'s size, pushing towards\n        either very small remainders or sufficiently large (useful) ones.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit (capacity >= item size)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Extract capacities for only the fitting bins\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # If no bin can fit the item, return priorities initialized to -inf\n    if fitting_bins_remain_cap.size == 0:\n        return priorities\n\n    # Calculate potential remaining capacity if the item were placed\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # --- Core Priority Calculation (Best Fit component) ---\n    # We negate the potential remaining capacity so that a smaller remainder\n    # results in a higher (less negative) priority score.\n    # Example: remaining 0.1 -> score -0.1; remaining 0.5 -> score -0.5.\n    calculated_priorities = -potential_remaining_cap\n\n    # --- Hybrid/Non-linear/Adaptive Components ---\n\n    # 1. Exact Fit Bonus: A strong, discrete non-linear bonus for perfect fits.\n    # This highly prioritizes bins that can be perfectly filled.\n    EXACT_FIT_THRESHOLD = 1e-9  # Tolerance for floating point comparisons to zero\n    EXACT_FIT_BONUS = 1000.0    # A large bonus to ensure exact fits are top priority\n\n    exact_fit_mask = np.isclose(potential_remaining_cap, 0.0, atol=EXACT_FIT_THRESHOLD)\n    calculated_priorities[exact_fit_mask] += EXACT_FIT_BONUS\n\n    # 2. Fragmentation Penalty: Penalize creating small, non-zero remnants.\n    # This aims to avoid \"awkward\" remaining capacities that are too small\n    # to be easily useful for typical future items, especially if they are\n    # a significant fraction of the current item's size.\n    \n    # Apply penalty only if the item size is positive to avoid division by zero.\n    # Item sizes in BPP are typically positive.\n    if item > EXACT_FIT_THRESHOLD:\n        # Define the \"fragmentation zone\": remaining capacities that are\n        # non-zero but less than or equal to the current item's size.\n        # This range is problematic as it's not an exact fit, but also not\n        # large enough to easily accommodate another item of the same size.\n        fragment_zone_mask = (potential_remaining_cap > EXACT_FIT_THRESHOLD) & \\\n                             (potential_remaining_cap <= item)\n\n        if np.any(fragment_zone_mask):\n            # Normalize the remaining capacity within this zone by the item's size.\n            # This makes the penalty adaptive to the scale of the current item.\n            normalized_fragment_rem = potential_remaining_cap[fragment_zone_mask] / item\n\n            # Apply a penalty that increases as the normalized remainder\n            # approaches 1 (i.e., remaining capacity is close to item's size).\n            # This strongly discourages leaving a bin with a capacity just\n            # slightly less than the item, effectively making it a \"dead space\".\n            # PENALTY_FACTOR is a tunable parameter controlling the penalty's strength.\n            PENALTY_FACTOR = 5.0 # Example: A factor of 5.0\n\n            penalty = -PENALTY_FACTOR * normalized_fragment_rem\n            calculated_priorities[fragment_zone_mask] += penalty\n\n    # Assign the calculated priorities to the fitting bins in the main array\n    priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.038691663342641,
    "SLOC": 22.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter16_response0.txt_stdout.txt",
    "code_path": "problem_iter16_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, ideal_remainder_ratio: float = 0.31342951204947844) -> np.ndarray:\n    \"\"\"Returns priority with which to add an item to each bin.\n\n    This implementation aims for a \"Target Remainder Fit\" strategy. It seeks to\n    assign an item to a bin such that the bin's remaining capacity after placement\n    is close to a predefined 'ideal' target value. This approach is more adaptive\n    and holistic than a simple Best Fit, as it manages the distribution of\n    remaining bin capacities, potentially leaving more \"useful\" space for\n    future items and promoting a more diverse set of bin states.\n\n    The 'ideal_remainder_ratio' parameter determines the target remaining capacity\n    as a fraction of the total bin capacity. A value of 0.0 would revert to a\n    pure Best Fit (minimizing remaining capacity). A value closer to 1.0 would\n    lean towards Worst Fit (maximizing remaining capacity). A moderate value\n    (e.g., 0.25) attempts to maintain a balanced bin state.\n\n    Assumptions:\n    1. The problem implies a fixed `BIN_CAPACITY` for all bins. Since it's not\n       an explicit argument, we infer it from `bins_remain_cap`. In online BPP,\n       new bins are typically added at full capacity. Therefore, `np.max(bins_remain_cap)`\n       is used as a reasonable proxy for `BIN_CAPACITY`, assuming at least one\n       bin is either empty (full capacity) or has the largest possible remaining capacity.\n       For extreme edge cases (e.g., all bins are almost full and no new empty bin has been opened yet),\n       this inference might be inaccurate, but it's a common practical approach.\n    2. `item` and `bins_remain_cap` values are in consistent units.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of current remaining capacities for each bin.\n        ideal_remainder_ratio: A heuristic parameter (between 0.0 and 1.0)\n                               determining the target remaining capacity as a\n                               fraction of the total bin capacity.\n                               0.0 for Best Fit, ~0.2-0.3 for a more balanced \"middle-fit\".\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Handle case where no bins are available or bins_remain_cap is empty\n    if bins_remain_cap.size == 0:\n        return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 4.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter18_response1.txt_stdout.txt",
    "code_path": "problem_iter18_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns a priority score for each bin, incorporating advanced heuristics\n    for online Bin Packing.\n\n    This heuristic combines \"Best Fit\" with an aggressively incentivized\n    \"Bin Completion\" strategy, a sophisticated \"Fragmentation Avoidance\"\n    (targeting a \"Valley of Despair\"), and a \"Quality of Large Space\" bonus.\n    It adapts non-linearly to the current item's size to optimize space utilization.\n\n    The priority calculation is composed of:\n    1.  **Core Best Fit Principle:** Bins with smaller remaining capacity after\n        placement get a base higher priority.\n    2.  **Exact Fit / Bin Completion Bonus (Aggressive Non-linear):**\n        Applies a very high and sharply decaying exponential bonus for bins\n        where the item fits perfectly or near-perfectly (remaining capacity\n        is very close to zero). This strongly encourages closing bins efficiently.\n    3.  **Fragmentation Penalty (\"Valley of Despair\" - Non-linear & Adaptive):**\n        Introduces a significant penalty for bins that, after placing the item,\n        would be left with a non-zero, \"awkward\" amount of remaining capacity.\n        This penalty is shaped like an inverted Gaussian curve, being harshest\n        for mid-range remainders (e.g., 30-50% of the item's size) and tapering\n        off for very small (near zero) or larger remainders. This discourages\n        creating fragmented spaces that are neither useful for larger items\n        nor small enough to be easily ignored or filled by tiny items.\n    4.  **Quality of Large Remaining Space Bonus (Logarithmic & Adaptive):**\n        Provides a moderate, logarithmically increasing bonus for bins that are\n        left with a substantial amount of free capacity (e.g., more than double\n        the current item's size). This incentivizes keeping bins with genuinely\n        useful large spaces available for future large items, promoting overall\n        bin utility rather than just minimal remaining space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins where the item cannot fit will have a priority of -np.inf.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Tolerance for floating point comparisons to avoid issues with near-zero values\n    TOLERANCE_EPS = 1e-9\n\n    # Mask for bins where the item can fit (capacity >= item size, with a small tolerance)\n    can_fit_mask = bins_remain_cap >= item - TOLERANCE_EPS\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # If no bin can fit the item, return priorities initialized to -inf\n    if fitting_bins_remain_cap.size == 0:\n        return priorities\n\n    # Calculate potential remaining capacity if the item were placed\n    potential_remaining_cap = fitting_bins_remain_cap - item\n    # Ensure no negative remainders due to floating point inaccuracies when item == capacity\n    potential_remaining_cap[potential_remaining_cap < 0] = 0.0\n\n    # --- Core Priority Calculation (Enhanced Best Fit / Space Quality) ---\n    # Base: The Best Fit principle inherently prefers smaller remaining capacities.\n    # We use a negative linear relationship initially.\n    calculated_priorities = -potential_remaining_cap\n\n    # --- Non-linear & Adaptive Components ---\n\n    # 1. Exact Fit / Bin Completion Bonus:\n    # A very strong, rapidly decaying exponential bonus for near-perfect fits.\n    # This ensures that bins resulting in almost zero remaining capacity are highly prioritized.\n    EXACT_FIT_BONUS_MAGNITUDE = 5000.0  # High magnitude to make exact fits dominant\n    EXACT_FIT_DECAY_RATE = 50.0         # High decay rate for a very sharp peak at 0\n    \n    exact_fit_bonus = EXACT_FIT_BONUS_MAGNITUDE * np.exp(-EXACT_FIT_DECAY_RATE * potential_remaining_cap)\n    calculated_priorities += exact_fit_bonus\n\n    # 2. Fragmentation Penalty (\"Valley of Despair\"):\n    # Penalizes bins that would be left with an \"awkward\" non-zero remaining capacity.\n    # This penalty uses a negative Gaussian (bell curve) shape, peaking for remainders\n    # that are a problematic fraction of the item's size (e.g., 30-50%).\n    \n    if item > TOLERANCE_EPS: # Only apply if item size is meaningful for relative calculations\n        # Define the range of remaining capacities to consider for fragmentation penalties.\n        # This covers non-zero remainders up to 1.5 times the item's size.\n        fragment_consideration_mask = (potential_remaining_cap > TOLERANCE_EPS) & \\\n                                      (potential_remaining_cap <= 1.5 * item)\n\n        if np.any(fragment_consideration_mask):\n            # Normalize the remaining capacity by the item's size for adaptive scaling.\n            normalized_fragment_rem = potential_remaining_cap[fragment_consideration_mask] / item\n\n            # Parameters for the Gaussian penalty curve. These are tunable.\n            FRAGMENT_PENALTY_PEAK_RATIO = 0.4 # Peak penalty when remaining capacity is 40% of item size\n            FRAGMENT_PENALTY_STD_DEV = 0.2    # Standard deviation: controls the width of the penalty zone\n            PENALTY_MAGNITUDE = 100.0         # Maximum strength of the fragmentation penalty\n\n            # Calculate the Gaussian penalty. It's negative, so it subtracts from priority.\n            # `exp(-(x-mu)^2 / (2*sigma^2))`\n            penalty = -PENALTY_MAGNITUDE * np.exp(\n                -((normalized_fragment_rem - FRAGMENT_PENALTY_PEAK_RATIO)**2) / (2 * FRAGMENT_PENALTY_STD_DEV**2)\n            )\n            calculated_priorities[fragment_consideration_mask] += penalty\n\n    # 3. Quality of Large Remaining Space Bonus:\n    # Provides a bonus for bins that maintain a substantial amount of useful free capacity\n    # after the item is placed (e.g., more than double the current item's size).\n    # This encourages keeping bins with significant \"potential\" for future large items.\n    \n    if item > TOLERANCE_EPS:\n        # Define \"large enough\" remaining capacity relative to the item size.\n        LARGE_REM_THRESHOLD_MULTIPLE = 2.0\n        \n        large_rem_mask = potential_remaining_cap > (LARGE_REM_THRESHOLD_MULTIPLE * item)\n\n        if np.any(large_rem_mask):\n            # The bonus increases logarithmically with the ratio of remaining capacity\n            # to the threshold. Logarithmic growth provides diminishing returns for\n            # extremely large remaining spaces, preventing them from dominating excessively.\n            LARGE_REM_BONUS_FACTOR = 20.0 # Moderate bonus strength\n\n            # Scale the argument for log1p to ensure positive values and manage sensitivity.\n            # Cap the argument to avoid excessively large bonus if item is tiny and remaining is huge.\n            scaled_log_arg = potential_remaining_cap[large_rem_mask] / (item * LARGE_REM_THRESHOLD_MULTIPLE)\n            log_bonus_amount = LARGE_REM_BONUS_FACTOR * np.log1p(np.minimum(scaled_log_arg, 100.0)) # Cap scaled_arg at 100\n\n            calculated_priorities[large_rem_mask] += log_bonus_amount\n\n    # Assign the calculated priorities back to the original array for fitting bins\n    priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.0043877143996833,
    "SLOC": 45.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter20_response7.txt_stdout.txt",
    "code_path": "problem_iter20_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins combining best fit, aggressive exponential near-fit bonuses,\n    a nuanced Gaussian (valley of despair) fragmentation penalty, and a bonus for\n    leaving large useful space. Aims for global efficacy by intelligently managing bin states.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit (capacity >= item size)\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return priorities initialized to -inf\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Extract capacities for only the fitting bins\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate potential remaining capacity if the item were placed\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # --- Core Priority Calculation (Best Fit component) ---\n    # We negate the potential remaining capacity so that a smaller remainder\n    # results in a higher (less negative) priority score.\n    calculated_priorities = -potential_remaining_cap\n\n    # --- Hybrid/Non-linear/Adaptive Components ---\n\n    # Tolerance for floating point comparisons to zero\n    EXACT_FIT_THRESHOLD = 1e-9\n\n    # 1. Aggressive Near-Exact Fit Bonus (Exponential Decay):\n    # This provides a very strong, non-linear incentive for bins that can be\n    # almost perfectly filled, with the bonus decaying rapidly as remainder grows.\n    NEAR_EXACT_FIT_BONUS_MAGNITUDE = 5000.0  # Max bonus for perfect fit\n    NEAR_EXACT_FIT_BONUS_DECAY = 50.0       # Steepness of the exponential decay\n\n    # Apply this bonus for very small non-zero remainders (e.g., less than 10% of item size)\n    near_exact_mask = (potential_remaining_cap > EXACT_FIT_THRESHOLD) & \\\n                      (potential_remaining_cap < 0.1 * item) # Or a fixed small absolute threshold\n\n    if np.any(near_exact_mask):\n        # Bonus: A * exp(-B * remainder)\n        bonus = NEAR_EXACT_FIT_BONUS_MAGNITUDE * np.exp(-NEAR_EXACT_FIT_BONUS_DECAY * potential_remaining_cap[near_exact_mask])\n        calculated_priorities[near_exact_mask] += bonus\n\n    # For perfect fits (zero remainder), assign highest possible priority (infinity).\n    # This ensures exact fits are always chosen first.\n    perfect_fit_mask = np.isclose(potential_remaining_cap, 0.0, atol=EXACT_FIT_THRESHOLD)\n    if np.any(perfect_fit_mask):\n        calculated_priorities[perfect_fit_mask] = np.inf\n\n    # 2. Gaussian Fragmentation Penalty (\"Valley of Despair\"):\n    # Applies a strong, non-linear penalty for leaving \"awkward\" small-to-medium\n    # remnants that are difficult to fill. The penalty peaks around a specific\n    # problematic remainder size, creating a \"valley of despair\".\n    if item > EXACT_FIT_THRESHOLD: # Penalty only relevant for positive item sizes\n        GAUSSIAN_PENALTY_AMPLITUDE = 200.0  # Strength of the penalty\n        # The remainder size (as a ratio of item size) that is most undesirable.\n        GAUSSIAN_PENALTY_CENTER_RATIO = 0.4 # E.g., 40% of item's size\n        # Standard deviation of the Gaussian, controls the \"width\" of the valley.\n        GAUSSIAN_PENALTY_SPREAD = 0.1 * item # Adaptive to item size\n\n        # Define the range where this penalty is relevant (non-zero up to 1.5 * item)\n        penalty_zone_mask = (potential_remaining_cap > EXACT_FIT_THRESHOLD) & \\\n                            (potential_remaining_cap <= 1.5 * item)\n\n        if np.any(penalty_zone_mask):\n            # Calculate the \"target\" (undesirable) remainder size\n            target_fragment_remainder = GAUSSIAN_PENALTY_CENTER_RATIO * item\n\n            # Squared difference from the target fragment remainder\n            diff_sq = (potential_remaining_cap[penalty_zone_mask] - target_fragment_remainder)**2\n\n            # Apply a negative Gaussian penalty: -A * exp(-(x - mu)^2 / (2 * sigma^2))\n            # Add a small epsilon to spread to avoid division by zero for extremely small items.\n            penalty = -GAUSSIAN_PENALTY_AMPLITUDE * np.exp(-diff_sq / (2 * (GAUSSIAN_PENALTY_SPREAD + EXACT_FIT_THRESHOLD)**2))\n            calculated_priorities[penalty_zone_mask] += penalty\n\n    # 3. Useful Large Space Bonus (Logarithmic):\n    # Incentivizes leaving a bin with a significantly large remaining capacity\n    # (e.g., enough for another item of similar size). This encourages maintaining\n    # flexible bins that can accommodate future larger items, with diminishing returns.\n    USEFUL_SPACE_BONUS_FACTOR = 10.0 # Magnitude of this bonus\n    MIN_USEFUL_SPACE_FOR_BONUS_RATIO = 1.0 # Remainder must be at least 'item' size\n\n    # Mask for remaining capacities that are considered \"large and useful\"\n    large_useful_mask = potential_remaining_cap >= MIN_USEFUL_SPACE_FOR_BONUS_RATIO * item\n\n    if np.any(large_useful_mask):\n        # Logarithmic bonus: scales with remaining capacity but with diminishing returns.\n        # Use log1p (log(1+x)) for numerical stability.\n        bonus = USEFUL_SPACE_BONUS_FACTOR * np.log1p(potential_remaining_cap[large_useful_mask])\n        calculated_priorities[large_useful_mask] += bonus\n\n    # Assign the calculated priorities to the fitting bins in the main array\n    priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 1.9944156362185879,
    "SLOC": 27.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter21_response3.txt_stdout.txt",
    "code_path": "problem_iter21_code3.py",
    "code": "import numpy as np\n\n# Assume BIN_CAPACITY is a constant for the problem.\n# In typical normalized Bin Packing Problems, bin capacity is 1.0.\n# If bins can have different initial capacities, this constant would need\n# to be a parameter or derived for each bin, making the problem more complex.\nBIN_CAPACITY = 1.0 \n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using a sophisticated hybrid approach, building upon v1\n    with enhanced adaptivity, multi-zone penalties, and strategic space management.\n\n    Key improvements over v1:\n    - More aggressive and adaptive near-exact fit bonus.\n    - Multi-zone fragmentation penalty:\n        - A primary Gaussian 'valley of despair' (mid-range fragments).\n        - A sharp 'dust' penalty for tiny, unusable remainders.\n    - Strategic 'quality of space' bonus: uses a sigmoid to incentivize\n      maintaining versatile large capacities for future large items,\n      with a smoother and more controlled effect.\n    - Parameters are more dynamically linked to the 'item' size or BIN_CAPACITY,\n      making the heuristic more adaptive.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit (capacity >= item size)\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return priorities initialized to -inf\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Extract capacities for only the fitting bins\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Calculate potential remaining capacity if the item were placed\n    potential_remaining_cap = fitting_bins_remain_cap - item\n\n    # --- Core Priority Calculation (Best Fit component) ---\n    # We negate the potential remaining capacity so that a smaller remainder\n    # results in a higher (less negative) priority score. This is the base for Best-Fit.\n    calculated_priorities = -potential_remaining_cap\n\n    # --- Hybrid/Non-linear/Adaptive Components ---\n\n    # Tolerance for floating point comparisons to zero\n    EXACT_FIT_THRESHOLD = 1e-9\n\n    # 1. Enhanced Completion Bonus:\n    #    a. Perfect Fit: Assigns the highest possible priority (infinity).\n    #       Ensures perfect fits are always chosen first.\n    perfect_fit_mask = np.isclose(potential_remaining_cap, 0.0, atol=EXACT_FIT_THRESHOLD)\n    if np.any(perfect_fit_mask):\n        calculated_priorities[perfect_fit_mask] = np.inf\n\n    #    b. Aggressive Near-Exact Fit (Exponential Decay):\n    #       Provides a very strong, non-linear incentive for bins that can be\n    #       almost perfectly filled, with the bonus decaying rapidly as remainder grows.\n    #       Magnitude and decay are increased and made adaptive to item size.\n    NEAR_EXACT_FIT_BONUS_MAGNITUDE = 10000.0  # Increased maximum bonus\n    # Decay factor scales inversely with item size and direct with BIN_CAPACITY for higher sensitivity\n    NEAR_EXACT_FIT_BONUS_DECAY = 150.0 / (item + EXACT_FIT_THRESHOLD) * BIN_CAPACITY \n\n    # Apply this bonus for very small non-zero remainders. Threshold is adaptive.\n    near_exact_mask = (potential_remaining_cap > EXACT_FIT_THRESHOLD) & \\\n                      (potential_remaining_cap < max(0.05 * item, 0.01 * BIN_CAPACITY))\n\n    if np.any(near_exact_mask):\n        # Bonus: A * exp(-B * remainder)\n        bonus = NEAR_EXACT_FIT_BONUS_MAGNITUDE * np.exp(-NEAR_EXACT_FIT_BONUS_DECAY * potential_remaining_cap[near_exact_mask])\n        calculated_priorities[near_exact_mask] += bonus\n\n    # 2. Multi-Zone Adaptive Fragmentation Penalty:\n    #    Applies strong, non-linear penalties for leaving \"awkward\" remnants.\n    #    Distinguishes between mid-range frustrating fragments and tiny, unusable \"dust\".\n    if item > EXACT_FIT_THRESHOLD: \n        # a. Primary Gaussian Penalty (\"Valley of Despair\"):\n        #    Penalizes mid-range fragments that are too small to be generally useful\n        #    but too large to be considered \"full\". Amplitude made stronger for larger items.\n        GAUSSIAN_PENALTY_AMPLITUDE = 300.0 * (1.0 + item / BIN_CAPACITY) # Stronger penalty for larger items\n        GAUSSIAN_PENALTY_CENTER_RATIO = 0.35 # The undesirable remainder size is 35% of the item's size\n        GAUSSIAN_PENALTY_SPREAD = 0.1 * item # Adaptive spread based on item size\n\n        # Define the range where this Gaussian penalty is most relevant\n        penalty_zone_mask_gaussian = (potential_remaining_cap > max(0.05 * item, 0.02 * BIN_CAPACITY)) & \\\n                                     (potential_remaining_cap < 0.7 * BIN_CAPACITY) # Avoid applying to extremely small or very large remainders\n\n        if np.any(penalty_zone_mask_gaussian):\n            target_fragment_remainder = GAUSSIAN_PENALTY_CENTER_RATIO * item\n            diff_sq = (potential_remaining_cap[penalty_zone_mask_gaussian] - target_fragment_remainder)**2\n            # Apply a negative Gaussian penalty: -A * exp(-(x - mu)^2 / (2 * sigma^2))\n            penalty = -GAUSSIAN_PENALTY_AMPLITUDE * np.exp(-diff_sq / (2 * (GAUSSIAN_PENALTY_SPREAD + EXACT_FIT_THRESHOLD)**2))\n            calculated_priorities[penalty_zone_mask_gaussian] += penalty\n\n        # b. \"Dust\" / Micro-Fragment Penalty:\n        #    A very sharp penalty for creating tiny, non-zero remainders that are too small\n        #    to be useful for any reasonable future item (\"crumbs\" or \"dust\").\n        DUST_PENALTY_MAGNITUDE = 800.0 # High magnitude for sharp penalty\n        # The maximum remainder size to be considered \"dust\". Adaptive, but with a small absolute max.\n        dust_threshold = min(0.05 * item, 0.03 * BIN_CAPACITY) \n        \n        dust_mask = (potential_remaining_cap > EXACT_FIT_THRESHOLD) & \\\n                    (potential_remaining_cap <= dust_threshold)\n\n        if np.any(dust_mask):\n            # Penalize inverse of remainder to be very high for tiny remainders.\n            # Add small epsilon to avoid division by zero.\n            penalty = -DUST_PENALTY_MAGNITUDE / (potential_remaining_cap[dust_mask] + EXACT_FIT_THRESHOLD)\n            calculated_priorities[dust_mask] += penalty\n\n    # 3. Strategic 'Quality of Space' Bonus (Sigmoid-modulated):\n    #    Incentivizes leaving a bin with a significantly large and versatile remaining capacity.\n    #    Uses a sigmoid function to provide a smooth, increasing bonus for larger useful spaces,\n    #    encouraging the maintenance of flexible bins for future larger items.\n    STRATEGIC_SPACE_BONUS_MAGNITUDE = 80.0 # Max bonus for leaving large useful space\n    # The threshold at which the bonus starts becoming significant (as ratio of BIN_CAPACITY)\n    STRATEGIC_SPACE_BONUS_THRESHOLD_RATIO = 0.5 \n    # Steepness of the sigmoid curve. Higher means sharper transition.\n    STRATEGIC_SPACE_BONUS_STEEPNESS = 12.0 \n\n    # Criteria for \"useful\" space: at least a certain percentage of bin capacity,\n    # or larger than the current item itself if item is small.\n    useful_space_criteria = max(STRATEGIC_SPACE_BONUS_THRESHOLD_RATIO * BIN_CAPACITY, item * 1.0)\n    \n    strategic_space_mask = potential_remaining_cap >= useful_space_criteria\n\n    if np.any(strategic_space_mask):\n        # Normalize potential remaining capacity and threshold by BIN_CAPACITY for sigmoid input\n        x_scaled = potential_remaining_cap[strategic_space_mask] / BIN_CAPACITY\n        x0_scaled = useful_space_criteria / BIN_CAPACITY\n\n        # Apply logistic sigmoid function: 1 / (1 + exp(-k * (x - x0)))\n        sigmoid_output = 1 / (1 + np.exp(-STRATEGIC_SPACE_BONUS_STEEPNESS * (x_scaled - x0_scaled)))\n        \n        # Scale the bonus by the sigmoid output\n        bonus = STRATEGIC_SPACE_BONUS_MAGNITUDE * sigmoid_output\n        calculated_priorities[strategic_space_mask] += bonus\n\n    # Assign the calculated priorities back to the fitting bins in the main array\n    priorities[can_fit_mask] = calculated_priorities\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 2.2038292780215536,
    "SLOC": 50.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  }
]