{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns a priority score for each bin, incorporating advanced heuristics\n    for online Bin Packing.\n\n    This heuristic combines \"Best Fit\" with an aggressively incentivized\n    \"Bin Completion\" strategy, a sophisticated \"Fragmentation Avoidance\"\n    (targeting a \"Valley of Despair\"), and a \"Quality of Large Space\" bonus.\n    It adapts non-linearly to the current item's size to optimize space utilization.\n\n    The priority calculation is composed of:\n    1.  **Core Best Fit Principle:** Bins with smaller remaining capacity after\n        placement get a base higher priority.\n    2.  **Exact Fit / Bin Completion Bonus (Aggressive Non-linear):**\n        Applies a very high and sharply decaying exponential bonus for bins\n        where the item fits perfectly or near-perfectly (remaining capacity\n        is very close to zero). This strongly encourages closing bins efficiently.\n    3.  **Fragmentation Penalty (\"Valley of Despair\" - Non-linear & Adaptive):**\n        Introduces a significant penalty for bins that, after placing the item,\n        would be left with a non-zero, \"awkward\" amount of remaining capacity.\n        This penalty is shaped like an inverted Gaussian curve, being harshest\n        for mid-range remainders (e.g., 30-50% of the item's size) and tapering\n        off for very small (near zero) or larger remainders. This discourages\n        creating fragmented spaces that are neither useful for larger items\n        nor small enough to be easily ignored or filled by tiny items.\n    4.  **Quality of Large Remaining Space Bonus (Logarithmic & Adaptive):**\n        Provides a moderate, logarithmically increasing bonus for bins that are\n        left with a substantial amount of free capacity (e.g., more than double\n        the current item's size). This incentivizes keeping bins with genuinely\n        useful large spaces available for future large items, promoting overall\n        bin utility rather than just minimal remaining space.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Bins where the item cannot fit will have a priority of -np.inf.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Tolerance for floating point comparisons to avoid issues with near-zero values\n    TOLERANCE_EPS = 1e-9\n\n    # Mask for bins where the item can fit (capacity >= item size, with a small tolerance)\n    can_fit_mask = bins_remain_cap >= item - TOLERANCE_EPS\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # If no bin can fit the item, return priorities initialized to -inf\n    if fitting_bins_remain_cap.size == 0:\n        return priorities\n\n    # Calculate potential remaining capacity if the item were placed\n    potential_remaining_cap = fitting_bins_remain_cap - item\n    # Ensure no negative remainders due to floating point inaccuracies when item == capacity\n    potential_remaining_cap[potential_remaining_cap < 0] = 0.0\n\n    # --- Core Priority Calculation (Enhanced Best Fit / Space Quality) ---\n    # Base: The Best Fit principle inherently prefers smaller remaining capacities.\n    # We use a negative linear relationship initially.\n    calculated_priorities = -potential_remaining_cap\n\n    # --- Non-linear & Adaptive Components ---\n\n    # 1. Exact Fit / Bin Completion Bonus:\n    # A very strong, rapidly decaying exponential bonus for near-perfect fits.\n    # This ensures that bins resulting in almost zero remaining capacity are highly prioritized.\n    EXACT_FIT_BONUS_MAGNITUDE = 5000.0  # High magnitude to make exact fits dominant\n    EXACT_FIT_DECAY_RATE = 50.0         # High decay rate for a very sharp peak at 0\n    \n    exact_fit_bonus = EXACT_FIT_BONUS_MAGNITUDE * np.exp(-EXACT_FIT_DECAY_RATE * potential_remaining_cap)\n    calculated_priorities += exact_fit_bonus\n\n    # 2. Fragmentation Penalty (\"Valley of Despair\"):\n    # Penalizes bins that would be left with an \"awkward\" non-zero remaining capacity.\n    # This penalty uses a negative Gaussian (bell curve) shape, peaking for remainders\n    # that are a problematic fraction of the item's size (e.g., 30-50%).\n    \n    if item > TOLERANCE_EPS: # Only apply if item size is meaningful for relative calculations\n        # Define the range of remaining capacities to consider for fragmentation penalties.\n        # This covers non-zero remainders up to 1.5 times the item's size.\n        fragment_consideration_mask = (potential_remaining_cap > TOLERANCE_EPS) & \\\n                                      (potential_remaining_cap <= 1.5 * item)\n\n        if np.any(fragment_consideration_mask):\n            # Normalize the remaining capacity by the item's size for adaptive scaling.\n            normalized_fragment_rem = potential_remaining_cap[fragment_consideration_mask] / item\n\n            # Parameters for the Gaussian penalty curve. These are tunable.\n            FRAGMENT_PENALTY_PEAK_RATIO = 0.4 # Peak penalty when remaining capacity is 40% of item size\n            FRAGMENT_PENALTY_STD_DEV = 0.2    # Standard deviation: controls the width of the penalty zone\n            PENALTY_MAGNITUDE = 100.0         # Maximum strength of the fragmentation penalty\n\n            # Calculate the Gaussian penalty. It's negative, so it subtracts from priority.\n            # `exp(-(x-mu)^2 / (2*sigma^2))`\n            penalty = -PENALTY_MAGNITUDE * np.exp(\n                -((normalized_fragment_rem - FRAGMENT_PENALTY_PEAK_RATIO)**2) / (2 * FRAGMENT_PENALTY_STD_DEV**2)\n            )\n            calculated_priorities[fragment_consideration_mask] += penalty\n\n    # 3. Quality of Large Remaining Space Bonus:\n    # Provides a bonus for bins that maintain a substantial amount of useful free capacity\n    # after the item is placed (e.g., more than double the current item's size).\n    # This encourages keeping bins with significant \"potential\" for future large items.\n    \n    if item > TOLERANCE_EPS:\n        # Define \"large enough\" remaining capacity relative to the item size.\n        LARGE_REM_THRESHOLD_MULTIPLE = 2.0\n        \n        large_rem_mask = potential_remaining_cap > (LARGE_REM_THRESHOLD_MULTIPLE * item)\n\n        if np.any(large_rem_mask):\n            # The bonus increases logarithmically with the ratio of remaining capacity\n            # to the threshold. Logarithmic growth provides diminishing returns for\n            # extremely large remaining spaces, preventing them from dominating excessively.\n            LARGE_REM_BONUS_FACTOR = 20.0 # Moderate bonus strength\n\n            # Scale the argument for log1p to ensure positive values and manage sensitivity.\n            # Cap the argument to avoid excessively large bonus if item is tiny and remaining is huge.\n            scaled_log_arg = potential_remaining_cap[large_rem_mask] / (item * LARGE_REM_THRESHOLD_MULTIPLE)\n            log_bonus_amount = LARGE_REM_BONUS_FACTOR * np.log1p(np.minimum(scaled_log_arg, 100.0)) # Cap scaled_arg at 100\n\n            calculated_priorities[large_rem_mask] += log_bonus_amount\n\n    # Assign the calculated priorities back to the original array for fitting bins\n    priorities[can_fit_mask] = calculated_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority for 'Best Fit' by maximizing the effective filled capacity.\n\n    Prioritizes bins that achieve the highest fill level after placing the item,\n    yielding positive scores for valid fits and penalizing impossible ones.\n    \"\"\"\n    # Initialize all priorities to an extremely low value, ensuring bins that cannot\n    # accommodate the item are never selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify which bins possess sufficient remaining capacity for the item.\n    fits_mask = bins_remain_cap >= item\n\n    # For bins where the item demonstrably fits, calculate a priority score.\n    # This score, 2 * item - bins_remain_cap[fits_mask], maximizes the resulting\n    # effective filled capacity relative to the item's size. A perfect fit\n    # (where the bin's remaining capacity becomes zero) yields the highest positive\n    # score (equal to `item`), while less efficient fits yield lower positive scores.\n    # This combines the efficiency of masking first with a positive-scaled Best Fit score.\n    priorities[fits_mask] = 2 * item - bins_remain_cap[fits_mask]\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (4th), we see a significant jump in heuristic sophistication. The top-ranked (1st) introduces an \"aggressively incentivized\" exponential exact-fit bonus (magnitude 5000.0, decay 50.0), a \"Valley of Despair\" fragmentation penalty shaped like a negative Gaussian curve (peaking at 40% of item size), and a \"Quality of Large Remaining Space Bonus\" (logarithmic). In contrast, (4th) uses a discrete exact fit bonus (1000.0) and a simpler linear fragmentation penalty (proportional to normalized remainder, up to item size). The non-linear, adaptive components in (1st) provide far more nuanced control over bin states, encouraging aggressive bin closure and intelligent space management beyond simple minimization.\n\nComparing (4th) vs (8th), the fragmentation penalty shifts from a linearly scaled penalty based on `normalized_fragment_rem` (in 4th) to a fixed `TINY_REMAINDER_PENALTY` applied if the remainder is below `TINY_REMAINDER_THRESHOLD` (in 8th). The exact fit bonus is discrete and high in both (1000.0). The explicit threshold-based penalty in (8th) is simpler but less adaptive than the item-size-normalized linear penalty in (4th), which is itself less sophisticated than the Gaussian penalty in (1st). The higher performance of (4th) suggests its fragmentation handling, though linear, is more effective than a simple \"tiny remainder\" threshold.\n\nComparing (8th) vs (9th), (8th) employs a \"Hybrid Fit\" with a \"Perfect/Near-Perfect Fit Bonus\" and a \"Tiny Remainder Penalty\", while (9th) is a pure \"Best Fit\" strategy, prioritizing only the smallest remaining capacity. The clear improvement from (9th) to (8th) highlights the critical importance of explicitly incentivizing bin completion and penalizing awkward fragment creation.\n\nComparing (9th) vs (13th), both start with a \"Best Fit\" core. (13th) adds a small `used_bin_bonus` (1e-6) to prefer existing, partially filled bins over new ones. This subtle consolidation preference in (13th) leads to better performance than plain Best Fit (9th), indicating that even a minor bias towards existing bins can be beneficial for reducing the total bin count.\n\nComparing (13th) vs (14th), both aim for \"Best Fit\" plus a \"Consolidation Bonus\". (13th) explicitly takes `bin_capacity` as an argument and uses a very small `used_bin_bonus` of 1e-6. (14th) infers `BIN_CAPACITY` from `np.max(bins_remain_cap)` and uses a larger `CONSOLIDATION_BONUS` of 0.01. The stronger consolidation bias and adaptive `BIN_CAPACITY` inference in (14th) likely contribute to its better performance.\n\nComparing (14th) vs (20th), (14th) utilizes Best Fit plus a consolidation bonus, whereas (20th) implements a \"Target Remainder Fit\", aiming to leave a specific `ideal_remainder_ratio` (hardcoded 0.25) of the bin capacity. The higher ranking of (14th) suggests that a combination of Best Fit and consolidation is generally more robust and effective than solely targeting an ideal remainder, which might lead to suboptimal packing if the chosen target isn't universally beneficial across different item sizes.\n\nComparing (20th) vs (16th), (20th) attempts a \"Target Remainder Fit\" by minimizing the absolute difference from a target remaining capacity. In stark contrast, (16th) simply returns `np.zeros_like`, assigning equal priority to all fitting bins, which is equivalent to an arbitrary selection (e.g., First Fit). The functional strategy in (20th), despite its specific approach, vastly outperforms the null strategy of (16th).\n\nComparing (1st/2nd/3rd) with each other, they are identical code snippets. The same applies to (4th/5th/6th), (7th/10th), (8th/11th), (9th/12th), (14th/15th), (16th/17th), and (18th/19th). This suggests that the ranking differences within these identical groups are negligible or due to external factors not present in the code.\n\nOverall: The best heuristics employ a multi-faceted approach, balancing immediate best fit with future bin state management. This includes strong incentives for bin completion, nuanced penalties for fragmentation, and strategies for maintaining useful large spaces. Non-linear, adaptive functions are crucial for expressing these complex preferences. Simple Best Fit is a good baseline, but more sophisticated additions consistently improve performance. Purely arbitrary choices are the worst.\n- \n*   **Keywords:** Adaptive, Non-linear, Hybrid, State-aware, Quality-of-space.\n*   **Advice:** Design hybrid heuristics with adaptive, non-linear rules targeting critical bin states (completion, fragmentation). Use functions like exponential/Gaussian/logarithmic for specific incentives. Prioritize remaining space *quality* via tunable parameters.\n*   **Avoid:** Sole reliance on monotonic/linear relationships, simple additive bonuses, or shying away from non-linear complexity. Do not over-emphasize basic greedy without state awareness of bin characteristics.\n*   **Explanation:** Non-linear, state-aware functions capture nuanced problem dynamics, providing precise incentives/penalties (e.g., sharp fragmentation costs, strong completion bonuses) that linear or overly simplistic approaches miss, yielding superior solutions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}