```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Combines advanced, non-linear heuristics for online Bin Packing,
    integrating aggressive bin completion, nuanced fragmentation avoidance
    (Valley of Despair), and quality-of-large-space incentivization.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Tolerance for floating point comparisons to avoid issues with near-zero values
    TOLERANCE_EPS = 1e-9

    # Mask for bins where the item can fit (capacity >= item size, with a small tolerance)
    can_fit_mask = bins_remain_cap >= item - TOLERANCE_EPS

    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # If no bin can fit the item, return priorities initialized to -inf
    if fitting_bins_remain_cap.size == 0:
        return priorities

    # Calculate potential remaining capacity if the item were placed
    potential_remaining_cap = fitting_bins_remain_cap - item
    # Ensure no negative remainders due to floating point inaccuracies when item == capacity
    potential_remaining_cap[potential_remaining_cap < 0] = 0.0

    # --- Core Priority Calculation (Enhanced Best Fit / Space Quality) ---
    # Base: The Best Fit principle inherently prefers smaller remaining capacities.
    # This serves as a foundational component for the overall priority.
    calculated_priorities = -potential_remaining_cap

    # --- Non-linear & Adaptive Components (Leveraging insights from analysis) ---

    # 1. Exact Fit / Bin Completion Bonus (Aggressive Non-linear):
    # Applies a very high and sharply decaying exponential bonus for bins
    # where the item fits perfectly or near-perfectly (remaining capacity
    # is very close to zero). This strongly encourages closing bins efficiently,
    # as identified as a key "better code" characteristic.
    EXACT_FIT_BONUS_MAGNITUDE = 5000.0  # High magnitude to make exact fits dominant
    EXACT_FIT_DECAY_RATE = 50.0         # High decay rate for a very sharp peak at 0
    
    exact_fit_bonus = EXACT_FIT_BONUS_MAGNITUDE * np.exp(-EXACT_FIT_DECAY_RATE * potential_remaining_cap)
    calculated_priorities += exact_fit_bonus

    # 2. Fragmentation Penalty ("Valley of Despair" - Non-linear & Adaptive):
    # Introduces a significant penalty for bins that, after placing the item,
    # would be left with a non-zero, "awkward" amount of remaining capacity.
    # This penalty is shaped like an inverted Gaussian curve, being harshest
    # for mid-range remainders (e.g., 30-50% of the item's size) and tapering
    # off for very small (near zero) or larger remainders. This discourages
    # creating fragmented spaces that are neither useful for larger items
    # nor small enough to be easily ignored or filled by tiny items.
    # This sophisticated non-linear penalty is a critical improvement.
    if item > TOLERANCE_EPS: # Only apply if item size is meaningful for relative calculations
        # Define the range of remaining capacities to consider for fragmentation penalties.
        # This covers non-zero remainders up to 1.5 times the item's size.
        fragment_consideration_mask = (potential_remaining_cap > TOLERANCE_EPS) & \
                                      (potential_remaining_cap <= 1.5 * item)

        if np.any(fragment_consideration_mask):
            # Normalize the remaining capacity by the item's size for adaptive scaling.
            normalized_fragment_rem = potential_remaining_cap[fragment_consideration_mask] / item

            # Parameters for the Gaussian penalty curve.
            FRAGMENT_PENALTY_PEAK_RATIO = 0.4 # Peak penalty when remaining capacity is 40% of item size
            FRAGMENT_PENALTY_STD_DEV = 0.2    # Standard deviation: controls the width of the penalty zone
            PENALTY_MAGNITUDE = 100.0         # Maximum strength of the fragmentation penalty

            # Calculate the Gaussian penalty. It's negative, so it subtracts from priority.
            # `exp(-(x-mu)^2 / (2*sigma^2))`
            penalty = -PENALTY_MAGNITUDE * np.exp(
                -((normalized_fragment_rem - FRAGMENT_PENALTY_PEAK_RATIO)**2) / (2 * FRAGMENT_PENALTY_STD_DEV**2)
            )
            calculated_priorities[fragment_consideration_mask] += penalty

    # 3. Quality of Large Remaining Space Bonus (Logarithmic & Adaptive):
    # Provides a moderate, logarithmically increasing bonus for bins that are
    # left with a substantial amount of useful free capacity (e.g., more than double
    # the current item's size). This incentivizes keeping bins with genuinely
    # useful large spaces available for future large items, promoting overall
    # bin utility rather than just minimal remaining space. This is crucial
    # for maintaining flexibility for subsequent items.
    if item > TOLERANCE_EPS:
        # Define "large enough" remaining capacity relative to the item size.
        LARGE_REM_THRESHOLD_MULTIPLE = 2.0
        
        large_rem_mask = potential_remaining_cap > (LARGE_REM_THRESHOLD_MULTIPLE * item)

        if np.any(large_rem_mask):
            # The bonus increases logarithmically with the ratio of remaining capacity
            # to the threshold. Logarithmic growth provides diminishing returns for
            # extremely large remaining spaces, preventing them from dominating excessively.
            LARGE_REM_BONUS_FACTOR = 20.0 # Moderate bonus strength

            # Scale the argument for log1p to ensure positive values and manage sensitivity.
            scaled_log_arg = potential_remaining_cap[large_rem_mask] / (item * LARGE_REM_THRESHOLD_MULTIPLE)
            log_bonus_amount = LARGE_REM_BONUS_FACTOR * np.log1p(np.minimum(scaled_log_arg, 100.0)) # Cap scaled_arg to prevent extreme values

            calculated_priorities[large_rem_mask] += log_bonus_amount

    # Assign the calculated priorities back to the original array for fitting bins
    priorities[can_fit_mask] = calculated_priorities

    return priorities
```
