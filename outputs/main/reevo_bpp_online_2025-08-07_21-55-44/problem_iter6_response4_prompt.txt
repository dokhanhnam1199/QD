{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Compute dynamic priority scores for bins in an online Bin Packing Problem.\n\n    The priority blends best\u2011fit (tight leftover) and worst\u2011fit (large leftover)\n    using a data\u2011driven weight \u03b1 that adapts to the current distribution of\n    leftovers.  A linear waste penalty is subtracted, and a temperature\n    scaling smooths the scores.  A tiny negative tie\u2011breaker based on the\n    bin index resolves exact ties.  Feasible bins that cannot accommodate the\n    item receive -inf and are never selected.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities for each bin.\n\n    Returns\n    -------\n    np.ndarray\n        Array of priority scores (same shape as ``bins_remain_cap``).\n    \"\"\"\n    # Ensure numeric array\n    caps = np.asarray(bins_remain_cap, dtype=float)\n\n    # Feasibility mask\n    feasible = caps >= item\n\n    # If no feasible bin, return -inf for all\n    if not np.any(feasible):\n        return np.full_like(caps, -np.inf, dtype=float)\n\n    # Compute leftover capacity if the item is placed\n    leftover = caps[feasible] - item  # >= 0\n\n    # ----- Dynamic alpha based on leftover distribution -----\n    max_leftover = np.max(leftover)\n    if max_leftover > 0:\n        mean_leftover = np.mean(leftover)\n        alpha = np.clip(mean_leftover / max_leftover, 0.0, 1.0)\n    else:\n        # All feasible bins are exact fits\n        alpha = 0.5\n\n    # ----- Normalize leftover to [0,1] for stable scoring -----\n    if max_leftover > 0:\n        leftover_norm = leftover / max_leftover\n    else:\n        leftover_norm = leftover  # all zeros\n\n    # ----- Best\u2011fit and worst\u2011fit components -----\n    best_score = -leftover_norm   # tighter fit \u2192 higher\n    worst_score = leftover_norm   # larger leftover \u2192 higher\n\n    # ----- Linear blend -----\n    combined = (1.0 - alpha) * best_score + alpha * worst_score\n\n    # ----- Waste penalty -----\n    base_penalty = 0.1\n    penalty_factor = base_penalty * np.mean(leftover_norm)\n    combined -= penalty_factor * leftover_norm\n\n    # ----- Temperature scaling -----\n    std_norm = np.std(leftover_norm)\n    temperature = 1.0 + std_norm  # higher variation \u2192 smoother scores\n    if temperature <= 0.0:\n        temperature = 1e-8\n    combined /= temperature\n\n    # ----- Tie\u2011breaker based on bin index (avoid fixed epsilon) -----\n    idx = np.arange(caps.size, dtype=float)\n    epsilon = 1e-6 * (np.max(np.abs(combined)) + 1e-12)\n    tie_break = -epsilon * idx\n\n    # ----- Assemble final scores -----\n    full_scores = np.full_like(caps, -np.inf, dtype=float)\n    full_scores[feasible] = combined + tie_break[feasible]\n\n    return full_scores\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    *,\n    temperature: float = 0.5,\n    alpha: float = 0.1,\n    exact_fit_bonus: float = 1e6,\n    epsilon: float = 1e-12,\n    next_item: float | None = None,\n) -> np.ndarray:\n    \"\"\"\n    Advanced priority function for online Bin Packing.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities of the currently opened bins.\n    temperature : float, optional\n        Temperature for the softmax scaling. Lower values amplify differences.\n    alpha : float, optional\n        Coefficient of the quadratic waste penalty (enables online adaptation).\n    exact_fit_bonus : float, optional\n        Large positive bonus added when an item exactly fits a bin.\n    epsilon : float, optional\n        Tiny index\u2011based tie\u2011breaker; smaller index gets a slight advantage.\n    next_item : float | None, optional\n        Size of the next item (if known). Bins that can also accommodate the\n        next item receive a small additional boost.\n\n    Returns\n    -------\n    np.ndarray\n        Priority scores for each bin (higher is better). Infeasible bins are\n        assigned ``-np.inf`` (they will never be selected).\n    \"\"\"\n    # Ensure a NumPy array of floats.\n    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)\n\n    # Compute waste (remaining capacity after placing the current item).\n    waste = bins_remain_cap - item\n    feasible = waste >= 0\n\n    # Linear + quadratic waste penalty.\n    base = -waste - alpha * np.square(waste)\n\n    # Exact\u2011fit bonus (massive boost for zero slack).\n    exact_fit_mask = np.isclose(waste, 0.0, atol=1e-12)\n    base = np.where(exact_fit_mask, base + exact_fit_bonus, base)\n\n    # Optional look\u2011ahead for the next item.\n    if next_item is not None:\n        future_waste = waste - next_item\n        can_fit_next = future_waste >= 0\n        # Small boost for bins that can also host the next item.\n        # Additionally, penalise large leftover after both items.\n        lookahead_bonus = np.where(\n            can_fit_next,\n            0.05 - 0.01 * future_waste,  # 0.05 is a base boost.\n            0.0,\n        )\n        base += lookahead_bonus\n\n    # Temperature\u2011scaled softmax (monotonic transformation).\n    if np.any(feasible):\n        scaled = base / temperature\n        # Stabilise exponentiation by subtracting the max feasible value.\n        max_scaled = np.max(scaled[feasible])\n        exp_scores = np.exp(scaled - max_scaled)\n        # Infeasible bins become -inf so they are never chosen.\n        scores = np.where(feasible, exp_scores, -np.inf)\n    else:\n        # No feasible bin \u2013 all scores are -inf (caller should open a new bin).\n        scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Deterministic tie\u2011breaker: favour lower indices by a tiny epsilon.\n    scores = scores - epsilon * np.arange(bins_remain_cap.size)\n\n    return scores\n\n[Reflection]\nUse simple waste penalties, exact\u2011fit boost, softmax temperature, optional lookahead, stable normalization, tiny index tie\u2011breaker.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}