<|channel|>commentary<|message|>We must output improved function priority_v2. We have to design a better priority function based on reflection: "Prioritize exact fits, mildly penalize slack, use simple index tie‑breakers; avoid complex exponentials." So we need to produce a Python code only, inside code block. Should we include imports? Possibly we need import numpy as np.

We need to design priority_v2 function that returns priority array for each bin. The priority function should reflect best-fit style, but with exact fit reward and mild slack penalty. Also incorporate index tie-breaker. Should be deterministic. Should avoid complex exponentials.

We should consider param: item: float, bins_remain_cap: np.ndarray. Optionally bin_capacity? But best to keep it simple. But we can compute waste = bins_remain_cap - item. Feasible bins: waste >= 0. If no feasible bins, maybe open a new bin? But this function just returns priority; caller will decide.

So priority for feasible bins: maybe set priority = -waste (so less slack yields higher priority). Also reward exact fit: if waste is zero or near zero within tolerance, set priority to huge reward. But we don't want to break ranking if there is an exact fit but also other bins with negative waste? Actually negative waste is impossible; we only consider feasible bins. So exact fit is the best (waste zero). So we can set priority = huge positive value, or we can set priority = 0 or something high. But if we use negative waste, waste zero is priority 0; other waste positive yields negative priority. So exact fit is highest (0). So we can keep that. But we might want to boost exact fit by adding extra penalty or reward.

But the reflection says: "Prioritize exact fits, mildly penalize slack, use simple index tie‑breakers; avoid complex exponentials." So we need to incorporate a small penalty for slack: maybe a small constant times waste. Actually we already have negative waste; that's a penalty that increases with waste. It's not too mild; but we could weight it small: maybe -0.5 * waste. But we can just keep -waste, as in v1. But we can add a small linear penalty: -waste (already negative). But we can add also maybe -0.1 * waste or something. But negative waste is enough.

Also we need tie-breaker: we can use negative index times small epsilon. In v1 they used -np.arange(bins_remain_cap.size) * epsilon. That gives preference to lower index if priorities are equal. Good.

Also we can incorporate some additional heuristics: e.g., bins that will be nearly full after placement: maybe compute fullness after placement: new_fullness = 1 - (bins_remain_cap - item)/bin_capacity = 1 - waste / bin_capacity. The best-fit chooses smallest waste, which is same as largest fullness. So it's same.

Also maybe include some penalty for bins that are too close to full? But no.

We can also incorporate a lookahead boost: if we have avg_future_item. But reflection says avoid complex exponentials; but lookahead boost can be simple linear. But not necessary. But we can keep it optional: function signature: def priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0, avg_future_item: float | None = None) -> np.ndarray:

But the reflection didn't mention bin_capacity or avg_future_item. But we could still include them. But better to keep signature simple: only item and bins_remain_cap. We can incorporate bin_capacity as 1.0 by default, but maybe we don't need it. But we can compute waste.

We can also compute priority as negative waste if feasible else -np.inf.

Add reward for exact fit: maybe priority += large constant if waste <= tolerance. But we can also just let negative waste be zero for exact fit, which is already highest priority if other bins produce negative priority. So not necessary to add extra reward.

But the reflection says "Prioritize exact fits, mildly penalize slack." That is what we have: zero waste yields highest priority, positive waste yields lower priority. So we can implement that.

So priority_v2 could be similar to v1 but maybe slightly different. Let's propose:

```
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    feasible = waste >= 0
    # base priority: smaller waste = larger priority
    base_priority = np.where(feasible, -waste, -np.inf)
    # mild penalty: multiply waste by a small factor to reduce difference between close bins
    penalty_factor = 0.5  # optional
    base_priority[feasible] *= penalty_factor
    # tie-breaker: prefer lower index
    epsilon = 1e-12
    tie_breaker = -np.arange(bins_remain_cap.size) * epsilon
    priorities = base_priority + tie_breaker
    return priorities
```

But that uses penalty factor. But maybe we don't need it.

Alternatively, we can incorporate an additional small boost for exact fit:

```
exact_fit_mask = feasible & np.isclose(waste, 0.0, atol=1e-9)
base_priority[exact_fit_mask] += 1e6
```

But that will produce huge difference. But we might prefer a smaller reward, like +0.5 or something. But we can just leave as is: waste=0 yields -0 = 0. Others produce negative priority. So 0 is highest.

But if there are bins with waste negative? That cannot happen if feasible. So safe.

Also we can incorporate bin_capacity for scaling: e.g., -waste/bin_capacity? But if bin_capacity is 1.0, same.

We might also consider some future lookahead: if we have avg_future_item estimate, we can boost bins that have slack > avg_future_item by some linear factor. But reflection says avoid complex exponentials, but a linear boost is fine. But we can keep it optional. However, to keep function signature simple, we might not include avg_future_item.

But the improved function should be better than v1. But v1 is already fairly simple. So we can propose v2 that maybe uses different scoring: e.g., uses waste squared to penalize large slack more heavily but still mild. Or use waste ratio: waste / bin_capacity. But we can keep it simple.

Alternatively, we can incorporate an exponential of waste, but reflection says avoid complex exponentials.

Better: we can incorporate linear or quadratic penalty: priority = -waste**2. That penalizes large slack more heavily. But small slack remains small penalty.

But small slack might produce small negative numbers; we can use -waste**2.

But maybe we can combine with base priority: -waste**2 is fine.

But we might want to differentiate between exactly zero slack and small slack: we can add an extra reward for exact fit: +1e6.

So we can implement:

```
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)
    waste = bins_remain_cap - item
    feasible = waste >= 0
    base_priority = np.where(feasible, -waste**2, -np.inf)
    # reward exact fit
    exact_fit_mask = feasible & np.isclose(waste, 0.0, atol=1e-9)
    base_priority[exact_fit_mask] += 1e6
    # tie-breaker
    epsilon = 1e-12
    tie_breaker = -np.arange(bins_remain_cap.size) * epsilon
    priorities = base_priority + tie_breaker
    return priorities
```

But is this improvement? It penalizes slack more heavily for larger slack. But reflection says "mildly penalize slack." Quadratic penalty might be stronger than linear. But we can use linear penalty: -waste. That is mild.

But v1 uses -waste. So maybe we can add a small reward for slack that is close to zero: e.g., add 0.1 * (some slack?). But maybe we can incorporate the concept of "best fit" which is same.

But maybe we can incorporate something like: priority = -waste + (waste < epsilon? 1e-6: 0). But no.

Alternatively, we can incorporate an offset: priority = -waste + (waste < 1e-9? 1e-5: 0). But small.

But reflection: "Prioritize exact fits, mildly penalize slack, use simple index tie‑breakers; avoid complex exponentials." So the simple approach is: -waste. So v1 already does that. So what improvement? Maybe we can incorporate some improvement like using a small fractional penalty for slack: maybe use -waste * (1 + small constant). But no.

Maybe we can incorporate a "gap" measure: small gap should be prioritized. This is same as -waste.

Maybe we can incorporate a reward for bins that have less capacity remaining after placement than others, but that is same.

Maybe we can incorporate a penalty for bins that are almost full before placement? But we want to pack as tightly as possible. But best fit chooses smallest slack. That implies bin with smallest remaining capacity after placement, which is same as most full bin after placement. That is best fit.

But maybe we can incorporate a lookahead if we have average future item: we can slightly boost bins that can accommodate future items too. But reflection didn't mention that.

Alternatively, we can incorporate a small penalty for bins with extremely small remaining capacity after placement? But that would reduce the best-fit. But we want to fill bins as much as possible.

Thus, v2 might just be the same as v1 but with a slightly improved tie-breaker or small difference. Maybe we can incorporate a deterministic random tie-breaker, but we want deterministic. So use index tie-breaker.

Also we might include a check: if no feasible bins, return np.full_like(bins_remain_cap, -np.inf). But v1 already returns -inf for infeasible bins.

Maybe we can incorporate some simple smoothing: if waste <= epsilon: treat as exact fit reward. But we can just use tolerance.

Also maybe we can incorporate a small penalty if bin is extremely full before placement: we want to avoid placing an item in a bin that will become too full relative to its capacity. But it's okay.

Alright.

Maybe we can incorporate an "overflow" detection: If waste is extremely small but negative due to floating error, treat as feasible. But we already check >=0.

Alright.

Now, let's produce improved code. Let's create priority_v2 with the following features:

- Accepts item, bins_remain_cap
- Compute waste = bins_remain_cap - item
- Feasible = waste >= 0
- base_priority = np.where(feasible, -waste, -np.inf)
- If exact fit: waste <= tolerance: base_priority += huge positive reward (e.g., 1e6) but we can also set to 0? But we already set to -0 = 0. So maybe we can set huge reward to guarantee no tie. But if there are multiple exact fits, they get same priority. We can break tie by index. So it's okay.
- tie_breaker: -np.arange(bins_remain_cap.size) * epsilon
- Optionally, a slight penalty for slack: use a factor: priority = -waste * (1 + 0.01). But maybe not needed.
- Optionally, we can incorporate some mild reward for bins that have large slack relative to average item: but no.

Better: Let's incorporate a simple lookahead boost: if avg_future_item is given and if waste >= avg_future_item: add small boost: e.g., 0.1 * (waste - avg_future_item) / bin_capacity. But we can omit.

Thus priority_v2 will be essentially same as v1 but with potential small improvement: maybe using squared waste: -waste**2, which penalizes large slack more. But reflection says "mildly penalize slack." Squared is stronger penalty. But we can consider linear penalty plus a small factor to penalize large slack more. But we can simply use -waste**2, which might be too strong.

Alternatively, we can use -waste * (1 + w/ some). But simpler: -waste + (waste < 1e-9? 1e-9: 0). But no.

Ok, we can keep base_priority = -waste. Add a small linear scaling factor: maybe -waste * 0.9? But that reduces penalty. But not needed.

Thus, priority_v2 will be similar to v1 but maybe with an optional parameter for exact_fit_reward. But we can keep it simple.

Wait, the user wants "improved function `priority_v2`" according to the reflection. So we must produce improved function. v1 is best-fit. But we can further improve: maybe we can incorporate a small penalty for slack but not huge. We can also incorporate "close to full" preference: a bin with small slack is preferred. We can also incorporate "minimize wasted space" and "minimize number of bins" by penalizing bins that are almost full but have slack? But no.

We could also incorporate the concept of "two-phase" approach: first, if there is a bin with exact fit, choose it. If not, choose the bin with smallest slack. This is essentially best fit. So we can separate them.

We could also incorporate a small boost for bins that are currently less used (i.e., have more capacity left before placement) but after placement will have less capacity left. Actually best-fit tries to fill bins as fully as possible. So it uses smallest slack. So it's fine.

Ok, maybe we can also incorporate a factor that takes into account current fullness: we want to avoid leaving large empty bins. But best fit already does that.

Ok, maybe we can incorporate "some capacity leftover" reward: e.g., for bins that are extremely full before placement (waste close to capacity, i.e., near full after placement), we can boost them. But we can't differentiate.

Alternatively, we can incorporate a simple difference: penalty = -waste, but also maybe add an offset: priority = -waste + (0.1 * something). But no.

We can also incorporate "weight" to bin index: lower index gets slightly better tie-breaker. Already done.

Ok, then we can produce code.

Let's produce a function priority_v2 as follows:

```
def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Priority function for online Bin Packing that prioritizes exact fits
    and mildly penalizes slack. The bin with the highest priority score
    will be selected for the incoming item.

    Parameters
    ----------
    item : float
        Size of the incoming item.
    bins_remain_cap : np.ndarray
        Array of remaining capacities of the currently opened bins.

    Returns
    -------
    np.ndarray
        Priority scores for each bin; infeasible bins receive -np.inf.
    """
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)

    # Remaining capacity after placing the item
    waste = bins_remain_cap - item

    # Feasibility mask
    feasible = waste >= 0

    # Base priority: higher when waste is smaller
    # Use -waste to get higher score for smaller slack.
    base_priority = np.where(feasible, -waste, -np.inf)

    # Extra reward for exact fits
    exact_fit_mask = feasible & np.isclose(waste, 0.0, atol=1e-9)
    if np.any(exact_fit_mask):
        # Add a large constant so that exact fits are always preferred
        base_priority[exact_fit_mask] += 1e6

    # Tiny tie‑breaker: favor lower index for equal priorities
    epsilon = 1e-12
    tie_breaker = -np.arange(bins_remain_cap.size) * epsilon
    priorities = base_priority + tie_breaker

    return priorities
```

But we might want to avoid using huge constant; but it's okay. But we might also want to avoid adding large constant that could affect floating point precision. But we can also use a moderate constant: e.g., 1e-3. But if waste values are around 0.1 or 0.2, then 1e-3 is small relative. But we want to ensure exact fit is strictly better than any other bin. But we can achieve that by using -waste (waste=0 yields 0) and any positive slack yields negative. So no need to add extra reward. But if we add large constant, we risk messing with ordering of different slacks? But because all other priorities are negative, large constant will be huge positive. So fine.

But we might want to avoid the constant; but we can keep it small: e.g., 1e-6, but then if there is a bin with slack -0.0001, that is -0.0001, while exact fit gets +1e-6. So exact fit wins. But we can just keep no reward. But we can keep constant to be safe.

But we can just set base_priority = -waste. Then for exact fit, base_priority is 0. So it's best. No need for extra constant.

But the code can keep extra constant for clarity.

Ok, we can produce final code.

But we might also incorporate an optional "bin_capacity" param: but we can avoid.

Also we might incorporate optional "avg_future_item" for lookahead: but reflection didn't ask for it. We can keep simple.

We should add imports: import numpy as np at top.

But user asked: "Please write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."

Thus we need to output only code. We can include import. So our code block should include the function. Usually we need to include necessary imports.

Thus we output:

```
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    ...
```

Thus entire code inside code block.

Ok.

But maybe we can add docstring. Provide explanation.

Also we can mention that infeasible bins receive -np.inf.

Ok, produce final code.

But we might incorporate optional param `bin_capacity`, but we can just ignore.

Ok, final.

Wait, reflect: "Prioritize exact fits, mildly penalize slack, use simple index tie‑breakers; avoid complex exponentials."

Thus we should do:

- penalty = -waste
- small reward for exact fits: +1e-9? Not necessary.

- tie-breaker: -np.arange(n)*epsilon.

Thus final code.

Let's output:

```
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Compute priority scores for online Bin Packing.

    Parameters
    ----------
    item : float
        Size of the incoming item.
    bins_remain_cap : np.ndarray
        Remaining capacities of the currently opened bins.

    Returns
    -------
    np.ndarray
        Priority scores; higher scores mean more desirable bins.
        Infeasible bins receive -np.inf.
    """
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)

    # Remaining capacity after placement
    waste = bins_remain_cap - item

    # Feasibility mask
    feasible = waste >= 0

    # Base priority: less waste -> higher priority
    # Using -waste gives 0 for an exact fit and negative values otherwise.
    base_priority = np.where(feasible, -waste, -np.inf)

    # Tiny tie‑breaker: favor lower index for equal priorities
    epsilon = 1e-12
    tie_breaker = -np.arange(bins_remain_cap.size) * epsilon

    return base_priority + tie_breaker
```

We can also add optional reward for exact fits: but it's already 0. But maybe we want to ensure exact fits are strictly better than other bins with small slack. Since other bins have negative priority, they will always be less. So no need.

Thus this is improved function.

But perhaps we can add optional exact fit reward: we can incorporate `if np.any(exact_fit_mask)` etc. But it's optional.

Let's decide. If we don't add extra constant, exact fits produce 0. Bins with slack 1e-9 produce -1e-9. So exact fit wins. Good.

Ok, we can produce this code. Provide only code. Use the code block.

But we might also want to handle case of no feasible bins: In that case, base_priority will be -inf for all. We may want to also return -inf. It's fine.

Ok.

But we might also want to use an epsilon tolerance: e.g., treat waste >= -1e-12 as feasible. But we can ignore.

Thus code ready.

Double-check for errors:

- np.asarray(bins_remain_cap, dtype=float). Good.

- waste = bins_remain_cap - item. Good.

- feasible = waste >= 0. Good.

- base_priority = np.where(feasible, -waste, -np.inf). Good.

- tie_breaker = -np.arange(bins_remain_cap.size) * epsilon. Good.

- return base_priority + tie_breaker.

Works.

Also we might want to convert to float dtype. That's fine.

Ok.

Let's output code accordingly.

But we must check that we didn't include any non-code in the final answer.

We will output only code inside code block. We'll not add any explanation outside. The code block will contain import and function. That meets the requirement.

We must ensure the code block starts with ```python and ends with ```. Let's produce that.

We also need to include necessary imports at top: import numpy as np.

Thus final.

But maybe we should include a small comment at top that this is improved function.

Ok, produce final.<|end|>
```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Compute priority scores for online Bin Packing.

    The priority is higher for bins that will have less remaining capacity
    after placing the item (i.e., best‑fit).  Exact fits receive the
    highest score (0), and infeasible bins get -np.inf.  A tiny
    deterministic tie‑breaker favors bins with lower indices.

    Parameters
    ----------
    item : float
        Size of the incoming item.
    bins_remain_cap : np.ndarray
        Remaining capacities of the currently opened bins.

    Returns
    -------
    np.ndarray
        Priority scores; higher scores mean more desirable bins.
        Infeasible bins receive -np.inf.
    """
    bins_remain_cap = np.asarray(bins_remain_cap, dtype=float)

    # Remaining capacity after placement
    waste = bins_remain_cap - item

    # Feasibility mask
    feasible = waste >= 0

    # Base priority: less waste -> higher priority (exact fit → 0)
    base_priority = np.where(feasible, -waste, -np.inf)

    # Tiny deterministic tie‑breaker: lower index gets a slight edge
    epsilon = 1e-12
    tie_breaker = -np.arange(bins_remain_cap.size) * epsilon

    return base_priority + tie_breaker
```
