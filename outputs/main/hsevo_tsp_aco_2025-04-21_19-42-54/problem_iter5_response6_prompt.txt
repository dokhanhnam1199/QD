{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, adaptive density scaling, and center proximity\n    to balance exploitation and exploration in TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Reciprocal Rank (exploration)\n    reciprocal_rank = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        distances = distance_matrix[i, :]\n        ranks = np.argsort(distances)\n        ranked_distances = np.argsort(ranks) + 1\n        reciprocal_rank[i, :] = 1 / ranked_distances\n\n    # 3. Adaptive Density Scaling (adjust exploration)\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 4. Center Proximity Factor (global context)\n    center_x = np.mean(np.arange(n))\n    center_y = np.mean(np.arange(n))\n    center_distances = np.sqrt((np.arange(n) - center_x)**2 + (np.arange(n) - center_y)**2)\n    center_proximity_factor = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            center_proximity_factor[i, j] = 1.0 - (abs(center_distances[i] - center_distances[j]) / (np.max(center_distances) + 1e-9))\n\n\n    # Combine factors: Inverse distance + adaptive exploration + global context.\n    heuristic_matrix = inverse_distance + density_scaling * reciprocal_rank + 0.2 * center_proximity_factor\n\n    # Normalize\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, stochasticity, node degree, and a global penalization.\n    Balances exploitation and exploration, prevents premature convergence.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    node_degree = np.sum(inverse_distance, axis=0)\n    node_degree_matrix = np.outer(node_degree, node_degree)\n\n    min_dist_to_node = np.min(distance_matrix + np.diag([np.inf]*n), axis=0)\n    node_min_dist_matrix = np.outer(min_dist_to_node,min_dist_to_node)\n    penalization_matrix = distance_matrix * (node_min_dist_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                stochasticity = np.random.rand() * (distance_matrix[i,j] / max_distance)\n                heuristic_matrix[i, j] = inverse_distance[i,j] + stochasticity + 0.01 * node_degree_matrix[i,j] - 0.00001 * penalization_matrix[i,j]\n            else:\n                heuristic_matrix[i, j] = 0\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristic utilizes an adaptive temperature based on edge variance, sparsification, and node desirability, while the bottom heuristic relies on inverse distance, stochasticity, node degree, and global penalization without normalization or adaptive parameters. (2nd best) vs (second worst) are identical, implying the ranker does not differentiate between them. Comparing (1st) vs (2nd), we see only hyperparameter differences. (3rd) vs (4th) are identical, implying the ranker does not differentiate between them. Comparing (second worst) vs (worst), heuristics 19 and 20 are exactly the same. Overall: The best heuristics incorporate adaptive mechanisms like temperature scaling based on variance and dynamic sparsification, whereas the worst rely on static combinations of factors, often without normalization, adaptive elements, or clear mechanisms to balance exploration and exploitation. Normalization and adaptive parameters are key differentiators.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined definition of \"Current Self-Reflection\" focused on designing better heuristics, built around your feedback and designed to avoid ineffective practices:\n\n**Current Self-Reflection (Refined):**\n\n*   **Keywords:** Adaptive, Dynamic, Normalized, Contextual, Exploration/Exploitation.\n*   **Advice:** Design heuristics that dynamically adapt their components (e.g., parameters, search strategies) based on the problem context and search progress. Normalize data before combining factors. Prioritize balancing exploration and exploitation throughout the search.\n*   **Avoid:** Static parameters, combinations of unnormalized factors, ignoring global context.\n*   **Explanation:** Effective heuristics need to react intelligently to the problem instance and search trajectory. Rigidity and a lack of awareness leads to suboptimal performance. Normalization ensures fair evaluation and contribution of features or components.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}