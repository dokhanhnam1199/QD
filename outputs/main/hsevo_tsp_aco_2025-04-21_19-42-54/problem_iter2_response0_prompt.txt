{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristics for the Traveling Salesman Problem (TSP) based on a combination of\n    inverse distance, savings heuristic, and random perturbations, inspired\n    by quantum mechanics principles (stochastic sampling with \"path integrals\").\n\n    Args:\n        distance_matrix: A numpy ndarray representing the distance matrix between cities.\n\n    Returns:\n        A numpy ndarray of the same shape as distance_matrix, representing the\n        prior probability of including each edge in a solution.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance (short distances are preferred)\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero\n\n    # Savings heuristic (inspired by Clarke-Wright algorithm)\n    #  Higher savings indicate a higher likelihood of inclusion.\n    savings = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Savings from merging i and j into a route\n                savings[i, j] = distance_matrix[i, 0] + distance_matrix[0, j] - distance_matrix[i, j] # assuming start node is 0\n                savings[j, i] = distance_matrix[j, 0] + distance_matrix[0, i] - distance_matrix[j, i]\n\n    # Combine inverse distance and savings\n    heuristics = inverse_distance + savings # maybe add scaling factors\n\n    # Add a stochastic element -  \"quantum fluctuations\"\n    # This introduces some randomness to allow for exploration of different paths\n    # even if some edges seem less promising initially.\n    random_perturbation = np.random.normal(0, 0.1, size=(n, n)) # scaled random noise\n    heuristics += random_perturbation\n\n    # Ensure the diagonal is zero and all values are non-negative.\n\n    for i in range(n):\n        heuristics[i, i] = 0\n\n    heuristics = np.maximum(heuristics, 0) # Ensure probabilities are non-negative\n\n\n    # Normalize the heuristics\n    total_sum = np.sum(heuristics)\n    if total_sum > 0:\n        heuristics /= total_sum  # Convert to probabilities (optional, but useful)\n    else:\n        heuristics = np.ones_like(distance_matrix) / (n * (n -1))\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A heuristic function for the Traveling Salesman Problem based on Newton's laws and physical intuition.\n    This function incorporates a combination of inverse distance (gravity),\n    a temperature-based exploration factor, and a path coherence term.\n\n    Args:\n        distance_matrix (np.ndarray): A matrix where distance_matrix[i, j] is the distance between city i and city j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, containing heuristic values for each edge.\n                     Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse Distance (Gravitational Analogy): Closer cities are more attractive.\n    inverse_distance = 1 / (distance_matrix + 1e-9)  # Add a small constant to avoid division by zero.\n\n    # 2. Temperature-based Exploration: Encourage exploration initially, focusing as the search progresses.\n    temperature = 1.0  # Initial temperature (can be tuned). It decreases over time implicitly\n    exploration_factor = np.exp(-distance_matrix / temperature)\n\n    # 3. Path Coherence: Encourages edges that connect to nodes that are \"well-connected\" in general,\n    #    considering all edge connections in the graph to create a smooth and promising initial path\n    path_coherence = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Compute the sum of inverse distances from i and j to all other nodes\n                attraction_i = np.sum(inverse_distance[i, :])\n                attraction_j = np.sum(inverse_distance[j, :])\n                path_coherence[i, j] = (attraction_i + attraction_j)\n    \n    # 4. Combine the factors, weighting each appropriately. These weightings can be optimized through trials.\n    alpha = 0.6  # Weight for inverse distance\n    beta = 0.2   # Weight for exploration factor\n    gamma = 0.2   # Weight for path coherence\n\n    heuristics = alpha * inverse_distance + beta * exploration_factor + gamma * path_coherence\n\n    # Set diagonal elements (distance to self) to zero, avoiding self-loops.\n    np.fill_diagonal(heuristics, 0)\n\n    return heuristics\n\n### Analyze & experience\n- *   Comparing (1st) vs (20th), we see that the best heuristic incorporates inverse distance, node degree preference, and global distance context, penalizing long edges connected to nodes already having short edges, while the worst focuses on inverse distance, node degree, and penalizing long edges connected to nodes already having short edges. The key difference lies in the inclusion of the global context component in the best heuristic, along with degree component implemented by average distance, making it superior.\n*   Comparing (2nd) vs (19th), we observe that the second-best heuristic combines inverse distance with a \"gravity\" effect encouraging exploration of less-visited edges and randomness to escape local optima. The 19th heuristic uses inverse distance, a temperature-based exploration factor, and path coherence. The added randomness and gravity component makes the 2nd better than 19th.\n*   Comparing (1st) vs (2nd), the first prioritizes a more balanced tour with global distance awareness using average distances, while the second opts for exploration through a gravity effect and stochastic noise. The focus on a balanced tour appears to be more effective.\n*   Comparing (3rd) vs (4th), the 3rd heuristic prioritizes shorter distances and edges connecting to nodes with longer average distances, while the 4th incorporates inverse distance, a greedy start bias based on summed distances, and a global connection boost. The node importance heuristic seems better than greedy start bias.\n*   Comparing (2nd worst) vs (worst), the 19th relies on temperature-controlled exploration and path coherence, while the 20th uses node degree preference and avoidance of long edges connected to nodes with short edges. The path coherence mechanism appears less robust than degree-based exploration.\n*   Overall: Effective heuristics incorporate a balance between exploitation (favoring shorter distances) and exploration (avoiding local optima). They often factor in node connectivity, either through degree penalties/preferences or gravity-like effects. The weighting of different components and the specific implementation of exploration mechanisms (randomness, temperature, savings heuristic) plays a significant role in performance. Global context awareness is also crucial for achieving superior results.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current self-reflection\" focused on designing better heuristics, avoiding common pitfalls, and a roadmap for improvement:\n\n*   **Keywords:** Heuristic design, exploitation-exploration balance, global context, premature convergence, adaptive strategies, performance metrics.\n\n*   **Advice:** Design heuristics that dynamically adapt their exploration/exploitation ratio based on search progress and problem characteristics. Incorporate global information strategically, using it to guide, not dictate, the search. Rigorously test and benchmark heuristics against diverse problem instances.\n\n*   **Avoid:** Over-reliance on any single strategy (e.g., pure exploitation), neglecting global problem context, ignoring performance metrics and feedback loops.\n\n*   **Explanation:** Effective heuristic design requires a nuanced understanding of the problem landscape. Balance short-term gains with broader exploration guided by global awareness and adapt your strategy based on real-world performance data.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}