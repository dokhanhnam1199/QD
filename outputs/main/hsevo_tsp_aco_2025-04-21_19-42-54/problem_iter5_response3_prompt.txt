{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction (inverse square distance), node degree bias (favoring less-connected nodes),\n    and a simulated annealing-inspired temperature factor with adaptive cooling to balance exploration and exploitation.\n    Sparsifies the matrix by setting unpromising elements to zero.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution. Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Handle potential division by zero or near-zero distances:\n    epsilon = 1e-9  # A small value to avoid division by zero\n\n    # Temperature parameter - Decreases as algorithm runs, adaptive cooling.\n    initial_temperature = 10.0\n    temperature = initial_temperature  # Initial Temperature - High initially allows exploration.\n    min_temperature = 0.01 # Minimum Temperature\n    cooling_rate = 0.995\n    adaptive_cooling_threshold = 0.05 #When to trigger a slower cooling\n\n    # Node degree bias (favor less-connected nodes - initially uniform).  Will become dynamic during search (simulated annealing will be used to select edges)\n    node_degrees = np.ones(n)  # Initially all nodes are considered equally unconnected\n\n\n    # Gravitational attraction component (scaled and temperature modulated):\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #Edge score combination\n                distance_factor = 1.0 / (distance_matrix[i, j]**2 + epsilon)\n                degree_factor = node_degrees[i] * node_degrees[j] #Promotes connections to nodes with lower degree\n                heuristics[i, j] = distance_factor * degree_factor * temperature\n            else:\n                heuristics[i, j] = 0.0  # No self-loops\n\n    #Sparsification (attempt to cut off edges that are considered too long, only if temperature is low enough)\n    sparsification_threshold = np.mean(heuristics) * 0.1 # Dynamic threshold relative to average heuristic value. Can be further tuned.\n    if temperature < 1.0:\n      heuristics[heuristics < sparsification_threshold] = 0.0\n\n    # Annealing Schedule - Decay of the temperature (adaptive)\n    if np.sum(heuristics > 0) / (n * (n - 1)) < adaptive_cooling_threshold: #If many edges are already zeroed out, cool more slowly\n      cooling_rate = 0.999 #Slow down cooling\n    temperature *= cooling_rate #Reduce temperature each step (simulates cooling)\n    temperature = max(temperature, min_temperature) #Ensure the temperature doesnt drop below the minimum\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, node importance, and global connection boost.\n    Dynamically balances exploitation/exploration.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    heuristics = 1.0 / (distance_matrix + 1e-9)\n\n    # Node importance\n    node_importance = np.sum(distance_matrix, axis=1)\n    for i in range(n):\n        for j in range(n):\n           heuristics[i,j] = heuristics[i, j] * ((1/(node_importance[i]+ 1e-9)) + (1/(node_importance[j] + 1e-9)))\n\n    # Global Connection Boost\n    for i in range(n):\n        for j in range(n):\n            avg_dist_to_neighbors_i = np.mean(distance_matrix[i,:])\n            avg_dist_to_neighbors_j = np.mean(distance_matrix[j,:])\n            heuristics[i,j] = heuristics[i,j] * (avg_dist_to_neighbors_i + avg_dist_to_neighbors_j)\n\n    # Sparsify (Adaptive)\n    threshold = np.mean(heuristics) * 0.1 #dynamic sparsification\n    heuristics[heuristics < threshold] = 0\n\n    # Zero out self-loops\n    for i in range(n):\n        heuristics[i, i] = 0.0\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristic utilizes an adaptive temperature based on edge variance, sparsification, and node desirability, while the bottom heuristic relies on inverse distance, stochasticity, node degree, and global penalization without normalization or adaptive parameters. (2nd best) vs (second worst) are identical, implying the ranker does not differentiate between them. Comparing (1st) vs (2nd), we see only hyperparameter differences. (3rd) vs (4th) are identical, implying the ranker does not differentiate between them. Comparing (second worst) vs (worst), heuristics 19 and 20 are exactly the same. Overall: The best heuristics incorporate adaptive mechanisms like temperature scaling based on variance and dynamic sparsification, whereas the worst rely on static combinations of factors, often without normalization, adaptive elements, or clear mechanisms to balance exploration and exploitation. Normalization and adaptive parameters are key differentiators.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined definition of \"Current Self-Reflection\" focused on designing better heuristics, built around your feedback and designed to avoid ineffective practices:\n\n**Current Self-Reflection (Refined):**\n\n*   **Keywords:** Adaptive, Dynamic, Normalized, Contextual, Exploration/Exploitation.\n*   **Advice:** Design heuristics that dynamically adapt their components (e.g., parameters, search strategies) based on the problem context and search progress. Normalize data before combining factors. Prioritize balancing exploration and exploitation throughout the search.\n*   **Avoid:** Static parameters, combinations of unnormalized factors, ignoring global context.\n*   **Explanation:** Effective heuristics need to react intelligently to the problem instance and search trajectory. Rigidity and a lack of awareness leads to suboptimal performance. Normalization ensures fair evaluation and contribution of features or components.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}