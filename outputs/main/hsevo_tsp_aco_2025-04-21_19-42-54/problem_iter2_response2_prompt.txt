{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Einstein's heuristics for TSP using stochastic solution sampling,\n    considering both distance and potential cluster structures.\n\n    Args:\n        distance_matrix: A numpy array representing the distances between cities.\n\n    Returns:\n        A numpy array of the same shape as distance_matrix, representing the\n        prior indicators of how promising it is to include each edge in a solution.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n\n    # 1. Inverse distance: Shorter distances are generally more promising.\n    heuristic_matrix = 1 / (distance_matrix + 1e-9)  # Adding a small constant to avoid division by zero\n\n    # 2. Node degree centrality adjustment: Nodes with fewer close neighbors might be more critical.\n    degree_centrality = np.sum(heuristic_matrix, axis=0)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristic_matrix[i, j] /= (degree_centrality[i] * degree_centrality[j])**0.25 # Adjust centrality influence with exponent\n    \n    # 3. Adjust for Triangle Inequality violations (proxy for \"shortcuts\"):\n    #    If going from i to k to j is significantly shorter than i to j, it hints\n    #    that i to j may be a useful long-range connection in some cases.\n    #    We *decrease* its heuristic value, penalizing direct connections where shortcuts exist\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                shortcut_factor = 0.0\n                for k in range(n):\n                    if i != k and j != k:\n                         potential_shortcut = distance_matrix[i, k] + distance_matrix[k, j]\n                         shortcut_factor += np.maximum(0, (distance_matrix[i, j] - potential_shortcut) / distance_matrix[i,j])\n                heuristic_matrix[i, j] /= (1 + shortcut_factor/ n ) # Average, normalized impact of shortcuts\n\n    # 4. Symmetry correction:  Ensure the matrix is symmetric (if it wasn't already)\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T) / 2\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristics for the Traveling Salesman Problem (TSP) using a combination of inverse distance,\n    savings heuristic, and a touch of randomness to encourage exploration.\n\n    Args:\n        distance_matrix: A NumPy ndarray representing the distance matrix between cities.\n\n    Returns:\n        A NumPy ndarray of the same shape as distance_matrix, representing the prior\n        indicator of how promising each edge is. Higher values indicate more promising edges.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Inverse distance (basic heuristic)\n    inverse_distance = 1 / (distance_matrix + np.eye(n))  # Add identity to avoid division by zero on the diagonal\n    heuristics += inverse_distance\n\n    # 2. Savings Heuristic:  Measure the \"savings\" from directly linking two cities rather than going back to a depot (city 0).  Normalize savings by distance.\n\n    depot = 0  # Arbitrarily choose city 0 as the depot/starting point. This can be randomized as well.\n    savings = np.zeros_like(distance_matrix)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j: # Avoid savings between same city\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    #scale savings by overall distances involved.\n    heuristics += savings / (distance_matrix + np.eye(n))\n    # 3. Randomness (Encourage exploration)\n    randomness = np.random.rand(n, n) * 0.1  # Small random values\n\n    heuristics += randomness\n\n    # 4. Make sure diagnal is small/zero\n\n    heuristics[np.diag_indices_from(heuristics)] = -1\n    # 5. Normalize the heuristics\n    heuristics = (heuristics - np.min(heuristics)) / (np.max(heuristics) - np.min(heuristics))  # Scale to [0, 1]\n\n\n    return heuristics\n\n### Analyze & experience\n- *   Comparing (1st) vs (20th), we see that the best heuristic incorporates inverse distance, node degree preference, and global distance context, penalizing long edges connected to nodes already having short edges, while the worst focuses on inverse distance, node degree, and penalizing long edges connected to nodes already having short edges. The key difference lies in the inclusion of the global context component in the best heuristic, along with degree component implemented by average distance, making it superior.\n*   Comparing (2nd) vs (19th), we observe that the second-best heuristic combines inverse distance with a \"gravity\" effect encouraging exploration of less-visited edges and randomness to escape local optima. The 19th heuristic uses inverse distance, a temperature-based exploration factor, and path coherence. The added randomness and gravity component makes the 2nd better than 19th.\n*   Comparing (1st) vs (2nd), the first prioritizes a more balanced tour with global distance awareness using average distances, while the second opts for exploration through a gravity effect and stochastic noise. The focus on a balanced tour appears to be more effective.\n*   Comparing (3rd) vs (4th), the 3rd heuristic prioritizes shorter distances and edges connecting to nodes with longer average distances, while the 4th incorporates inverse distance, a greedy start bias based on summed distances, and a global connection boost. The node importance heuristic seems better than greedy start bias.\n*   Comparing (2nd worst) vs (worst), the 19th relies on temperature-controlled exploration and path coherence, while the 20th uses node degree preference and avoidance of long edges connected to nodes with short edges. The path coherence mechanism appears less robust than degree-based exploration.\n*   Overall: Effective heuristics incorporate a balance between exploitation (favoring shorter distances) and exploration (avoiding local optima). They often factor in node connectivity, either through degree penalties/preferences or gravity-like effects. The weighting of different components and the specific implementation of exploration mechanisms (randomness, temperature, savings heuristic) plays a significant role in performance. Global context awareness is also crucial for achieving superior results.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current self-reflection\" focused on designing better heuristics, avoiding common pitfalls, and a roadmap for improvement:\n\n*   **Keywords:** Heuristic design, exploitation-exploration balance, global context, premature convergence, adaptive strategies, performance metrics.\n\n*   **Advice:** Design heuristics that dynamically adapt their exploration/exploitation ratio based on search progress and problem characteristics. Incorporate global information strategically, using it to guide, not dictate, the search. Rigorously test and benchmark heuristics against diverse problem instances.\n\n*   **Avoid:** Over-reliance on any single strategy (e.g., pure exploitation), neglecting global problem context, ignoring performance metrics and feedback loops.\n\n*   **Explanation:** Effective heuristic design requires a nuanced understanding of the problem landscape. Balance short-term gains with broader exploration guided by global awareness and adapt your strategy based on real-world performance data.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}