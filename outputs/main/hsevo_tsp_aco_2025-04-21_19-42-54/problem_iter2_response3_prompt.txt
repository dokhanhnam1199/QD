{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Improved heuristics for the Traveling Salesman Problem based on distance and neighborhood.\n    Prioritizes shorter distances and edges connecting to nodes with longer average distances to other nodes.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix representing the distances between nodes.\n                                       distance_matrix[i][j] is the distance between node i and node j.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n                      indicates the desirability of including the corresponding edge in the TSP tour.\n    \"\"\"\n    n = distance_matrix.shape[0]\n\n    # 1. Inverse distance heuristic: shorter distances are more desirable\n    inverse_distance = 1 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero\n\n    # 2. Node Importance Heuristic: Give higher weight to edges connecting nodes with overall larger distances\n    #    to others. This encourages exploring less-connected parts of the graph early on,\n    #    potentially preventing premature convergence to local optima.\n    node_importance = np.sum(distance_matrix, axis=1)  # Sum of distances for each node\n    node_importance_matrix = np.tile(node_importance, (n, 1)) # Replicate node importance for matrix operations\n    node_importance_heuristic = node_importance_matrix + node_importance_matrix.T\n    #3. combine heuristics\n    heuristic_matrix = inverse_distance * (node_importance_heuristic/np.max(node_importance_heuristic))\n\n    # Set diagonal elements to zero to avoid self-loops.  The inverse_distance also handles this, but added for extra safety.\n    np.fill_diagonal(heuristic_matrix, 0)\n\n    return heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Improved heuristic for TSP based on a combination of distance,\n    reciprocal rank, and adaptive weighting based on local density.\n    Higher values indicate a more promising edge.\n\n    Args:\n        distance_matrix: A NumPy array representing the distance matrix.\n                         distance_matrix[i, j] is the distance between node i and node j.\n\n    Returns:\n        A NumPy array of the same shape as distance_matrix,\n        representing the heuristic scores for each edge.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf) # ensure we don't choose self-loops\n\n    # 1. Inverse Distance: Shorter distances are generally more desirable\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Reciprocal Rank: Reward edges that are short relative to others from the same node.\n    reciprocal_rank = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        # Get distances from node i to all other nodes\n        distances = distance_matrix[i, :]\n        # Get the rank of each distance from node i (lower distance = lower rank)\n        ranks = np.argsort(distances)  # Indices sorted by distance\n        ranked_distances = np.argsort(ranks) + 1  # Rank starts from 1\n        reciprocal_rank[i, :] = 1 / ranked_distances\n\n    # 3. Adaptive weighting: Account for local node density. Denser areas mean even short edges must be handled with caution\n    #   - We calculate node density using a simple averaging of neighboring inverse distances.  High inverse distance == high node density nearby\n    #   - Low local density gives greater weight to reciprocal rank to aggressively explore shorter paths, preventing traps\n    #   - High local density deemphasizes reciprocal rank, and instead focuses on pure distance.\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    # Weight scaling factor based on density. Higher local density, reduces exploration.\n    density_scaling = 1.0 / (1.0 + node_densities) # Scale densities so they become penalty coefficients.\n    density_scaling = np.tile(density_scaling, (n,1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T) #take lower density factor between connected nodes\n\n\n    # Combine heuristics: inverse distance contributes heavily to base utility, exploration reciprocal rank to boost better ones\n    heuristic_matrix = inverse_distance + density_scaling * reciprocal_rank # density acts as a weighting penalty on exploration.\n\n    # Normalize heuristic matrix so stochastic sampling isn't overly sensitive to scale.\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9) # scaled [0,1]\n\n    return heuristic_matrix\n\n### Analyze & experience\n- *   Comparing (1st) vs (20th), we see that the best heuristic incorporates inverse distance, node degree preference, and global distance context, penalizing long edges connected to nodes already having short edges, while the worst focuses on inverse distance, node degree, and penalizing long edges connected to nodes already having short edges. The key difference lies in the inclusion of the global context component in the best heuristic, along with degree component implemented by average distance, making it superior.\n*   Comparing (2nd) vs (19th), we observe that the second-best heuristic combines inverse distance with a \"gravity\" effect encouraging exploration of less-visited edges and randomness to escape local optima. The 19th heuristic uses inverse distance, a temperature-based exploration factor, and path coherence. The added randomness and gravity component makes the 2nd better than 19th.\n*   Comparing (1st) vs (2nd), the first prioritizes a more balanced tour with global distance awareness using average distances, while the second opts for exploration through a gravity effect and stochastic noise. The focus on a balanced tour appears to be more effective.\n*   Comparing (3rd) vs (4th), the 3rd heuristic prioritizes shorter distances and edges connecting to nodes with longer average distances, while the 4th incorporates inverse distance, a greedy start bias based on summed distances, and a global connection boost. The node importance heuristic seems better than greedy start bias.\n*   Comparing (2nd worst) vs (worst), the 19th relies on temperature-controlled exploration and path coherence, while the 20th uses node degree preference and avoidance of long edges connected to nodes with short edges. The path coherence mechanism appears less robust than degree-based exploration.\n*   Overall: Effective heuristics incorporate a balance between exploitation (favoring shorter distances) and exploration (avoiding local optima). They often factor in node connectivity, either through degree penalties/preferences or gravity-like effects. The weighting of different components and the specific implementation of exploration mechanisms (randomness, temperature, savings heuristic) plays a significant role in performance. Global context awareness is also crucial for achieving superior results.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a redefinition of \"Current self-reflection\" focused on designing better heuristics, avoiding common pitfalls, and a roadmap for improvement:\n\n*   **Keywords:** Heuristic design, exploitation-exploration balance, global context, premature convergence, adaptive strategies, performance metrics.\n\n*   **Advice:** Design heuristics that dynamically adapt their exploration/exploitation ratio based on search progress and problem characteristics. Incorporate global information strategically, using it to guide, not dictate, the search. Rigorously test and benchmark heuristics against diverse problem instances.\n\n*   **Avoid:** Over-reliance on any single strategy (e.g., pure exploitation), neglecting global problem context, ignoring performance metrics and feedback loops.\n\n*   **Explanation:** Effective heuristic design requires a nuanced understanding of the problem landscape. Balance short-term gains with broader exploration guided by global awareness and adapt your strategy based on real-world performance data.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}