{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive edge prioritization with node desirability and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    temperature = 1.0\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * temperature\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j])\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n    heuristics[heuristics < threshold] = 0.0\n\n    temperature *= 0.995\n    return heuristics\n\n[Heuristics 4th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive edge prioritization with node desirability and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    temperature = 1.0\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * temperature\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j])\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n    heuristics[heuristics < threshold] = 0.0\n\n    temperature *= 0.995\n    return heuristics\n\n[Heuristics 5th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive heuristic: Combines gravitat. attraction & node degree bias,\n    with dynamic temperature & sparsification for balanced exploration/exploitation.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 10.0\n    temperature = initial_temperature\n    min_temperature = 0.01\n    cooling_rate = 0.995\n    adaptive_cooling_threshold = 0.05\n    node_degrees = np.ones(n)\n\n    # Gravitational attraction and node degree\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                distance_factor = 1.0 / (distance_matrix[i, j]**2 + epsilon)\n                degree_factor = node_degrees[i] * node_degrees[j]\n                heuristics[i, j] = distance_factor * degree_factor * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    #Sparsification\n    sparsification_threshold = np.mean(heuristics) * 0.1\n    if temperature < 1.0:\n      heuristics[heuristics < sparsification_threshold] = 0.0\n\n    # Annealing Schedule\n    if np.sum(heuristics > 0) / (n * (n - 1)) < adaptive_cooling_threshold:\n      cooling_rate = 0.999\n    temperature *= cooling_rate\n    temperature = max(temperature, min_temperature)\n\n    # Node importance from v1, normalized\n    node_importance = np.sum(distance_matrix, axis=1)\n    node_importance = (node_importance - np.min(node_importance)) / (np.max(node_importance) - np.min(node_importance) + epsilon) #Normalize node importance\n    for i in range(n):\n        for j in range(n):\n           heuristics[i,j] = heuristics[i, j] * (node_importance[i] + node_importance[j])\n\n    return heuristics\n\n[Heuristics 6th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive heuristic: Combines gravitat. attraction & node degree bias,\n    with dynamic temperature & sparsification for balanced exploration/exploitation.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 10.0\n    temperature = initial_temperature\n    min_temperature = 0.01\n    cooling_rate = 0.995\n    adaptive_cooling_threshold = 0.05\n    node_degrees = np.ones(n)\n\n    # Gravitational attraction and node degree\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                distance_factor = 1.0 / (distance_matrix[i, j]**2 + epsilon)\n                degree_factor = node_degrees[i] * node_degrees[j]\n                heuristics[i, j] = distance_factor * degree_factor * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    #Sparsification\n    sparsification_threshold = np.mean(heuristics) * 0.1\n    if temperature < 1.0:\n      heuristics[heuristics < sparsification_threshold] = 0.0\n\n    # Annealing Schedule\n    if np.sum(heuristics > 0) / (n * (n - 1)) < adaptive_cooling_threshold:\n      cooling_rate = 0.999\n    temperature *= cooling_rate\n    temperature = max(temperature, min_temperature)\n\n    # Node importance from v1, normalized\n    node_importance = np.sum(distance_matrix, axis=1)\n    node_importance = (node_importance - np.min(node_importance)) / (np.max(node_importance) - np.min(node_importance) + epsilon) #Normalize node importance\n    for i in range(n):\n        for j in range(n):\n           heuristics[i,j] = heuristics[i, j] * (node_importance[i] + node_importance[j])\n\n    return heuristics\n\n[Heuristics 7th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining shortest paths, gravitational attraction, and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n\n    # Shortest Path Influence\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n    shortest_path_factor = np.exp(-shortest_path_matrix / mean_distance)\n\n    # Inverse distance with centrality boost from v1\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n    node_centrality = np.sum(inverse_distance, axis=1)\n    edge_centrality = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            edge_centrality[i, j] = node_centrality[i] * node_centrality[j]\n    centrality_boost = inverse_distance * (edge_centrality**0.5 + 1e-9)\n\n    # Gravitational attraction\n    gravitational_attraction = 1.0 / (distance_matrix**2 + epsilon)\n\n    # Combine factors with normalization\n    combined = shortest_path_factor + gravitational_attraction + centrality_boost\n\n    # Normalizing the matrix values to range between 0 and 1\n    combined = (combined - np.min(combined)) / (np.max(combined) - np.min(combined))\n    heuristics = combined\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], 20)\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining shortest paths, gravitational attraction, and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n\n    # Shortest Path Influence\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n    shortest_path_factor = np.exp(-shortest_path_matrix / mean_distance)\n\n    # Inverse distance with centrality boost from v1\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n    node_centrality = np.sum(inverse_distance, axis=1)\n    edge_centrality = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            edge_centrality[i, j] = node_centrality[i] * node_centrality[j]\n    centrality_boost = inverse_distance * (edge_centrality**0.5 + 1e-9)\n\n    # Gravitational attraction\n    gravitational_attraction = 1.0 / (distance_matrix**2 + epsilon)\n\n    # Combine factors with normalization\n    combined = shortest_path_factor + gravitational_attraction + centrality_boost\n\n    # Normalizing the matrix values to range between 0 and 1\n    combined = (combined - np.min(combined)) / (np.max(combined) - np.min(combined))\n    heuristics = combined\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], 20)\n    heuristics[heuristics < threshold] = 0\n\n    return heuristics\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Adaptive Density Scaling\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 3. Sparsification (adaptive threshold)\n    threshold = np.mean(inverse_distance) * 0.5  # Dynamic threshold\n    sparse_inverse_distance = np.where(inverse_distance > threshold, inverse_distance, 0)\n\n    # Combine factors\n    heuristic_matrix = sparse_inverse_distance * density_scaling\n\n    # Normalize\n    max_val = np.max(heuristic_matrix)\n    min_val = np.min(heuristic_matrix)\n    if max_val > min_val: # Avoid division by zero\n        heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n    else:\n        heuristic_matrix = np.zeros_like(heuristic_matrix)\n\n\n    return heuristic_matrix\n\n[Heuristics 10th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Adaptive Density Scaling\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 3. Sparsification (adaptive threshold)\n    threshold = np.mean(inverse_distance) * 0.5  # Dynamic threshold\n    sparse_inverse_distance = np.where(inverse_distance > threshold, inverse_distance, 0)\n\n    # Combine factors\n    heuristic_matrix = sparse_inverse_distance * density_scaling\n\n    # Normalize\n    max_val = np.max(heuristic_matrix)\n    min_val = np.min(heuristic_matrix)\n    if max_val > min_val: # Avoid division by zero\n        heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n    else:\n        heuristic_matrix = np.zeros_like(heuristic_matrix)\n\n\n    return heuristic_matrix\n\n[Heuristics 11th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Adaptive Density Scaling\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 3. Sparsification (adaptive threshold)\n    threshold = np.mean(inverse_distance) * 0.5  # Dynamic threshold\n    sparse_inverse_distance = np.where(inverse_distance > threshold, inverse_distance, 0)\n\n    # Combine factors\n    heuristic_matrix = sparse_inverse_distance * density_scaling\n\n    # Normalize\n    max_val = np.max(heuristic_matrix)\n    min_val = np.min(heuristic_matrix)\n    if max_val > min_val: # Avoid division by zero\n        heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n    else:\n        heuristic_matrix = np.zeros_like(heuristic_matrix)\n\n\n    return heuristic_matrix\n\n[Heuristics 12th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Adaptive Density Scaling\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 3. Sparsification (adaptive threshold)\n    threshold = np.mean(inverse_distance) * 0.5  # Dynamic threshold\n    sparse_inverse_distance = np.where(inverse_distance > threshold, inverse_distance, 0)\n\n    # Combine factors\n    heuristic_matrix = sparse_inverse_distance * density_scaling\n\n    # Normalize\n    max_val = np.max(heuristic_matrix)\n    min_val = np.min(heuristic_matrix)\n    if max_val > min_val: # Avoid division by zero\n        heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n    else:\n        heuristic_matrix = np.zeros_like(heuristic_matrix)\n\n\n    return heuristic_matrix\n\n[Heuristics 13th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic: Combines shortest path, edge attraction, and adaptive temperature scaling.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 1.0\n    current_temperature = initial_temperature\n\n    # Shortest Path Influence\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                shortest_path_factor = np.exp(-shortest_path_matrix[i, j] / mean_distance)\n                heuristics[i, j] = (edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) + shortest_path_factor) * current_temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n    heuristics[heuristics < threshold] = 0\n\n    edge_values = heuristics[heuristics > 0]\n    if len(edge_values) > 0:\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0\n\n    if edge_variance > 0.1:\n        current_temperature *= 0.99\n    else:\n        current_temperature *= 0.999\n\n    current_temperature = max(current_temperature, 0.01)\n\n    return heuristics\n\n[Heuristics 14th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining node desirability, edge attraction, and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 1.0\n    sparsification_percentile = 40.0\n    high_variance_threshold = 0.1\n    cooling_high_variance = 0.99\n    cooling_low_variance = 0.999\n    min_temperature = 0.01\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction (inverse square distance)\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    # Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    savings = savings / (np.max(savings) + epsilon)\n\n    # Initial Heuristic Combination\n    temperature = initial_temperature\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (edge_attraction[i, j] + savings[i,j]) * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive Cooling\n    edge_values = heuristics[heuristics > 0]\n    if len(edge_values) > 0:\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0\n\n    if edge_variance > high_variance_threshold:\n        temperature *= cooling_high_variance\n    else:\n        temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)\n\n    return heuristics\n\n[Heuristics 15th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining node desirability, edge attraction, and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 1.0\n    sparsification_percentile = 40.0\n    high_variance_threshold = 0.1\n    cooling_high_variance = 0.99\n    cooling_low_variance = 0.999\n    min_temperature = 0.01\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction (inverse square distance)\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    # Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    savings = savings / (np.max(savings) + epsilon)\n\n    # Initial Heuristic Combination\n    temperature = initial_temperature\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (edge_attraction[i, j] + savings[i,j]) * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive Cooling\n    edge_values = heuristics[heuristics > 0]\n    if len(edge_values) > 0:\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0\n\n    if edge_variance > high_variance_threshold:\n        temperature *= cooling_high_variance\n    else:\n        temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)\n\n    return heuristics\n\n[Heuristics 16th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic combining node desirability, edge attraction, and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    initial_temperature = 1.0\n    sparsification_percentile = 40.0\n    high_variance_threshold = 0.1\n    cooling_high_variance = 0.99\n    cooling_low_variance = 0.999\n    min_temperature = 0.01\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction (inverse square distance)\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    # Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    savings = savings / (np.max(savings) + epsilon)\n\n    # Initial Heuristic Combination\n    temperature = initial_temperature\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (edge_attraction[i, j] + savings[i,j]) * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive Cooling\n    edge_values = heuristics[heuristics > 0]\n    if len(edge_values) > 0:\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0\n\n    if edge_variance > high_variance_threshold:\n        temperature *= cooling_high_variance\n    else:\n        temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)\n\n    return heuristics\n\n[Heuristics 17th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray,\n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability)  # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Edge Attraction (Gravity Model):\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # 3. Contextual Node Connectivity (New):\n    #    Penalize edges connecting nodes that are already highly connected to each other.\n    node_connectivity = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Count common neighbors\n                common_neighbors = 0\n                for k in range(n):\n                    if k != i and k != j:\n                        common_neighbors += (1 if (distance_matrix[i, k] < np.inf and distance_matrix[j, k] < np.inf) else 0) # Check for valid edges, instead of 0 distance\n                node_connectivity[i, j] = common_neighbors / (n - 2 + epsilon) # normalized\n            else:\n                node_connectivity[i, j] = 0.0\n\n    node_connectivity = 1 - node_connectivity #Inverse: High value if nodes *aren't* connected.\n\n    # 4. Adaptive Temperature:\n    temperature = initial_temperature\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * node_connectivity[i,j] * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 5. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top X%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 18th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic with node desirability, sparsification, and stochasticity.\"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Node Desirability (similar to v0)\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Inverse Distance (similar to v1)\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    inverse_distance /= np.max(inverse_distance)  # Normalize\n\n    # Stochasticity (similar to v1, but scaled)\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n    stochasticity_matrix = np.random.rand(n, n) * (distance_matrix / (max_distance + epsilon))\n    stochasticity_matrix /= np.max(stochasticity_matrix)  # Normalize\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (\n                    0.4 * inverse_distance[i, j] +\n                    0.3 * stochasticity_matrix[i, j] +\n                    0.3 * (node_desirability[i] + node_desirability[j]) / 2.0  # Average desirability\n                )\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    sparsification_percentile = 40.0  #Static sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    return heuristics\n\n[Heuristics 19th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic with node desirability, sparsification, and stochasticity.\"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Node Desirability (similar to v0)\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Inverse Distance (similar to v1)\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    inverse_distance /= np.max(inverse_distance)  # Normalize\n\n    # Stochasticity (similar to v1, but scaled)\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n    stochasticity_matrix = np.random.rand(n, n) * (distance_matrix / (max_distance + epsilon))\n    stochasticity_matrix /= np.max(stochasticity_matrix)  # Normalize\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (\n                    0.4 * inverse_distance[i, j] +\n                    0.3 * stochasticity_matrix[i, j] +\n                    0.3 * (node_desirability[i] + node_desirability[j]) / 2.0  # Average desirability\n                )\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    sparsification_percentile = 40.0  #Static sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    return heuristics\n\n[Heuristics 20th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Adaptive heuristic with node desirability, sparsification, and stochasticity.\"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Node Desirability (similar to v0)\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Inverse Distance (similar to v1)\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    inverse_distance /= np.max(inverse_distance)  # Normalize\n\n    # Stochasticity (similar to v1, but scaled)\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n    stochasticity_matrix = np.random.rand(n, n) * (distance_matrix / (max_distance + epsilon))\n    stochasticity_matrix /= np.max(stochasticity_matrix)  # Normalize\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (\n                    0.4 * inverse_distance[i, j] +\n                    0.3 * stochasticity_matrix[i, j] +\n                    0.3 * (node_desirability[i] + node_desirability[j]) / 2.0  # Average desirability\n                )\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    sparsification_percentile = 40.0  #Static sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    return heuristics\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}