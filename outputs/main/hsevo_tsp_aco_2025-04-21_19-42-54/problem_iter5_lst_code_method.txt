{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 2nd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 3rd]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, \n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n[Heuristics 7th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction (inverse square distance), node degree bias (favoring less-connected nodes),\n    and a simulated annealing-inspired temperature factor with adaptive cooling to balance exploration and exploitation.\n    Sparsifies the matrix by setting unpromising elements to zero.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution. Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Handle potential division by zero or near-zero distances:\n    epsilon = 1e-9  # A small value to avoid division by zero\n\n    # Temperature parameter - Decreases as algorithm runs, adaptive cooling.\n    initial_temperature = 10.0\n    temperature = initial_temperature  # Initial Temperature - High initially allows exploration.\n    min_temperature = 0.01 # Minimum Temperature\n    cooling_rate = 0.995\n    adaptive_cooling_threshold = 0.05 #When to trigger a slower cooling\n\n    # Node degree bias (favor less-connected nodes - initially uniform).  Will become dynamic during search (simulated annealing will be used to select edges)\n    node_degrees = np.ones(n)  # Initially all nodes are considered equally unconnected\n\n\n    # Gravitational attraction component (scaled and temperature modulated):\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                #Edge score combination\n                distance_factor = 1.0 / (distance_matrix[i, j]**2 + epsilon)\n                degree_factor = node_degrees[i] * node_degrees[j] #Promotes connections to nodes with lower degree\n                heuristics[i, j] = distance_factor * degree_factor * temperature\n            else:\n                heuristics[i, j] = 0.0  # No self-loops\n\n    #Sparsification (attempt to cut off edges that are considered too long, only if temperature is low enough)\n    sparsification_threshold = np.mean(heuristics) * 0.1 # Dynamic threshold relative to average heuristic value. Can be further tuned.\n    if temperature < 1.0:\n      heuristics[heuristics < sparsification_threshold] = 0.0\n\n    # Annealing Schedule - Decay of the temperature (adaptive)\n    if np.sum(heuristics > 0) / (n * (n - 1)) < adaptive_cooling_threshold: #If many edges are already zeroed out, cool more slowly\n      cooling_rate = 0.999 #Slow down cooling\n    temperature *= cooling_rate #Reduce temperature each step (simulates cooling)\n    temperature = max(temperature, min_temperature) #Ensure the temperature doesnt drop below the minimum\n\n    return heuristics\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction, shortest path considerations,\n    and adaptive temperature to prioritize promising edges while maintaining diversity.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                      Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9  # Avoid division by zero\n\n    # Shortest Path Influence:\n    #  - Estimate shortest path distances between all node pairs using Floyd-Warshall or Dijkstra.\n    #  - Edges on or close to these shortest paths are likely to be useful.\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    # Adaptive Temperature:\n    # - Start with a high temperature for exploration and reduce it adaptively based on the\n    #   average edge distance in the current best solution (if available, otherwise use initial average distance).\n    initial_temperature = 1.0\n    current_temperature = initial_temperature\n\n    #Calculate mean distance\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    # Gravitational attraction and shortest path influence:\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Prioritize edges on or near shortest paths:\n                shortest_path_factor = np.exp(-shortest_path_matrix[i, j] / mean_distance) # Higher if closer to shortest path\n\n                #Gravitational attraction with temperature modulation:\n                gravitational_attraction = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * current_temperature\n\n                #Combine the factors:\n                heuristics[i, j] = gravitational_attraction + shortest_path_factor\n\n            else:\n                heuristics[i, j] = 0.0  # No self-loops\n\n    # Sparsification: Remove edges with very low heuristic values to focus on the most promising ones.\n    threshold = np.percentile(heuristics[heuristics > 0], 20)  # Keep top 80%\n    heuristics[heuristics < threshold] = 0\n\n    # Adaptive Cooling: Adjust temperature dynamically (example - based on some iteration count or a performance metric)\n    current_temperature *= 0.995\n\n    return heuristics\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction, shortest path considerations,\n    and adaptive temperature to prioritize promising edges while maintaining diversity.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                      Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9  # Avoid division by zero\n\n    # Shortest Path Influence:\n    #  - Estimate shortest path distances between all node pairs using Floyd-Warshall or Dijkstra.\n    #  - Edges on or close to these shortest paths are likely to be useful.\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    # Adaptive Temperature:\n    # - Start with a high temperature for exploration and reduce it adaptively based on the\n    #   average edge distance in the current best solution (if available, otherwise use initial average distance).\n    initial_temperature = 1.0\n    current_temperature = initial_temperature\n\n    #Calculate mean distance\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    # Gravitational attraction and shortest path influence:\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Prioritize edges on or near shortest paths:\n                shortest_path_factor = np.exp(-shortest_path_matrix[i, j] / mean_distance) # Higher if closer to shortest path\n\n                #Gravitational attraction with temperature modulation:\n                gravitational_attraction = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * current_temperature\n\n                #Combine the factors:\n                heuristics[i, j] = gravitational_attraction + shortest_path_factor\n\n            else:\n                heuristics[i, j] = 0.0  # No self-loops\n\n    # Sparsification: Remove edges with very low heuristic values to focus on the most promising ones.\n    threshold = np.percentile(heuristics[heuristics > 0], 20)  # Keep top 80%\n    heuristics[heuristics < threshold] = 0\n\n    # Adaptive Cooling: Adjust temperature dynamically (example - based on some iteration count or a performance metric)\n    current_temperature *= 0.995\n\n    return heuristics\n\n[Heuristics 10th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction (inverse square distance), node degree desirability,\n    and a simulated annealing-inspired temperature factor with sparsification\n    to prioritize shorter edges, encourage balanced node degrees, and prevent premature convergence.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                      Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Handle potential division by zero or near-zero distances:\n    epsilon = 1e-9  # A small value to avoid division by zero\n\n    # Temperature parameter - Decreases as algorithm runs\n    temperature = 1.0  # Initial Temperature - High initially allows exploration.\n\n    # Node Degree Desirability Component:\n    # Encourages nodes to have a degree close to 2 (typical for TSP solutions)\n    degree_penalty = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty[i, j] = 1.0 # Initialize to 1, then penalize bad edges\n\n    # Gravitational attraction component (scaled and temperature modulated):\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * temperature * degree_penalty[i, j]\n            else:\n                heuristics[i, j] = 0.0  # No self loops\n\n    # Annealing Schedule - Decay of the temperature\n    temperature *= 0.995  # Reduce temperature each step (simulates cooling)\n\n    # Sparsification:  Zero out less promising edges\n    threshold = np.mean(heuristics) * 0.1 # Dynamic threshold based on mean\n    heuristics[heuristics < threshold] = 0.0 # Zero out low-priority edges.\n\n    return heuristics\n\n[Heuristics 11th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, savings, node degree, and triangle inequality.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / (distance_matrix + 1e-9)\n\n    # 2. Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    heuristic_matrix += savings / (distance_matrix + 1e-9)\n\n    # 3. Node degree centrality adjustment\n    degree_centrality = np.sum(heuristic_matrix, axis=0)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristic_matrix[i, j] /= (degree_centrality[i] * degree_centrality[j])**0.25\n\n    # 4. Triangle Inequality violation penalty\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                shortcut_factor = 0.0\n                for k in range(n):\n                    if i != k and j != k:\n                         potential_shortcut = distance_matrix[i, k] + distance_matrix[k, j]\n                         shortcut_factor += np.maximum(0, (distance_matrix[i, j] - potential_shortcut) / distance_matrix[i,j])\n                heuristic_matrix[i, j] /= (1 + shortcut_factor/ n )\n\n    # 5. Symmetry correction\n    heuristic_matrix = (heuristic_matrix + heuristic_matrix.T) / 2\n\n    # 6. Normalization to encourage exploration (focus on diversity)\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix))\n    \n    return heuristic_matrix\n\n[Heuristics 12th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density scaling, and center proximity\n    to balance exploitation and exploration in TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Reciprocal Rank (exploration)\n    reciprocal_rank = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        distances = distance_matrix[i, :]\n        ranks = np.argsort(distances)\n        ranked_distances = np.argsort(ranks) + 1\n        reciprocal_rank[i, :] = 1 / ranked_distances\n\n    # 3. Adaptive Density Scaling (adjust exploration)\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 4. Center Proximity Factor (global context)\n    center_x = np.mean(np.arange(n))\n    center_y = np.mean(np.arange(n))\n    center_distances = np.sqrt((np.arange(n) - center_x)**2 + (np.arange(n) - center_y)**2)\n    center_proximity_factor = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            center_proximity_factor[i, j] = 1.0 - (abs(center_distances[i] - center_distances[j]) / (np.max(center_distances) + 1e-9))\n\n\n    # Combine factors: Inverse distance + adaptive exploration + global context.\n    heuristic_matrix = inverse_distance + density_scaling * reciprocal_rank + 0.2 * center_proximity_factor\n\n    # Normalize\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n\n    return heuristic_matrix\n\n[Heuristics 13th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, adaptive density scaling, and center proximity\n    to balance exploitation and exploration in TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Reciprocal Rank (exploration)\n    reciprocal_rank = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        distances = distance_matrix[i, :]\n        ranks = np.argsort(distances)\n        ranked_distances = np.argsort(ranks) + 1\n        reciprocal_rank[i, :] = 1 / ranked_distances\n\n    # 3. Adaptive Density Scaling (adjust exploration)\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 4. Center Proximity Factor (global context)\n    center_x = np.mean(np.arange(n))\n    center_y = np.mean(np.arange(n))\n    center_distances = np.sqrt((np.arange(n) - center_x)**2 + (np.arange(n) - center_y)**2)\n    center_proximity_factor = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            center_proximity_factor[i, j] = 1.0 - (abs(center_distances[i] - center_distances[j]) / (np.max(center_distances) + 1e-9))\n\n\n    # Combine factors: Inverse distance + adaptive exploration + global context.\n    heuristic_matrix = inverse_distance + density_scaling * reciprocal_rank + 0.2 * center_proximity_factor\n\n    # Normalize\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n\n    return heuristic_matrix\n\n[Heuristics 14th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse distance, gravity, and degree for TSP.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n    inverse_distance = 1.0 / (distance_matrix + 1e-6)\n    total_distances = np.sum(distance_matrix, axis=0)\n    gravity_potential = total_distances / np.mean(total_distances)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristic_matrix[i, j] = inverse_distance[i, j] * (gravity_potential[i] + gravity_potential[j])\n            else:\n                heuristic_matrix[i, j] = 0\n\n    node_degree = np.sum(heuristic_matrix > 0, axis=0)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty = 1 / (node_degree[i] + node_degree[j] + 1e-9)\n                heuristic_matrix[i,j] = heuristic_matrix[i, j] * degree_penalty\n    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9)\n    return heuristic_matrix\n\n[Heuristics 15th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    TSP heuristics: Combines inverse distance, savings, node centrality, and randomness.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n    heuristics += inverse_distance\n\n    # Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    heuristics += savings / (distance_matrix + np.eye(n))\n\n    # Node centrality\n    node_centrality = np.sum(inverse_distance, axis=1)\n    edge_centrality = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            edge_centrality[i, j] = node_centrality[i] * node_centrality[j]\n    heuristics += inverse_distance * (edge_centrality**0.5 + 1e-9)\n\n    # Randomness\n    randomness = np.random.rand(n, n) * 0.05\n    heuristics += randomness\n\n    heuristics[np.diag_indices_from(heuristics)] = -1\n\n    heuristics = (heuristics - np.min(heuristics)) / (np.max(heuristics) - np.min(heuristics))\n\n    return heuristics\n\n[Heuristics 16th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node importance, and global connection boost.\n    Dynamically balances exploitation/exploration.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    heuristics = 1.0 / (distance_matrix + 1e-9)\n\n    # Node importance\n    node_importance = np.sum(distance_matrix, axis=1)\n    for i in range(n):\n        for j in range(n):\n           heuristics[i,j] = heuristics[i, j] * ((1/(node_importance[i]+ 1e-9)) + (1/(node_importance[j] + 1e-9)))\n\n    # Global Connection Boost\n    for i in range(n):\n        for j in range(n):\n            avg_dist_to_neighbors_i = np.mean(distance_matrix[i,:])\n            avg_dist_to_neighbors_j = np.mean(distance_matrix[j,:])\n            heuristics[i,j] = heuristics[i,j] * (avg_dist_to_neighbors_i + avg_dist_to_neighbors_j)\n\n    # Sparsify (Adaptive)\n    threshold = np.mean(heuristics) * 0.1 #dynamic sparsification\n    heuristics[heuristics < threshold] = 0\n\n    # Zero out self-loops\n    for i in range(n):\n        heuristics[i, i] = 0.0\n\n    return heuristics\n\n[Heuristics 17th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, node importance, and global connection boost.\n    Dynamically balances exploitation/exploration.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix, dtype=float)\n\n    # Inverse distance\n    heuristics = 1.0 / (distance_matrix + 1e-9)\n\n    # Node importance\n    node_importance = np.sum(distance_matrix, axis=1)\n    for i in range(n):\n        for j in range(n):\n           heuristics[i,j] = heuristics[i, j] * ((1/(node_importance[i]+ 1e-9)) + (1/(node_importance[j] + 1e-9)))\n\n    # Global Connection Boost\n    for i in range(n):\n        for j in range(n):\n            avg_dist_to_neighbors_i = np.mean(distance_matrix[i,:])\n            avg_dist_to_neighbors_j = np.mean(distance_matrix[j,:])\n            heuristics[i,j] = heuristics[i,j] * (avg_dist_to_neighbors_i + avg_dist_to_neighbors_j)\n\n    # Sparsify (Adaptive)\n    threshold = np.mean(heuristics) * 0.1 #dynamic sparsification\n    heuristics[heuristics < threshold] = 0\n\n    # Zero out self-loops\n    for i in range(n):\n        heuristics[i, i] = 0.0\n\n    return heuristics\n\n[Heuristics 18th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, stochasticity, node degree, and a global penalization.\n    Balances exploitation and exploration, prevents premature convergence.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    node_degree = np.sum(inverse_distance, axis=0)\n    node_degree_matrix = np.outer(node_degree, node_degree)\n\n    min_dist_to_node = np.min(distance_matrix + np.diag([np.inf]*n), axis=0)\n    node_min_dist_matrix = np.outer(min_dist_to_node,min_dist_to_node)\n    penalization_matrix = distance_matrix * (node_min_dist_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                stochasticity = np.random.rand() * (distance_matrix[i,j] / max_distance)\n                heuristic_matrix[i, j] = inverse_distance[i,j] + stochasticity + 0.01 * node_degree_matrix[i,j] - 0.00001 * penalization_matrix[i,j]\n            else:\n                heuristic_matrix[i, j] = 0\n\n    return heuristic_matrix\n\n[Heuristics 19th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, stochasticity, node degree, and a global penalization.\n    Balances exploitation and exploration, prevents premature convergence.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    node_degree = np.sum(inverse_distance, axis=0)\n    node_degree_matrix = np.outer(node_degree, node_degree)\n\n    min_dist_to_node = np.min(distance_matrix + np.diag([np.inf]*n), axis=0)\n    node_min_dist_matrix = np.outer(min_dist_to_node,min_dist_to_node)\n    penalization_matrix = distance_matrix * (node_min_dist_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                stochasticity = np.random.rand() * (distance_matrix[i,j] / max_distance)\n                heuristic_matrix[i, j] = inverse_distance[i,j] + stochasticity + 0.01 * node_degree_matrix[i,j] - 0.00001 * penalization_matrix[i,j]\n            else:\n                heuristic_matrix[i, j] = 0\n\n    return heuristic_matrix\n\n[Heuristics 20th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance, stochasticity, node degree, and a global penalization.\n    Balances exploitation and exploration, prevents premature convergence.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)\n\n    node_degree = np.sum(inverse_distance, axis=0)\n    node_degree_matrix = np.outer(node_degree, node_degree)\n\n    min_dist_to_node = np.min(distance_matrix + np.diag([np.inf]*n), axis=0)\n    node_min_dist_matrix = np.outer(min_dist_to_node,min_dist_to_node)\n    penalization_matrix = distance_matrix * (node_min_dist_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                stochasticity = np.random.rand() * (distance_matrix[i,j] / max_distance)\n                heuristic_matrix[i, j] = inverse_distance[i,j] + stochasticity + 0.01 * node_degree_matrix[i,j] - 0.00001 * penalization_matrix[i,j]\n            else:\n                heuristic_matrix[i, j] = 0\n\n    return heuristic_matrix\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}