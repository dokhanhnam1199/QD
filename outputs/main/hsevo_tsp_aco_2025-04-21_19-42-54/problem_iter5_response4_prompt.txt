{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction, shortest path considerations,\n    and adaptive temperature to prioritize promising edges while maintaining diversity.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                      Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9  # Avoid division by zero\n\n    # Shortest Path Influence:\n    #  - Estimate shortest path distances between all node pairs using Floyd-Warshall or Dijkstra.\n    #  - Edges on or close to these shortest paths are likely to be useful.\n    shortest_path_matrix = np.copy(distance_matrix)\n    for k in range(n):\n        for i in range(n):\n            for j in range(n):\n                shortest_path_matrix[i, j] = min(shortest_path_matrix[i, j], shortest_path_matrix[i, k] + shortest_path_matrix[k, j])\n\n    # Adaptive Temperature:\n    # - Start with a high temperature for exploration and reduce it adaptively based on the\n    #   average edge distance in the current best solution (if available, otherwise use initial average distance).\n    initial_temperature = 1.0\n    current_temperature = initial_temperature\n\n    #Calculate mean distance\n    mean_distance = np.mean(distance_matrix[distance_matrix != 0])\n\n    # Gravitational attraction and shortest path influence:\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Prioritize edges on or near shortest paths:\n                shortest_path_factor = np.exp(-shortest_path_matrix[i, j] / mean_distance) # Higher if closer to shortest path\n\n                #Gravitational attraction with temperature modulation:\n                gravitational_attraction = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * current_temperature\n\n                #Combine the factors:\n                heuristics[i, j] = gravitational_attraction + shortest_path_factor\n\n            else:\n                heuristics[i, j] = 0.0  # No self-loops\n\n    # Sparsification: Remove edges with very low heuristic values to focus on the most promising ones.\n    threshold = np.percentile(heuristics[heuristics > 0], 20)  # Keep top 80%\n    heuristics[heuristics < threshold] = 0\n\n    # Adaptive Cooling: Adjust temperature dynamically (example - based on some iteration count or a performance metric)\n    current_temperature *= 0.995\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    TSP heuristics: Combines inverse distance, savings, node centrality, and randomness.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Inverse distance\n    inverse_distance = 1 / (distance_matrix + np.eye(n))\n    heuristics += inverse_distance\n\n    # Savings Heuristic\n    depot = 0\n    savings = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                savings[i, j] = distance_matrix[depot, i] + distance_matrix[depot, j] - distance_matrix[i, j]\n    heuristics += savings / (distance_matrix + np.eye(n))\n\n    # Node centrality\n    node_centrality = np.sum(inverse_distance, axis=1)\n    edge_centrality = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            edge_centrality[i, j] = node_centrality[i] * node_centrality[j]\n    heuristics += inverse_distance * (edge_centrality**0.5 + 1e-9)\n\n    # Randomness\n    randomness = np.random.rand(n, n) * 0.05\n    heuristics += randomness\n\n    heuristics[np.diag_indices_from(heuristics)] = -1\n\n    heuristics = (heuristics - np.min(heuristics)) / (np.max(heuristics) - np.min(heuristics))\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristic utilizes an adaptive temperature based on edge variance, sparsification, and node desirability, while the bottom heuristic relies on inverse distance, stochasticity, node degree, and global penalization without normalization or adaptive parameters. (2nd best) vs (second worst) are identical, implying the ranker does not differentiate between them. Comparing (1st) vs (2nd), we see only hyperparameter differences. (3rd) vs (4th) are identical, implying the ranker does not differentiate between them. Comparing (second worst) vs (worst), heuristics 19 and 20 are exactly the same. Overall: The best heuristics incorporate adaptive mechanisms like temperature scaling based on variance and dynamic sparsification, whereas the worst rely on static combinations of factors, often without normalization, adaptive elements, or clear mechanisms to balance exploration and exploitation. Normalization and adaptive parameters are key differentiators.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined definition of \"Current Self-Reflection\" focused on designing better heuristics, built around your feedback and designed to avoid ineffective practices:\n\n**Current Self-Reflection (Refined):**\n\n*   **Keywords:** Adaptive, Dynamic, Normalized, Contextual, Exploration/Exploitation.\n*   **Advice:** Design heuristics that dynamically adapt their components (e.g., parameters, search strategies) based on the problem context and search progress. Normalize data before combining factors. Prioritize balancing exploration and exploitation throughout the search.\n*   **Avoid:** Static parameters, combinations of unnormalized factors, ignoring global context.\n*   **Explanation:** Effective heuristics need to react intelligently to the problem instance and search trajectory. Rigidity and a lack of awareness leads to suboptimal performance. Normalization ensures fair evaluation and contribution of features or components.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}