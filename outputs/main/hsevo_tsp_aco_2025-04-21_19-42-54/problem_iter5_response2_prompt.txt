{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Enhanced Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction (inverse square distance), node degree desirability,\n    and a simulated annealing-inspired temperature factor with sparsification\n    to prioritize shorter edges, encourage balanced node degrees, and prevent premature convergence.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                      Higher values suggest higher priority.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Handle potential division by zero or near-zero distances:\n    epsilon = 1e-9  # A small value to avoid division by zero\n\n    # Temperature parameter - Decreases as algorithm runs\n    temperature = 1.0  # Initial Temperature - High initially allows exploration.\n\n    # Node Degree Desirability Component:\n    # Encourages nodes to have a degree close to 2 (typical for TSP solutions)\n    degree_penalty = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                degree_penalty[i, j] = 1.0 # Initialize to 1, then penalize bad edges\n\n    # Gravitational attraction component (scaled and temperature modulated):\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * temperature * degree_penalty[i, j]\n            else:\n                heuristics[i, j] = 0.0  # No self loops\n\n    # Annealing Schedule - Decay of the temperature\n    temperature *= 0.995  # Reduce temperature each step (simulates cooling)\n\n    # Sparsification:  Zero out less promising edges\n    threshold = np.mean(heuristics) * 0.1 # Dynamic threshold based on mean\n    heuristics[heuristics < threshold] = 0.0 # Zero out low-priority edges.\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the top heuristic utilizes an adaptive temperature based on edge variance, sparsification, and node desirability, while the bottom heuristic relies on inverse distance, stochasticity, node degree, and global penalization without normalization or adaptive parameters. (2nd best) vs (second worst) are identical, implying the ranker does not differentiate between them. Comparing (1st) vs (2nd), we see only hyperparameter differences. (3rd) vs (4th) are identical, implying the ranker does not differentiate between them. Comparing (second worst) vs (worst), heuristics 19 and 20 are exactly the same. Overall: The best heuristics incorporate adaptive mechanisms like temperature scaling based on variance and dynamic sparsification, whereas the worst rely on static combinations of factors, often without normalization, adaptive elements, or clear mechanisms to balance exploration and exploitation. Normalization and adaptive parameters are key differentiators.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined definition of \"Current Self-Reflection\" focused on designing better heuristics, built around your feedback and designed to avoid ineffective practices:\n\n**Current Self-Reflection (Refined):**\n\n*   **Keywords:** Adaptive, Dynamic, Normalized, Contextual, Exploration/Exploitation.\n*   **Advice:** Design heuristics that dynamically adapt their components (e.g., parameters, search strategies) based on the problem context and search progress. Normalize data before combining factors. Prioritize balancing exploration and exploitation throughout the search.\n*   **Avoid:** Static parameters, combinations of unnormalized factors, ignoring global context.\n*   **Explanation:** Effective heuristics need to react intelligently to the problem instance and search trajectory. Rigidity and a lack of awareness leads to suboptimal performance. Normalization ensures fair evaluation and contribution of features or components.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}