{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n                  epsilon: float = 2.5408635945870555e-07,\n                  initial_temperature: float = 3.301010802589493,\n                  sparsification_percentile: float = 49.22462731911484,\n                  high_variance_threshold: float = 0.46775928005587575,\n                  cooling_high_variance: float = 0.9952012128425347,\n                  cooling_low_variance: float = 0.9959702645502414,\n                  min_temperature: float = 0.056679815478619666) -> np.ndarray:\n    \"\"\"\n    Adaptive Heuristics for TSP Edge Prioritization:\n    Combines gravitational attraction with node-based desirability scores and\n    an adaptive temperature to balance exploration and exploitation. Also includes sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the TSP instance.\n        epsilon (float): Small value to avoid division by zero. Default is 1e-9.\n        initial_temperature (float): Initial temperature for exploration. Default is 1.0.\n        sparsification_percentile (float): Percentile for sparsification. Default is 40.0.\n        high_variance_threshold (float): Threshold for high variance in edge values. Default is 0.1.\n        cooling_high_variance (float): Cooling factor when variance is high. Default is 0.99.\n        cooling_low_variance (float): Cooling factor when variance is low. Default is 0.999.\n        min_temperature (float): Minimum temperature to prevent it from becoming too small. Default is 0.01.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     Higher values suggest higher priority. Sparsified.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Node Desirability Scores:\n    #    Nodes connected to very long edges are considered \"undesirable\" and vice-versa.\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))  # Sum of inverse distances\n    node_desirability /= np.max(node_desirability) # Normalize 0-1.  More connected nodes = higher score\n\n    # 2. Adaptive Temperature:\n    #    Start with a high temperature for exploration, decrease adaptively based on the variance\n    #    of edge costs in the current solution. If variance is high, explore more.\n    temperature = initial_temperature\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon))\n            else:\n                edge_attraction[i, j] = 0.0\n\n    edge_attraction = edge_attraction / np.max(edge_attraction)  # Normalize between 0 and 1\n\n    # Combine all factors and sparsify\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j]) * temperature\n            else:\n                heuristics[i, j] = 0.0\n\n    # 3. Sparsification: Zero out low-probability edges to focus search\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)  # Keep top 60%\n    heuristics[heuristics < threshold] = 0.0\n\n    # Adaptive cooling\n    edge_values = heuristics[heuristics > 0]\n\n    if len(edge_values) > 0: #Prevent errors if all zero.\n        edge_variance = np.var(edge_values)\n    else:\n        edge_variance = 0.0 #No edges.\n\n    if edge_variance > high_variance_threshold:  #High Variance explore more\n         temperature *= cooling_high_variance\n    else: #Low variance, exploit more\n         temperature *= cooling_low_variance\n\n    temperature = max(temperature, min_temperature)  # Prevent temperature from becoming too small\n\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines inverse distance, adaptive density, and sparsification for TSP.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Avoid division by zero and self-loops\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n    np.fill_diagonal(distance_matrix, np.inf)\n\n    # 1. Inverse Distance\n    inverse_distance = 1 / distance_matrix\n\n    # 2. Adaptive Density Scaling\n    node_densities = np.zeros(n)\n    for i in range(n):\n        node_densities[i] = np.mean(inverse_distance[i, :])\n\n    density_scaling = 1.0 / (1.0 + node_densities)\n    density_scaling = np.tile(density_scaling, (n, 1))\n    density_scaling = np.minimum(density_scaling, density_scaling.T)\n\n    # 3. Sparsification (adaptive threshold)\n    threshold = np.mean(inverse_distance) * 0.5  # Dynamic threshold\n    sparse_inverse_distance = np.where(inverse_distance > threshold, inverse_distance, 0)\n\n    # Combine factors\n    heuristic_matrix = sparse_inverse_distance * density_scaling\n\n    # Normalize\n    max_val = np.max(heuristic_matrix)\n    min_val = np.min(heuristic_matrix)\n    if max_val > min_val: # Avoid division by zero\n        heuristic_matrix = (heuristic_matrix - min_val) / (max_val - min_val)\n    else:\n        heuristic_matrix = np.zeros_like(heuristic_matrix)\n\n\n    return heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses adaptive temperature scaling based on edge variance, a concept absent in the worst, which uses fixed weights for inverse distance, stochasticity, and node desirability. (2nd best) vs (second worst) are identical, suggesting their ranking may be due to random variation during evaluation. Comparing (1st) vs (2nd), we see only difference in parameters. (3rd) vs (4th) are also identical, indicating a similar situation. Comparing (second worst) vs (worst), they are identical. Overall: The top heuristics focus on adaptive parameters based on the evolving search state, while the worst relies on fixed weighting of simpler components. Incorporating adaptive mechanisms is key to better performance. The use of shortest paths, savings heuristics, and contextual node connectivity appear sporadically across the rankings.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined perspective on self-reflection for designing better heuristics:\n\n*   **Keywords:** Dynamic Calibration, Contextual Awareness, Exploration-Exploitation Balance, Adaptive Strategies.\n*   **Advice:** Focus on creating self-aware heuristics that adapt based on real-time performance and global solution context. Use dynamic calibration to continuously optimize the weights of different heuristic components.\n*   **Avoid:** Static parameter settings and heuristic combinations lacking a feedback loop. Avoid relying solely on local information.\n*   **Explanation:** Design heuristics that \"learn\" during the search, dynamically adjusting parameters and strategies based on their impact on solution quality. Integrate global context to guide the search towards promising regions and prevent stagnation.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}