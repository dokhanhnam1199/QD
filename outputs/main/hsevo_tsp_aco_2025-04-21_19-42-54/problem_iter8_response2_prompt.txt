{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive edge prioritization with node desirability and sparsification.\"\"\"\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n    epsilon = 1e-9\n    temperature = 1.0\n\n    # Node Desirability\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Edge Attraction\n    edge_attraction = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                edge_attraction[i, j] = (1.0 / (distance_matrix[i, j]**2 + epsilon)) * temperature\n            else:\n                edge_attraction[i, j] = 0.0\n    edge_attraction = edge_attraction / np.max(edge_attraction)\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = edge_attraction[i, j] * (node_desirability[i] + node_desirability[j])\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], 40)\n    heuristics[heuristics < threshold] = 0.0\n\n    temperature *= 0.995\n    return heuristics\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive heuristic with node desirability, sparsification, and stochasticity.\"\"\"\n    n = distance_matrix.shape[0]\n    epsilon = 1e-9\n    heuristics = np.zeros_like(distance_matrix)\n\n    # Node Desirability (similar to v0)\n    node_desirability = np.zeros(n)\n    for i in range(n):\n        node_desirability[i] = np.sum(1.0 / (distance_matrix[i, :] + epsilon))\n    node_desirability /= np.max(node_desirability)\n\n    # Inverse Distance (similar to v1)\n    inverse_distance = 1.0 / (distance_matrix + epsilon)\n    inverse_distance /= np.max(inverse_distance)  # Normalize\n\n    # Stochasticity (similar to v1, but scaled)\n    max_distance = np.max(distance_matrix[np.isfinite(distance_matrix)])\n    stochasticity_matrix = np.random.rand(n, n) * (distance_matrix / (max_distance + epsilon))\n    stochasticity_matrix /= np.max(stochasticity_matrix)  # Normalize\n\n    # Combine factors\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                heuristics[i, j] = (\n                    0.4 * inverse_distance[i, j] +\n                    0.3 * stochasticity_matrix[i, j] +\n                    0.3 * (node_desirability[i] + node_desirability[j]) / 2.0  # Average desirability\n                )\n            else:\n                heuristics[i, j] = 0.0\n\n    # Sparsification\n    sparsification_percentile = 40.0  #Static sparsification\n    threshold = np.percentile(heuristics[heuristics > 0], sparsification_percentile)\n    heuristics[heuristics < threshold] = 0.0\n\n    return heuristics\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses adaptive temperature scaling based on edge variance, a concept absent in the worst, which uses fixed weights for inverse distance, stochasticity, and node desirability. (2nd best) vs (second worst) are identical, suggesting their ranking may be due to random variation during evaluation. Comparing (1st) vs (2nd), we see only difference in parameters. (3rd) vs (4th) are also identical, indicating a similar situation. Comparing (second worst) vs (worst), they are identical. Overall: The top heuristics focus on adaptive parameters based on the evolving search state, while the worst relies on fixed weighting of simpler components. Incorporating adaptive mechanisms is key to better performance. The use of shortest paths, savings heuristics, and contextual node connectivity appear sporadically across the rankings.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, here's a refined perspective on self-reflection for designing better heuristics:\n\n*   **Keywords:** Dynamic Calibration, Contextual Awareness, Exploration-Exploitation Balance, Adaptive Strategies.\n*   **Advice:** Focus on creating self-aware heuristics that adapt based on real-time performance and global solution context. Use dynamic calibration to continuously optimize the weights of different heuristic components.\n*   **Avoid:** Static parameter settings and heuristic combinations lacking a feedback loop. Avoid relying solely on local information.\n*   **Explanation:** Design heuristics that \"learn\" during the search, dynamically adjusting parameters and strategies based on their impact on solution quality. Integrate global context to guide the search towards promising regions and prevent stagnation.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}