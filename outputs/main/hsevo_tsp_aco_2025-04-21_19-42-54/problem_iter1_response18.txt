```python
import numpy as np

def heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:
    """
    Improved heuristic for TSP based on a combination of distance,
    reciprocal rank, and adaptive weighting based on local density.
    Higher values indicate a more promising edge.

    Args:
        distance_matrix: A NumPy array representing the distance matrix.
                         distance_matrix[i, j] is the distance between node i and node j.

    Returns:
        A NumPy array of the same shape as distance_matrix,
        representing the heuristic scores for each edge.
    """

    n = distance_matrix.shape[0]
    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)

    # Avoid division by zero and self-loops
    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)
    np.fill_diagonal(distance_matrix, np.inf) # ensure we don't choose self-loops

    # 1. Inverse Distance: Shorter distances are generally more desirable
    inverse_distance = 1 / distance_matrix

    # 2. Reciprocal Rank: Reward edges that are short relative to others from the same node.
    reciprocal_rank = np.zeros_like(distance_matrix, dtype=float)
    for i in range(n):
        # Get distances from node i to all other nodes
        distances = distance_matrix[i, :]
        # Get the rank of each distance from node i (lower distance = lower rank)
        ranks = np.argsort(distances)  # Indices sorted by distance
        ranked_distances = np.argsort(ranks) + 1  # Rank starts from 1
        reciprocal_rank[i, :] = 1 / ranked_distances

    # 3. Adaptive weighting: Account for local node density. Denser areas mean even short edges must be handled with caution
    #   - We calculate node density using a simple averaging of neighboring inverse distances.  High inverse distance == high node density nearby
    #   - Low local density gives greater weight to reciprocal rank to aggressively explore shorter paths, preventing traps
    #   - High local density deemphasizes reciprocal rank, and instead focuses on pure distance.
    node_densities = np.zeros(n)
    for i in range(n):
        node_densities[i] = np.mean(inverse_distance[i, :])

    # Weight scaling factor based on density. Higher local density, reduces exploration.
    density_scaling = 1.0 / (1.0 + node_densities) # Scale densities so they become penalty coefficients.
    density_scaling = np.tile(density_scaling, (n,1))
    density_scaling = np.minimum(density_scaling, density_scaling.T) #take lower density factor between connected nodes


    # Combine heuristics: inverse distance contributes heavily to base utility, exploration reciprocal rank to boost better ones
    heuristic_matrix = inverse_distance + density_scaling * reciprocal_rank # density acts as a weighting penalty on exploration.

    # Normalize heuristic matrix so stochastic sampling isn't overly sensitive to scale.
    heuristic_matrix = (heuristic_matrix - np.min(heuristic_matrix)) / (np.max(heuristic_matrix) - np.min(heuristic_matrix) + 1e-9) # scaled [0,1]

    return heuristic_matrix
```
