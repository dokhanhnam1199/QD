{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines Best Fit with strong preference for tight/perfect fits.\n    Uses inverse of remaining waste + epsilon for numerical stability,\n    and clear disqualification for unfit bins with -inf.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity (waste) if the item is placed.\n    # This value will be non-negative for bins where the item fits.\n    remaining_waste = bins_remain_cap[can_fit_mask] - item\n\n    # Add a small epsilon to the denominator for numerical stability.\n    # This prevents division by zero for perfect fits (waste = 0) and\n    # ensures very large, but finite, priority for them.\n    # It also handles very small positive waste values gracefully.\n    epsilon = np.finfo(float).eps\n\n    # The priority is the inverse of (remaining_waste + epsilon).\n    # This creates a strong non-linear preference: smaller waste yields\n    # disproportionately higher priority. Perfect fits receive the highest score (1/epsilon).\n    priorities[can_fit_mask] = 1.0 / (remaining_waste + epsilon)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which to add item to each bin, implementing an Adaptive Median Fit heuristic.\n\n    This heuristic dynamically adjusts its target remaining capacity based on the current\n    distribution of *potential* remaining capacities among bins that can fit the item.\n    It aims to select the bin that, after placing the item, would result in a remaining\n    capacity closest to the median of all possible remaining capacities among valid bins.\n\n    This approach seeks to:\n    1.  Promote a more uniform distribution of remaining bin capacities across the system,\n        reducing fragmentation where some bins are left with tiny, unusable gaps while\n        others are very large. This addresses \"Problem-aware Robustness\".\n    2.  Provide \"Adaptive Search\" by not always pursuing the absolute tightest fit (as Best Fit does),\n        but rather a fit that aligns with the current central tendency of available options.\n        This allows for more flexible bin states for future items.\n    3.  Offer \"Informed Exploration\" by considering the overall landscape of available fits\n        (the distribution of potential remaining spaces) rather than just a local minimum.\n        It subtly guides the packing towards a more balanced, sustainable state.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Scores are designed such that a perfect match to the 'target_remaining_capacity'\n        gets the highest possible score (0), while bins that cannot fit the item get -inf.\n    \"\"\"\n    # Initialize all priorities to negative infinity. This ensures that\n    # any bin that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Identify bins where the item can actually fit (remaining space >= 0).\n    can_fit_mask = potential_remaining_space >= 0\n\n    # Get the potential remaining spaces for only those bins that can fit the item.\n    fitting_bins_potential_rem_space = potential_remaining_space[can_fit_mask]\n\n    # Handle edge case: if no bin can fit the item, all priorities remain -inf.\n    # If only one bin fits, its priority will be 0 (as median is itself), ensuring it's chosen.\n    if len(fitting_bins_potential_rem_space) == 0:\n        return priorities\n\n    # Calculate the median of the potential remaining spaces among fitting bins.\n    # This acts as our dynamic \"target remaining capacity\". By seeking the median,\n    # we aim to homogenize the bin states, avoiding extremes (too tight or too loose).\n    target_remaining_capacity = np.median(fitting_bins_potential_rem_space)\n\n    # For bins where the item can fit, assign a priority based on how close\n    # their potential remaining space is to the target_remaining_capacity.\n    # We use negative absolute difference: a smaller difference (closer to target)\n    # yields a higher priority (closer to 0, which is the maximum score).\n    priorities[can_fit_mask] = -np.abs(fitting_bins_potential_rem_space - target_remaining_capacity)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd) vs (4th) vs (9th), we see they are virtually identical implementations of Best Fit with an inverse-waste priority. Their top ranking suggests this simple, non-linear preference for tight fits, using `np.finfo(float).eps` for numerical stability, is highly effective.\n\nComparing (2nd) vs (3rd), and (4th) vs (3rd), 3rd is ranked worse despite similar core logic. The key differences are that 3rd uses a hardcoded `epsilon` value as a function argument and includes unused imports (`random`, `math`, `scipy`, `torch`). This suggests that using `np.finfo(float).eps` for maximum numerical robustness and keeping imports clean contributes to better heuristic design and possibly performance.\n\nComparing (4th) vs (5th), and also the general cluster of 1st,2nd,4th,9th (simple inverse Best Fit) against 5th, 7th, 8th, 10th (fragmentation aversion/dead space penalty), the simpler inverse Best Fit consistently ranks higher. This indicates that explicitly trying to avoid \"fragmentation\" or \"dead space\" with additional penalty logic, even if well-intentioned, might introduce complexity that hinders overall performance compared to directly prioritizing the smallest waste. The non-linear `1/(waste+epsilon)` already inherently penalizes larger waste, including small \"awkward\" ones, by giving them disproportionately lower scores.\n\nComparing (6th) vs (7th), 7th is a more complex version of fragmentation aversion with multiple thresholds and bonuses/penalties. It's ranked worse than 6th (which is effectively a basic Best Fit with issues). This reinforces that increased algorithmic complexity with multiple tunable parameters can be detrimental.\n\nComparing (8th) vs (9th), 9th (simple inverse Best Fit) is better than 8th (Best Fit with dead space penalty). This further supports the strength of the `1/(waste+epsilon)` approach over explicit penalty schemes.\n\nComparing (10th) vs (11th), 11th (Adaptive Best Fit with explicit high score for perfect fits) is better than 10th (dead space penalty). This indicates that making perfect fits unequivocally dominant via a very large, fixed score (like `1e12`) can be an effective refinement, though it's still ranked below the top simple inverse Best Fit.\n\nComparing (14th) vs (15th), and analyzing the entire bottom cluster (15th-20th) of \"Adaptive Median Fit\" against the higher-ranked heuristics, the \"Adaptive Median Fit\" consistently performs the worst. This heuristic tries to select a bin whose remaining capacity is closest to the *median* of all possible remaining capacities, aiming for a \"balanced\" bin state. This strong negative result suggests that aiming for uniformity or balancing remaining bin capacities is counterproductive for bin packing, which typically benefits from aggressive waste minimization and closing bins efficiently, even if it leads to some highly filled and some empty bins.\n\nOverall: The best heuristics are robust Best Fit variants with a strong, non-linear preference for tight fits (implicit or explicit) and good numerical stability. Added complexity like fragmentation aversion or \"balancing\" strategies tend to degrade performance.\n- \n### Current self-reflection\n*   **Keywords**: Predictive State, Adaptive Logic, Global Utility, Hybrid Search.\n*   **Advice**: Design scoring that anticipates future bin configurations. Integrate adaptive strategies for dynamic bin management. Explore controlled look-ahead or multi-objective trade-offs for superior global utility.\n*   **Avoid**: Over-emphasizing only immediate greedy fits. Merely stating implementation robustness (e.g., epsilon) or error handling as core design principles.\n*   **Explanation**: Advancing heuristics requires foresight into bin states and dynamic adaptation, moving beyond local optimality. Focus on conceptual logic, not just robust implementation.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}