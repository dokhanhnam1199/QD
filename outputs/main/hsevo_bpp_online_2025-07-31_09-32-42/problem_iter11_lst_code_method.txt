{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, implementing a variation of Best Fit.\n\n    This heuristic aims to select the bin that will have the least remaining capacity\n    after the item is placed, effectively trying to \"fill up\" a bin as much as possible.\n    Bins that cannot accommodate the item receive the lowest possible priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Scores are designed such that a perfect fit (remaining capacity = 0) gets the\n        highest possible score (0), while bins that cannot fit the item get -inf.\n    \"\"\"\n    # Calculate the remaining space in each bin if the item were placed.\n    # A smaller positive value here means a \"better fit\".\n    potential_remaining_space = bins_remain_cap - item\n\n    # Initialize all priorities to negative infinity. This ensures that\n    # any bin that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit (remaining space >= 0).\n    can_fit_mask = potential_remaining_space >= 0\n\n    # For bins where the item can fit, assign a priority based on the negative\n    # of the potential remaining space.\n    # By maximizing this value, we are effectively minimizing the potential remaining space.\n    # A perfect fit (0 remaining space) will result in a priority of 0 (the highest).\n    # A larger remaining space (e.g., 0.5) will result in a lower priority (-0.5).\n    priorities[can_fit_mask] = -potential_remaining_space[can_fit_mask]\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for placing an item into bins, implementing a multi-objective\n    heuristic that combines Best Fit with strategic bin consolidation and exact fit preference.\n\n    This heuristic extends the Best Fit strategy by introducing weighted bonuses to prioritize\n    actions that lead to more efficient bin utilization and fewer open bins.\n\n    Objectives:\n    1.  **Prioritize Exact Fits (Consolidation):** Give a very high bonus to bins where the item\n        fits perfectly, resulting in 0 remaining capacity. This encourages \"closing\" bins.\n    2.  **Minimize Remaining Capacity (Best Fit):** For non-exact fits, prefer the bin that\n        will have the least remaining capacity, as per the standard Best Fit approach.\n    3.  **Prefer Partially Filled Bins (Strategic Consolidation):** Give a small bonus to bins\n        that are already partially filled (i.e., not completely empty) over completely empty bins\n        (new bins). This aims to consolidate items into existing bins, reducing the total number\n        of bins used over time.\n\n    Args:\n        item: Size of item to be added to the bin (expected to be > 0 and <= BIN_CAPACITY).\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         Assumes capacities are normalized (e.g., BIN_CAPACITY = 1.0).\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Bins that cannot accommodate the item receive the lowest possible priority (-inf).\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.full(num_bins, -np.inf, dtype=float)\n\n    # Assume a standard bin capacity for normalization. This is crucial for\n    # distinguishing between partially filled and completely empty bins.\n    BIN_CAPACITY = 1.0\n\n    # Define constants for bonus scores and floating-point comparisons.\n    # EXACT_FIT_BONUS must be significantly larger than any possible base Best Fit score\n    # (which ranges from -BIN_CAPACITY to 0).\n    EXACT_FIT_BONUS = 100.0\n\n    # PARTIALLY_FILLED_BIN_BONUS should be small enough not to override a significantly\n    # better Best Fit score, but large enough to break ties or give a slight preference.\n    PARTIALLY_FILLED_BIN_BONUS = 0.005\n\n    # Epsilon for robust floating-point comparisons, accounts for precision errors.\n    # Using np.finfo(float).eps * a_factor is generally robust for numerical stability.\n    EPSILON = np.finfo(float).eps * 100\n\n    # 1. Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # 2. Identify bins where the item can actually fit.\n    # We use a small negative tolerance for `potential_remaining_space` to account for\n    # minor floating-point inaccuracies that might make a valid fit appear slightly negative.\n    can_fit_mask = potential_remaining_space >= -EPSILON\n\n    # For scoring, ensure remaining space is non-negative. If it's slightly negative\n    # due to precision (but within fit tolerance), treat it as 0.\n    potential_remaining_space_for_scoring = np.where(potential_remaining_space < 0, 0, potential_remaining_space)\n\n    # 3. Base Score: Apply the Best Fit logic.\n    # A smaller potential_remaining_space results in a higher score (less negative).\n    # A perfect fit (0 remaining space) will get a base score of 0.\n    priorities[can_fit_mask] = -potential_remaining_space_for_scoring[can_fit_mask]\n\n    # 4. Apply Exact Fit Bonus: Strongly prioritize bins that become perfectly full.\n    # A bin is considered an exact fit if its remaining space after placing the item is\n    # very close to zero.\n    exact_fit_mask = can_fit_mask & (potential_remaining_space_for_scoring < EPSILON)\n    priorities[exact_fit_mask] += EXACT_FIT_BONUS\n\n    # 5. Apply Partially Filled Bin Bonus: Promote consolidation into existing bins.\n    # Identify bins that are not completely empty (i.e., already contain items)\n    # and are not already handled by the exact fit bonus.\n    is_empty_bin_mask = np.isclose(bins_remain_cap, BIN_CAPACITY, atol=EPSILON)\n    is_partially_filled_bin_mask = can_fit_mask & ~is_empty_bin_mask & ~exact_fit_mask\n\n    priorities[is_partially_filled_bin_mask] += PARTIALLY_FILLED_BIN_BONUS\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with Strong Perfect Fit Preference, Inverse Waste Scoring,\n    and an Adaptive Consolidation Bias to encourage filling existing bins faster.\n    \"\"\"\n    BIN_CAPACITY = 1.0\n    PERFECT_FIT_TOLERANCE = 1e-6\n    EPSILON = np.finfo(float).eps\n    MAX_PRIORITY_SCORE = 1e12\n    # A factor to weigh the consolidation bias. A small positive value ensures it\n    # influences choices among bins with similar inverse waste scores without dominating.\n    CONSOLIDATION_FACTOR = 0.1\n\n    # Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Initialize all priorities to negative infinity. Bins that cannot fit the item\n    # will retain this value, ensuring they are never chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    # We allow a tiny negative remaining space due to floating-point precision.\n    can_fit_mask = potential_remaining_space >= -PERFECT_FIT_TOLERANCE\n\n    # Extract the potential remaining spaces only for bins where the item can fit.\n    fitting_potential_rem_space = potential_remaining_space[can_fit_mask]\n    \n    # Get the original remaining capacities for these fitting bins.\n    # This is needed to calculate the current \"fullness\" before placing the item.\n    fitting_bins_original_cap = bins_remain_cap[can_fit_mask]\n\n    # Initialize priorities for fitting bins.\n    fitting_priorities = np.zeros_like(fitting_potential_rem_space)\n\n    # Identify perfect or near-perfect fits based on the defined tolerance.\n    perfect_fit_mask = np.abs(fitting_potential_rem_space) <= PERFECT_FIT_TOLERANCE\n\n    # Assign the maximal priority score for perfect fits. This strongly encourages closing bins.\n    fitting_priorities[perfect_fit_mask] = MAX_PRIORITY_SCORE\n\n    # For bins that are not a perfect fit, apply an inverse waste scoring\n    # and add a consolidation bias.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    \n    # Calculate waste for inverse scoring. Since perfect_fit_mask handles near-zero,\n    # fitting_potential_rem_space[non_perfect_fit_mask] will be positive.\n    waste_for_inverse = fitting_potential_rem_space[non_perfect_fit_mask]\n\n    # Calculate current bin fullness: (BIN_CAPACITY - current_remaining_capacity) / BIN_CAPACITY.\n    # A bin that is already 'fuller' (i.e., has less remaining capacity) gets a higher fullness score.\n    # This term acts as an adaptive, linear bias, encouraging consolidation by slightly preferring\n    # bins that are already well-filled, given similar waste.\n    current_bin_fullness = (BIN_CAPACITY - fitting_bins_original_cap[non_perfect_fit_mask]) / BIN_CAPACITY\n    \n    # Apply the inverse waste function. Adding EPSILON prevents division by zero.\n    # Then add the consolidation bias.\n    fitting_priorities[non_perfect_fit_mask] = (\n        1 / (waste_for_inverse + EPSILON) + CONSOLIDATION_FACTOR * current_bin_fullness\n    )\n\n    # Update the main priorities array with the scores for fitting bins.\n    priorities[can_fit_mask] = fitting_priorities\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines robust Best Fit with adaptive bin consolidation and explicit perfect fit prioritization.\n    Prioritizes minimal waste, encourages using fuller bins, and ensures perfect fits are optimal.\n    \"\"\"\n    # Initialize all priorities to negative infinity, unequivocally marking bins\n    # that cannot accommodate the item as unavailable.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify which bins have sufficient capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return the initialized priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Step 1: Calculate core Best Fit and Adaptive Consolidation scores ---\n\n    # Potential remaining space (waste) if the item were placed in fit-capable bins.\n    potential_remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Small epsilon for numerical stability, especially for near-zero remaining space.\n    # This prevents division by zero and ensures very small waste values don't cause issues.\n    EPSILON = np.finfo(float).eps\n\n    # Core Best Fit Component: Assign priority as the inverse of (potential remaining space + epsilon).\n    # This strongly favors bins that result in minimal remaining space (tightest fit).\n    best_fit_scores = 1.0 / (potential_remaining_space + EPSILON)\n\n    # Adaptive Consolidation Bias Component: Promote packing into already fuller bins.\n    # This encourages overall bin consolidation, aiming to reduce the total number of bins used.\n    # We use the maximum remaining capacity among *all* bins as a dynamic reference\n    # for what an \"empty\" bin looks like in the current system state.\n    max_rem_cap = np.max(bins_remain_cap)\n\n    # Calculate the relative \"fullness\" of each fitting bin. A lower bins_remain_cap\n    # indicates a fuller bin, resulting in a higher fullness score (closer to 1).\n    # Add epsilon to denominator to prevent division by zero if max_rem_cap is 0.\n    relative_fullness = (max_rem_cap - bins_remain_cap[can_fit_mask]) / (max_rem_cap + EPSILON)\n\n    # Tunable weight for the consolidation bias. A small positive value ensures this\n    # bias influences decisions without overpowering the primary waste minimization.\n    CONSOLIDATION_BIAS_WEIGHT = 0.05\n\n    # Combine the Best Fit score with the adaptive consolidation bias.\n    # This forms the base priority for all fit-capable bins.\n    priorities[can_fit_mask] = best_fit_scores + CONSOLIDATION_BIAS_WEIGHT * relative_fullness\n\n    # --- Step 2: Explicitly boost perfect fits to ensure their absolute priority ---\n\n    # Tolerance for identifying near-perfect fits to handle floating-point inaccuracies.\n    # A small absolute value is used for robustness across different bin capacities/item sizes.\n    PERFECT_FIT_TOLERANCE = 1e-9\n\n    # A very high score to ensure perfect fits are chosen over any other option.\n    # This value must be significantly larger than any possible score derived from Step 1\n    # to guarantee that a perfect fit always receives the highest priority.\n    PERFECT_FIT_SCORE = 1e30 \n\n    # Identify perfect or near-perfect fits within the fitting bins.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n\n    # Apply the super-high score to bins that are perfect fits.\n    # This overrides any score calculated in Step 1 for these specific bins,\n    # making perfect fits the unequivocally best choice.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns a priority score for each bin, implementing a Best Fit variant with\n    strategic fragmentation avoidance.\n\n    This heuristic aims to:\n    1. Strongly prioritize bins that can perfectly fit the item (leaving 0 remaining capacity).\n    2. For non-perfect fits, generally follow a Best Fit approach (preferring smaller remaining capacity).\n    3. Strategically penalize bins that would be left with a very small, potentially unusable,\n       positive remaining capacity (fragmentation avoidance).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score for each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Bins that cannot accommodate the item receive the lowest possible priority (-np.inf).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Assume a standard bin capacity for scaling purposes.\n    # In many BPP contexts, item sizes and bin capacities are normalized to 1.0.\n    # If bins can have truly different max capacities, this constant should be adapted\n    # (e.g., passed as an argument or inferred from the problem context).\n    BIN_CAPACITY = 1.0 \n    \n    # Epsilon for robust float comparisons (to distinguish true zero from very small positive floats)\n    EPSILON = 1e-9\n\n    # Mask for bins where the item can actually fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the potential remaining space if the item were placed in fit-able bins\n    potential_remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Initialize a temporary array for valid priorities to avoid modifying 'priorities' directly\n    # until all calculations for valid bins are done.\n    priorities_valid = np.zeros_like(potential_remaining_space, dtype=float)\n\n    # --- Scoring Logic ---\n\n    # 1. Highest Priority: Perfect Fit (remaining space is 0)\n    perfect_fit_mask_in_valid = (np.abs(potential_remaining_space) < EPSILON)\n    priorities_valid[perfect_fit_mask_in_valid] = 1000.0  # Assign a very high, distinct score\n\n    # 2. Non-Perfect Fits: Apply a combination of Best Fit and Strategic Fragmentation Avoidance\n    non_perfect_fit_mask_in_valid = ~perfect_fit_mask_in_valid\n    \n    # Get the remaining spaces for non-perfect fits\n    r_non_perfect = potential_remaining_space[non_perfect_fit_mask_in_valid]\n\n    # Base score for non-perfect fits: Best Fit component.\n    # A smaller 'r' (closer to 0) results in a higher score.\n    # Scale this to be in a more manageable range (e.g., 0 to 100)\n    # 0 remaining space (conceptually) gets 100, full bin (conceptually) gets 0.\n    best_fit_component = (BIN_CAPACITY - r_non_perfect) / BIN_CAPACITY * 100.0\n\n    # Strategic component: Fragmentation Penalty\n    # Penalize leaving very small, positive remaining spaces. These are often \"dead space\"\n    # that is hard to utilize for future items, leading to more bins being opened.\n    # The penalty is adaptive, considering the current item's size.\n    \n    # Define thresholds for \"fragmented\" space:\n    # - Relative to the item size: If remaining space is less than a fraction of the item size.\n    # - Absolute: If remaining space is below a fixed small value (e.g., 10% of bin capacity).\n    FRAGMENT_THRESHOLD_RELATIVE_TO_ITEM = 0.5  # e.g., < 50% of current item size\n    FRAGMENT_THRESHOLD_ABSOLUTE = 0.1 * BIN_CAPACITY # e.g., < 10% of total bin capacity\n\n    # Identify fragmented remaining spaces\n    is_fragmented_space = (r_non_perfect > EPSILON) & \\\n                          (r_non_perfect < np.minimum(item * FRAGMENT_THRESHOLD_RELATIVE_TO_ITEM, FRAGMENT_THRESHOLD_ABSOLUTE))\n\n    # Apply a penalty for fragmented spaces. The penalty's magnitude can be tuned.\n    # Penalizing based on item size makes the penalty more significant for larger items\n    # that create small, useless gaps.\n    fragmentation_penalty = np.where(is_fragmented_space, item * 50.0, 0.0) # Larger items incur higher penalty\n\n    # Combine best fit component with strategic penalty\n    strategic_priorities_non_perfect = best_fit_component - fragmentation_penalty\n    \n    # Assign the calculated scores back to the 'priorities_valid' array for non-perfect fits\n    priorities_valid[non_perfect_fit_mask_in_valid] = strategic_priorities_non_perfect\n\n    # Place the calculated valid priorities back into the main 'priorities' array\n    priorities[can_fit_mask] = priorities_valid\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\n# Assume a normalized bin capacity. In a real application, this could be\n# an argument to the function or configured based on problem specifics.\n# Using 1.0 is common when item sizes are normalized relative to bin capacity.\nBIN_CAPACITY = 1.0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which to add an item to each bin, implementing a multi-objective\n    heuristic that combines Best Fit principles with strategic considerations for\n    fragmentation, bin consolidation, and adaptive behavior based on item size.\n\n    This heuristic aims to:\n    1. Strongly prioritize exact fits (remaining capacity = 0).\n    2. Beyond exact fits, prefer bins that will have less remaining capacity (Best Fit).\n    3. Adaptively adjust priority based on the item size and the resulting bin state:\n       - Discourage creating very small, \"fragmented\" remaining spaces, especially for small items.\n       - Encourage efficient use of bins by giving a slight preference to more empty bins\n         when placing small items (a \"Worst Fit\" tendency for small items) to keep\n         nearly-full bins available for larger items to complete them.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score for each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Scores are designed such that a perfect fit receives the highest possible score,\n        while bins that cannot fit the item receive -inf.\n    \"\"\"\n    # 1. Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Initialize all priorities to negative infinity. This ensures that\n    # any bin that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit (remaining space >= 0).\n    can_fit_mask = potential_remaining_space >= 0\n    \n    # Extract relevant values for bins where the item can fit\n    valid_remaining_space = potential_remaining_space[can_fit_mask]\n    \n    # 2. Base Priority: Best Fit Component\n    # This core component prioritizes bins that will have the least remaining space.\n    # A smaller positive remaining space (closer to a perfect fit) leads to a higher\n    # (less negative, closer to 0) base score. A perfect fit (0 remaining space)\n    # gets a base score of 0.\n    priorities[can_fit_mask] = -valid_remaining_space\n\n    # --- Strategic & Adaptive Adjustments ---\n    # These constants can be tuned based on problem characteristics or desired behavior.\n    PERFECT_FIT_BONUS = 1000.0  # Large bonus to guarantee exact fits are highest priority\n    NEAR_PERFECT_FIT_BONUS = 10.0 # Bonus for very tight fits (e.g., 0.001 remaining)\n    \n    # Define a \"fragmentation zone\" for remaining capacities that are small but not trivial.\n    # Placing an item that results in remaining space within this zone might be undesirable\n    # for small items, as it leaves an awkward, hard-to-use gap.\n    FRAGMENTATION_ZONE_LOW = 0.01 * BIN_CAPACITY  # E.g., 1% of bin capacity\n    FRAGMENTATION_ZONE_HIGH = 0.10 * BIN_CAPACITY # E.g., 10% of bin capacity\n    FRAGMENTATION_PENALTY_MAGNITUDE = 5.0 # How much to penalize fragmentation\n\n    # Define what constitutes a \"small item\". Behavior changes for such items.\n    SMALL_ITEM_THRESHOLD = 0.20 * BIN_CAPACITY # E.g., items smaller than 20% of bin capacity\n\n    # Apply Perfect Fit Bonus: If the item perfectly fills a bin, it's the absolute best.\n    perfect_fit_mask = (valid_remaining_space == 0)\n    priorities[can_fit_mask][perfect_fit_mask] += PERFECT_FIT_BONUS\n\n    # Apply Near Perfect Fit Bonus: Reward bins that are almost perfectly filled.\n    # This differentiates a 0.001 remaining space from a 0.05 remaining space,\n    # making the tighter one preferred even if neither is exactly zero.\n    near_perfect_fit_mask = (valid_remaining_space > 0) & (valid_remaining_space < FRAGMENTATION_ZONE_LOW)\n    priorities[can_fit_mask][near_perfect_fit_mask] += NEAR_PERFECT_FIT_BONUS\n\n    # Fragmentation Avoidance Penalty (Adaptive):\n    # This penalty applies if placing the item would result in a remaining space\n    # that falls within the defined fragmentation zone.\n    # The penalty is primarily applied when the item itself is small, as placing\n    # a small item and leaving a medium-sized, awkward gap might be inefficient.\n    # For large items, a small remaining space usually just means the bin is well-filled,\n    # which is desirable, so we don't penalize.\n    fragmentation_zone_mask = (valid_remaining_space >= FRAGMENTATION_ZONE_LOW) & \\\n                              (valid_remaining_space < FRAGMENTATION_ZONE_HIGH)\n    \n    if item < SMALL_ITEM_THRESHOLD:\n        # Calculate a penalty that increases as the remaining space gets deeper into the\n        # fragmentation zone (from low to high threshold).\n        # This makes leaving a 0.02 gap slightly better than a 0.08 gap.\n        penalty_scale = (valid_remaining_space[fragmentation_zone_mask] - FRAGMENTATION_ZONE_LOW) / \\\n                        (FRAGMENTATION_ZONE_HIGH - FRAGMENTATION_ZONE_LOW + 1e-9) # Add epsilon to prevent div by zero\n        priorities[can_fit_mask][fragmentation_zone_mask] -= FRAGMENTATION_PENALTY_MAGNITUDE * penalty_scale\n\n    # Strategic Placement for Small Items (\"Worst Fit\" tendency):\n    # For very small items, sometimes it's better to place them in a bin that is\n    # currently more empty, to keep other partially-filled bins available for\n    # larger items to complete them. This helps consolidate small items into\n    # fewer bins and prevents creation of many partially-filled bins.\n    if item < SMALL_ITEM_THRESHOLD:\n        # Give a bonus proportional to the current remaining capacity of the bin.\n        # A bin with more space gets a higher bonus, pulling small items towards\n        # more empty bins. The magnitude of this bonus can be tuned.\n        emptiness_preference_bonus = 0.5 * (bins_remain_cap[can_fit_mask] / BIN_CAPACITY)\n        priorities[can_fit_mask] += emptiness_preference_bonus\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse waste (Best Fit) with explicit perfect fit prioritization\n    and a severe penalty for creating small, potentially unusable gaps, enhancing\n    global bin utilization and reducing fragmentation.\n    \"\"\"\n    PERFECT_FIT_TOLERANCE = 1e-9  # Tolerance for floating point perfect fit check\n    PERFECT_FIT_SCORE = 1e12    # Very high score for perfect fits\n    STABILITY_EPSILON = np.finfo(float).eps # Small epsilon for numerical stability in inverse division\n\n    SMALL_GAP_THRESHOLD = 0.05  # Threshold for what constitutes a \"small gap\"\n    # A large negative penalty to significantly de-prioritize bins leaving small, bad gaps.\n    # This must be large enough to make the penalized inverse score worse than any desirable non-small-gap score.\n    SMALL_GAP_SEVERE_PENALTY = 1000.0 \n\n    # Initialize all priorities to negative infinity. Bins that cannot fit the item\n    # will retain this lowest priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can actually fit.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining space in these bins if the item were placed.\n    potential_remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Initialize priorities for currently valid bins.\n    current_bin_priorities = np.zeros_like(potential_remaining_space)\n\n    # 1. Assign Perfect Fit Score: If remaining space is zero (within tolerance), it's a perfect fit.\n    # These should always be the highest priority.\n    perfect_fit_mask = potential_remaining_space <= PERFECT_FIT_TOLERANCE\n    current_bin_priorities[perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. Assign Inverse Waste Score for Non-Perfect Fits: For bins where the item fits\n    # but some space remains, prioritize inversely to the remaining waste. Smaller waste\n    # yields higher priority. Add epsilon for numerical stability.\n    non_perfect_fit_mask = potential_remaining_space > PERFECT_FIT_TOLERANCE\n    current_bin_priorities[non_perfect_fit_mask] = \\\n        1.0 / (potential_remaining_space[non_perfect_fit_mask] + STABILITY_EPSILON)\n\n    # 3. Apply Small Gap Penalty: Identify non-perfect fits that result in a \"small gap\".\n    # These are highly undesirable as they fragment bin capacity. Apply a severe penalty\n    # to make them less attractive than larger, more usable remaining spaces, or even opening a new bin.\n    small_gap_to_penalize_mask = (potential_remaining_space > PERFECT_FIT_TOLERANCE) & \\\n                                 (potential_remaining_space < SMALL_GAP_THRESHOLD)\n    \n    current_bin_priorities[small_gap_to_penalize_mask] -= SMALL_GAP_SEVERE_PENALTY\n\n    # Assign the calculated priorities back to the main priorities array for bins that can fit.\n    priorities[can_fit_mask] = current_bin_priorities\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\n# Assume a global BIN_CAPACITY for context. This is typical in Bin Packing Problem (BPP)\n# where items and bin capacities are often normalized, e.g., to 1.0.\n# The thresholds below are defined relative to this capacity.\nBIN_CAPACITY = 1.0\n\n# Define heuristic parameters for strategic adjustments.\n# These values are empirical and might require tuning for specific problem distributions.\n\n# EPSILON_CLOSE_TO_ZERO: Tolerance for floating-point comparisons to detect \"exact\" fits.\nEPSILON_CLOSE_TO_ZERO = 1e-6\n\n# FRAGMENTATION_LOW_THRESHOLD & FRAGMENTATION_HIGH_THRESHOLD:\n# Define a range for \"fragmented\" gaps. These are remaining capacities that are\n# too small to be widely useful for future items, but not small enough to be\n# considered a \"perfect fit\". Placing an item here might lead to wasted space.\nFRAGMENTATION_LOW_THRESHOLD = 0.05 * BIN_CAPACITY  # e.g., 5% of bin capacity\nFRAGMENTATION_HIGH_THRESHOLD = 0.20 * BIN_CAPACITY # e.g., 20% of bin capacity\n\n# FRAGMENTATION_PENALTY: The amount by which to reduce the priority for creating\n# a fragmented gap. A higher value makes fragmented gaps significantly less desirable.\n# This value is relative to the base Best Fit score (which is typically small negative numbers).\nFRAGMENTATION_PENALTY = 0.5\n\n# LARGE_GAP_THRESHOLD: Defines a threshold for what's considered a \"large\" remaining gap.\n# If no tight fit is available, leaving a large gap might be strategically beneficial\n# to accommodate larger future items, thus keeping the bin \"open\".\nLARGE_GAP_THRESHOLD = 0.50 * BIN_CAPACITY # e.g., 50% of bin capacity\n\n# LARGE_GAP_BONUS: The amount by which to increase the priority for leaving a large gap.\n# This bonus should be small enough not to override truly tight fits, but\n# sufficient to make large gaps slightly more attractive than medium-sized,\n# potentially fragmented ones, when no tight fit is an option.\nLARGE_GAP_BONUS = 0.1\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns a priority score for placing an item in each bin, enhancing the\n    Best Fit (BF) heuristic with strategic considerations for bin consolidation,\n    fragmentation avoidance, and maintaining bin utility.\n\n    This heuristic aims to:\n    1.  **Strongly Prioritize Exact Fits:** Always choose a bin that can be\n        perfectly (or near-perfectly) filled by the item, ensuring maximum bin utilization.\n    2.  **Discourage Fragmentation:** Penalize bins that would result in a\n        small, potentially unusable \"fragmented\" remaining capacity after placement.\n        This encourages filling bins more completely or leaving larger, more useful gaps.\n    3.  **Encourage Large Usable Gaps (Strategic Worst Fit):** If no tight fit is\n        available, slightly prefer bins that leave a substantial remaining capacity.\n        This keeps bins strategically \"open\" to accommodate larger future items,\n        reducing the chance of using many bins with small, varied, and hard-to-fill gaps.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: A NumPy array containing the remaining capacity of each bin.\n\n    Return:\n        A NumPy array of the same size as `bins_remain_cap`, with a priority score\n        for each bin. A higher score indicates a higher priority for placing the\n        item in that bin. Bins that cannot accommodate the item receive -infinity.\n    \"\"\"\n    # Calculate the remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Initialize all priorities to negative infinity. This ensures that\n    # any bin that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit.\n    # We allow for very small negative remaining space due to floating point inaccuracies,\n    # treating them as effectively zero.\n    can_fit_mask = potential_remaining_space >= -EPSILON_CLOSE_TO_ZERO\n\n    # Extract the potential remaining space for only the bins where the item can fit.\n    fitting_rem_space = potential_remaining_space[can_fit_mask]\n\n    # --- Step 1: Base Priority (Best Fit logic) ---\n    # The base priority is the negative of the potential remaining space.\n    # This means a smaller remaining space (closer to zero) results in a\n    # higher (less negative or zero) priority score.\n    # A perfect fit (0 remaining space) gets a base score of 0.\n    current_priorities_for_fit_bins = -fitting_rem_space\n\n    # --- Step 2: Strategic Adjustments ---\n\n    # 2.1. Very High Priority for Exact/Near-Exact Fits:\n    # Identify bins where the item perfectly or very nearly perfectly fills the bin.\n    # Assign them an extremely high, distinct priority to ensure they are always chosen if available.\n    is_exact_or_near_fit = np.abs(fitting_rem_space) < EPSILON_CLOSE_TO_ZERO\n    current_priorities_for_fit_bins[is_exact_or_near_fit] = np.finfo(float).max / 2.0 # Use a very large float value\n\n    # For subsequent adjustments, we only consider bins that are not an exact fit,\n    # as their priority has already been set to the highest possible value.\n    non_exact_mask = ~is_exact_or_near_fit\n\n    # 2.2. Penalty for Fragmented Gaps:\n    # Identify bins that, if chosen, would result in a remaining capacity falling\n    # within the \"fragmentation zone\" (e.g., too small to be useful for many items,\n    # but not a perfect fit). Reduce their priority to discourage their selection.\n    is_fragmented_zone_gap = (fitting_rem_space > FRAGMENTATION_LOW_THRESHOLD) & \\\n                             (fitting_rem_space < FRAGMENTATION_HIGH_THRESHOLD) & \\\n                             non_exact_mask\n    current_priorities_for_fit_bins[is_fragmented_zone_gap] -= FRAGMENTATION_PENALTY\n\n    # 2.3. Bonus for Large Usable Gaps:\n    # Identify bins that, if chosen, would leave a large remaining capacity.\n    # If no exact or very tight fit is available, it might be strategically better\n    # to leave a large, reusable space rather than a fragmented one. This applies\n    # a small bonus to make these bins slightly more attractive among non-tight options.\n    is_large_usable_gap = (fitting_rem_space >= LARGE_GAP_THRESHOLD) & \\\n                          non_exact_mask\n    current_priorities_for_fit_bins[is_large_usable_gap] += LARGE_GAP_BONUS\n\n    # Assign the calculated priorities for fitting bins back into the main priorities array.\n    priorities[can_fit_mask] = current_priorities_for_fit_bins\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines robust Best Fit with adaptive bin consolidation and explicit perfect fit prioritization.\n    Prioritizes minimal waste, encourages using fuller bins, and ensures perfect fits are optimal.\n    \"\"\"\n    # Initialize all priorities to negative infinity, unequivocally marking bins\n    # that cannot accommodate the item as unavailable.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify which bins have sufficient capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return the initialized priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Step 1: Calculate core Best Fit and Adaptive Consolidation scores ---\n\n    # Potential remaining space (waste) if the item were placed in fit-capable bins.\n    potential_remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Small epsilon for numerical stability, especially for near-zero remaining space.\n    # This prevents division by zero and ensures very small waste values don't cause issues.\n    EPSILON = np.finfo(float).eps\n\n    # Core Best Fit Component: Assign priority as the inverse of (potential remaining space + epsilon).\n    # This strongly favors bins that result in minimal remaining space (tightest fit).\n    best_fit_scores = 1.0 / (potential_remaining_space + EPSILON)\n\n    # Adaptive Consolidation Bias Component: Promote packing into already fuller bins.\n    # This encourages overall bin consolidation, aiming to reduce the total number of bins used.\n    # We use the maximum remaining capacity among *all* bins as a dynamic reference\n    # for what an \"empty\" bin looks like in the current system state.\n    max_rem_cap = np.max(bins_remain_cap)\n\n    # Calculate the relative \"fullness\" of each fitting bin. A lower bins_remain_cap\n    # indicates a fuller bin, resulting in a higher fullness score (closer to 1).\n    # Add epsilon to denominator to prevent division by zero if max_rem_cap is 0.\n    relative_fullness = (max_rem_cap - bins_remain_cap[can_fit_mask]) / (max_rem_cap + EPSILON)\n\n    # Tunable weight for the consolidation bias. A small positive value ensures this\n    # bias influences decisions without overpowering the primary waste minimization.\n    CONSOLIDATION_BIAS_WEIGHT = 0.05\n\n    # Combine the Best Fit score with the adaptive consolidation bias.\n    # This forms the base priority for all fit-capable bins.\n    priorities[can_fit_mask] = best_fit_scores + CONSOLIDATION_BIAS_WEIGHT * relative_fullness\n\n    # --- Step 2: Explicitly boost perfect fits to ensure their absolute priority ---\n\n    # Tolerance for identifying near-perfect fits to handle floating-point inaccuracies.\n    # A small absolute value is used for robustness across different bin capacities/item sizes.\n    PERFECT_FIT_TOLERANCE = 1e-9\n\n    # A very high score to ensure perfect fits are chosen over any other option.\n    # This value must be significantly larger than any possible score derived from Step 1\n    # to guarantee that a perfect fit always receives the highest priority.\n    PERFECT_FIT_SCORE = 1e30 \n\n    # Identify perfect or near-perfect fits within the fitting bins.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n\n    # Apply the super-high score to bins that are perfect fits.\n    # This overrides any score calculated in Step 1 for these specific bins,\n    # making perfect fits the unequivocally best choice.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority for placing an item into bins, implementing a multi-objective\n    heuristic that combines Best Fit with strategic bin consolidation and exact fit preference.\n\n    This heuristic extends the Best Fit strategy by introducing weighted bonuses to prioritize\n    actions that lead to more efficient bin utilization and fewer open bins.\n\n    Objectives:\n    1.  **Prioritize Exact Fits (Consolidation):** Give a very high bonus to bins where the item\n        fits perfectly, resulting in 0 remaining capacity. This encourages \"closing\" bins.\n    2.  **Minimize Remaining Capacity (Best Fit):** For non-exact fits, prefer the bin that\n        will have the least remaining capacity, as per the standard Best Fit approach.\n    3.  **Prefer Partially Filled Bins (Strategic Consolidation):** Give a small bonus to bins\n        that are already partially filled (i.e., not completely empty) over completely empty bins\n        (new bins). This aims to consolidate items into existing bins, reducing the total number\n        of bins used over time.\n\n    Args:\n        item: Size of item to be added to the bin (expected to be > 0 and <= BIN_CAPACITY).\n        bins_remain_cap: Array of current remaining capacities for each bin.\n                         Assumes capacities are normalized (e.g., BIN_CAPACITY = 1.0).\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Bins that cannot accommodate the item receive the lowest possible priority (-inf).\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.full(num_bins, -np.inf, dtype=float)\n\n    # Assume a standard bin capacity for normalization. This is crucial for\n    # distinguishing between partially filled and completely empty bins.\n    BIN_CAPACITY = 1.0\n\n    # Define constants for bonus scores and floating-point comparisons.\n    # EXACT_FIT_BONUS must be significantly larger than any possible base Best Fit score\n    # (which ranges from -BIN_CAPACITY to 0).\n    EXACT_FIT_BONUS = 100.0\n\n    # PARTIALLY_FILLED_BIN_BONUS should be small enough not to override a significantly\n    # better Best Fit score, but large enough to break ties or give a slight preference.\n    PARTIALLY_FILLED_BIN_BONUS = 0.005\n\n    # Epsilon for robust floating-point comparisons, accounts for precision errors.\n    # Using np.finfo(float).eps * a_factor is generally robust for numerical stability.\n    EPSILON = np.finfo(float).eps * 100\n\n    # 1. Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # 2. Identify bins where the item can actually fit.\n    # We use a small negative tolerance for `potential_remaining_space` to account for\n    # minor floating-point inaccuracies that might make a valid fit appear slightly negative.\n    can_fit_mask = potential_remaining_space >= -EPSILON\n\n    # For scoring, ensure remaining space is non-negative. If it's slightly negative\n    # due to precision (but within fit tolerance), treat it as 0.\n    potential_remaining_space_for_scoring = np.where(potential_remaining_space < 0, 0, potential_remaining_space)\n\n    # 3. Base Score: Apply the Best Fit logic.\n    # A smaller potential_remaining_space results in a higher score (less negative).\n    # A perfect fit (0 remaining space) will get a base score of 0.\n    priorities[can_fit_mask] = -potential_remaining_space_for_scoring[can_fit_mask]\n\n    # 4. Apply Exact Fit Bonus: Strongly prioritize bins that become perfectly full.\n    # A bin is considered an exact fit if its remaining space after placing the item is\n    # very close to zero.\n    exact_fit_mask = can_fit_mask & (potential_remaining_space_for_scoring < EPSILON)\n    priorities[exact_fit_mask] += EXACT_FIT_BONUS\n\n    # 5. Apply Partially Filled Bin Bonus: Promote consolidation into existing bins.\n    # Identify bins that are not completely empty (i.e., already contain items)\n    # and are not already handled by the exact fit bonus.\n    is_empty_bin_mask = np.isclose(bins_remain_cap, BIN_CAPACITY, atol=EPSILON)\n    is_partially_filled_bin_mask = can_fit_mask & ~is_empty_bin_mask & ~exact_fit_mask\n\n    priorities[is_partially_filled_bin_mask] += PARTIALLY_FILLED_BIN_BONUS\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and bin consolidation bonus.\n    Prioritizes perfect fits. For others, balances tight fit (inverse waste) with\n    a subtle bonus for fuller bins to encourage closure.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, all priorities remain -inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remaining_space = fitting_bins_remain_cap - item\n\n    # Constants for numerical stability and scoring.\n    STABILITY_EPSILON = np.finfo(float).eps  # Smallest representable float\n    PERFECT_FIT_TOLERANCE = 1e-6             # Absolute tolerance for near-perfect fits\n    PERFECT_FIT_SCORE = 1e12                 # High score for perfect fits to guarantee selection\n    ALPHA_FULLNESS_BONUS = 0.01              # Weight for the bin fullness component (0.01-0.05 is often a good start)\n\n    # 1. Explicitly identify and prioritize perfect or near-perfect fits.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n    \n    # Apply the highest score to perfect fit bins.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, calculate a combined score.\n    non_perfect_fit_mask_in_fitting = ~perfect_fit_mask_in_fitting\n    \n    # Only proceed if there are bins that are not perfect fits but can still fit the item.\n    if np.any(non_perfect_fit_mask_in_fitting):\n        non_perfect_potential_rem = potential_remaining_space[non_perfect_fit_mask_in_fitting]\n        non_perfect_bins_remain_cap = fitting_bins_remain_cap[non_perfect_fit_mask_in_fitting]\n\n        # Primary component: Inverse of remaining waste (Best Fit).\n        # This gives disproportionately higher scores to tighter fits.\n        best_fit_score = 1.0 / (non_perfect_potential_rem + STABILITY_EPSILON)\n\n        # Secondary component: Inverse of remaining capacity (Fullness Bonus).\n        # Gives higher scores to bins that are already more full (smaller remaining capacity),\n        # acting as a subtle bias towards closing bins.\n        bin_fullness_score = 1.0 / (non_perfect_bins_remain_cap + STABILITY_EPSILON)\n\n        # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n        # The '1.0 + alpha * ...' ensures the primary best_fit_score is amplified, not replaced,\n        # by the fullness component, balancing immediate fit quality with bin consolidation.\n        combined_score = best_fit_score * (1.0 + ALPHA_FULLNESS_BONUS * bin_fullness_score)\n        \n        # Assign these combined scores to the non-perfect fitting bins.\n        priorities[can_fit_mask][non_perfect_fit_mask_in_fitting] = combined_score\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid Best Fit with explicit Perfect Fit priority and adaptive bin consolidation.\n    Combines inverse waste minimization with a multiplicative bonus for bin fullness,\n    while ensuring perfect fits are chosen first.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit the item, return priorities as is (-inf for all).\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity (waste) if the item is placed.\n    # Only for bins that can fit the item.\n    current_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remaining_waste = current_bins_remain_cap - item\n\n    # Constants for heuristic scoring\n    # A very high score to ensure perfect fits are chosen over any other option.\n    PERFECT_FIT_SCORE = 1e12\n    # A small tolerance for robust floating-point comparison to identify perfect fits.\n    # Using an absolute tolerance suitable for general floating-point comparisons.\n    PERFECT_FIT_TOLERANCE = 1e-9\n    # Epsilon for numerical stability, preventing division by zero, especially for small waste or remaining capacity.\n    STABILITY_EPSILON = np.finfo(float).eps\n    # Weighting factor for the bin fullness bonus.\n    # A small alpha ensures that the primary Best Fit (waste minimization) remains dominant,\n    # but the fullness component provides a subtle, adaptive influence towards bin closure.\n    ALPHA_FULLNESS_BONUS = 0.01\n\n    # 1. Apply Perfect Fit Bonus: Identify and highly prioritize near-perfect fits.\n    # These are cases where potential_remaining_waste is very close to zero.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_waste) < PERFECT_FIT_TOLERANCE\n    \n    # Assign the high perfect fit score to the corresponding bins.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, apply a hybrid Best Fit strategy with a fullness bonus.\n    non_perfect_fit_mask_in_fitting = ~perfect_fit_mask_in_fitting\n    \n    # Select the relevant subsets for non-perfect fits.\n    non_perfect_waste = potential_remaining_waste[non_perfect_fit_mask_in_fitting]\n    non_perfect_bins_remain_cap = current_bins_remain_cap[non_perfect_fit_mask_in_fitting]\n\n    # Calculate the primary Best Fit component: Inverse of (waste + epsilon).\n    # This gives disproportionately high scores to tighter fits.\n    best_fit_score = 1.0 / (non_perfect_waste + STABILITY_EPSILON)\n\n    # Calculate the Bin Fullness component: Inverse of (bins_remain_cap + epsilon).\n    # This term gives higher scores to bins that are already more full (smaller bins_remain_cap).\n    # It acts as a bias towards closing bins.\n    bin_fullness_score = 1.0 / (non_perfect_bins_remain_cap + STABILITY_EPSILON)\n\n    # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n    # The '1.0 + ALPHA_FULLNESS_BONUS * ...' ensures the base best_fit_score is always present\n    # and is only amplified by the fullness, not replaced. This hybrid approach\n    # aims for immediate efficiency (Best Fit) while adaptively guiding towards bin closure.\n    combined_score = best_fit_score * (1.0 + ALPHA_FULLNESS_BONUS * bin_fullness_score)\n\n    # Assign the combined scores to the non-perfect fitting bins.\n    priorities[can_fit_mask][non_perfect_fit_mask_in_fitting] = combined_score\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Adaptive Hybrid Heuristic: Combines robust best-fit, explicit perfect-fit,\n    and consolidation bias, scaled for problem awareness.\n    \"\"\"\n    # Initialize all priorities to negative infinity. Bins unable to fit are disqualified.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can be placed.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return the initialized (all -inf) priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the potential remaining space (waste) in suitable bins after placing the item.\n    potential_waste = bins_remain_cap[can_fit_mask] - item\n\n    # --- Problem-aware constants for numerical stability and scaling ---\n    # EPSILON: A small value for numerical stability in inverse calculations, scaled by bin_capacity\n    # to maintain relative precision across different bin sizes.\n    EPSILON = np.finfo(float).eps * bin_capacity \n    if EPSILON == 0.0: # Safeguard for very small bin_capacity values\n        EPSILON = np.finfo(float).eps\n\n    # PERFECT_FIT_TOLERANCE: Tolerance for identifying \"perfect fits\" to account for\n    # floating-point inaccuracies, scaled by bin_capacity.\n    PERFECT_FIT_TOLERANCE = 1e-9 * bin_capacity \n\n    # PERFECT_FIT_SCORE: A very high score for perfect fits, scaled by bin_capacity\n    # to ensure they are overwhelmingly prioritized regardless of problem scale.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n\n    # CONSOLIDATION_FACTOR: Weight for the consolidation bonus. This additive factor\n    # encourages selecting already fuller bins, promoting their closure.\n    CONSOLIDATION_FACTOR = 0.1 \n\n    # --- Assign priorities based on fit characteristics ---\n\n    # 1. Prioritize perfect fits (waste is zero or very close to zero).\n    perfect_fit_mask = np.isclose(potential_waste, 0, atol=PERFECT_FIT_TOLERANCE)\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits (some waste remains), use a combined score.\n    non_perfect_fit_mask = ~perfect_fit_mask \n\n    if np.any(non_perfect_fit_mask):\n        # Extract waste for only the non-perfect fitting bins.\n        current_potential_waste = potential_waste[non_perfect_fit_mask]\n\n        # Primary score: Inverse of waste. This strongly rewards tighter fits\n        # by making smaller wastes result in disproportionately higher scores.\n        best_fit_score = 1.0 / (current_potential_waste + EPSILON)\n\n        # Secondary score: Bonus based on the bin's current fullness (before placing item).\n        # This is normalized (0 to 1) and encourages selecting bins closer to full.\n        current_fullness = (bin_capacity - bins_remain_cap[can_fit_mask][non_perfect_fit_mask]) / bin_capacity\n\n        # Additive consolidation bonus.\n        consolidation_bonus = CONSOLIDATION_FACTOR * current_fullness\n\n        # Combine the scores. Best Fit is dominant, but fuller bins get a slight edge.\n        priorities[can_fit_mask][non_perfect_fit_mask] = best_fit_score + consolidation_bonus\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Adaptive Hybrid Heuristic: Combines robust best-fit, explicit perfect-fit,\n    and consolidation bias, scaled for problem awareness.\n    \"\"\"\n    # Initialize all priorities to negative infinity. Bins unable to fit are disqualified.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can be placed.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return the initialized (all -inf) priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the potential remaining space (waste) in suitable bins after placing the item.\n    potential_waste = bins_remain_cap[can_fit_mask] - item\n\n    # --- Problem-aware constants for numerical stability and scaling ---\n    # EPSILON: A small value for numerical stability in inverse calculations, scaled by bin_capacity\n    # to maintain relative precision across different bin sizes.\n    EPSILON = np.finfo(float).eps * bin_capacity \n    if EPSILON == 0.0: # Safeguard for very small bin_capacity values\n        EPSILON = np.finfo(float).eps\n\n    # PERFECT_FIT_TOLERANCE: Tolerance for identifying \"perfect fits\" to account for\n    # floating-point inaccuracies, scaled by bin_capacity.\n    PERFECT_FIT_TOLERANCE = 1e-9 * bin_capacity \n\n    # PERFECT_FIT_SCORE: A very high score for perfect fits, scaled by bin_capacity\n    # to ensure they are overwhelmingly prioritized regardless of problem scale.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n\n    # CONSOLIDATION_FACTOR: Weight for the consolidation bonus. This additive factor\n    # encourages selecting already fuller bins, promoting their closure.\n    CONSOLIDATION_FACTOR = 0.1 \n\n    # --- Assign priorities based on fit characteristics ---\n\n    # 1. Prioritize perfect fits (waste is zero or very close to zero).\n    perfect_fit_mask = np.isclose(potential_waste, 0, atol=PERFECT_FIT_TOLERANCE)\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits (some waste remains), use a combined score.\n    non_perfect_fit_mask = ~perfect_fit_mask \n\n    if np.any(non_perfect_fit_mask):\n        # Extract waste for only the non-perfect fitting bins.\n        current_potential_waste = potential_waste[non_perfect_fit_mask]\n\n        # Primary score: Inverse of waste. This strongly rewards tighter fits\n        # by making smaller wastes result in disproportionately higher scores.\n        best_fit_score = 1.0 / (current_potential_waste + EPSILON)\n\n        # Secondary score: Bonus based on the bin's current fullness (before placing item).\n        # This is normalized (0 to 1) and encourages selecting bins closer to full.\n        current_fullness = (bin_capacity - bins_remain_cap[can_fit_mask][non_perfect_fit_mask]) / bin_capacity\n\n        # Additive consolidation bonus.\n        consolidation_bonus = CONSOLIDATION_FACTOR * current_fullness\n\n        # Combine the scores. Best Fit is dominant, but fuller bins get a slight edge.\n        priorities[can_fit_mask][non_perfect_fit_mask] = best_fit_score + consolidation_bonus\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and fullness bonus.\n    Prioritizes perfect fits, then combines inverse waste minimization with a\n    multiplicative bonus for bin fullness to encourage consolidation.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Extract relevant data for bins that can fit, to avoid re-indexing.\n    relevant_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remain_after_fit = relevant_bins_remain_cap - item\n\n    # Epsilon for numerical stability, using a small standard machine epsilon.\n    epsilon = np.finfo(float).eps\n\n    # 2. Prioritize Perfect Fits: Assign a very high, distinct score.\n    # Use a tolerance for floating point comparisons, scaled by bin_capacity for robustness\n    # across different problem scales.\n    PERFECT_FIT_TOLERANCE = epsilon * bin_capacity \n    \n    # Mask for perfect fits *within the set of bins that can fit*.\n    perfect_fit_sub_mask = (np.abs(potential_remain_after_fit) < PERFECT_FIT_TOLERANCE)\n    \n    # A large, bin_capacity-scaled score ensures perfect fits are unequivocally dominant.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n    \n    # Assign perfect fit scores to the identified bins.\n    priorities[can_fit_mask][perfect_fit_sub_mask] = PERFECT_FIT_SCORE\n\n    # 3. Process Non-Perfect Fits: Apply the hybrid Best Fit + Fullness logic.\n    # Mask for non-perfect fits *within the set of bins that can fit*.\n    non_perfect_fit_sub_mask = ~perfect_fit_sub_mask\n\n    # Get the remaining waste and bin remaining capacities specifically for non-perfect fits.\n    remaining_waste_non_perfect = potential_remain_after_fit[non_perfect_fit_sub_mask]\n    bins_remain_cap_non_perfect = relevant_bins_remain_cap[non_perfect_fit_sub_mask]\n\n    # Primary Best Fit component: Inverse of (remaining_waste + epsilon).\n    # This gives disproportionately high scores to tighter (but not perfect) fits.\n    best_fit_score = 1.0 / (remaining_waste_non_perfect + epsilon)\n\n    # Bin Fullness component: Inverse of (bins_remain_cap + epsilon).\n    # This term gives higher scores to bins that are already more full (smaller bins_remain_cap),\n    # acting as a bias towards closing bins.\n    bin_fullness_score = 1.0 / (bins_remain_cap_non_perfect + epsilon)\n\n    # Weighting factor for the bin fullness bonus.\n    # A small alpha ensures that the primary Best Fit (waste minimization) remains dominant,\n    # but the fullness component provides a subtle, adaptive influence, encouraging bin closure.\n    alpha = 0.01  # Tunable parameter: Adjust based on desired balance.\n\n    # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n    # The '1.0 + alpha * ...' ensures the base best_fit_score is always present\n    # and is only amplified by the fullness, not replaced.\n    combined_score = best_fit_score * (1.0 + alpha * bin_fullness_score)\n\n    # Assign these combined scores to the corresponding non-perfect fit bins.\n    priorities[can_fit_mask][non_perfect_fit_sub_mask] = combined_score\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and fullness bonus.\n    Prioritizes perfect fits, then combines inverse waste minimization with a\n    multiplicative bonus for bin fullness to encourage consolidation.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Extract relevant data for bins that can fit, to avoid re-indexing.\n    relevant_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remain_after_fit = relevant_bins_remain_cap - item\n\n    # Epsilon for numerical stability, using a small standard machine epsilon.\n    epsilon = np.finfo(float).eps\n\n    # 2. Prioritize Perfect Fits: Assign a very high, distinct score.\n    # Use a tolerance for floating point comparisons, scaled by bin_capacity for robustness\n    # across different problem scales.\n    PERFECT_FIT_TOLERANCE = epsilon * bin_capacity \n    \n    # Mask for perfect fits *within the set of bins that can fit*.\n    perfect_fit_sub_mask = (np.abs(potential_remain_after_fit) < PERFECT_FIT_TOLERANCE)\n    \n    # A large, bin_capacity-scaled score ensures perfect fits are unequivocally dominant.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n    \n    # Assign perfect fit scores to the identified bins.\n    priorities[can_fit_mask][perfect_fit_sub_mask] = PERFECT_FIT_SCORE\n\n    # 3. Process Non-Perfect Fits: Apply the hybrid Best Fit + Fullness logic.\n    # Mask for non-perfect fits *within the set of bins that can fit*.\n    non_perfect_fit_sub_mask = ~perfect_fit_sub_mask\n\n    # Get the remaining waste and bin remaining capacities specifically for non-perfect fits.\n    remaining_waste_non_perfect = potential_remain_after_fit[non_perfect_fit_sub_mask]\n    bins_remain_cap_non_perfect = relevant_bins_remain_cap[non_perfect_fit_sub_mask]\n\n    # Primary Best Fit component: Inverse of (remaining_waste + epsilon).\n    # This gives disproportionately high scores to tighter (but not perfect) fits.\n    best_fit_score = 1.0 / (remaining_waste_non_perfect + epsilon)\n\n    # Bin Fullness component: Inverse of (bins_remain_cap + epsilon).\n    # This term gives higher scores to bins that are already more full (smaller bins_remain_cap),\n    # acting as a bias towards closing bins.\n    bin_fullness_score = 1.0 / (bins_remain_cap_non_perfect + epsilon)\n\n    # Weighting factor for the bin fullness bonus.\n    # A small alpha ensures that the primary Best Fit (waste minimization) remains dominant,\n    # but the fullness component provides a subtle, adaptive influence, encouraging bin closure.\n    alpha = 0.01  # Tunable parameter: Adjust based on desired balance.\n\n    # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n    # The '1.0 + alpha * ...' ensures the base best_fit_score is always present\n    # and is only amplified by the fullness, not replaced.\n    combined_score = best_fit_score * (1.0 + alpha * bin_fullness_score)\n\n    # Assign these combined scores to the corresponding non-perfect fit bins.\n    priorities[can_fit_mask][non_perfect_fit_sub_mask] = combined_score\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and bin consolidation bonus.\n    Prioritizes perfect fits. For others, balances tight fit (inverse waste) with\n    a subtle bonus for fuller bins to encourage closure.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, all priorities remain -inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remaining_space = fitting_bins_remain_cap - item\n\n    # Constants for numerical stability and scoring.\n    STABILITY_EPSILON = np.finfo(float).eps  # Smallest representable float\n    PERFECT_FIT_TOLERANCE = 1e-6             # Absolute tolerance for near-perfect fits\n    PERFECT_FIT_SCORE = 1e12                 # High score for perfect fits to guarantee selection\n    ALPHA_FULLNESS_BONUS = 0.01              # Weight for the bin fullness component (0.01-0.05 is often a good start)\n\n    # 1. Explicitly identify and prioritize perfect or near-perfect fits.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n    \n    # Apply the highest score to perfect fit bins.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, calculate a combined score.\n    non_perfect_fit_mask_in_fitting = ~perfect_fit_mask_in_fitting\n    \n    # Only proceed if there are bins that are not perfect fits but can still fit the item.\n    if np.any(non_perfect_fit_mask_in_fitting):\n        non_perfect_potential_rem = potential_remaining_space[non_perfect_fit_mask_in_fitting]\n        non_perfect_bins_remain_cap = fitting_bins_remain_cap[non_perfect_fit_mask_in_fitting]\n\n        # Primary component: Inverse of remaining waste (Best Fit).\n        # This gives disproportionately higher scores to tighter fits.\n        best_fit_score = 1.0 / (non_perfect_potential_rem + STABILITY_EPSILON)\n\n        # Secondary component: Inverse of remaining capacity (Fullness Bonus).\n        # Gives higher scores to bins that are already more full (smaller remaining capacity),\n        # acting as a subtle bias towards closing bins.\n        bin_fullness_score = 1.0 / (non_perfect_bins_remain_cap + STABILITY_EPSILON)\n\n        # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n        # The '1.0 + alpha * ...' ensures the primary best_fit_score is amplified, not replaced,\n        # by the fullness component, balancing immediate fit quality with bin consolidation.\n        combined_score = best_fit_score * (1.0 + ALPHA_FULLNESS_BONUS * bin_fullness_score)\n        \n        # Assign these combined scores to the non-perfect fitting bins.\n        priorities[can_fit_mask][non_perfect_fit_mask_in_fitting] = combined_score\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and fullness bonus.\n    Prioritizes perfect fits, then combines inverse waste minimization with a\n    multiplicative bonus for bin fullness to encourage consolidation.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # 1. Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Extract relevant data for bins that can fit, to avoid re-indexing.\n    relevant_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remain_after_fit = relevant_bins_remain_cap - item\n\n    # Epsilon for numerical stability, using a small standard machine epsilon.\n    epsilon = np.finfo(float).eps\n\n    # 2. Prioritize Perfect Fits: Assign a very high, distinct score.\n    # Use a tolerance for floating point comparisons, scaled by bin_capacity for robustness\n    # across different problem scales.\n    PERFECT_FIT_TOLERANCE = epsilon * bin_capacity \n    \n    # Mask for perfect fits *within the set of bins that can fit*.\n    perfect_fit_sub_mask = (np.abs(potential_remain_after_fit) < PERFECT_FIT_TOLERANCE)\n    \n    # A large, bin_capacity-scaled score ensures perfect fits are unequivocally dominant.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n    \n    # Assign perfect fit scores to the identified bins.\n    priorities[can_fit_mask][perfect_fit_sub_mask] = PERFECT_FIT_SCORE\n\n    # 3. Process Non-Perfect Fits: Apply the hybrid Best Fit + Fullness logic.\n    # Mask for non-perfect fits *within the set of bins that can fit*.\n    non_perfect_fit_sub_mask = ~perfect_fit_sub_mask\n\n    # Get the remaining waste and bin remaining capacities specifically for non-perfect fits.\n    remaining_waste_non_perfect = potential_remain_after_fit[non_perfect_fit_sub_mask]\n    bins_remain_cap_non_perfect = relevant_bins_remain_cap[non_perfect_fit_sub_mask]\n\n    # Primary Best Fit component: Inverse of (remaining_waste + epsilon).\n    # This gives disproportionately high scores to tighter (but not perfect) fits.\n    best_fit_score = 1.0 / (remaining_waste_non_perfect + epsilon)\n\n    # Bin Fullness component: Inverse of (bins_remain_cap + epsilon).\n    # This term gives higher scores to bins that are already more full (smaller bins_remain_cap),\n    # acting as a bias towards closing bins.\n    bin_fullness_score = 1.0 / (bins_remain_cap_non_perfect + epsilon)\n\n    # Weighting factor for the bin fullness bonus.\n    # A small alpha ensures that the primary Best Fit (waste minimization) remains dominant,\n    # but the fullness component provides a subtle, adaptive influence, encouraging bin closure.\n    alpha = 0.01  # Tunable parameter: Adjust based on desired balance.\n\n    # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n    # The '1.0 + alpha * ...' ensures the base best_fit_score is always present\n    # and is only amplified by the fullness, not replaced.\n    combined_score = best_fit_score * (1.0 + alpha * bin_fullness_score)\n\n    # Assign these combined scores to the corresponding non-perfect fit bins.\n    priorities[can_fit_mask][non_perfect_fit_sub_mask] = combined_score\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Hybrid Best Fit with explicit perfect fit priority and bin consolidation bonus.\n    Prioritizes perfect fits. For others, balances tight fit (inverse waste) with\n    a subtle bonus for fuller bins to encourage closure.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, all priorities remain -inf.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    potential_remaining_space = fitting_bins_remain_cap - item\n\n    # Constants for numerical stability and scoring.\n    STABILITY_EPSILON = np.finfo(float).eps  # Smallest representable float\n    PERFECT_FIT_TOLERANCE = 1e-6             # Absolute tolerance for near-perfect fits\n    PERFECT_FIT_SCORE = 1e12                 # High score for perfect fits to guarantee selection\n    ALPHA_FULLNESS_BONUS = 0.01              # Weight for the bin fullness component (0.01-0.05 is often a good start)\n\n    # 1. Explicitly identify and prioritize perfect or near-perfect fits.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n    \n    # Apply the highest score to perfect fit bins.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, calculate a combined score.\n    non_perfect_fit_mask_in_fitting = ~perfect_fit_mask_in_fitting\n    \n    # Only proceed if there are bins that are not perfect fits but can still fit the item.\n    if np.any(non_perfect_fit_mask_in_fitting):\n        non_perfect_potential_rem = potential_remaining_space[non_perfect_fit_mask_in_fitting]\n        non_perfect_bins_remain_cap = fitting_bins_remain_cap[non_perfect_fit_mask_in_fitting]\n\n        # Primary component: Inverse of remaining waste (Best Fit).\n        # This gives disproportionately higher scores to tighter fits.\n        best_fit_score = 1.0 / (non_perfect_potential_rem + STABILITY_EPSILON)\n\n        # Secondary component: Inverse of remaining capacity (Fullness Bonus).\n        # Gives higher scores to bins that are already more full (smaller remaining capacity),\n        # acting as a subtle bias towards closing bins.\n        bin_fullness_score = 1.0 / (non_perfect_bins_remain_cap + STABILITY_EPSILON)\n\n        # Combine: Multiply the best-fit score by a factor that increases with bin fullness.\n        # The '1.0 + alpha * ...' ensures the primary best_fit_score is amplified, not replaced,\n        # by the fullness component, balancing immediate fit quality with bin consolidation.\n        combined_score = best_fit_score * (1.0 + ALPHA_FULLNESS_BONUS * bin_fullness_score)\n        \n        # Assign these combined scores to the non-perfect fitting bins.\n        priorities[can_fit_mask][non_perfect_fit_mask_in_fitting] = combined_score\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Adaptive Hybrid Heuristic: Combines robust best-fit, explicit perfect-fit,\n    and consolidation bias, scaled for problem awareness.\n    \"\"\"\n    # Initialize all priorities to negative infinity. Bins unable to fit are disqualified.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can be placed.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return the initialized (all -inf) priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the potential remaining space (waste) in suitable bins after placing the item.\n    potential_waste = bins_remain_cap[can_fit_mask] - item\n\n    # --- Problem-aware constants for numerical stability and scaling ---\n    # EPSILON: A small value for numerical stability in inverse calculations, scaled by bin_capacity\n    # to maintain relative precision across different bin sizes.\n    EPSILON = np.finfo(float).eps * bin_capacity \n    if EPSILON == 0.0: # Safeguard for very small bin_capacity values\n        EPSILON = np.finfo(float).eps\n\n    # PERFECT_FIT_TOLERANCE: Tolerance for identifying \"perfect fits\" to account for\n    # floating-point inaccuracies, scaled by bin_capacity.\n    PERFECT_FIT_TOLERANCE = 1e-9 * bin_capacity \n\n    # PERFECT_FIT_SCORE: A very high score for perfect fits, scaled by bin_capacity\n    # to ensure they are overwhelmingly prioritized regardless of problem scale.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity \n\n    # CONSOLIDATION_FACTOR: Weight for the consolidation bonus. This additive factor\n    # encourages selecting already fuller bins, promoting their closure.\n    CONSOLIDATION_FACTOR = 0.1 \n\n    # --- Assign priorities based on fit characteristics ---\n\n    # 1. Prioritize perfect fits (waste is zero or very close to zero).\n    perfect_fit_mask = np.isclose(potential_waste, 0, atol=PERFECT_FIT_TOLERANCE)\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits (some waste remains), use a combined score.\n    non_perfect_fit_mask = ~perfect_fit_mask \n\n    if np.any(non_perfect_fit_mask):\n        # Extract waste for only the non-perfect fitting bins.\n        current_potential_waste = potential_waste[non_perfect_fit_mask]\n\n        # Primary score: Inverse of waste. This strongly rewards tighter fits\n        # by making smaller wastes result in disproportionately higher scores.\n        best_fit_score = 1.0 / (current_potential_waste + EPSILON)\n\n        # Secondary score: Bonus based on the bin's current fullness (before placing item).\n        # This is normalized (0 to 1) and encourages selecting bins closer to full.\n        current_fullness = (bin_capacity - bins_remain_cap[can_fit_mask][non_perfect_fit_mask]) / bin_capacity\n\n        # Additive consolidation bonus.\n        consolidation_bonus = CONSOLIDATION_FACTOR * current_fullness\n\n        # Combine the scores. Best Fit is dominant, but fuller bins get a slight edge.\n        priorities[can_fit_mask][non_perfect_fit_mask] = best_fit_score + consolidation_bonus\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}