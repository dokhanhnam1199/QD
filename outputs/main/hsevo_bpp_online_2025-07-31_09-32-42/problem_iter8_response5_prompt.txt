{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Consolidation-Aware Best Fit: Prioritizes minimal waste (Best Fit) while subtly\n    favoring bins that are already fuller. This encourages faster bin closure\n    and more efficient bin consolidation, integrating adaptive logic.\n\n    Args:\n        item (float): The size of the item to be placed.\n        bins_remain_cap (np.ndarray): Remaining capacity in each bin. Assumes capacities\n                                      are normalized (e.g., max capacity of 1.0).\n\n    Returns:\n        np.ndarray: Priority scores for each bin. Bins unable to fit the item get -np.inf.\n    \"\"\"\n    # Initialize priorities to negative infinity, ensuring bins unable to fit\n    # the item are unequivocally disqualified.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Determine which bins have sufficient remaining capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining space (waste) in suitable bins after placing the item.\n    # A smaller waste indicates a more efficient, tighter fit.\n    remaining_waste = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the current fullness of bins that can fit the item.\n    # This assumes a maximum bin capacity of 1.0 for normalized problems.\n    # Higher fullness implies the bin is closer to being 'closed'.\n    current_fullness = 1.0 - bins_remain_cap[can_fit_mask]\n\n    # A small positive constant for numerical stability and to ensure a finite,\n    # very high priority for perfect fits (waste = 0).\n    epsilon = np.finfo(float).eps\n\n    # Define a consolidation factor. This value is chosen to be small enough\n    # to ensure the primary Best Fit term (inverse waste) remains dominant,\n    # but large enough to provide a subtle preference among bins with\n    # very similar waste values. It acts as an adaptive consolidation incentive.\n    CONSOLIDATION_FACTOR = 0.1\n\n    # Primary score: Inverse of waste. This strongly rewards tight fits.\n    best_fit_score = 1.0 / (remaining_waste + epsilon)\n\n    # Secondary score: Bonus for current fullness. This nudges the selection\n    # towards bins that are already more filled, encouraging their closure.\n    consolidation_bonus = CONSOLIDATION_FACTOR * current_fullness\n\n    # Combine the scores. The Best Fit dominates, but bins closer to full\n    # receive a slight advantage, reflecting a more global optimization strategy.\n    priorities[can_fit_mask] = best_fit_score + consolidation_bonus\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins using an adaptive Best Fit approach, emphasizing tight fits for\n    global utility and robustly handling edge cases for predictive packing.\n\n    This heuristic combines the effectiveness of inverse waste minimization with\n    adaptive numerical stability, aiming for bin configurations that enhance\n    future packing opportunities. It assigns the highest priority to perfect fits\n    and uses a non-linear inverse function for others, subtly scaled by bin\n    capacity to be problem-aware.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        bin_capacity: The total capacity of a single bin. Default is 1.0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize all priorities to negative infinity. This ensures bins that cannot fit\n    # the item are never chosen, providing a clear baseline for infeasible options.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can be placed without exceeding capacity.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the potential remaining capacity (waste) after placing the item\n    # for only those bins that can accommodate it.\n    potential_remain_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Separate valid bins into two categories: perfect fits and non-perfect fits (some waste).\n    perfect_fit_mask = (potential_remain_after_fit == 0)\n    non_perfect_fit_mask = (potential_remain_after_fit > 0)\n\n    # 1. Prioritize perfect fits with a very high, distinct finite score.\n    # Scaling by `bin_capacity` makes this score adaptive to the problem's scale,\n    # ensuring perfect fits are overwhelmingly dominant regardless of the bin size.\n    PERFECT_FIT_SCORE = 1e6 * bin_capacity # A large multiplier ensures dominance.\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, prioritize inversely to the remaining waste.\n    # A smaller waste results in a higher priority. The `STABILITY_EPSILON` is\n    # dynamically scaled by `bin_capacity` to maintain numerical stability and\n    # problem-awareness across different bin scales (e.g., a tiny absolute waste\n    # in a large bin might be effectively 'zero'). This avoids explicit \"dead space\"\n    # penalties, letting the inverse relationship naturally de-prioritize larger wastes.\n    STABILITY_EPSILON = np.finfo(float).eps * bin_capacity \n    \n    # Ensure epsilon is not zero, especially for very small bin_capacity values if they could occur.\n    if STABILITY_EPSILON == 0.0:\n        STABILITY_EPSILON = np.finfo(float).eps \n\n    priorities[can_fit_mask][non_perfect_fit_mask] = 1.0 / (potential_remain_after_fit[non_perfect_fit_mask] + STABILITY_EPSILON)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd), we observe a shift from fixed, categorical gap management to a more continuous, adaptive approach. The best heuristic (1st) combines a core inverse Best Fit (prioritizing minimal waste) with an *adaptive*, linear bias towards selecting already fuller bins. This bias dynamically references the `max_rem_cap` in the system, making it more robust across varying bin states. In contrast, the second-best heuristic (2nd) employs a fixed `SMALL_GAP_THRESHOLD` and `BAD_GAP_SCORE_OFFSET` to categorize and penalize small, unusable gaps. The adaptive, continuous nature of the consolidation bias in (1st) likely provides a smoother and more effective optimization over (2nd)'s discrete, threshold-based logic.\n\nComparing (2nd) vs (3rd), the significant improvement lies in (2nd)'s explicit awareness of \"bad gaps.\" While (3rd) is a simple Best Fit (`-potential_remaining_space`), (2nd) introduces a foresight mechanism by heavily penalizing bin placements that would leave very small, hard-to-use remaining capacities. This strategic penalty in (2nd) helps reduce fragmentation and improves global utility over (3rd)'s purely greedy, linear approach.\n\nComparing (3rd) vs (4th), the major upgrade is the introduction of `inverse waste` scoring and a strong `PERFECT_FIT_TOLERANCE` with a `MAX_PRIORITY_SCORE` in (4th). (3rd) uses a linear negative score for waste, while (4th)'s `1 / (waste + EPSILON)` disproportionately favors tighter fits (non-linear penalization of waste) and explicitly ensures perfect fits are unequivocally the best choice. This aggressive preference for tight and perfect fits significantly improves performance over simple linear scoring.\n\nComparing (4th) vs (5th), both incorporate inverse waste, but (5th) (and its identical counterparts 8th, 9th) introduces a *multiplicative* bonus for bin fullness. This differs from (1st)'s additive bias, and (4th)'s explicit `MAX_PRIORITY_SCORE`. The multiplicative `(1.0 + alpha * bin_fullness_score)` in (5th) amplifies the primary Best Fit score based on current fullness, aiming for consolidation. However, such multiplicative factors can sometimes lead to less predictable score distributions compared to additive biases.\n\nComparing (5th) vs (6th), both aim for consolidation alongside Best Fit. (5th) uses a *multiplicative* fullness bonus, whereas (6th) employs an *additive* consolidation bonus (`CONSOLIDATION_FACTOR * current_fullness`). Additive biases are generally considered easier to tune and less prone to distorting the primary ranking significantly. The fact that (6th) is ranked below (5th) suggests that for the specific test cases, the multiplicative amplification or the particular fullness definition in (5th) provided a slight advantage.\n\nComparing (6th) vs (10th), (6th) uses inverse waste and an additive consolidation bonus, aiming for smooth, continuous improvement. (10th) reverts to a linear Best Fit with a distinct, fixed `SMALL_GAP_PENALTY`. The continuous nature of inverse waste (in 6th) generally outperforms discrete penalty thresholds (in 10th) by providing a more nuanced ranking of suboptimal fits.\n\nComparing (10th) vs (11th), (11th) (and its identical counterparts 14th, 17th, 18th, 19th) significantly improves by moving from (10th)'s linear Best Fit with a rigid \"small gap\" penalty to a robust `inverse waste` scoring system for non-perfect fits, coupled with an explicit `PERFECT_FIT_SCORE` identified via `PERFECT_FIT_TOLERANCE`. This robust inverse approach implicitly handles undesirable gaps more smoothly and effectively than a fixed penalty.\n\nComparing (11th) vs (12th), the primary difference is how perfect fits are identified. (11th) uses `np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE`, which is numerically robust for floating-point comparisons. (12th) uses a direct equality check `(valid_potential_remaining_space == 0)`. The `tolerance`-based approach in (11th) correctly identifies near-perfect fits, preventing potential misclassifications due to minor floating-point inaccuracies, thus making it slightly superior.\n\nComparing (12th) vs (13th), (13th) (and its identical counterparts 15th, 16th, 20th) introduces a crucial improvement for generalizability: scaling the `PERFECT_FIT_SCORE` and `STABILITY_EPSILON` by `bin_capacity`. This makes the heuristic \"problem-aware\" and robust across different bin scales. While theoretically superior for wider application, its lower ranking suggests that for the specific test cases, this scaling either did not yield benefit or introduced minor complexities that were not advantageous, or the `bin_capacity` was uniform.\n\nOverall: The progression from worst to best demonstrates a clear evolution from simple greedy approaches to sophisticated, hybrid strategies. Key themes include: aggressively prioritizing perfect fits, employing non-linear waste penalization (inverse functions), proactively managing fragmentation (either by direct penalty or implicit via inverse functions), and incorporating adaptive biases for bin consolidation. Numerical stability through `epsilon` and `tolerance` is consistently vital. The top-performing heuristic effectively balances immediate waste minimization with long-term bin consolidation using adaptive mechanisms. The observed redundancies in the provided list's ranking suggest that performance can be highly sensitive to specific test sets or external factors not captured in the code snippets.\n- \nHere's a redefined self-reflection for designing better heuristics:\n\n*   **Keywords:** Adaptive, Strategic, Multi-objective, Predictive, Robustness.\n*   **Advice:** Design multi-stage heuristics blending immediate gains with strategic bin consolidation and fragmentation minimization. Employ adaptive parameters responding to problem state. Prioritize exact fits. Ensure numerical robustness via scale-aware comparisons and integer representations.\n*   **Avoid:** Relying purely on single-pass greedy methods. Neglecting explicit fragmentation or balanced bin states as core objectives. Fixed thresholds or simplistic non-adaptive scoring. Superficial epsilon-only numerical fixes.\n*   **Explanation:** Better heuristics leverage foresight: multi-objective optimization balancing current placement with future bin utility, and dynamic adaptation to problem complexity, for robust, generalizable performance.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}