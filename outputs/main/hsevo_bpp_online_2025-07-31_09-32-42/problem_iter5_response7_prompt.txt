{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which to add an item to each bin, implementing a Best Fit\n    variant with a dynamic 'dead space' penalty for problem-aware robustness and\n    informed exploration.\n\n    This heuristic extends the standard Best Fit approach by actively discouraging\n    the creation of small, potentially unusable gaps (referred to as \"dead space\")\n    within bins. While Best Fit prioritizes minimizing remaining capacity, this\n    `priority_v2` introduces an adaptive penalty. If placing an item results in a\n    remaining capacity that falls into a predefined 'dead space' range (too small\n    to be generally useful, but not perfectly zero), the priority for that bin\n    is significantly reduced. This aims to prevent bin fragmentation, improve\n    overall bin utilization, and encourage more robust packing solutions, especially\n    when dealing with items that could leave awkward small residuals.\n\n    The penalty ensures that creating such \"dead space\" is often considered worse\n    than placing the item into a much emptier bin, thereby nudging the heuristic\n    away from locally optimal but globally inefficient choices.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n        bin_capacity: The total capacity of a single bin. This is crucial for\n                      scaling the 'dead space' threshold and penalty value. Default is 1.0.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Scores are designed such that:\n        - A perfect fit (remaining capacity = 0) receives the highest possible score (0).\n        - Bins that create \"dead space\" after placement receive a heavily penalized score.\n        - Bins that cannot accommodate the item receive the lowest possible priority (-inf).\n    \"\"\"\n    # Define thresholds and penalty for 'dead space'. These are dynamically\n    # scaled relative to the bin_capacity for problem-aware robustness.\n    # A remaining space is considered 'dead space' if it's greater than 0\n    # but less than a certain fraction (e.g., 10%) of the bin's total capacity.\n    DEAD_SPACE_THRESHOLD = 0.1 * bin_capacity\n\n    # The penalty value is set to the full bin capacity. This makes creating\n    # a dead space gap significantly less desirable than placing the item\n    # into a brand new, empty bin (which would result in a score of -bin_capacity).\n    PENALTY_VALUE = bin_capacity\n\n    # Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Initialize all priorities to negative infinity. This ensures that\n    # any bin that cannot fit the item will never be chosen.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Create a boolean mask for bins where the item can actually fit (remaining space >= 0).\n    can_fit_mask = potential_remaining_space >= 0\n\n    # Apply the base Best Fit priority: negative of the potential remaining space.\n    # A smaller remaining space results in a higher (less negative) priority.\n    # A perfect fit (0 remaining space) will result in a priority of 0 (the highest possible base score).\n    priorities[can_fit_mask] = -potential_remaining_space[can_fit_mask]\n\n    # Identify bins that, if chosen, would result in 'dead space'.\n    # This applies to bins where:\n    # 1. The item *can* fit (already covered by can_fit_mask).\n    # 2. The resulting `potential_remaining_space` is greater than 0 (not a perfect fit).\n    # 3. The resulting `potential_remaining_space` is less than the `DEAD_SPACE_THRESHOLD`.\n    dead_space_creation_mask = (potential_remaining_space > 0) & (potential_remaining_space < DEAD_SPACE_THRESHOLD)\n\n    # Combine the masks: apply penalty only to bins that can fit the item AND create dead space.\n    apply_penalty_mask = can_fit_mask & dead_space_creation_mask\n\n    # Apply the significant penalty to the priorities of identified 'dead space' bins.\n    # This actively discourages choices that lead to awkward, unusable gaps,\n    # thereby introducing 'problem-aware robustness' into the heuristic.\n    priorities[apply_penalty_mask] -= PENALTY_VALUE\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Adaptive Best Fit: Prioritizes bins to minimize waste, with distinct high score for perfect fits and robust inverse for others.\"\"\"\n\n    # Initialize priorities to negative infinity. This explicitly excludes bins\n    # that cannot fit the item from consideration, making them the lowest possible priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins where the item can be placed.\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate the potential remaining capacity (waste) after placing the item\n    # for only those bins that can accommodate it.\n    potential_remain_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Separate valid bins into two categories: perfect fits and non-perfect fits (some waste).\n    perfect_fit_mask = (potential_remain_after_fit == 0)\n    non_perfect_fit_mask = (potential_remain_after_fit > 0)\n\n    # 1. Prioritize perfect fits with a very high, distinct finite score.\n    # This guarantees they are chosen over any other fit while maintaining\n    # numerical stability (avoiding np.inf).\n    PERFECT_FIT_SCORE = 1e12  # A large number ensuring perfect fit dominance\n    priorities[can_fit_mask][perfect_fit_mask] = PERFECT_FIT_SCORE\n\n    # 2. For non-perfect fits, prioritize inversely to the remaining waste.\n    # A smaller waste yields a higher priority. A small epsilon is added\n    # to the denominator to ensure numerical stability, especially for very\n    # small positive waste values, preventing potential overflows.\n    STABILITY_EPSILON = 1e-9 # Ensures division by zero or near-zero is handled\n    priorities[can_fit_mask][non_perfect_fit_mask] = 1.0 / (potential_remain_after_fit[non_perfect_fit_mask] + STABILITY_EPSILON)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (2nd) vs (4th) vs (9th), we see they are virtually identical implementations of Best Fit with an inverse-waste priority. Their top ranking suggests this simple, non-linear preference for tight fits, using `np.finfo(float).eps` for numerical stability, is highly effective.\n\nComparing (2nd) vs (3rd), and (4th) vs (3rd), 3rd is ranked worse despite similar core logic. The key differences are that 3rd uses a hardcoded `epsilon` value as a function argument and includes unused imports (`random`, `math`, `scipy`, `torch`). This suggests that using `np.finfo(float).eps` for maximum numerical robustness and keeping imports clean contributes to better heuristic design and possibly performance.\n\nComparing (4th) vs (5th), and also the general cluster of 1st,2nd,4th,9th (simple inverse Best Fit) against 5th, 7th, 8th, 10th (fragmentation aversion/dead space penalty), the simpler inverse Best Fit consistently ranks higher. This indicates that explicitly trying to avoid \"fragmentation\" or \"dead space\" with additional penalty logic, even if well-intentioned, might introduce complexity that hinders overall performance compared to directly prioritizing the smallest waste. The non-linear `1/(waste+epsilon)` already inherently penalizes larger waste, including small \"awkward\" ones, by giving them disproportionately lower scores.\n\nComparing (6th) vs (7th), 7th is a more complex version of fragmentation aversion with multiple thresholds and bonuses/penalties. It's ranked worse than 6th (which is effectively a basic Best Fit with issues). This reinforces that increased algorithmic complexity with multiple tunable parameters can be detrimental.\n\nComparing (8th) vs (9th), 9th (simple inverse Best Fit) is better than 8th (Best Fit with dead space penalty). This further supports the strength of the `1/(waste+epsilon)` approach over explicit penalty schemes.\n\nComparing (10th) vs (11th), 11th (Adaptive Best Fit with explicit high score for perfect fits) is better than 10th (dead space penalty). This indicates that making perfect fits unequivocally dominant via a very large, fixed score (like `1e12`) can be an effective refinement, though it's still ranked below the top simple inverse Best Fit.\n\nComparing (14th) vs (15th), and analyzing the entire bottom cluster (15th-20th) of \"Adaptive Median Fit\" against the higher-ranked heuristics, the \"Adaptive Median Fit\" consistently performs the worst. This heuristic tries to select a bin whose remaining capacity is closest to the *median* of all possible remaining capacities, aiming for a \"balanced\" bin state. This strong negative result suggests that aiming for uniformity or balancing remaining bin capacities is counterproductive for bin packing, which typically benefits from aggressive waste minimization and closing bins efficiently, even if it leads to some highly filled and some empty bins.\n\nOverall: The best heuristics are robust Best Fit variants with a strong, non-linear preference for tight fits (implicit or explicit) and good numerical stability. Added complexity like fragmentation aversion or \"balancing\" strategies tend to degrade performance.\n- \n### Current self-reflection\n*   **Keywords**: Predictive State, Adaptive Logic, Global Utility, Hybrid Search.\n*   **Advice**: Design scoring that anticipates future bin configurations. Integrate adaptive strategies for dynamic bin management. Explore controlled look-ahead or multi-objective trade-offs for superior global utility.\n*   **Avoid**: Over-emphasizing only immediate greedy fits. Merely stating implementation robustness (e.g., epsilon) or error handling as core design principles.\n*   **Explanation**: Advancing heuristics requires foresight into bin states and dynamic adaptation, moving beyond local optimality. Focus on conceptual logic, not just robust implementation.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}