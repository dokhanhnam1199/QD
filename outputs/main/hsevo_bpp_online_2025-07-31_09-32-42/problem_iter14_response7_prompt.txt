{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes tight fits (Best Fit) and perfect fits, with an additive bonus\n    for bins that are already fuller, promoting efficient bin closure.\n    \"\"\"\n    # Initialize all priorities to negative infinity, unequivocally marking bins\n    # that cannot accommodate the item as unavailable.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify which bins have sufficient capacity for the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit the item, return the initialized priorities.\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Potential remaining space (waste) if the item were placed in fit-capable bins.\n    potential_remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Small epsilon for numerical stability, especially for near-zero remaining space.\n    EPSILON = np.finfo(float).eps\n\n    # Core Best Fit Component: Assign priority as the inverse of (potential remaining space + epsilon).\n    # This strongly favors bins that result in minimal remaining space (tightest fit).\n    best_fit_scores = 1.0 / (potential_remaining_space + EPSILON)\n\n    # Additive Consolidation Bias Component: Promote packing into already fuller bins.\n    # Based on analysis, a bonus directly related to individual bin fullness is preferred.\n    # A smaller bins_remain_cap (fuller bin) results in a higher inverse value,\n    # thereby giving a higher additive bonus.\n    CONSOLIDATION_BIAS_WEIGHT = 0.05\n    consolidation_bonus = 1.0 / (bins_remain_cap[can_fit_mask] + EPSILON)\n\n    # Combine the Best Fit score with the additive consolidation bias.\n    priorities[can_fit_mask] = best_fit_scores + CONSOLIDATION_BIAS_WEIGHT * consolidation_bonus\n\n    # Explicitly boost perfect fits to ensure their absolute priority.\n    # Tolerance for identifying near-perfect fits to handle floating-point inaccuracies.\n    PERFECT_FIT_TOLERANCE = 1e-9\n    # A very high score to ensure perfect fits are chosen over any other option.\n    PERFECT_FIT_SCORE = 1e30 \n\n    # Identify perfect or near-perfect fits within the fitting bins.\n    perfect_fit_mask_in_fitting = np.abs(potential_remaining_space) < PERFECT_FIT_TOLERANCE\n\n    # Apply the super-high score to bins that are perfect fits, overriding previous calculations.\n    priorities[can_fit_mask][perfect_fit_mask_in_fitting] = PERFECT_FIT_SCORE\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority for placing an item in each bin, implementing a strategy\n    that favors keeping bins with more remaining capacity flexible for future items,\n    akin to a Worst Fit approach, and designed with \"emergent behavior\" in mind.\n\n    This heuristic assigns a higher priority to bins that would have more space remaining\n    after the item is placed, provided the item can fit. This aims to keep bins\n    \"open\" and versatile for subsequent, potentially larger, items, letting a global\n    packing strategy emerge from these local decisions rather than explicit optimization.\n    Bins that cannot accommodate the item receive a low, but finite, priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority for placing the item in that bin.\n        Scores are designed to reflect the desirability of a bin, with unfitting\n        bins receiving a score strictly lower than any possible valid placement.\n    \"\"\"\n    # Initialize all priorities to a sufficiently small negative value.\n    # This ensures that any bin where the item cannot fit will always have\n    # a lower priority than any bin where it can fit, without using -np.inf.\n    # Assuming capacities and item sizes are positive, the minimum valid\n    # remaining capacity (for a perfect fit) is 0. Thus, any negative value\n    # effectively excludes non-fitting bins.\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n\n    # Calculate the potential remaining space in each bin if the item were placed.\n    potential_remaining_space = bins_remain_cap - item\n\n    # Create a boolean mask for bins where the item can actually fit\n    # (i.e., where potential_remaining_space is non-negative).\n    can_fit_mask = potential_remaining_space >= 0\n\n    # For bins where the item can fit, assign a priority based on the\n    # potential remaining space. A larger positive value means more space\n    # would be left, which is preferred in this \"Worst Fit\" type of strategy.\n    # This promotes keeping bins less full, allowing for more flexibility\n    # in subsequent item placements.\n    priorities[can_fit_mask] = potential_remaining_space[can_fit_mask]\n\n    return priorities\n\n### Analyze & experience\n- Comparing the provided heuristics, it's evident that strategies focusing on maximizing bin utilization and minimizing wasted space perform significantly better. The list contains several identical heuristics, which will be noted in the comparisons.\n\n*   **Comparing (1st) vs (2nd):** The 1st heuristic, ranked best, employs a robust Best Fit core and a strong, fixed `PERFECT_FIT_BONUS` (1000.0). In contrast, the 2nd heuristic introduces a subtle, proportional consolidation bonus and a lower `EXACT_FIT_BONUS` (100.0). The superior performance of 1st suggests that a very high, overriding bonus for exact fits is more critical than a nuanced, proportional consolidation strategy. Simplicity and dominance of the perfect fit rule likely contribute to its effectiveness.\n\n*   **Comparing (2nd) vs (3rd):** The 2nd heuristic uses a linear Best Fit score, while the 3rd uses an inverse Best Fit score (`1.0 / (potential_remaining_space + EPSILON)`) and an inverse fullness consolidation bonus. The 3rd also uses an even higher `PERFECT_FIT_SCORE` (1e30). While the higher perfect fit score in 3rd is beneficial, the inverse scoring for Best Fit and consolidation can make the heuristic highly sensitive to small numerical differences, potentially leading to instability or less predictable behavior compared to the more stable linear approach in 2nd. The ranking suggests that the linear Best Fit combined with a strong exact fit bonus (as in 2nd) is more effective than the inverse-based scaling of 3rd, despite 3rd's even higher perfect fit constant.\n    *   *Note:* Heuristics 6th and 8th are identical to 3rd.\n\n*   **Comparing (3rd) vs (4th):** The 3rd heuristic uses inverse scoring for its Best Fit and consolidation components. The 4th heuristic reverts to a linear Best Fit score (`-potential_remaining_space`) combined with a dominant exact fit bonus and a *fixed additive* `PARTIALLY_FILLED_BIN_BONUS`. The 4th's superior ranking over 3rd indicates that a simpler, fixed consolidation bonus is more effective or robust than a complex inverse-proportional one. It reinforces the idea that linear Best Fit scoring is preferable.\n    *   *Note:* Heuristic 7th is identical to 4th.\n\n*   **Comparing (4th) vs (5th):** The 4th heuristic employs a consolidation bonus to fill partially used bins, while the 5th heuristic introduces a *penalty* for creating \"fragmented\" gaps. The better performance of 4th suggests that a positive incentive for consolidation (filling existing bins) is generally more effective or easier to tune than a negative incentive for avoiding specific types of gaps. Both aim to improve space utilization but consolidation seems to yield better overall results in this ranking.\n\n*   **Comparing (5th) vs (9th):** The 5th heuristic is a refined Best Fit variant. The 9th heuristic introduces a \"relative utilization\" approach (`item / bins_remain_cap`), prioritizing bins where the item consumes a larger *proportion* of the remaining capacity. The significant drop in rank shows that classic Best Fit (minimizing absolute remaining space) is vastly superior to relative utilization for general bin packing, as the latter might lead to many partially filled bins if not carefully balanced.\n\n*   **Comparing (9th) vs (10th):** The 9th heuristic (relative utilization) is outperformed by the 10th, which is a pure Best Fit implementation (`-potential_remaining_space`). This clearly demonstrates that Best Fit, by directly aiming to minimize wasted space within a bin, is a stronger foundational heuristic for reducing the total number of bins compared to a relative fit approach.\n\n*   **Comparing (10th) vs (11th):** The 10th heuristic (Pure Best Fit) ranks much higher than the 11th (Worst Fit based on potential remaining space). This is a well-known result in bin packing: Best Fit generally minimizes the number of bins by packing them tightly, while Worst Fit aims to distribute items, often leading to more open bins and higher bin counts.\n\n*   **Comparing (11th) vs (12th):** Both are Worst Fit variants. The 11th prioritizes bins that will have the *most remaining space after placement*. The 12th prioritizes bins that *currently have the most remaining capacity*. The marginally better ranking of 11th suggests that considering the *post-placement* state for a Worst Fit strategy is slightly more effective than solely relying on the pre-placement state.\n    *   *Note:* Heuristics 13th, 14th, 15th, 17th, 19th, 20th are identical to 12th. Heuristics 16th and 18th are identical to 11th.\n\n**Overall:**\nThe best heuristics are always variants of Best Fit. The most critical factor for performance in bin packing is the overwhelming priority given to exact or near-exact fits. Additional strategic bonuses for consolidating items into existing, partially-filled bins can provide a further edge, but simpler, fixed additive bonuses often perform more robustly than complex proportional or inverse weighting schemes. Worst Fit strategies consistently perform poorly for minimizing bin count.\n- \n*   **Keywords**: Direct, Greedy, Simplicity, Waste Minimization, Fixed.\n*   **Advice**: Prioritize direct waste minimization with immediate, greedy choices. Apply only fixed, additive adjustments for highly desirable, simple outcomes like perfect fits.\n*   **Avoid**: Complex, non-linear, or multiplicative scoring; adaptive parameters; explicit fragmentation penalties or bin balancing; general numerical robustness as a design principle.\n*   **Explanation**: Effective heuristics remain simple, direct, and focused on immediate optimal outcomes. Overcomplicating objectives or using indirect, complex strategies often leads to suboptimal performance and instability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}