{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines penalties for over-filling and rewards for near-full bins using adaptive scaling.\"\"\"\n    new_remain_cap = bins_remain_cap - item\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n    reward = 1 / (1 + np.exp(-new_remain_cap / (bins_remain_cap + 0.1)))\n    priority_score = penalty + reward\n    return priority_score\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic uses an adaptive reward/penalty system combined with exact fit rewards and scaled priorities, which efficiently utilizes bin space. The worst heuristic lacks a structured reward/penalty system, instead relying on primarily penalties and sigmoid rewards which is less adaptive and less effective in handling a wide range of scenarios. \nComparing (second best) vs (second worst), we see the second best prioritizes closing bins with adaptive scaling and exact fit rewards, improving compactness and efficiency. The second worst uses an inverse remaining capacity method, which works well for bins close to capacity but fails to account for potential exact fits and efficiency. \nComparing (1st) vs (2nd), we see the 1st strategy offers a more balanced approach with adaptive scaling, rewarding exact fits, and penalizing inefficient bins. The 2nd strategy only uses an inverse capacity method, leading to less diversity in bin utilization and efficiency.\nComparing (3rd) vs (4th), both are essentially the same adaptive reward/penalty system. \nComparing (second worst) vs (worst), the second worst employs a combination of penalty and sigmoid reward with adaptive scaling, which is slightly better than the excessive reliance on sigmoid rewards and penalties in the worst heuristic.\nOverall, the better heuristics excel due to their adaptive scaling, exact fit rewards, and penalty mechanisms, promoting efficient bin packing.\n- \n- **Keywords**: Efficiency, Adaptability, Simplicity, Balance\n- **Advice**: Emphasize adaptability and simplicity in designing heuristics, focusing on incremental adjustments rather than complex reward and penalty systems.\n- **Avoid**: Detailed balancing with specific penalties/rewards and adaptive scaling leading to overly complex heuristics.\n- **Explanation**: Effective heuristics should be adaptable to different scenarios without relying on a complex system of penalties and rewards. Simplicity ensures they remain flexible and easier to implement, improving their overall efficiency. Overly detailed balancing can introduce unnecessary complexity, leading to heuristics that are less effective and harder to refine.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}