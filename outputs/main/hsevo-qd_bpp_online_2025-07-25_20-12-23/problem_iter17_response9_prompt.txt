{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    item: float, \n    bins_remain_cap: np.ndarray, \n    penalty: float = -3586.2115429161977,\n    sigmoid_threshold_min: float = 0.7607170824887122,\n    sigmoid_threshold_max: float = 0.42411238386705796) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with adaptive sigmoid reward to balance filling bins effectively.\n    \"\"\"\n    priority_scores = np.zeros_like(bins_remain_cap)\n    \n    # Exact fit gets highest priority\n    exact_fit_mask = bins_remain_cap == item\n    priority_scores[exact_fit_mask] = 1.0\n    \n    # Non-exact fits are rewarded based on a sigmoid function for capacity utilization\n    non_exact_fit_mask = np.logical_and(bins_remain_cap >= item, ~exact_fit_mask)\n    if np.any(non_exact_fit_mask):\n        new_remain_cap = bins_remain_cap[non_exact_fit_mask] - item\n        reward = 1 / (1 + np.exp(-new_remain_cap))\n        priority_scores[non_exact_fit_mask] = reward\n    \n    # Penalize bins that cannot fit the item\n    priority_scores = np.where(bins_remain_cap < item, penalty, priority_scores)\n    \n    return priority_scores\n\n### Analyze & experience\n- Comparing (best) vs (worst), we see that the best heuristic uses an adaptive scaling mechanism with rewards for exact fills and penalties for small remaining capacities, promoting efficient use of space. The worst heuristic is repetitive and does not adapt to the situation beyond simple inverse priority based on remaining capacity.\n(second best) vs (second worst) reveals similar patterns, with the second best using an adaptive scaling and sigmoid functions to balance rewards and penalties effectively. The second worst provides less adaptability and relies solely on inverse capacity, disregarding specific rewards and penalties.\nComparing (1st) vs (2nd), we see that the first heuristic incorporates a more nuanced adaptive scaling with exact fit rewards and small capacity penalties. The second heuristic only uses a simple inverse of remaining capacity, lacking adaptations to different scenarios.\n(3rd) vs (4th) continues the trend of the third heuristic being an adaptive scaling version with strategic rewards and penalties, whereas the fourth solely uses the inverse of remaining capacity.\nComparing (second worst) vs (worst), we see that the second worst still contains the adaptive sigmoid mechanism, which is more advanced than the worst's simple inverse function.\n\nOverall, the top-ranked heuristics are distinguished by their adaptive scaling and reward/penalty logic, ensuring efficient and strategic bin filling, whereas the lower-ranked ones rely solely on simple inverse priority measures.\n- \n- **Keywords**: Adaptive scaling, strategic rewards, underutilization penalties, nuanced control.\n- **Advice**: Develop flexible heuristics with dynamic rewards for exact fits and strategic penalties for inefficient placements, balancing bin usage precisely to avoid large leftovers.\n- **Avoid**: Simplicity-driven shortcuts (e.g., simplistic priority measures) and single-factor optimizations (e.g., balancing bin usage alone).\n- **Explanation**: By focusing on adaptive scaling and strategic rewards/penalties, you enhance your heuristics' ability to navigate complex scenarios effectively, avoiding inefficiencies and leveraging optimal packing conditions without resorting to overly simplistic approaches.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}