{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This design implements a heuristic based on the remaining capacity of the bins.\n    Is a priority score based on filling tighter bins first, aiming to close the \n    bins as much as possible to trigger first fit for new bins sooner.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Inverse of remaining capacity (sinusoidal function to prioritize the almost full bins)\n    return 1.0 / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority score for each bin using a combination of penalty and reward,\n    balancing leftover space and efficiency with adaptive scaling.\n    \"\"\"\n    ifbinsfit = bins_remain_cap >= item\n    penalty = np.where(ifbinsfit, 0, -np.inf)\n    reward = np.where(ifbinsfit, 1.0 / (bins_remain_cap - item + 0.1), 0)\n    priority_score = penalty + reward\n    return priority_score\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version uses adaptive scaling and a reward/penalty system to prioritize bins that are more likely to close, promoting efficient use of space.\n    \n    Adaptive scaling adjusts the priority based on the ratio of item size to remaining capacity, encouraging filling of bins to a similar extent.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Base priority: Adaptive scaling based on the item size relative to the remaining capacity\n    base_priority = item / (bins_remain_cap + 0.1)  # adding a small epsilon to avoid division by zero\n    \n    # Specific reward for bins that would be filled to capacity by this item\n    exact_fill_reward = np.where(bins_remain_cap == item, 1.0, 0.0)\n    \n    # Penalty for very small remaining capacities to avoid precision issues with very small numbers\n    small_capacity_penalty = np.where((bins_remain_cap < item) & (bins_remain_cap > 0.1), 0.1, 0.0)\n    \n    # Combined priority score\n    priority_score = base_priority + exact_fill_reward - small_capacity_penalty\n    \n    return priority_score\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority score for each bin, combining exact fit rewards with adaptive scaling.\"\"\"\n    priority_scores = np.zeros_like(bins_remain_cap)\n    \n    # Exact fit has highest priority\n    exact_fit_mask = bins_remain_cap == item\n    priority_scores[exact_fit_mask] = 1.0\n    \n    # For non-exact fits, prioritize based on how close they are to being full, with adaptive scaling\n    non_exact_fit_mask = np.logical_and(bins_remain_cap >= item, ~exact_fit_mask)\n    if np.any(non_exact_fit_mask):\n        priority_scores[non_exact_fit_mask] = 0.5 + 0.5 * (bins_remain_cap[non_exact_fit_mask] - item) / bins_remain_cap[non_exact_fit_mask]\n    \n    # Penalize bins with less remaining capacity than the item size\n    priority_scores = np.where(bins_remain_cap < item, -1000, priority_scores)\n    \n    return priority_scores\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for leftover space with incentives for exact fits using adaptive scaling and sigmoid rewards.\n    \"\"\"\n    # Ensure the item can fit in the bin\n    can_fit = bins_remain_cap >= item\n    \n    # Calculate space left after adding the item\n    space_left = bins_remain_cap - item\n    \n    # Priority for exact fits with weight\n    exact_fit_weight = 1.5\n    exact_fit_priority = can_fit * (bins_remain_cap == item) * exact_fit_weight\n    \n    # Use a sigmoid function to reward bins that are fuller after adding the item\n    sigmoid_reward = 1 / (1 + np.exp(-space_left))\n    \n    # Combine penalties and rewards\n    priority = exact_fit_priority + sigmoid_reward * can_fit\n    \n    return priority\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for leftover space with incentives for exact fits using adaptive scaling and sigmoid rewards.\n    \"\"\"\n    # Ensure the item can fit in the bin\n    can_fit = bins_remain_cap >= item\n    \n    # Calculate space left after adding the item\n    space_left = bins_remain_cap - item\n    \n    # Priority for exact fits with weight\n    exact_fit_weight = 1.5\n    exact_fit_priority = can_fit * (bins_remain_cap == item) * exact_fit_weight\n    \n    # Use a sigmoid function to reward bins that are fuller after adding the item\n    sigmoid_reward = 1 / (1 + np.exp(-space_left))\n    \n    # Combine penalties and rewards\n    priority = exact_fit_priority + sigmoid_reward * can_fit\n    \n    return priority\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for leftover space with incentives for exact fits using adaptive scaling and sigmoid rewards.\n    \"\"\"\n    # Ensure the item can fit in the bin\n    can_fit = bins_remain_cap >= item\n    \n    # Calculate space left after adding the item\n    space_left = bins_remain_cap - item\n    \n    # Priority for exact fits with weight\n    exact_fit_weight = 1.5\n    exact_fit_priority = can_fit * (bins_remain_cap == item) * exact_fit_weight\n    \n    # Use a sigmoid function to reward bins that are fuller after adding the item\n    sigmoid_reward = 1 / (1 + np.exp(-space_left))\n    \n    # Combine penalties and rewards\n    priority = exact_fit_priority + sigmoid_reward * can_fit\n    \n    return priority\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for leftover space with incentives for exact fits using adaptive scaling and sigmoid rewards.\n    \"\"\"\n    # Ensure the item can fit in the bin\n    can_fit = bins_remain_cap >= item\n    \n    # Calculate space left after adding the item\n    space_left = bins_remain_cap - item\n    \n    # Priority for exact fits with weight\n    exact_fit_weight = 1.5\n    exact_fit_priority = can_fit * (bins_remain_cap == item) * exact_fit_weight\n    \n    # Use a sigmoid function to reward bins that are fuller after adding the item\n    sigmoid_reward = 1 / (1 + np.exp(-space_left))\n    \n    # Combine penalties and rewards\n    priority = exact_fit_priority + sigmoid_reward * can_fit\n    \n    return priority\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with adaptive sigmoid reward to balance filling bins effectively.\n    \"\"\"\n    priority_scores = np.zeros_like(bins_remain_cap)\n    \n    # Exact fit gets highest priority\n    exact_fit_mask = bins_remain_cap == item\n    priority_scores[exact_fit_mask] = 1.0\n    \n    # Non-exact fits are rewarded based on a sigmoid function for capacity utilization\n    non_exact_fit_mask = np.logical_and(bins_remain_cap >= item, ~exact_fit_mask)\n    if np.any(non_exact_fit_mask):\n        new_remain_cap = bins_remain_cap[non_exact_fit_mask] - item\n        reward = 1 / (1 + np.exp(-new_remain_cap))\n        priority_scores[non_exact_fit_mask] = reward\n    \n    # Penalize bins that cannot fit the item\n    priority_scores = np.where(bins_remain_cap < item, -1000, priority_scores)\n    \n    return priority_scores\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(\n    item: float, \n    bins_remain_cap: np.ndarray, \n    penalty: float = -3586.2115429161977,\n    sigmoid_threshold_min: float = 0.7607170824887122,\n    sigmoid_threshold_max: float = 0.42411238386705796) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with adaptive sigmoid reward to balance filling bins effectively.\n    \"\"\"\n    priority_scores = np.zeros_like(bins_remain_cap)\n    \n    # Exact fit gets highest priority\n    exact_fit_mask = bins_remain_cap == item\n    priority_scores[exact_fit_mask] = 1.0\n    \n    # Non-exact fits are rewarded based on a sigmoid function for capacity utilization\n    non_exact_fit_mask = np.logical_and(bins_remain_cap >= item, ~exact_fit_mask)\n    if np.any(non_exact_fit_mask):\n        new_remain_cap = bins_remain_cap[non_exact_fit_mask] - item\n        reward = 1 / (1 + np.exp(-new_remain_cap))\n        priority_scores[non_exact_fit_mask] = reward\n    \n    # Penalize bins that cannot fit the item\n    priority_scores = np.where(bins_remain_cap < item, penalty, priority_scores)\n    \n    return priority_scores\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit prioritization with adaptive sigmoid reward to balance filling bins effectively.\n    \"\"\"\n    priority_scores = np.zeros_like(bins_remain_cap)\n    \n    # Exact fit gets highest priority\n    exact_fit_mask = bins_remain_cap == item\n    priority_scores[exact_fit_mask] = 1.0\n    \n    # Non-exact fits are rewarded based on a sigmoid function for capacity utilization\n    non_exact_fit_mask = np.logical_and(bins_remain_cap >= item, ~exact_fit_mask)\n    if np.any(non_exact_fit_mask):\n        new_remain_cap = bins_remain_cap[non_exact_fit_mask] - item\n        reward = 1 / (1 + np.exp(-new_remain_cap))\n        priority_scores[non_exact_fit_mask] = reward\n    \n    # Penalize bins that cannot fit the item\n    priority_scores = np.where(bins_remain_cap < item, -1000, priority_scores)\n    \n    return priority_scores\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Balances penalties for leftover space and rewards for bins close to item size, adjusting for bin capacity.\"\"\"\n    # Prioritize bins that can fit the item with minimal leftover space\n    feasible_caps = np.where(bins_remain_cap - item >= 0, bins_remain_cap - item, -np.inf)\n    # Penalty for larger spaces left after placing the item\n    space_penalty = -feasible_caps\n    # Reward for bins closer to being full after adding the item\n    close_fit_reward = np.clip(bins_remain_cap - item, 0, 1) * 2\n    # Normalize the rewards and penalties\n    norm_reward = close_fit_reward / (np.max(close_fit_reward) + 1e-9)\n    norm_penalty = space_penalty / (np.max(-space_penalty) + 1e-9)\n    # Combined priority score with adaptive scaling\n    priority_score = norm_penalty + norm_reward\n    return priority_score\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for inefficiency and rewards for minimal space left, using a sigmoid for balance.\n    \"\"\"\n    # Calculate new remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n\n    # Penalize bins where the item would make the new remaining capacity negative\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n\n    # Reward bins that leave minimal space after adding the item using a sigmoid\n    reward = 1 / (1 + np.exp(-new_remain_cap))\n\n    # Penalize the last bin to avoid opening a new bin unless strictly necessary\n    penalty_for_new_bin = np.linspace(0, 10.0, len(bins_remain_cap))\n\n    # Combine penalties and rewards into a priority score\n    priority_score = reward + penalty - penalty_for_new_bin\n\n    return priority_score\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for inefficiency and rewards for minimal space left, using a sigmoid for balance.\n    \"\"\"\n    # Calculate new remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n\n    # Penalize bins where the item would make the new remaining capacity negative\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n\n    # Reward bins that leave minimal space after adding the item using a sigmoid\n    reward = 1 / (1 + np.exp(-new_remain_cap))\n\n    # Penalize the last bin to avoid opening a new bin unless strictly necessary\n    penalty_for_new_bin = np.linspace(0, 10.0, len(bins_remain_cap))\n\n    # Combine penalties and rewards into a priority score\n    priority_score = reward + penalty - penalty_for_new_bin\n\n    return priority_score\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines penalties for inefficiency and rewards for minimal space left, using a sigmoid for balance.\n    \"\"\"\n    # Calculate new remaining capacity after adding the item\n    new_remain_cap = bins_remain_cap - item\n\n    # Penalize bins where the item would make the new remaining capacity negative\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n\n    # Reward bins that leave minimal space after adding the item using a sigmoid\n    reward = 1 / (1 + np.exp(-new_remain_cap))\n\n    # Penalize the last bin to avoid opening a new bin unless strictly necessary\n    penalty_for_new_bin = np.linspace(0, 10.0, len(bins_remain_cap))\n\n    # Combine penalties and rewards into a priority score\n    priority_score = reward + penalty - penalty_for_new_bin\n\n    return priority_score\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines penalties for over-filling and rewards for near-full bins using adaptive scaling.\"\"\"\n    new_remain_cap = bins_remain_cap - item\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n    reward = 1 / (1 + np.exp(-new_remain_cap / (bins_remain_cap + 0.1)))\n    priority_score = penalty + reward\n    return priority_score\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines penalties for over-filling and rewards for near-full bins using adaptive scaling.\"\"\"\n    new_remain_cap = bins_remain_cap - item\n    penalty = np.where(new_remain_cap < 0, -100, 0)\n    reward = 1 / (1 + np.exp(-new_remain_cap / (bins_remain_cap + 0.1)))\n    priority_score = penalty + reward\n    return priority_score\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}