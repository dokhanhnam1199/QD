{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Calculates bin priorities considering fit, waste, fill level, and spread.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fit check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)  # Minimize waste\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap / 2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)  # Half-full target\n\n    scaled_bins = bins_remain_cap[valid_bins] ** 2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6)  # Spread items\n\n    # Adaptive exploration: higher randomness for nearly full bins\n    nearly_full = bins_remain_cap[valid_bins] < item * 1.1\n    random_weight = np.where(nearly_full, 0.02, 0.01)  # more exploration if nearly full\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * random_weight\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Calculates bin priorities considering fit, waste, fill level, and spread.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fit check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)  # Minimize waste\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap / 2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)  # Half-full target\n\n    scaled_bins = bins_remain_cap[valid_bins] ** 2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6)  # Spread items\n\n    # Adaptive exploration: higher randomness for nearly full bins\n    nearly_full = bins_remain_cap[valid_bins] < item * 1.1\n    random_weight = np.where(nearly_full, 0.02, 0.01)  # more exploration if nearly full\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * random_weight\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Calculates bin priorities considering fit, waste, fill level, and spread.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fit check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)  # Minimize waste\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap / 2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)  # Half-full target\n\n    scaled_bins = bins_remain_cap[valid_bins] ** 2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6)  # Spread items\n\n    # Adaptive exploration: higher randomness for nearly full bins\n    nearly_full = bins_remain_cap[valid_bins] < item * 1.1\n    random_weight = np.where(nearly_full, 0.02, 0.01)  # more exploration if nearly full\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * random_weight\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0\n\n    priorities[valid_bins] = 1\n\n    # Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Prioritize bins closer to half-full after adding the item\n    half_full_diff = np.abs(remaining_cap_after_add[valid_bins] - np.mean(bins_remain_cap) / 2) # Try to make bins uniformly occupied\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)\n\n    #Scale the bin_remain_cap to emphasize almost full bins\n    scaled_bins_remain_cap = bins_remain_cap[valid_bins]**2\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += scaled_bins_remain_cap / np.sum(scaled_bins_remain_cap + 1e-6)\n\n    # Add a small random component to break ties and explore different solutions\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assign priority to bins based on fit, wasted space, and occupancy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1\n\n    remaining_cap = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + remaining_cap)\n    \n    mean_cap = np.mean(bins_remain_cap)\n    priorities[valid_bins] += 1.0 / (1e-6 + np.abs(remaining_cap - mean_cap / 2))\n\n    scaled_cap = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += scaled_cap / np.sum(scaled_cap + 1e-6)\n    \n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assign priority to bins based on fit, wasted space, and occupancy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1\n\n    remaining_cap = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + remaining_cap)\n    \n    mean_cap = np.mean(bins_remain_cap)\n    priorities[valid_bins] += 1.0 / (1e-6 + np.abs(remaining_cap - mean_cap / 2))\n\n    scaled_cap = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += scaled_cap / np.sum(scaled_cap + 1e-6)\n    \n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assign priority to bins based on fit, wasted space, and occupancy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1\n\n    remaining_cap = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + remaining_cap)\n    \n    mean_cap = np.mean(bins_remain_cap)\n    priorities[valid_bins] += 1.0 / (1e-6 + np.abs(remaining_cap - mean_cap / 2))\n\n    scaled_cap = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += scaled_cap / np.sum(scaled_cap + 1e-6)\n    \n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best aspects of v1 and introduces dynamic scaling and capacity awareness.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fits check\n\n    # Minimize wasted space, scale based on item size\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += (item / (np.mean(bins_remain_cap) + 1e-6)) / (1e-6 + wasted_space)\n\n    # Target Half-full, but adjust the target based on item size\n    mean_cap = np.mean(bins_remain_cap)\n    target_level = max(mean_cap / 2, item)  # Adjust the target\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - target_level)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)\n\n    # Spread items, but normalize by mean remaining capacity\n    scaled_bins = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += (scaled_bins / (np.sum(scaled_bins) + 1e-6)) * (mean_cap / (np.max(bins_remain_cap)+1e-6))\n\n    # Introduce more nuanced tie-breaking: favor bins closer to item size.\n    tiebreak_values = np.random.rand(np.sum(valid_bins))\n    size_diffs = np.abs(bins_remain_cap[valid_bins] - item)\n    normalized_size_diffs = size_diffs / (np.max(bins_remain_cap) + 1e-6)\n    priorities[valid_bins] += tiebreak_values * (1 - normalized_size_diffs) * 0.01\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines fit ratio and energy landscape concepts for priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    \n    # Fit ratio component: prioritize closer fits, avoid near-full bins\n    fit_ratio = item / bins_remain_cap[fit_mask]\n    priorities[fit_mask] = (1 - np.abs(fit_ratio - 1)) - 0.01 * (bins_remain_cap[fit_mask] - item)\n    \n    # Energy landscape component: disincentivize largely empty bins\n    priorities[fit_mask] /= (bins_remain_cap[fit_mask] + 1e-9) # Avoid division by zero\n    \n    # Set low priority for bins where item doesn't fit\n    priorities[~fit_mask] = -np.inf\n    \n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more sophisticated priority function for online bin packing, incorporating\n    adaptive strategies, nuanced weighting, and controlled exploration.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # 1. Feasibility Boost: Ensure only valid bins are considered.\n    priorities[valid_bins] += 1.0\n\n    # 2. Wasted Space Minimization (Adaptive Weighting):\n    wasted_space = bins_remain_cap[valid_bins] - item\n    # Softer penalty for slightly exceeding, to allow for better distribution later\n    wasted_space_priority = 1.0 / (1e-6 + wasted_space)\n\n    # Make it adaptive based on item size: If item is small, waste matters more.\n    wasted_space_weight = min(1.0, item)  # Scale down priority for larger items\n    priorities[valid_bins] += wasted_space_priority * wasted_space_weight\n\n    # 3. Target Fill Level (Dynamic Adjustment):\n    target_fill = np.mean(bins_remain_cap) / 2.0 # Attempt to target half full\n    fill_diff = np.abs(bins_remain_cap[valid_bins] - item - target_fill)\n    fill_priority = 1.0 / (1e-6 + fill_diff)\n\n    # Dynamic weight based on how full bins are.  If all bins are nearly full,\n    # the fill difference matters less.\n    fill_weight = 1 - (np.mean(bins_remain_cap) / np.max(bins_remain_cap))\n    priorities[valid_bins] += fill_priority * fill_weight\n\n    # 4. Bin Fragmentation Penalty (Nuanced):\n    # Encourages filling bins that already have items. The more items, the more important.\n    # This is a very simplistic proxy, but we lack history in an online context.\n    occupied_space = np.max(bins_remain_cap) - bins_remain_cap[valid_bins]\n    fragmentation_priority = occupied_space / (np.max(bins_remain_cap) + 1e-6)\n    priorities[valid_bins] += fragmentation_priority\n\n    # 5. Encourage Spread (Non-linear Scaling):\n    # Encourages using empty bins, but not too strongly if other bins are filling up nicely.\n    empty_bin_bonus = (np.max(bins_remain_cap) - bins_remain_cap[valid_bins])**2\n    priorities[valid_bins] += empty_bin_bonus / (np.sum(empty_bin_bonus) + 1e-6)\n\n    # 6. Controlled Exploration (Simulated Annealing):\n    temperature = 0.1 # Higher values cause more exploration\n    random_noise = np.random.rand(np.sum(valid_bins)) * temperature\n    priorities[valid_bins] += random_noise\n\n    # 7. Prioritize bins closer to item sizes.\n    size_diff = np.abs(bins_remain_cap[valid_bins] - item)\n    size_priority = 1.0 / (1e-6 + size_diff)\n    size_weight = 0.5 # Adjust the weight given to size preference.\n    priorities[valid_bins] += size_priority * size_weight\n\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n    \n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    priorities[valid_bins] = 1\n\n    #Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += bins_remain_cap[valid_bins] / np.sum(bins_remain_cap[valid_bins] + 1e-6)\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n    \n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    priorities[valid_bins] = 1\n\n    #Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += bins_remain_cap[valid_bins] / np.sum(bins_remain_cap[valid_bins] + 1e-6)\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more sophisticated priority function for online bin packing, incorporating\n    adaptive strategies, nuanced weighting, and controlled exploration.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # 1. Feasibility Boost: Ensure only valid bins are considered.\n    priorities[valid_bins] += 1.0\n\n    # 2. Wasted Space Minimization (Adaptive Weighting):\n    wasted_space = bins_remain_cap[valid_bins] - item\n    # Softer penalty for slightly exceeding, to allow for better distribution later\n    wasted_space_priority = 1.0 / (1e-6 + wasted_space)\n\n    # Make it adaptive based on item size: If item is small, waste matters more.\n    wasted_space_weight = min(1.0, item)  # Scale down priority for larger items\n    priorities[valid_bins] += wasted_space_priority * wasted_space_weight\n\n    # 3. Target Fill Level (Dynamic Adjustment):\n    target_fill = np.mean(bins_remain_cap) / 2.0 # Attempt to target half full\n    fill_diff = np.abs(bins_remain_cap[valid_bins] - item - target_fill)\n    fill_priority = 1.0 / (1e-6 + fill_diff)\n\n    # Dynamic weight based on how full bins are.  If all bins are nearly full,\n    # the fill difference matters less.\n    fill_weight = 1 - (np.mean(bins_remain_cap) / np.max(bins_remain_cap))\n    priorities[valid_bins] += fill_priority * fill_weight\n\n    # 4. Bin Fragmentation Penalty (Nuanced):\n    # Encourages filling bins that already have items. The more items, the more important.\n    # This is a very simplistic proxy, but we lack history in an online context.\n    occupied_space = np.max(bins_remain_cap) - bins_remain_cap[valid_bins]\n    fragmentation_priority = occupied_space / (np.max(bins_remain_cap) + 1e-6)\n    priorities[valid_bins] += fragmentation_priority\n\n    # 5. Encourage Spread (Non-linear Scaling):\n    # Encourages using empty bins, but not too strongly if other bins are filling up nicely.\n    empty_bin_bonus = (np.max(bins_remain_cap) - bins_remain_cap[valid_bins])**2\n    priorities[valid_bins] += empty_bin_bonus / (np.sum(empty_bin_bonus) + 1e-6)\n\n    # 6. Controlled Exploration (Simulated Annealing):\n    temperature = 0.1 # Higher values cause more exploration\n    random_noise = np.random.rand(np.sum(valid_bins)) * temperature\n    priorities[valid_bins] += random_noise\n\n    # 7. Prioritize bins closer to item sizes.\n    size_diff = np.abs(bins_remain_cap[valid_bins] - item)\n    size_priority = 1.0 / (1e-6 + size_diff)\n    size_weight = 0.5 # Adjust the weight given to size preference.\n    priorities[valid_bins] += size_priority * size_weight\n\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n    \n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    priorities[valid_bins] = 1\n\n    #Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += bins_remain_cap[valid_bins] / np.sum(bins_remain_cap[valid_bins] + 1e-6)\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An enhanced priority function for the online bin packing problem.\n\n    This version incorporates adaptive elements and problem-specific knowledge\n    to achieve improved packing efficiency. It considers aspects like\n    remaining capacity distribution, item size relative to bin sizes, and\n    introduces a dynamic exploration factor.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # Base priority for valid bins\n    priorities[valid_bins] = 1.0\n\n    # Minimize wasted space - scaled by item size\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += item / (1e-6 + wasted_space)\n\n    # Encourage filling bins closer to a target level (e.g., 75% full)\n    target_fill = 0.75  # Adjust as needed\n    bin_size = bins_remain_cap.max() + item # Assuming all bins have same initial capacity\n    target_capacity = target_fill * bin_size\n\n    fill_level_diff = np.abs(bins_remain_cap[valid_bins] - (bin_size - item) - target_capacity)\n    priorities[valid_bins] += 1.0 / (1e-6 + fill_level_diff)\n\n    # Balance bin usage - penalize bins with significantly larger remaining capacity\n    capacity_ratio = bins_remain_cap[valid_bins] / (bin_size + 1e-6)\n    priorities[valid_bins] += (1 - capacity_ratio)**2  # Higher priority for fuller bins\n\n    # Adaptive exploration factor: Adjusts randomness based on problem state\n    # More randomness when bins are relatively empty or very full\n    avg_capacity = np.mean(bins_remain_cap)\n    exploration_factor = np.clip(1 - np.abs(bins_remain_cap[valid_bins] - avg_capacity) / (bin_size + 1e-6), 0.01, 0.1) # Scale exploration with difference from mean capacity\n\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * exploration_factor\n\n    # Favor bins that are close to the item size\n    size_difference = np.abs(bins_remain_cap[valid_bins] - item)\n    priorities[valid_bins] += 1.0 / (1e-6 + size_difference)\n    \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An enhanced priority function for the online bin packing problem.\n\n    This version incorporates adaptive elements and problem-specific knowledge\n    to achieve improved packing efficiency. It considers aspects like\n    remaining capacity distribution, item size relative to bin sizes, and\n    introduces a dynamic exploration factor.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # Base priority for valid bins\n    priorities[valid_bins] = 1.0\n\n    # Minimize wasted space - scaled by item size\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += item / (1e-6 + wasted_space)\n\n    # Encourage filling bins closer to a target level (e.g., 75% full)\n    target_fill = 0.75  # Adjust as needed\n    bin_size = bins_remain_cap.max() + item # Assuming all bins have same initial capacity\n    target_capacity = target_fill * bin_size\n\n    fill_level_diff = np.abs(bins_remain_cap[valid_bins] - (bin_size - item) - target_capacity)\n    priorities[valid_bins] += 1.0 / (1e-6 + fill_level_diff)\n\n    # Balance bin usage - penalize bins with significantly larger remaining capacity\n    capacity_ratio = bins_remain_cap[valid_bins] / (bin_size + 1e-6)\n    priorities[valid_bins] += (1 - capacity_ratio)**2  # Higher priority for fuller bins\n\n    # Adaptive exploration factor: Adjusts randomness based on problem state\n    # More randomness when bins are relatively empty or very full\n    avg_capacity = np.mean(bins_remain_cap)\n    exploration_factor = np.clip(1 - np.abs(bins_remain_cap[valid_bins] - avg_capacity) / (bin_size + 1e-6), 0.01, 0.1) # Scale exploration with difference from mean capacity\n\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * exploration_factor\n\n    # Favor bins that are close to the item size\n    size_difference = np.abs(bins_remain_cap[valid_bins] - item)\n    priorities[valid_bins] += 1.0 / (1e-6 + size_difference)\n    \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority: float = 1.3920066404433198,\n                wasted_space_epsilon: float = 4.827875296811026e-06,\n                half_full_epsilon: float = 7.010850237263482e-06,\n                spread_epsilon: float = 6.740380158224467e-06,\n                random_component_weight: float = 0.010327684384234433) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority: Priority to give valid bins.\n        wasted_space_epsilon: Epsilon to avoid division by zero when prioritizing least wasted space.\n        half_full_epsilon: Epsilon to avoid division by zero when prioritizing bins closer to half-full.\n        spread_epsilon: Epsilon to avoid division by zero when spreading items across bins.\n        random_component_weight: Weight of the random component added to break ties.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, fit_priority: float = 1.96691293969179, wasted_space_weight: float = 0.5594849641789597,\n                half_full_weight: float = 0.41937172867409367, spread_weight: float = 1.763440391628324, random_weight: float = 0.07086293885039939,\n                epsilon: float = 2.024658583252878e-06, half_full_scaling: float = 2.8217905038897073) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority: Priority score for bins where the item fits.\n        wasted_space_weight: Weight of the wasted space component.\n        half_full_weight: Weight of the half-full component.\n        spread_weight: Weight of the spread component.\n        random_weight: Weight of the random component.\n        epsilon: Small value to avoid division by zero.\n        half_full_scaling: Scaling exponent for bin remaining capacities.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority: float = 1.3920066404433198,\n                wasted_space_epsilon: float = 4.827875296811026e-06,\n                half_full_epsilon: float = 7.010850237263482e-06,\n                spread_epsilon: float = 6.740380158224467e-06,\n                random_component_weight: float = 0.010327684384234433) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority: Priority to give valid bins.\n        wasted_space_epsilon: Epsilon to avoid division by zero when prioritizing least wasted space.\n        half_full_epsilon: Epsilon to avoid division by zero when prioritizing bins closer to half-full.\n        spread_epsilon: Epsilon to avoid division by zero when spreading items across bins.\n        random_component_weight: Weight of the random component added to break ties.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}