[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n    \n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    priorities[valid_bins] = 1\n\n    #Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += bins_remain_cap[valid_bins] / np.sum(bins_remain_cap[valid_bins] + 1e-6)\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 49.82892142331044,
    "mi": 62.53484120287788,
    "token_count": 86.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines fit ratio and energy landscape concepts for priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    \n    # Fit ratio component: prioritize closer fits, avoid near-full bins\n    fit_ratio = item / bins_remain_cap[fit_mask]\n    priorities[fit_mask] = (1 - np.abs(fit_ratio - 1)) - 0.01 * (bins_remain_cap[fit_mask] - item)\n    \n    # Energy landscape component: disincentivize largely empty bins\n    priorities[fit_mask] /= (bins_remain_cap[fit_mask] + 1e-9) # Avoid division by zero\n    \n    # Set low priority for bins where item doesn't fit\n    priorities[~fit_mask] = -np.inf\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 218.7248250995196,
    "mi": 78.58204003001397,
    "token_count": 150.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0\n\n    priorities[valid_bins] = 1\n\n    # Prioritize bins that will have the least wasted space\n    wasted_space = remaining_cap_after_add[valid_bins]\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)\n\n    # Prioritize bins closer to half-full after adding the item\n    half_full_diff = np.abs(remaining_cap_after_add[valid_bins] - np.mean(bins_remain_cap) / 2) # Try to make bins uniformly occupied\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)\n\n    #Scale the bin_remain_cap to emphasize almost full bins\n    scaled_bins_remain_cap = bins_remain_cap[valid_bins]**2\n\n    # Try to spread items across bins if possible, less important when nearing full bins\n    priorities[valid_bins] += scaled_bins_remain_cap / np.sum(scaled_bins_remain_cap + 1e-6)\n\n    # Add a small random component to break ties and explore different solutions\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 2.7323494216194746,
    "SLOC": 17.0,
    "cyclomatic_complexity": 4.0,
    "halstead": 316.09556827246695,
    "mi": 81.03776293129587,
    "token_count": 238.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                fit_priority: float = 1.3920066404433198,\n                wasted_space_epsilon: float = 4.827875296811026e-06,\n                half_full_epsilon: float = 7.010850237263482e-06,\n                spread_epsilon: float = 6.740380158224467e-06,\n                random_component_weight: float = 0.010327684384234433) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        fit_priority: Priority to give valid bins.\n        wasted_space_epsilon: Epsilon to avoid division by zero when prioritizing least wasted space.\n        half_full_epsilon: Epsilon to avoid division by zero when prioritizing bins closer to half-full.\n        spread_epsilon: Epsilon to avoid division by zero when spreading items across bins.\n        random_component_weight: Weight of the random component added to break ties.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate remaining capacity after adding the item\n    remaining_cap_after_add = bins_remain_cap - item\n\n    # Give a high priority to bins where the item fits\n    valid_bins = remaining_cap_after_add >= 0\n    if not np.any(valid_bins):\n        return priorities # No valid bins, all priorities remain 0",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 24.0,
    "mi": 93.10857950438157,
    "token_count": 107.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines best aspects of v0 and v1 to create effective heuristic.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0 # Fits check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space) # Minimize wasted space\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap/2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff) # Target Half-full\n\n    scaled_bins = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6) # Spread items\n\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01 # Break ties\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.6924611088950936,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 256.46415084724833,
    "mi": 83.80970596468467,
    "token_count": 186.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    An enhanced priority function for the online bin packing problem.\n\n    This version incorporates adaptive elements and problem-specific knowledge\n    to achieve improved packing efficiency. It considers aspects like\n    remaining capacity distribution, item size relative to bin sizes, and\n    introduces a dynamic exploration factor.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # Base priority for valid bins\n    priorities[valid_bins] = 1.0\n\n    # Minimize wasted space - scaled by item size\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += item / (1e-6 + wasted_space)\n\n    # Encourage filling bins closer to a target level (e.g., 75% full)\n    target_fill = 0.75  # Adjust as needed\n    bin_size = bins_remain_cap.max() + item # Assuming all bins have same initial capacity\n    target_capacity = target_fill * bin_size\n\n    fill_level_diff = np.abs(bins_remain_cap[valid_bins] - (bin_size - item) - target_capacity)\n    priorities[valid_bins] += 1.0 / (1e-6 + fill_level_diff)\n\n    # Balance bin usage - penalize bins with significantly larger remaining capacity\n    capacity_ratio = bins_remain_cap[valid_bins] / (bin_size + 1e-6)\n    priorities[valid_bins] += (1 - capacity_ratio)**2  # Higher priority for fuller bins\n\n    # Adaptive exploration factor: Adjusts randomness based on problem state\n    # More randomness when bins are relatively empty or very full\n    avg_capacity = np.mean(bins_remain_cap)\n    exploration_factor = np.clip(1 - np.abs(bins_remain_cap[valid_bins] - avg_capacity) / (bin_size + 1e-6), 0.01, 0.1) # Scale exploration with difference from mean capacity\n\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * exploration_factor\n\n    # Favor bins that are close to the item size\n    size_difference = np.abs(bins_remain_cap[valid_bins] - item)\n    priorities[valid_bins] += 1.0 / (1e-6 + size_difference)\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.058635819704831,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 463.551887559856,
    "mi": 75.1514010445255,
    "token_count": 254.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Calculates bin priorities considering fit, waste, fill level, and spread.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fit check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)  # Minimize waste\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap / 2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)  # Half-full target\n\n    scaled_bins = bins_remain_cap[valid_bins] ** 2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6)  # Spread items\n\n    # Adaptive exploration: higher randomness for nearly full bins\n    nearly_full = bins_remain_cap[valid_bins] < item * 1.1\n    random_weight = np.where(nearly_full, 0.02, 0.01)  # more exploration if nearly full\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * random_weight\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 2.7024331870762004,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 166.4210625757214,
    "mi": 88.38041487868065,
    "token_count": 146.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response4.txt_stdout.txt",
    "code_path": "problem_iter9_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more sophisticated priority function for online bin packing, incorporating\n    adaptive strategies, nuanced weighting, and controlled exploration.\n    \"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    # 1. Feasibility Boost: Ensure only valid bins are considered.\n    priorities[valid_bins] += 1.0\n\n    # 2. Wasted Space Minimization (Adaptive Weighting):\n    wasted_space = bins_remain_cap[valid_bins] - item\n    # Softer penalty for slightly exceeding, to allow for better distribution later\n    wasted_space_priority = 1.0 / (1e-6 + wasted_space)\n\n    # Make it adaptive based on item size: If item is small, waste matters more.\n    wasted_space_weight = min(1.0, item)  # Scale down priority for larger items\n    priorities[valid_bins] += wasted_space_priority * wasted_space_weight\n\n    # 3. Target Fill Level (Dynamic Adjustment):\n    target_fill = np.mean(bins_remain_cap) / 2.0 # Attempt to target half full\n    fill_diff = np.abs(bins_remain_cap[valid_bins] - item - target_fill)\n    fill_priority = 1.0 / (1e-6 + fill_diff)\n\n    # Dynamic weight based on how full bins are.  If all bins are nearly full,\n    # the fill difference matters less.\n    fill_weight = 1 - (np.mean(bins_remain_cap) / np.max(bins_remain_cap))\n    priorities[valid_bins] += fill_priority * fill_weight\n\n    # 4. Bin Fragmentation Penalty (Nuanced):\n    # Encourages filling bins that already have items. The more items, the more important.\n    # This is a very simplistic proxy, but we lack history in an online context.\n    occupied_space = np.max(bins_remain_cap) - bins_remain_cap[valid_bins]\n    fragmentation_priority = occupied_space / (np.max(bins_remain_cap) + 1e-6)\n    priorities[valid_bins] += fragmentation_priority\n\n    # 5. Encourage Spread (Non-linear Scaling):\n    # Encourages using empty bins, but not too strongly if other bins are filling up nicely.\n    empty_bin_bonus = (np.max(bins_remain_cap) - bins_remain_cap[valid_bins])**2\n    priorities[valid_bins] += empty_bin_bonus / (np.sum(empty_bin_bonus) + 1e-6)\n\n    # 6. Controlled Exploration (Simulated Annealing):\n    temperature = 0.1 # Higher values cause more exploration\n    random_noise = np.random.rand(np.sum(valid_bins)) * temperature\n    priorities[valid_bins] += random_noise\n\n    # 7. Prioritize bins closer to item sizes.\n    size_diff = np.abs(bins_remain_cap[valid_bins] - item)\n    size_priority = 1.0 / (1e-6 + size_diff)\n    size_weight = 0.5 # Adjust the weight given to size preference.\n    priorities[valid_bins] += size_priority * size_weight\n\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 28.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 601.3738273490744,
    "mi": 76.58282637504686,
    "token_count": 356.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assign priority to bins based on fit, wasted space, and occupancy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1\n\n    remaining_cap = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + remaining_cap)\n    \n    mean_cap = np.mean(bins_remain_cap)\n    priorities[valid_bins] += 1.0 / (1e-6 + np.abs(remaining_cap - mean_cap / 2))\n\n    scaled_cap = bins_remain_cap[valid_bins]**2\n    priorities[valid_bins] += scaled_cap / np.sum(scaled_cap + 1e-6)\n    \n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * 0.01\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 2.7921818907060234,
    "SLOC": 17.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 325.3715058335023,
    "mi": 81.12302816555501,
    "token_count": 219.0,
    "exec_success": true
  }
]