{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Calculates bin priorities considering fit, waste, fill level, and spread.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap - item >= 0\n\n    if not np.any(valid_bins):\n        return priorities\n\n    priorities[valid_bins] = 1.0  # Fit check\n    wasted_space = bins_remain_cap[valid_bins] - item\n    priorities[valid_bins] += 1.0 / (1e-6 + wasted_space)  # Minimize waste\n\n    mean_cap = np.mean(bins_remain_cap)\n    half_full_diff = np.abs(bins_remain_cap[valid_bins] - item - mean_cap / 2)\n    priorities[valid_bins] += 1.0 / (1e-6 + half_full_diff)  # Half-full target\n\n    scaled_bins = bins_remain_cap[valid_bins] ** 2\n    priorities[valid_bins] += scaled_bins / (np.sum(scaled_bins) + 1e-6)  # Spread items\n\n    # Adaptive exploration: higher randomness for nearly full bins\n    nearly_full = bins_remain_cap[valid_bins] < item * 1.1\n    random_weight = np.where(nearly_full, 0.02, 0.01)  # more exploration if nearly full\n    priorities[valid_bins] += np.random.rand(np.sum(valid_bins)) * random_weight\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines fit ratio and energy landscape concepts for priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    \n    # Fit ratio component: prioritize closer fits, avoid near-full bins\n    fit_ratio = item / bins_remain_cap[fit_mask]\n    priorities[fit_mask] = (1 - np.abs(fit_ratio - 1)) - 0.01 * (bins_remain_cap[fit_mask] - item)\n    \n    # Energy landscape component: disincentivize largely empty bins\n    priorities[fit_mask] /= (bins_remain_cap[fit_mask] + 1e-9) # Avoid division by zero\n    \n    # Set low priority for bins where item doesn't fit\n    priorities[~fit_mask] = -np.inf\n    \n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the best heuristic incorporates fit, waste, fill level, and spread, while the worst only considers the ratio of item size to remaining capacity. The best also utilizes adaptive exploration with randomness, while the worst relies solely on logarithmic ratios.\n\nComparing (2nd) vs (19th), the same code is repeated, indicating that these heuristics are of equal (presumably poor) quality. They are also identical to (1st), showing an error in the ranking.\n\nComparing (1st) vs (4th), both consider fit, waste, half-full target, and spread. However, (1st) has adaptive exploration that increases randomness for nearly full bins, using `np.where`, while (4th) uses a fixed small random component.\n\nComparing (4th) vs (11th), (4th) targets a half-full state and scales bin remaining capacity, whereas (11th) only considers fit, waste, and a basic spread mechanism.\n\nComparing (9th) vs (10th), (9th) uses a fit ratio and energy landscape concept, potentially leading to instability with `-np.inf`. (10th) employs adaptive weighting for wasted space, dynamic adjustment for target fill, a bin fragmentation penalty, and controlled exploration (simulated annealing).\n\nComparing (17th) vs (18th), both introduce a large number of tunable parameters, but (18th) also includes weights for each component and a scaling exponent for the half-full target. However, neither includes any actual computations, so their relative ranking is moot.\n\nOverall: The better heuristics incorporate more factors (fit, waste, fill level, spread), use adaptive exploration strategies, and dynamically adjust weights. The worse heuristics tend to oversimplify the problem, lack adaptability, or introduce instability. Sophistication alone doesn't guarantee success. Simply adding tunable parameters without implementing the core logic doesn't improve performance.\n- \nHere's a redefined approach to \"Current Self-Reflection\" for designing better heuristics:\n\n*   **Keywords:** Problem context, dynamic weighting, adaptive exploration, performance measurement.\n\n*   **Advice:** Focus on designing heuristics that dynamically adapt to the specific problem instance and phase. Prioritize measurable performance improvements over theoretical elegance.\n\n*   **Avoid:** Premature optimization, reliance on static parameters, ignoring the problem's nuances and edge cases.\n\n*   **Explanation:** Effective self-reflection analyzes *why* a heuristic works or fails in specific scenarios, guiding iterative refinement informed by empirical data. Don't just consider factors; understand their *impact* and interaction.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}