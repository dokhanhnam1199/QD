```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin,
    implementing a highly refined Best Fit heuristic with multiple thresholds
    and non-linear scoring to aggressively minimize fragmentation and
    prioritize tight fits.

    This strategy extends the Best Fit principle by applying a multi-tiered
    non-linear penalty based on the remaining capacity after fitting
    the item. The goal is to maximize the filled portion of the bin while
    minimizing unusable leftover space.

    - **Perfect Fits (0 remaining capacity):** Receive the absolute highest priority (score 0).
    - **Tight Fits (0 < remaining_cap <= tight_fit_threshold):** A significantly
      reduced penalty is applied using a higher-order polynomial decay (e.g., quartic).
      This makes bins resulting in very small gaps extremely desirable, aggressively
      encouraging near-perfect fills to 'trap' items and prevent creation of many
      sparsely filled bins.
    - **Mid-Range Fits (tight_fit_threshold < remaining_cap <= loose_fit_threshold):**
      A standard linear Best Fit penalty is applied. These are good fits, but not
      as optimal as tight fits.
    - **Loose Fits (remaining_cap > loose_fit_threshold):** A super-linear penalty
      is applied to discourage very loose fits. This helps prevent items from being
      placed in bins where they leave a disproportionately large amount of space,
      potentially "saving" that bin for a larger item that might appear later, or
      forcing the current item into a new bin if all existing bins offer only very loose fits.

    This comprehensive approach aims to minimize overall fragmentation by strongly
    preferring bins that become nearly full, while also disincentivizing the creation
    of excessive empty space in partially filled bins.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of current remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Higher scores (less negative) indicate a more desirable bin.
    """
    # Initialize all priorities to a very low negative number (effectively -infinity)
    # for bins that cannot accommodate the item.
    scores = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    # Identify bins where the item can fit
    can_fit_mask = bins_remain_cap >= item

    # Calculate remaining capacity after fit for eligible bins
    remaining_after_fit = bins_remain_cap[can_fit_mask] - item

    # Define thresholds for differentiating fit quality.
    # These values might need tuning based on typical item sizes and bin capacities.
    # Assumes normalized capacities (e.g., bin capacity = 1.0).
    tight_fit_threshold = 0.05  # Remaining capacity up to 5% is considered 'tight'
    loose_fit_threshold = 0.25  # Remaining capacity over 25% is considered 'loose'

    # Power factors for non-linear scaling.
    # power_factor_tight: Higher value provides a more aggressive incentive for very small
    #   remaining capacities (closer to 0). E.g., 4 for quartic decay.
    # power_factor_loose: Value > 1 applies a super-linear penalty for very loose fits.
    #   E.g., 1.5 makes the penalty grow faster than linear but slower than quadratic.
    power_factor_tight = 4
    power_factor_loose = 1.5

    # Initialize scaled_remaining for eligible bins to standard linear penalty.
    # This will be the default for mid-range fits.
    scaled_remaining = remaining_after_fit

    # Apply non-linear penalty for 'tight fits': 0 < r <= tight_fit_threshold
    # The formula (r^k / T^(k-1)) ensures continuity at r=T (where it equals T)
    # and makes smaller 'r' values result in much less penalty.
    tight_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= tight_fit_threshold)
    if np.any(tight_fit_mask): # Avoid division by zero if threshold is 0 or power_factor is 1
        if tight_fit_threshold > 0 and power_factor_tight > 1:
            scaled_remaining[tight_fit_mask] = (
                (remaining_after_fit[tight_fit_mask] ** power_factor_tight) /
                (tight_fit_threshold ** (power_factor_tight - 1))
            )
        else: # Fallback to linear or handle edge cases if threshold is 0 or power is 1
            scaled_remaining[tight_fit_mask] = remaining_after_fit[tight_fit_mask]


    # Apply super-linear penalty for 'loose fits': r > loose_fit_threshold
    # The formula (r^p * L^(1-p)) ensures continuity at r=L (where it equals L)
    # and grows faster than linear for r > L when p > 1.
    loose_fit_mask = (remaining_after_fit > loose_fit_threshold)
    if np.any(loose_fit_mask):
        if loose_fit_threshold > 0 and power_factor_loose != 1:
            scaled_remaining[loose_fit_mask] = (
                (remaining_after_fit[loose_fit_mask] ** power_factor_loose) *
                (loose_fit_threshold ** (1 - power_factor_loose))
            )
        else: # Fallback to linear or handle edge cases if threshold is 0 or power is 1
            scaled_remaining[loose_fit_mask] = remaining_after_fit[loose_fit_mask]

    # If remaining_after_fit is exactly 0, scaled_remaining will be 0,
    # making perfect fits the highest priority (score 0).
    # Otherwise, the score is the negative of the scaled remaining capacity,
    # so lower scaled_remaining (less penalty) means a higher score (less negative).
    scores[can_fit_mask] = -scaled_remaining

    return scores
```
