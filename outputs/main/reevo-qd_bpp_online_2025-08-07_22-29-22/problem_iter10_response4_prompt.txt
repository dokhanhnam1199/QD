{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Global random generator (single instance)\n_rng = np.random.default_rng()\n\n# Exponential moving average parameters for item size statistics\n_alpha_ema = 0.1  # smoothing factor (0 < alpha <= 1)\n_ema_item_size = 0.0  # mean of seen items\n_ema_item_sq = 0.0    # mean of squared sizes\n_item_count = 0\n\n# Adaptive parameters\n_epsilon_factor = 0.05  # fraction of std used as exact\u2011fit tolerance\n_swap_weight = 0.05     # scaling for the swap\u2011improvement boost\n_exact_fit_offset = 1e9 # offset ensuring exact fits dominate priority\n\n\n    \"\"\"Update exponential moving averages of item size and squared size.\"\"\"\n    global _ema_item_size, _ema_item_sq, _item_count\n    _item_count += 1\n    # EMA update\n    _ema_item_size = (1 - _alpha_ema) * _ema_item_size + _alpha_ema * item\n    _ema_item_sq = (1 - _alpha_ema) * _ema_item_sq + _alpha_ema * (item * item)\n\n\n    \"\"\"Return the current estimated standard deviation of item sizes.\"\"\"\n    var = _ema_item_sq - _ema_item_size * _ema_item_size\n    if var < 0.0:\n        var = 0.0\n    return np.sqrt(var)\n\n\n    \"\"\"\n    Adaptive priority for online Bin Packing.\n\n    The priority reflects three principles:\n    1. Exact fits are always preferred (highest priority).\n    2. Among non\u2011exact feasible bins, smaller leftover capacity is better.\n    3. A small boost is added when the leftover after placement is close to\n       the average item size observed so far (swap\u2011improvement heuristic).\n    Ties are broken with a tiny random perturbation.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities of the existing bins.\n\n    Returns\n    -------\n    np.ndarray\n        Priority values for each bin (same shape as ``bins_remain_cap``). The\n        caller should select the bin with the maximum priority.\n    \"\"\"\n    # ---- Update global statistics with the current item ----\n    _update_item_stats(item)\n\n    # Adaptive tolerance for exact fit detection\n    std = _current_std()\n    eps = max(1e-9, _epsilon_factor * std)\n\n    # Remaining capacity after (theoretically) placing the item\n    leftover = bins_remain_cap - item\n\n    # Feasibility mask (allow a tiny negative due to eps)\n    feasible = leftover >= -eps\n\n    # Initialise all priorities to -inf (infeasible bins)\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    if not np.any(feasible):\n        # No feasible bin \u2013 return all -inf\n        return priorities\n\n    # Exact\u2011fit mask (within tolerance)\n    exact_fit = feasible & (np.abs(leftover) <= eps)\n\n    # ---- Exact fits: assign a huge offset plus a tiny random tie\u2011breaker ----\n    if np.any(exact_fit):\n        tie_noise = _rng.uniform(0.0, 1e-6, size=exact_fit.sum())\n        priorities[exact_fit] = _exact_fit_offset + tie_noise\n\n    # ---- Non\u2011exact feasible bins ----\n    non_exact = feasible & ~exact_fit\n    if np.any(non_exact):\n        # Primary component: negative leftover (smaller leftover \u2192 larger priority)\n        primary = -leftover[non_exact]\n\n        # Swap\u2011improvement boost:\n        # leftover close to the average item size is considered promising.\n        mean_size = _ema_item_size if _item_count > 0 else 1.0\n        distance_to_mean = np.abs(leftover[non_exact] - mean_size)\n        boost = _swap_weight / (distance_to_mean + eps)\n\n        # Combine components\n        combined = primary + boost\n\n        # Random tie\u2011breaker (very small)\n        combined += _rng.uniform(0.0, 1e-6, size=combined.shape)\n\n        priorities[non_exact] = combined\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# Global random generator for optional tie\u2011breaking noise\n_rng = np.random.default_rng()\n\n# ---------- Minimal EMA for item\u2011size statistics ----------\n_alpha_ema = 0.1          # smoothing factor\n_ema_item_size = 0.0      # exponential moving average of item size\n_ema_item_sq = 0.0        # exponential moving average of squared size\n_item_count = 0           # number of observed items\n\n\n    \"\"\"Update EMA statistics with a newly observed item.\"\"\"\n    global _ema_item_size, _ema_item_sq, _item_count\n    _item_count += 1\n    _ema_item_size = (1 - _alpha_ema) * _ema_item_size + _alpha_ema * item\n    _ema_item_sq = (1 - _alpha_ema) * _ema_item_sq + _alpha_ema * (item * item)\n\n\n    \"\"\"Return the current estimated standard deviation of item sizes.\"\"\"\n    var = _ema_item_sq - _ema_item_size ** 2\n    var = max(var, 0.0)          # guard against tiny negative due to rounding\n    return np.sqrt(var)\n\n\n    \"\"\"\n    Compute a small tolerance for feasibility checks.\n    The tolerance grows with the observed variability of item sizes.\n    \"\"\"\n    std = _current_std()\n    # 1\u202f% of the standard deviation (or a tiny absolute floor) is used as epsilon\n    return max(1e-9, 0.01 * std)\n\n\n    item: float,\n    bins_remain_cap: np.ndarray,\n    *,\n    temperature: float = 0.1,\n    random_noise: bool = False,\n    noise_scale: float = 1e-12,\n) -> np.ndarray:\n    \"\"\"\n    Softmax\u2011based priority for online Bin Packing.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities of the currently open bins.\n    temperature : float, optional (default=0.1)\n        Controls the sharpness of the softmax.  Lower values make the decision\n        more deterministic (approaching a greedy Best\u2011Fit), while higher values\n        spread the probability mass more uniformly.\n    random_noise : bool, optional (default=False)\n        If True, a tiny uniform noise (\u00b1``noise_scale``) is added to the logits\n        of feasible bins to break ties stochastically.\n    noise_scale : float, optional (default=1e-12)\n        Amplitude of the optional tie\u2011breaking noise.\n\n    Returns\n    -------\n    np.ndarray\n        Priority scores for each bin (same shape as ``bins_remain_cap``).  Infeasible\n        bins receive a priority of 0, and the scores of feasible bins sum to 1.\n    \"\"\"\n    if temperature <= 0.0:\n        raise ValueError(\"temperature must be a positive float\")\n\n    # Update EMA statistics (used only for a tiny feasibility tolerance)\n    _update_item_stats(item)\n\n    # Convert input to a float64 NumPy array for stable arithmetic\n    caps = np.asarray(bins_remain_cap, dtype=np.float64)\n\n    # Feasibility mask with a small tolerance to guard against floating\u2011point errors\n    eps = _tolerance()\n    feasible = caps >= item - eps\n\n    # Initialise priority vector (zeros for infeasible bins)\n    priorities = np.zeros_like(caps, dtype=np.float64)\n\n    if not np.any(feasible):\n        # No bin can accommodate the item \u2013 return the zero vector\n        return priorities\n\n    # Slack (unused capacity) after placing the item in each feasible bin\n    slack = caps[feasible] - item  # non\u2011negative by construction\n\n    # Logits: tighter fit (smaller slack) yields a larger logit\n    logits = np.full_like(caps, -np.inf, dtype=np.float64)\n    logits[feasible] = -slack / temperature\n\n    # Optional tiny noise for stochastic tie\u2011breaking\n    if random_noise:\n        noise = _rng.uniform(-noise_scale, noise_scale, size=slack.shape)\n        logits[feasible] += noise\n\n    # Numerically stable softmax\n    max_logit = np.max(logits[feasible])          # safe: feasible is non\u2011empty\n    shifted = logits - max_logit                  # -inf stays -inf for infeasible bins\n    exp_shifted = np.exp(shifted)                 # exp(-inf) = 0\n    sum_exp = np.sum(exp_shifted)                # >0 because at least one entry is 1\n    priorities = exp_shifted / sum_exp\n\n    return priorities\n\n[Reflection]\nUse softmax with temperature, add tiny tie\u2011breaking noise, set adaptive epsilon, avoid huge constants, and normalize all priorities.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}