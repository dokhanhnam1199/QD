{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\n# Priority: inverse leftover + variance\u2011scaled jitter + tiny index bias\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse leftover, jitter scaled with remaining variance, and tiny index bias.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-12\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return priority\n    base = 1.0 / (remaining[feasible] + eps)\n    bias = -np.arange(n)[feasible] * 1e-6\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining.shape)[feasible] - 0.5) * jitter_scale\n    priority[feasible] = base + bias + jitter\n    return priority\n\n[Heuristics 2nd]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\n# Priority: inverse leftover + variance\u2011scaled jitter + tiny index bias\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse leftover, jitter scaled with remaining variance, and tiny index bias.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-12\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return priority\n    base = 1.0 / (remaining[feasible] + eps)\n    bias = -np.arange(n)[feasible] * 1e-6\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining.shape)[feasible] - 0.5) * jitter_scale\n    priority[feasible] = base + bias + jitter\n    return priority\n\n[Heuristics 3rd]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\n# Priority: inverse leftover + variance\u2011scaled jitter + tiny index bias\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse leftover, jitter scaled with remaining variance, and tiny index bias.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-12\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return priority\n    base = 1.0 / (remaining[feasible] + eps)\n    bias = -np.arange(n)[feasible] * 1e-6\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining.shape)[feasible] - 0.5) * jitter_scale\n    priority[feasible] = base + bias + jitter\n    return priority\n\n[Heuristics 4th]\nimport numpy as np\n\n_item_avg = None\n_item_avg_count = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best\u2011fit priority: inverse leftover with tightness, variance\u2011scaled jitter, adaptive to item avg, and index bias.\"\"\"\n    n = bins_remain_cap.size\n    priority = np.full(n, -np.inf, dtype=float)\n    if n == 0:\n        return priority\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    if not np.any(feasible):\n        return priority\n    global _item_avg, _item_avg_count\n    if _item_avg is None:\n        _item_avg = item\n        _item_avg_count = 1\n    else:\n        _item_avg += (item - _item_avg) / (_item_avg_count + 1)\n        _item_avg_count += 1\n    eps = 1e-12\n    rem = remaining[feasible]\n    base = 1.0 / (rem + eps)\n    tight = item / (item + rem + eps)\n    base *= tight\n    var_rem = np.var(rem) if rem.size > 1 else 0.0\n    factor = abs(item - _item_avg) / (_item_avg + eps)\n    jitter_scale = item * 0.005 * (1.0 + np.sqrt(var_rem)) * (1.0 + factor)\n    rng = np.random.default_rng()\n    jitter = (rng.random(rem.size) - 0.5) * jitter_scale\n    idx_bias = np.arange(n)[feasible] * 1e-6\n    priority[feasible] = base + jitter + idx_bias\n    return priority\n\n[Heuristics 5th]\nimport numpy as np\n\n_item_avg = None\n_item_avg_count = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best\u2011fit priority: inverse leftover with tightness, variance\u2011scaled jitter, adaptive to item avg, and index bias.\"\"\"\n    n = bins_remain_cap.size\n    priority = np.full(n, -np.inf, dtype=float)\n    if n == 0:\n        return priority\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    if not np.any(feasible):\n        return priority\n    global _item_avg, _item_avg_count\n    if _item_avg is None:\n        _item_avg = item\n        _item_avg_count = 1\n    else:\n        _item_avg += (item - _item_avg) / (_item_avg_count + 1)\n        _item_avg_count += 1\n    eps = 1e-12\n    rem = remaining[feasible]\n    base = 1.0 / (rem + eps)\n    tight = item / (item + rem + eps)\n    base *= tight\n    var_rem = np.var(rem) if rem.size > 1 else 0.0\n    factor = abs(item - _item_avg) / (_item_avg + eps)\n    jitter_scale = item * 0.005 * (1.0 + np.sqrt(var_rem)) * (1.0 + factor)\n    rng = np.random.default_rng()\n    jitter = (rng.random(rem.size) - 0.5) * jitter_scale\n    idx_bias = np.arange(n)[feasible] * 1e-6\n    priority[feasible] = base + jitter + idx_bias\n    return priority\n\n[Heuristics 6th]\nimport numpy as np\n\n_item_avg = None\n_item_count = 0\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins by inverse leftover, tightness factor, variance\u2011scaled jitter, tiny index bias.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return priority\n    # Update running average of seen items (light adaptivity)\n    global _item_avg, _item_count\n    if _item_avg is None:\n        _item_avg = item\n        _item_count = 1\n    else:\n        _item_count += 1\n        _item_avg += (item - _item_avg) / _item_count\n    eps = 1e-12\n    rem = remaining[feasible]\n    # Base: inverse leftover weighted by tightness (prefers tighter fit)\n    base = 1.0 / (rem + eps)\n    tight = item / (item + rem + eps)\n    base *= tight\n    # Jitter magnitude grows with variance of remaining capacities and item deviation from average\n    var_rem = np.var(rem) if rem.size > 1 else 0.0\n    jitter_coeff = 0.005 * (1.0 + np.sqrt(var_rem))\n    jitter_adj = 1.0 + abs(item - _item_avg) / (_item_avg + eps)\n    jitter_scale = item * jitter_coeff * jitter_adj\n    rng = np.random.default_rng()\n    jitter = (rng.random(rem.size) - 0.5) * jitter_scale\n    # Tiny index bias for deterministic tie\u2011breaks\n    idx_bias = np.arange(n)[feasible] * 1e-6\n    priority[feasible] = base + jitter + idx_bias\n    return priority\n\n[Heuristics 7th]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Inverse leftover priority with variance\u2011scaled jitter and tiny variance penalty.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return priority\n    eps = 1e-12\n    base = 1.0 / (remaining[feasible] + eps)\n    bias = -np.arange(n)[feasible] * 1e-6\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining[feasible].size) - 0.5) * jitter_scale\n    var_penalty = -((remaining[feasible] - remaining[feasible].mean())**2) * 1e-6\n    priority[feasible] = base + bias + jitter + var_penalty\n    return priority\n\n[Heuristics 8th]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Inverse leftover priority with variance\u2011scaled jitter and tiny variance penalty.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return priority\n    eps = 1e-12\n    base = 1.0 / (remaining[feasible] + eps)\n    bias = -np.arange(n)[feasible] * 1e-6\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining[feasible].size) - 0.5) * jitter_scale\n    var_penalty = -((remaining[feasible] - remaining[feasible].mean())**2) * 1e-6\n    priority[feasible] = base + bias + jitter + var_penalty\n    return priority\n\n[Heuristics 9th]\nimport numpy as np\n\n# Priority function for online bin packing: best-fit with small index bias, golden\u2011ratio deterministic jitter, and variance\u2011scaled random jitter.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Best\u2011fit priority: inverse leftover + tiny index bias + deterministic golden\u2011ratio jitter + variance\u2011scaled random jitter.\n    \"\"\"\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins - item\n    feasible = remaining >= 0\n    score = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return score\n    eps = 1e-9\n    idx = np.arange(n, dtype=float)\n    inv_leftover = 1.0 / (remaining[feasible] + eps)\n    bias = -idx[feasible] * 1e-6\n    phi = (1 + np.sqrt(5)) / 2\n    phi_jitter = ((phi * (idx[feasible] + 1) * item) % 1.0) * 1e-6 * item\n    std = np.std(remaining[feasible])\n    mean = np.mean(remaining[feasible]) + eps\n    coeff = std / mean\n    rng = np.random.default_rng()\n    rand_jitter = rng.random(feasible.sum()) * item * 0.01 * (1.0 + coeff)\n    score[feasible] = inv_leftover + bias + phi_jitter + rand_jitter\n    return score\n\n[Heuristics 10th]\nimport numpy as np\n\n# Priority function for online bin packing: best-fit with small index bias, golden\u2011ratio deterministic jitter, and variance\u2011scaled random jitter.\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Best\u2011fit priority: inverse leftover + tiny index bias + deterministic golden\u2011ratio jitter + variance\u2011scaled random jitter.\n    \"\"\"\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins - item\n    feasible = remaining >= 0\n    score = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return score\n    eps = 1e-9\n    idx = np.arange(n, dtype=float)\n    inv_leftover = 1.0 / (remaining[feasible] + eps)\n    bias = -idx[feasible] * 1e-6\n    phi = (1 + np.sqrt(5)) / 2\n    phi_jitter = ((phi * (idx[feasible] + 1) * item) % 1.0) * 1e-6 * item\n    std = np.std(remaining[feasible])\n    mean = np.mean(remaining[feasible]) + eps\n    coeff = std / mean\n    rng = np.random.default_rng()\n    rand_jitter = rng.random(feasible.sum()) * item * 0.01 * (1.0 + coeff)\n    score[feasible] = inv_leftover + bias + phi_jitter + rand_jitter\n    return score\n\n[Heuristics 11th]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combine inverse leftover, load\u2011balance, variance\u2011scaled jitter, tiny bias.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-12\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return priority\n    # Primary best\u2011fit score: higher for smaller leftover space\n    base = 1.0 / (remaining[feasible] + eps)\n    # Secondary term: prefers bins that become fuller after placement\n    load_balance = item / (item + remaining[feasible] + eps)\n    # Jitter scaled by variance of remaining capacities among feasible bins\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.005 * (1.0 + np.sqrt(var_rem))\n    jitter = (_rng.random(remaining.shape) - 0.5) * jitter_scale\n    # Tiny deterministic tie\u2011breaker (lower index slightly favored)\n    bias = -np.arange(n)[feasible] * 1e-6\n    priority[feasible] = base + load_balance + jitter + bias\n    return priority\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Score bins by inverse leftover + tightness, adding variance\u2011scaled jitter,\n    golden\u2011ratio deterministic jitter, and tiny index bias. Infeasible bins get -inf.\n    \"\"\"\n    # Ensure array type and get bin count\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n    # Remaining capacity after placing the item\n    remaining = bins - item\n    feasible = remaining >= 0\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n\n    eps = 1e-12\n    idx = np.arange(n, dtype=float)\n    rem_feas = remaining[feasible]\n\n    # Primary inverse\u2011leftover (best\u2011fit core)\n    inv_leftover = 1.0 / (rem_feas + eps)\n\n    # Secondary tightness term encourages higher fill\n    tightness = item / (item + rem_feas + eps)\n\n    # Deterministic golden\u2011ratio jitter (tiny perturbation)\n    phi = (1 + np.sqrt(5.0)) / 2.0\n    phi_jitter = ((phi * (idx[feasible] + 1.0) * item) % 1.0) * 1e-6 * item\n\n    # Tiny index bias to keep deterministic order on ties\n    bias = -idx[feasible] * 1e-6\n\n    # Random jitter scaled by coefficient of variation of remaining capacities\n    mean = np.mean(rem_feas)\n    std = np.std(rem_feas)\n    coeff = std / (mean + eps)\n    jitter_weight = 0.01 * (1.0 + coeff)\n    rng = np.random.default_rng()\n    rand_jitter = rng.random(rem_feas.shape) * jitter_weight * item\n\n    # Combine components\n    scores[feasible] = inv_leftover + tightness + phi_jitter + rand_jitter + bias\n    return scores\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Score bins by inverse leftover + tightness, adding variance\u2011scaled jitter,\n    golden\u2011ratio deterministic jitter, and tiny index bias. Infeasible bins get -inf.\n    \"\"\"\n    # Ensure array type and get bin count\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n    # Remaining capacity after placing the item\n    remaining = bins - item\n    feasible = remaining >= 0\n    scores = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return scores\n\n    eps = 1e-12\n    idx = np.arange(n, dtype=float)\n    rem_feas = remaining[feasible]\n\n    # Primary inverse\u2011leftover (best\u2011fit core)\n    inv_leftover = 1.0 / (rem_feas + eps)\n\n    # Secondary tightness term encourages higher fill\n    tightness = item / (item + rem_feas + eps)\n\n    # Deterministic golden\u2011ratio jitter (tiny perturbation)\n    phi = (1 + np.sqrt(5.0)) / 2.0\n    phi_jitter = ((phi * (idx[feasible] + 1.0) * item) % 1.0) * 1e-6 * item\n\n    # Tiny index bias to keep deterministic order on ties\n    bias = -idx[feasible] * 1e-6\n\n    # Random jitter scaled by coefficient of variation of remaining capacities\n    mean = np.mean(rem_feas)\n    std = np.std(rem_feas)\n    coeff = std / (mean + eps)\n    jitter_weight = 0.01 * (1.0 + coeff)\n    rng = np.random.default_rng()\n    rand_jitter = rng.random(rem_feas.shape) * jitter_weight * item\n\n    # Combine components\n    scores[feasible] = inv_leftover + tightness + phi_jitter + rand_jitter + bias\n    return scores\n\n[Heuristics 14th]\nimport numpy as np\n\n_rng = np.random.default_rng()\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Score bins using inverse leftover, tightness, variance\u2011scaled jitter, and tiny tie\u2011breakers.\"\"\"\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-9\n    remaining = bins - item\n    feasible = remaining >= 0\n    scores = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return scores\n    rem_f = remaining[feasible]\n    idx = np.arange(n, dtype=float)\n    idx_f = idx[feasible]\n    inv_leftover = 1.0 / (rem_f + eps)\n    tightness = item / (item + rem_f + eps)\n    bias = -idx_f * 1e-6\n    phi = (1 + np.sqrt(5.0)) / 2.0\n    phi_jitter = ((phi * (idx_f + 1) * item) % 1.0) * 1e-6 * item\n    mean_rem = np.mean(rem_f)\n    std_rem = np.std(rem_f)\n    coeff = std_rem / (mean_rem + eps)\n    jitter = _rng.random(rem_f.shape) * item * (0.01 + coeff * 0.01)\n    scores[feasible] = inv_leftover + tightness + bias + phi_jitter + jitter\n    return scores\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                eps: float = 2.7571374224357383e-11,\n                feasibility_threshold: float = -0.0019041439037806845,\n                weight_base: float = 0.17556819320803907,\n                weight_tight: float = 0.9031398098118371,\n                jitter_scale_factor: float = 0.001284745224474304,\n                bias_factor: float = 3.3289346372599506e-05,\n                jitter_center_offset: float = 0.42644513175496623) -> np.ndarray:\n    \"\"\"Inverse leftover with variance\u2011scaled jitter and adaptive item\u2011size bias for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                eps: float = 2.7571374224357383e-11,\n                feasibility_threshold: float = -0.0019041439037806845,\n                weight_base: float = 0.17556819320803907,\n                weight_tight: float = 0.9031398098118371,\n                jitter_scale_factor: float = 0.001284745224474304,\n                bias_factor: float = 3.3289346372599506e-05,\n                jitter_center_offset: float = 0.42644513175496623) -> np.ndarray:\n    \"\"\"Inverse leftover with variance\u2011scaled jitter and adaptive item\u2011size bias for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                eps: float = 2.7571374224357383e-11,\n                feasibility_threshold: float = -0.0019041439037806845,\n                weight_base: float = 0.17556819320803907,\n                weight_tight: float = 0.9031398098118371,\n                jitter_scale_factor: float = 0.001284745224474304,\n                bias_factor: float = 3.3289346372599506e-05,\n                jitter_center_offset: float = 0.42644513175496623) -> np.ndarray:\n    \"\"\"Inverse leftover with variance\u2011scaled jitter and adaptive item\u2011size bias for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                eps: float = 2.7571374224357383e-11,\n                feasibility_threshold: float = -0.0019041439037806845,\n                weight_base: float = 0.17556819320803907,\n                weight_tight: float = 0.9031398098118371,\n                jitter_scale_factor: float = 0.001284745224474304,\n                bias_factor: float = 3.3289346372599506e-05,\n                jitter_center_offset: float = 0.42644513175496623) -> np.ndarray:\n    \"\"\"Inverse leftover with variance\u2011scaled jitter and adaptive item\u2011size bias for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                eps: float = 2.7571374224357383e-11,\n                feasibility_threshold: float = -0.0019041439037806845,\n                weight_base: float = 0.17556819320803907,\n                weight_tight: float = 0.9031398098118371,\n                jitter_scale_factor: float = 0.001284745224474304,\n                bias_factor: float = 3.3289346372599506e-05,\n                jitter_center_offset: float = 0.42644513175496623) -> np.ndarray:\n    \"\"\"Inverse leftover with variance\u2011scaled jitter and adaptive item\u2011size bias for online bin packing.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Best\u2011fit with variance\u2011scaled jitter and tiny index bias for tie\u2011breaking.\"\"\"\n    rng = np.random.default_rng()\n    eps = 1e-12\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n\n    # remaining capacity after placing the item\n    remaining = bins - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not feasible.any():\n        return priority\n\n    # primary: inverse leftover (tighter fit gets higher score)\n    base = 1.0 / (remaining[feasible] + eps)\n\n    # deterministic tie\u2011breaker: tiny bias toward earlier bins\n    bias = -np.arange(n)[feasible] * 1e-6\n\n    # random jitter scaled by item size and variance of feasible leftovers\n    var_rem = np.var(remaining[feasible])\n    jitter_scale = item * 0.01 * (1.0 + np.sqrt(var_rem))\n    jitter = (rng.random(remaining.shape)[feasible] - 0.5) * jitter_scale\n\n    # mild penalty for bins far from the mean remaining capacity\n    mean_rem = np.mean(remaining[0] if False else remaining[feasible])  # placeholder for readability\n    penalty = -((remaining[feasible] - mean_rem) ** 2) * 1e-5\n\n    # combine all components\n    priority[feasible] = base + bias + jitter + penalty\n    return priority\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}