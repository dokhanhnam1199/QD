{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n# deterministic RNG for reproducible jitter\n_rng = np.random.default_rng(42)\n\n    \"\"\"Score bins: inverse leftover, tiny index bias, and scaled random jitter.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-9\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    priority[feasible] = 1.0 / (remaining[feasible] + eps)\n    # deterministic tie\u2011breaker: prefer lower index\n    priority[feasible] += -np.arange(n)[feasible] * 1e-5\n    # tiny reproducible jitter proportional to item size\n    jitter = _rng.random(n) * 0.01 * item\n    priority[feasible] += jitter[feasible]\n    return priority\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Priority: inverse leftover + tiny index bias + deterministic jitter + variance penalty.\"\"\"\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return priority\n    base = 1.0 / remaining[feasible]\n    idx_bias = np.arange(n)[feasible] * 1e-6\n    std_rem = np.std(remaining[feasible])\n    jitter_scale = item * 0.001 * (1 + std_rem / 10)\n    jitter = np.sin(item + np.arange(n)[feasible]) * jitter_scale\n    mean_rem = remaining[feasible].mean()\n    var_penalty = - ((remaining[feasible] - mean_rem) ** 2) * 1e-4\n    priority[feasible] = base + idx_bias + jitter + var_penalty\n    return priority\n\n### Analyze & experience\n- Comparing the 1st vs 20th, the top heuristic uses a variance\u2011based weight for adaptive jitter, scales random perturbations by item size, and adds a tiny index bias, whereas the bottom one relies on fixed sinusoidal/phi jitter and load\u2011aware randomness without variance scaling, leading to excessive randomness and poorer feasibility handling.  \nThe 2nd vs 19th pair shows a similar trend: the 2nd again uses std/mean for jitter weighting, while the 19th\u2019s load\u2011factor weighting can dampen exploration when load is low, reducing robustness.  \nIn the 3rd vs 18th comparison, the 3rd employs a deterministic RNG with reproducible jitter and a simple index bias, whereas the 18th adds a variance penalty and normal jitter but offers weaker bias and over\u2011penalizes moderate leftovers.  \nFor the 4th vs 17th pair, the 4th repeats the adaptive scheme of the 1st with a fresh RNG each call, balancing exploration; the 17th reverts to the deterministic jitter of the 3rd, lacking adaptive scaling and thus under\u2011exploring heterogeneous loads.  \nThe 5th vs 16th and 6th vs 15th pairs are identical to the 3rd, showing that deterministic jitter alone does not match the adaptive strategy.  \nThe 7th vs 14th pair illustrates that blending deterministic phi jitter with normal jitter scaled by load (7th) outperforms the 14th\u2019s variance penalty and sine jitter, as the former adapts better to load distribution.  \nComparing 8th vs 13th, 8th mirrors 7th\u2019s approach while 13th uses mean\u2011tracking item averages and CV\u2011based noise; the latter can be more robust but incurs higher computational cost.  \nIn the 9th vs 12th pair, 9th mixes sine, phi, and random jitter weighted by load, whereas 12th employs adaptive noise based on global mean and CV; the former is simpler but may mis\u2011rank under skewed loads.  \nFinally, the 10th vs 11th comparison shows that 11th\u2019s variance\u2011scaled weighting strategy is a balanced compromise similar to the 1st, whereas the 10th duplicates 9th\u2019s simpler scheme.  \nOverall, the top heuristics excel by: (1) filtering feasible bins early, (2) weighting deterministic and random components by the variance of remaining capacities, (3) scaling jitter to item size and load distribution, (4) providing deterministic tie\u2011breakers, and (5) guarding against division by zero with an epsilon.\n- \n- **Keywords**: feasibility, adaptive jitter, load\u2011aware scoring, vectorized, stochastic exploration.  \n- **Advice**: Use capacity\u2011aware scores with adaptive random perturbations scaled to item size and bin load; add feasibility penalties and dynamic tie\u2011breakers; keep implementation vectorized and clearly documented.  \n- **Avoid**: Fixed RNG seeds, deterministic index bias, static jitter, sole reliance on inverse\u2011leftover, redundant imports, dead\u2011code paths.  \n- **Explanation**: Adaptive stochastic components boost exploration while preserving reproducibility; feasibility filters enforce correctness; vectorization ensures speed; removing deterministic shortcuts prevents systematic bias.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}