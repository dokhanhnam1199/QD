{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Bin priority via inverse leftover, index bias, golden\u2011ratio jitter, and variance\u2011scaled random jitter.\"\"\"\n    bins = np.asarray(bins_remain_cap, dtype=float)\n    n = bins.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    remaining = bins - item\n    feasible = remaining >= 0\n    eps = 1e-9\n    idx = np.arange(n, dtype=float)\n    score = np.full(n, -np.inf, dtype=float)\n    if feasible.any():\n        inv_leftover = 1.0 / (remaining[feasible] + eps)\n        bias = -idx[feasible] * 1e-6\n        phi = (1 + np.sqrt(5)) / 2\n        phi_jitter = ((phi * (idx[feasible] + 1) * item) % 1.0) * 1e-6 * item\n        std = np.std(remaining[feasible])\n        mean = np.mean(remaining[feasible])\n        coeff = std / (mean + eps)\n        random_weight = 0.01 * (1.0 + coeff)\n        rng = np.random.default_rng()\n        rand_jitter = rng.random(feasible.sum()) * random_weight * item\n        score[feasible] = inv_leftover + bias + phi_jitter + rand_jitter\n    return score\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n_item_avg = None\n_item_avg_count = 0\n\n    \"\"\"Combines best-fit base with variance\u2011scaled random jitter and deterministic tie\u2011breakers.\"\"\"\n    global _item_avg, _item_avg_count\n    n = bins_remain_cap.size\n    if n == 0:\n        return np.empty(0, dtype=float)\n    eps = 1e-12\n    remaining = bins_remain_cap - item\n    feasible = remaining >= 0\n    priority = np.full(n, -np.inf, dtype=float)\n    if not np.any(feasible):\n        return priority\n    if _item_avg is None:\n        _item_avg = item\n        _item_avg_count = 1\n    else:\n        _item_avg += (item - _item_avg) / (_item_avg_count + 1)\n        _item_avg_count += 1\n    feas_rem = remaining[feasible]\n    mean_rem = feas_rem.mean()\n    std_rem = feas_rem.std()\n    cv = std_rem / (mean_rem + eps)\n    base = 1.0 / (feas_rem + eps)\n    bias = -np.arange(n)[feasible] * 1e-5\n    phi = 0.6180339887498949\n    det_jitter = (np.mod(phi * (np.arange(n)[feasible] + 1) * item, 1.0) - 0.5) * 1e-5 * item\n    sin_jitter = np.sin(item * np.arange(n)[feasible] + 1.0) * 1e-9 * item\n    rng = np.random.default_rng()\n    noise_sigma = item * np.exp(-cv)\n    factor = abs(item - _item_avg) / (_item_avg + eps)\n    noise_sigma *= (1 + factor)\n    rng_jitter = rng.standard_normal(feasible.sum()) * noise_sigma\n    priority[feasible] = base + bias + det_jitter + sin_jitter + rng_jitter\n    return priority\n\n### Analyze & experience\n- - **(1st\u202fvs\u202f20th)** \u2013\u202fHeuristic\u202f#1 has a concise docstring *\u201cScore bins by inverse leftover, variance\u2011scaled jitter, and tiny index bias.\u201d* It computes a simple inverse\u2011leftover score, adds a minimal variance\u2011scaled jitter, and a negligible index bias. Heuristic\u202f#20\u2019s docstring *\u201cCombines best\u2011fit base with variance\u2011scaled random jitter and deterministic tie\u2011breakers.\u201d* adds many components: deterministic golden\u2011ratio and sine jitter, adaptive random jitter based on coefficient of variation and a running item average, a larger index bias, and several hyper\u2011parameters. The extra randomness and tie\u2011break logic increase overhead and destabilise the core best\u2011fit signal, leading to poorer performance.\n\n- **(2nd\u202fvs\u202f19th)** \u2013\u202f#2 is identical to #1 (same docstring, same tiny jitter). #19 (same as #16\u201119) retains the inverse\u2011leftover, but inflates the index bias (\u20111e\u20115) and adds a variance\u2011penalty term `\u2011((rem\u2011mean)**2)*1e\u20114` plus jitter scaled by `item*(0.01+std*0.001)`. The larger bias and penalty introduce unnecessary noise, degrading solution quality.\n\n- **(3rd\u202fvs\u202f18th)** \u2013\u202f#3 mirrors #1. #18 is a duplicate of #19, so the same observations apply: added variance\u2011penalty and stronger jitter outweigh the benefits of the basic best\u2011fit metric.\n\n- **(4th\u202fvs\u202f17th)** \u2013\u202f#4 = #1. #17 repeats the pattern of #19/18, confirming that extra stochastic terms consistently push the heuristic down the ranking.\n\n- **(5th\u202fvs\u202f16th)** \u2013\u202f#5 = #1. #16 introduces the same variance\u2011scaled jitter and variance\u2011penalty as #19\u201118, showing that even modest increases in jitter amplitude and penalty harm robustness.\n\n- **(6th\u202fvs\u202f15th)** \u2013\u202f#6 adds a *tightness factor* (`item/(item+rem)`) to the inverse leftover and keeps a small jitter (`item`\u00b70.01\u00b7(1+\u221avar)). It still uses a tiny index bias. #15 (same as #13\u201115) replaces the inverse\u2011leftover base with a negative leftover, adds a Gaussian adaptive jitter based on the running item average, and imposes a variance penalty. The shift from a positive fit score to a negative leftover plus heavy stochasticity reduces predictability.\n\n- **(7th\u202fvs\u202f14th)** \u2013\u202f#7 equals #6. #14 (duplicate of #13\u201115) shares the adaptive Gaussian jitter and variance penalty, confirming that the extra adaptive noise makes the heuristic less effective than the simpler tightness\u2011augmented version.\n\n- **(8th\u202fvs\u202f13th)** \u2013\u202f#8\u2019s docstring notes *\u201cinverse leftover, index bias, golden\u2011ratio jitter, and variance\u2011scaled random jitter.\u201d* It blends deterministic phi\u2011jitter (tiny) with a random term scaled by the coefficient of variation. #13 uses a *negative leftover* base, a variance penalty, an index bias, and Gaussian jitter whose sigma adapts to the coefficient of variation and to deviation from a running item average. The richer adaptation adds complexity without improving fit quality.\n\n- **(9th\u202fvs\u202f12th)** \u2013\u202f#9 repeats #8. #12 is identical to #11, which adds a small variance penalty and adaptive random jitter on top of #8\u2019s components. The extra penalty and adaptive term again increase randomness and hurt consistency.\n\n- **(10th\u202fvs\u202f11th)** \u2013\u202f#10 (same as #8\u20119) combines inverse leftover, bias, deterministic phi jitter, and a coefficient\u2011of\u2011variation\u2011scaled random jitter. #11 adds a variance penalty and a more elaborate adaptive jitter (using the coefficient of variation and item\u2011average). The additional penalty and jitter disturb the clean best\u2011fit ordering, resulting in lower ranking.\n\nOverall, the progression shows that **simplicity (pure inverse leftover with minimal jitter) consistently outperforms increasingly elaborate stochastic tie\u2011breakers and variance penalties**.\n- \n- **Keywords** \u2013 adaptive jitter, feasibility filter, multi\u2011score, stochastic tie\u2011breaker, capacity awareness.  \n- **Advice** \u2013 use a primary inverse\u2011leftover score, augment with variance\u2011scaled random jitter proportional to item size and current bin load, and a secondary load\u2011balancing term; sample a small set of top candidates stochastically before deterministic fallback.  \n- **Avoid** \u2013 fixed RNG seeds, sole index bias, only deterministic tie\u2011breakers, static jitter magnitude, hard\u2011coded penalties, over\u2011engineered deterministic perturbations.  \n- **Explanation** \u2013 adaptive randomness improves exploration across instances while preserving speed; multi\u2011criteria scoring balances feasibility and load, and safeguards against pathological ties without sacrificing reproducibility.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}