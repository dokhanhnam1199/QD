**Analysis:**  

- **(1st vs 20th)** – Heuristic #1 has a concise docstring *“Score bins by inverse leftover, variance‑scaled jitter, and tiny index bias.”* It computes a simple inverse‑leftover score, adds a minimal variance‑scaled jitter, and a negligible index bias. Heuristic #20’s docstring *“Combines best‑fit base with variance‑scaled random jitter and deterministic tie‑breakers.”* adds many components: deterministic golden‑ratio and sine jitter, adaptive random jitter based on coefficient of variation and a running item average, a larger index bias, and several hyper‑parameters. The extra randomness and tie‑break logic increase overhead and destabilise the core best‑fit signal, leading to poorer performance.

- **(2nd vs 19th)** – #2 is identical to #1 (same docstring, same tiny jitter). #19 (same as #16‑19) retains the inverse‑leftover, but inflates the index bias (‑1e‑5) and adds a variance‑penalty term `‑((rem‑mean)**2)*1e‑4` plus jitter scaled by `item*(0.01+std*0.001)`. The larger bias and penalty introduce unnecessary noise, degrading solution quality.

- **(3rd vs 18th)** – #3 mirrors #1. #18 is a duplicate of #19, so the same observations apply: added variance‑penalty and stronger jitter outweigh the benefits of the basic best‑fit metric.

- **(4th vs 17th)** – #4 = #1. #17 repeats the pattern of #19/18, confirming that extra stochastic terms consistently push the heuristic down the ranking.

- **(5th vs 16th)** – #5 = #1. #16 introduces the same variance‑scaled jitter and variance‑penalty as #19‑18, showing that even modest increases in jitter amplitude and penalty harm robustness.

- **(6th vs 15th)** – #6 adds a *tightness factor* (`item/(item+rem)`) to the inverse leftover and keeps a small jitter (`item`·0.01·(1+√var)). It still uses a tiny index bias. #15 (same as #13‑15) replaces the inverse‑leftover base with a negative leftover, adds a Gaussian adaptive jitter based on the running item average, and imposes a variance penalty. The shift from a positive fit score to a negative leftover plus heavy stochasticity reduces predictability.

- **(7th vs 14th)** – #7 equals #6. #14 (duplicate of #13‑15) shares the adaptive Gaussian jitter and variance penalty, confirming that the extra adaptive noise makes the heuristic less effective than the simpler tightness‑augmented version.

- **(8th vs 13th)** – #8’s docstring notes *“inverse leftover, index bias, golden‑ratio jitter, and variance‑scaled random jitter.”* It blends deterministic phi‑jitter (tiny) with a random term scaled by the coefficient of variation. #13 uses a *negative leftover* base, a variance penalty, an index bias, and Gaussian jitter whose sigma adapts to the coefficient of variation and to deviation from a running item average. The richer adaptation adds complexity without improving fit quality.

- **(9th vs 12th)** – #9 repeats #8. #12 is identical to #11, which adds a small variance penalty and adaptive random jitter on top of #8’s components. The extra penalty and adaptive term again increase randomness and hurt consistency.

- **(10th vs 11th)** – #10 (same as #8‑9) combines inverse leftover, bias, deterministic phi jitter, and a coefficient‑of‑variation‑scaled random jitter. #11 adds a variance penalty and a more elaborate adaptive jitter (using the coefficient of variation and item‑average). The additional penalty and jitter disturb the clean best‑fit ordering, resulting in lower ranking.

Overall, the progression shows that **simplicity (pure inverse leftover with minimal jitter) consistently outperforms increasingly elaborate stochastic tie‑breakers and variance penalties**.

**Experience:**  
Keep heuristics simple—focus on a strong core metric (inverse leftover), use only tiny, variance‑scaled jitter for tie‑breaking, and avoid over‑engineered deterministic perturbations or heavy penalties. This yields robust, fast, and high‑quality solutions.