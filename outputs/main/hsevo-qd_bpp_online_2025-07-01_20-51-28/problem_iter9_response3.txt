```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    num_bins = len(bins_remain_cap)
    bin_capacity = bins_remain_cap[0] # Assuming all bins have same capacity


    # Heuristic 1: "Near Fit" - Prefer bins where the item fills a significant portion
    near_fit_threshold = 0.9
    near_fit_bonus_base = 10.0

    # Heuristic 2: Avoid Fragmentation - Penalize bins that would become highly fragmented
    fragmentation_penalty_exponent = 2
    small_fragment_threshold = 0.1
    large_fragment_penalty_base = -5.0

    # Heuristic 3: Try to completely fill
    complete_fill_bonus_base = 20.0

    # Heuristic 4: Balance utilization across bins.  Penalize bins that are already very full
    # to promote spreading items across bins, especially when items are small.
    utilization_penalty_exponent = 1 # Higher exponent means stronger penalty for high utilization
    high_utilization_threshold = 0.75 #Consider a bin highly utilized above this value
    high_utilization_penalty_factor = -2.0

    # Heuristic 5: Reward bins that have been empty for a while (delayed first fit)
    # This encourages usage of empty bins and reduces bin count.  This is very effective when items come sorted in decreasing order.
    # This needs to be coupled with good exploration of existing bins though, to avoid wasting space when items become smaller
    empty_bin_bonus_base = 5.0 #Give small boost for packing into completely empty bin. This has to be a small boost
    
    #Adaptive Weights
    near_fit_bonus = near_fit_bonus_base * (1 + (1-item/bin_capacity)) #Scale down when item is small, scale up when item is large
    large_fragment_penalty = large_fragment_penalty_base * (1 + (item / bin_capacity))  #Penalize less when item is already big
    complete_fill_bonus = complete_fill_bonus_base * (1 + (item/bin_capacity)) #scale up when item is large
    

    valid_bins = bins_remain_cap > 0
    if not np.any(valid_bins):
        return priorities

    possible_bins = bins_remain_cap >= item
    
    #Normalize remaining capacity (relative to bin capacity)
    normalized_remain_cap = bins_remain_cap / bin_capacity
    
    for i, remaining_cap in enumerate(bins_remain_cap):
        if remaining_cap < item:
            priorities[i] = -np.inf
            continue

        # Near Fit
        if item / remaining_cap >= near_fit_threshold:
            priorities[i] += near_fit_bonus

        # Complete Fill
        if item == remaining_cap:
            priorities[i] += complete_fill_bonus

        # Fragmentation
        new_remaining = remaining_cap - item
        if new_remaining > 0 and (new_remaining / bin_capacity) < small_fragment_threshold:
            priorities[i] += large_fragment_penalty

        # Utilization Balancing
        utilization = 1 - (remaining_cap / bin_capacity)
        if utilization >= high_utilization_threshold:
             priorities[i] += high_utilization_penalty_factor * (utilization**utilization_penalty_exponent)

        # Empty Bin Bonus (delayed first fit)
        if remaining_cap == bin_capacity:
            priorities[i] += empty_bin_bonus_base


    # If no bins can contain the item (after applying fragmentation penalty), slightly raise score of potential bins for placement
    if not np.any(priorities[possible_bins] > -np.inf):
        priorities[possible_bins] += 0.001

    return priorities
```
