[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: \"Near Fit\" - Prefer bins where the item fills a significant portion\n    #  but doesn't overflow. We'll add a penalty for overflow later.\n    near_fit_threshold = 0.9  # Adjust this to fine-tune \"nearness\"\n    near_fit_bonus = 10.0 # Significant boost for near fits\n\n    # Heuristic 2: Avoid Fragmentation - Penalize bins that would become highly fragmented\n    fragmentation_penalty_exponent = 2 # Higher exponent means stronger penalty for fragmentation\n    small_fragment_threshold = 0.1 # What counts as a small fragment relative to bin size?\n    large_fragment_penalty = -5.0 # Subtract points for producing a very small remaining fragment\n    \n    # Heuristic 3: Try to completely fill\n    complete_fill_bonus = 20.0\n\n    # Handle cases where remaining capacity is zero to avoid division by zero and also not allow packing to filled bins\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n    \n    \n    possible_bins = bins_remain_cap >= item\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        \n        if remaining_cap < item: # Disqualify bins that are too small\n            priorities[i] = -np.inf # Significantly low priority (lower than zero) to disallow item placement\n            continue\n        \n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n        \n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n        \n\n        # Fragmentation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bins_remain_cap[0]) < small_fragment_threshold:  #Compare against a base capacity\n                priorities[i] += large_fragment_penalty # Severe penalty\n\n    # If no bins can contain the item (after applying fragmentation penalty), slightly raise score of potential bins for placement\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.028719585161557,
    "SLOC": 10.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 88.0,
    "mi": 79.86649421043717,
    "token_count": 132.0,
    "exec_success": true
  }
]