[
  {
    "stdout_filepath": "problem_iter0_stdout0.txt",
    "code_path": "problem_iter0_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    ratios = item / bins_remain_cap\n    log_ratios = np.log(ratios)\n    priorities = -log_ratios\n    return priorities",
    "response_id": 0,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "halstead": 11.60964047443681,
    "mi": 94.04446327225541,
    "token_count": 47.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Heuristic 1: \"Near Fit\" - Prefer bins where the item fills a significant portion\n    #  but doesn't overflow. We'll add a penalty for overflow later.\n    near_fit_threshold = 0.9  # Adjust this to fine-tune \"nearness\"\n    near_fit_bonus = 10.0 # Significant boost for near fits\n\n    # Heuristic 2: Avoid Fragmentation - Penalize bins that would become highly fragmented\n    fragmentation_penalty_exponent = 2 # Higher exponent means stronger penalty for fragmentation\n    small_fragment_threshold = 0.1 # What counts as a small fragment relative to bin size?\n    large_fragment_penalty = -5.0 # Subtract points for producing a very small remaining fragment\n    \n    # Heuristic 3: Try to completely fill\n    complete_fill_bonus = 20.0\n\n    # Handle cases where remaining capacity is zero to avoid division by zero and also not allow packing to filled bins\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n    \n    \n    possible_bins = bins_remain_cap >= item\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        \n        if remaining_cap < item: # Disqualify bins that are too small\n            priorities[i] = -np.inf # Significantly low priority (lower than zero) to disallow item placement\n            continue\n        \n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n        \n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n        \n\n        # Fragmentation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bins_remain_cap[0]) < small_fragment_threshold:  #Compare against a base capacity\n                priorities[i] += large_fragment_penalty # Severe penalty\n\n    # If no bins can contain the item (after applying fragmentation penalty), slightly raise score of potential bins for placement\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n\n    return priorities",
    "response_id": 7,
    "tryHS": true,
    "obj": 4.028719585161557,
    "SLOC": 10.0,
    "cyclomatic_complexity": 3.0,
    "halstead": 88.0,
    "mi": 79.86649421043717,
    "token_count": 132.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines near-fit, fragmentation avoidance, and a capacity ratio to prioritize bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    near_fit_threshold = 0.9\n    near_fit_bonus = 10.0\n    small_fragment_threshold = 0.1\n    large_fragment_penalty = -5.0\n    complete_fill_bonus = 20.0\n    capacity_ratio_weight = 2.0  # Weight for the capacity ratio\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n    \n    possible_bins = bins_remain_cap >= item\n\n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = -np.inf\n            continue\n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n        \n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n\n        # Fragmentation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bins_remain_cap[0]) < small_fragment_threshold:\n            priorities[i] += large_fragment_penalty\n\n        # Capacity Ratio - more continuous measure of suitability\n        priorities[i] += (item / remaining_cap) * capacity_ratio_weight\n\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n        \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.068607897885915,
    "SLOC": 27.0,
    "cyclomatic_complexity": 9.0,
    "halstead": 358.95598430941163,
    "mi": 70.25600564940436,
    "token_count": 245.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version introduces dynamic weighting and considers bin fill level.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_capacity = bins_remain_cap[0]  # Assuming all bins have the same capacity.\n\n    # Heuristic 1: Near Fit (Dynamic bonus based on fill level)\n    near_fit_threshold_low = 0.75\n    near_fit_threshold_high = 0.95\n    near_fit_bonus_max = 15.0\n\n    # Heuristic 2: Fragmentation Penalty (Adaptive based on item size)\n    fragmentation_penalty_exponent = 2\n    small_fragment_threshold = 0.1\n    large_fragment_penalty_factor = -5.0\n\n    # Heuristic 3: Complete Fill Bonus\n    complete_fill_bonus = 25.0\n\n    # Heuristic 4: Fill Level Preference (Encourage filling emptier bins first, but temper)\n    fill_level_preference_weight = 2.0\n\n    # Heuristic 5: Avoid Overfill (Strong Negative Priority)\n    overfill_penalty = -np.inf\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n\n    possible_bins = bins_remain_cap >= item\n\n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = overfill_penalty\n            continue\n\n        # Complete Fill\n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n            continue #Short circuit to avoid other calculations\n\n        # Near Fit (Dynamic Bonus)\n        fill_ratio = item / remaining_cap\n        if near_fit_threshold_low <= fill_ratio <= near_fit_threshold_high:\n            # Scale bonus based on how close it is to the ideal near fit.\n            near_fit_bonus = near_fit_bonus_max * (1 - abs(fill_ratio - (near_fit_threshold_low + near_fit_threshold_high)/2) / ((near_fit_threshold_high - near_fit_threshold_low)/2))\n            priorities[i] += near_fit_bonus\n\n        # Fragmentation Penalty (Adaptive)\n        new_remaining = remaining_cap - item\n        if new_remaining > 0:\n            fragment_ratio = new_remaining / bin_capacity\n            if fragment_ratio < small_fragment_threshold:\n                # Scale penalty based on the item size.  Larger items causing fragmentation receive more penalty.\n                penalty = large_fragment_penalty_factor * (item / bin_capacity)\n                priorities[i] += penalty\n\n        # Fill Level Preference\n        priorities[i] += fill_level_preference_weight * (1 - (remaining_cap / bin_capacity))\n\n    # If no bins can contain the item (after applying fragmentation penalty), slightly raise score of potential bins for placement\n    if not np.any(priorities[possible_bins] > overfill_penalty):\n        priorities[possible_bins] += 0.001\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 5.115676106900674,
    "SLOC": 35.0,
    "cyclomatic_complexity": 10.0,
    "halstead": 451.50849518197793,
    "mi": 74.68370763706417,
    "token_count": 340.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float,\n                bins_remain_cap: np.ndarray,\n                near_fit_threshold: float = 0.29761772594302804,\n                near_fit_bonus: float = 16.302760571277386,\n                fragmentation_penalty_exponent: int = 1.044573988314493,\n                small_fragment_threshold: float = 0.16584727708016184,\n                large_fragment_penalty: float = -8.254364625617328,\n                complete_fill_bonus: float = 51.776707242150245,\n                raise_potential_bins_score: float = 0.47377604441116505) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        near_fit_threshold: Threshold for near fit heuristic.\n        near_fit_bonus: Bonus for near fit heuristic.\n        fragmentation_penalty_exponent: Exponent for fragmentation penalty.\n        small_fragment_threshold: Threshold for small fragment.\n        large_fragment_penalty: Penalty for large fragment.\n        complete_fill_bonus: Bonus for complete fill.\n        raise_potential_bins_score: Score for raising potential bins.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Handle cases where remaining capacity is zero to avoid division by zero and also not allow packing to filled bins\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "halstead": 11.60964047443681,
    "mi": 98.48956911045778,
    "token_count": 112.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines near-fit, fragmentation avoidance, capacity ratio, with handling for full bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    near_fit_threshold = 0.9\n    near_fit_bonus = 10.0\n    small_fragment_threshold = 0.1\n    large_fragment_penalty = -5.0\n    complete_fill_bonus = 20.0\n    capacity_ratio_weight = 2.0\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n    \n    possible_bins = bins_remain_cap >= item\n\n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = -np.inf\n            continue\n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n        \n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n\n        # Fragmentation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bins_remain_cap[0]) < small_fragment_threshold:\n            priorities[i] += large_fragment_penalty\n\n        # Capacity Ratio\n        priorities[i] += (item / remaining_cap) * capacity_ratio_weight\n\n    # If no possible bins are good, make all possible bins minimally acceptable.\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.068607897885915,
    "SLOC": 29.0,
    "cyclomatic_complexity": 8.0,
    "halstead": 473.1364966057107,
    "mi": 47.97181638389354,
    "token_count": 258.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version incorporates dynamic adaptation based on bin utilization\n    and more nuanced fragmentation handling, along with a first-fit consideration.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # --- Heuristic Parameters (Tunable) ---\n    near_fit_threshold = 0.9\n    near_fit_bonus = 15.0  # Increased bonus for near fit\n\n    fragmentation_penalty_exponent = 2.0\n    small_fragment_threshold = 0.1\n    large_fragment_penalty = -10.0 # Increased penalty for small fragments\n\n    complete_fill_bonus = 25.0 # Increased bonus for complete fill\n    first_fit_bonus = 5.0  # Bonus for placing in the first available bin\n\n    # Bin Utilization Awareness: Adjust near_fit_threshold based on avg bin capacity\n    avg_bin_capacity = np.mean(bins_remain_cap) if num_bins > 0 else 1.0 # avoid division by zero\n    if avg_bin_capacity < 0.3:\n      near_fit_threshold = 0.85 # be more aggressive if bins are filling up\n    elif avg_bin_capacity > 0.7:\n      near_fit_threshold = 0.95 # be stricter if bins are mostly empty\n\n    # --- Heuristic Logic ---\n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n\n    possible_bins = bins_remain_cap >= item\n\n    first_possible_bin_index = -1\n    for i in range(num_bins):\n        if bins_remain_cap[i] >= item:\n            first_possible_bin_index = i\n            break\n\n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = -np.inf\n            continue\n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n\n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n            \n\n        # Fragmentation - More nuanced penalty calculation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0:\n            fragment_ratio = new_remaining / bins_remain_cap[0]\n            if fragment_ratio < small_fragment_threshold:\n                priorities[i] += large_fragment_penalty  # Strong penalty\n\n        # First Fit Preference (slight nudge, if applicable)\n        if i == first_possible_bin_index and first_possible_bin_index != -1:\n            priorities[i] += first_fit_bonus\n\n    # If no bins can contain the item, slightly raise score of potential bins\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.617072197846027,
    "SLOC": 42.0,
    "cyclomatic_complexity": 16.0,
    "halstead": 463.551887559856,
    "mi": 72.7869516393646,
    "token_count": 363.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins considering near-fit, fragmentation, capacity ratio, and bin state.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n\n    # Adaptive near-fit bonus\n    near_fit_threshold = 0.8\n    near_fit_bonus_base = 10.0\n    \n    # Adaptive fragmentation penalty\n    small_fragment_threshold = 0.2\n    large_fragment_penalty_base = -5.0\n    \n    capacity_ratio_weight = 2.0\n\n    valid_bins = bins_remain_cap >= item\n    \n    if not np.any(valid_bins):\n        return priorities\n\n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = -np.inf\n            continue\n\n        # Near Fit - adaptive bonus\n        near_fit_ratio = item / remaining_cap\n        if near_fit_ratio >= near_fit_threshold:\n            priorities[i] += near_fit_bonus_base * (near_fit_ratio - near_fit_threshold) / (1 - near_fit_threshold)\n\n        # Fragmentation - adaptive penalty\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bins_remain_cap[0]) < small_fragment_threshold:\n            priorities[i] += large_fragment_penalty_base * (small_fragment_threshold - (new_remaining / bins_remain_cap[0])) / small_fragment_threshold\n\n        # Capacity Ratio\n        priorities[i] += (item / remaining_cap) * capacity_ratio_weight\n        \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.028719585161557,
    "SLOC": 30.0,
    "cyclomatic_complexity": 9.0,
    "halstead": 482.1561540675772,
    "mi": 73.97515945838714,
    "token_count": 295.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    num_bins = len(bins_remain_cap)\n    bin_capacity = bins_remain_cap[0] # Assuming all bins have same capacity\n\n\n    # Heuristic 1: \"Near Fit\" - Prefer bins where the item fills a significant portion\n    near_fit_threshold = 0.9\n    near_fit_bonus_base = 10.0\n\n    # Heuristic 2: Avoid Fragmentation - Penalize bins that would become highly fragmented\n    fragmentation_penalty_exponent = 2\n    small_fragment_threshold = 0.1\n    large_fragment_penalty_base = -5.0\n\n    # Heuristic 3: Try to completely fill\n    complete_fill_bonus_base = 20.0\n\n    # Heuristic 4: Balance utilization across bins.  Penalize bins that are already very full\n    # to promote spreading items across bins, especially when items are small.\n    utilization_penalty_exponent = 1 # Higher exponent means stronger penalty for high utilization\n    high_utilization_threshold = 0.75 #Consider a bin highly utilized above this value\n    high_utilization_penalty_factor = -2.0\n\n    # Heuristic 5: Reward bins that have been empty for a while (delayed first fit)\n    # This encourages usage of empty bins and reduces bin count.  This is very effective when items come sorted in decreasing order.\n    # This needs to be coupled with good exploration of existing bins though, to avoid wasting space when items become smaller\n    empty_bin_bonus_base = 5.0 #Give small boost for packing into completely empty bin. This has to be a small boost\n    \n    #Adaptive Weights\n    near_fit_bonus = near_fit_bonus_base * (1 + (1-item/bin_capacity)) #Scale down when item is small, scale up when item is large\n    large_fragment_penalty = large_fragment_penalty_base * (1 + (item / bin_capacity))  #Penalize less when item is already big\n    complete_fill_bonus = complete_fill_bonus_base * (1 + (item/bin_capacity)) #scale up when item is large\n    \n\n    valid_bins = bins_remain_cap > 0\n    if not np.any(valid_bins):\n        return priorities\n\n    possible_bins = bins_remain_cap >= item\n    \n    #Normalize remaining capacity (relative to bin capacity)\n    normalized_remain_cap = bins_remain_cap / bin_capacity\n    \n    for i, remaining_cap in enumerate(bins_remain_cap):\n        if remaining_cap < item:\n            priorities[i] = -np.inf\n            continue\n\n        # Near Fit\n        if item / remaining_cap >= near_fit_threshold:\n            priorities[i] += near_fit_bonus\n\n        # Complete Fill\n        if item == remaining_cap:\n            priorities[i] += complete_fill_bonus\n\n        # Fragmentation\n        new_remaining = remaining_cap - item\n        if new_remaining > 0 and (new_remaining / bin_capacity) < small_fragment_threshold:\n            priorities[i] += large_fragment_penalty\n\n        # Utilization Balancing\n        utilization = 1 - (remaining_cap / bin_capacity)\n        if utilization >= high_utilization_threshold:\n             priorities[i] += high_utilization_penalty_factor * (utilization**utilization_penalty_exponent)\n\n        # Empty Bin Bonus (delayed first fit)\n        if remaining_cap == bin_capacity:\n            priorities[i] += empty_bin_bonus_base\n\n\n    # If no bins can contain the item (after applying fragmentation penalty), slightly raise score of potential bins for placement\n    if not np.any(priorities[possible_bins] > -np.inf):\n        priorities[possible_bins] += 0.001\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.228161148783416,
    "SLOC": 36.0,
    "cyclomatic_complexity": 12.0,
    "halstead": 737.9696543405619,
    "mi": 73.27235603103533,
    "token_count": 368.0,
    "exec_success": true
  }
]