import numpy as np

def heuristics_v2(distance_matrix):
    """{This algorithm combines edge frequency sampling with a reinforcement learning approach to iteratively refine a heuristic matrix that guides TSP solution construction.}"""
    n = distance_matrix.shape[0]
    heuristics_matrix = np.ones_like(distance_matrix)  # Initialize heuristics

    num_episodes = 50
    num_steps = n * 2  # Number of steps per episode
    learning_rate = 0.1
    discount_factor = 0.9
    epsilon = 0.2

    for episode in range(num_episodes):
        state = np.random.randint(n)  # Start node
        visited = {state}
        tour = [state]

        for step in range(num_steps):
            # Epsilon-greedy action selection
            if np.random.rand() < epsilon:
                # Explore: Choose a random unvisited neighbor
                unvisited_neighbors = [node for node in range(n) if node not in visited]
                if unvisited_neighbors:
                    action = np.random.choice(unvisited_neighbors)
                else:
                    action = tour[0]  # Return to start if all visited
            else:
                # Exploit: Choose neighbor with highest heuristic value
                unvisited_neighbors = [node for node in range(n) if node not in visited]
                if unvisited_neighbors:
                    action = max(unvisited_neighbors, key=lambda node: heuristics_matrix[state, node])
                else:
                    action = tour[0]  # Return to start if all visited

            # Take action and observe reward
            reward = -distance_matrix[state, action]  # Negative distance as reward

            # Update heuristics matrix (Q-learning update)
            best_next_action = max([node for node in range(n) if node != state], key=lambda node: heuristics_matrix[action, node]) if len([node for node in range(n) if node != state])>0 else state
            heuristics_matrix[state, action] = (1 - learning_rate) * heuristics_matrix[state, action] + \
                                             learning_rate * (reward + discount_factor * heuristics_matrix[action, best_next_action])
            heuristics_matrix[action, state] = heuristics_matrix[state, action] #Symmetry

            # Update state and visited set
            state = action
            if state not in visited:
                visited.add(state)
            tour.append(state)

        epsilon *= 0.9  # Reduce exploration over time

    return heuristics_matrix
