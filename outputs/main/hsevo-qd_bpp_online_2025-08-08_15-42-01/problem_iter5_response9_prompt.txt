{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a penalty for wasted space with a bonus for achieving near-perfect fits,\n    while also incorporating a slight preference for bins with more remaining capacity\n    to keep options open for larger future items. Exploration is guided by this\n    preference rather than being purely random.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Penalty for wasted space (higher negative score for more waste)\n    # This encourages tighter fits.\n    wasted_space_penalty = -remaining_after_fit * 100.0  # Scale penalty\n\n    # Score 2: Bonus for near-perfect fits.\n    # Reward bins that leave very little space, up to a small tolerance.\n    # Perfect fit bonus is higher than near-perfect fit bonus.\n    near_perfect_fit_threshold = 0.05\n    perfect_fit_bonus = 10.0\n    near_perfect_fit_bonus = 5.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Score 3: Exploration-guided preference for bins with more remaining capacity.\n    # This is a \"soft\" preference, less impactful than tight fitting,\n    # to keep options open for potentially larger items later.\n    # We normalize this to avoid dominating the tight fit score.\n    # Using log to dampen the effect of very large capacities.\n    max_cap_preference = np.log1p(valid_bins_capacities) / np.log1p(np.max(valid_bins_capacities) + 1e-9) * 1.0 # Scaled preference\n\n    # Combine scores: Primarily penalize waste, reward good fits, with a soft exploration bias.\n    # The relative weights can be tuned.\n    combined_scores = wasted_space_penalty + fit_bonus + max_cap_preference\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** These heuristics are identical. The ranking suggests a potential issue with the evaluation or the ranking itself. However, assuming they represent a valid point in the ranking, their complexity and explicit handling of \"exploration candidates\" (top K fits and moderate capacity bins) is a notable design choice. The \"surplus penalty\" is a nuanced addition to pure tight-fitting.\n\n*   **Heuristics 2 vs 3 & 4:** Heuristics 3 and 4 are identical and simpler than Heuristic 2. They implement a basic epsilon-greedy approach (random choice vs. tight fit). Heuristic 2's \"adaptive exploration\" is more sophisticated, trying to guide exploration towards \"good enough\" bins rather than purely random ones. The penalty for large surpluses in Heuristic 2 also adds a layer of strategic thinking. The simpler epsilon-greedy in 3 & 4 is less likely to discover complex packing patterns.\n\n*   **Heuristics 3 & 4 vs 5 & 6:** Heuristics 5 and 6 are also simpler than 2 but introduce different mechanisms. Heuristic 6 is a pure \"Best Fit\" (minimizing remaining space). Heuristic 5 uses a softmax approach, turning \"fit scores\" into probabilities. This is an interesting probabilistic exploration. Comparing 3/4 (epsilon-greedy) to 5 (softmax) and 6 (pure best fit), the epsilon-greedy approach (3/4) is more direct in its exploration strategy. Best Fit (6) is purely exploitative, while Softmax (5) introduces a probabilistic distribution over good fits. The ranking suggests that these more direct or probabilistic approaches are less effective than the hybrid ones.\n\n*   **Heuristics 6 vs 7 & 8 & 9:** Heuristics 7, 8, and 9 are similar, focusing on tight fits and bonuses for perfect fits. Heuristic 6 (pure Best Fit) is simpler. Heuristics 7-9 add specific bonuses and sometimes penalties. Heuristic 8 & 9 are identical. The ranking indicates that adding these specific bonuses (like for perfect fits) and penalties (like for large remainders) improves performance over pure Best Fit. Heuristic 7 is more basic than 8/9.\n\n*   **Heuristics 7 vs 10 & 11:** Heuristic 10 is identical to 5. Heuristic 11 uses \"Almost Full Fit\" by maximizing `item - bins_remain_cap`, which is equivalent to minimizing `bins_remain_cap - item` (Best Fit). It does not explicitly add bonuses or penalties. The ranking suggests that the more nuanced approaches (like those in 7-9) are better than this direct Best Fit implementation.\n\n*   **Heuristics 11 vs 12:** Heuristic 12 is the most complex, combining penalties for wasted space, bonuses for near-perfect fits, and a \"soft\" preference for bins with more capacity (exploration-guided). Its higher ranking suggests that this multi-objective optimization within the heuristic is effective. Heuristic 11 is a simpler Best Fit variant.\n\n*   **Heuristics 12 vs 13:** Heuristic 13 is an FFD-inspired heuristic, prioritizing tight fits with a small penalty for slight oversights. It's simpler than 12. The ranking places 12 significantly higher, implying its combined strategy is superior.\n\n*   **Heuristics 13 vs 14 & 15 & 16:** Heuristic 14 is a simple Best Fit with explicit probabilities, but the loop implementation is inefficient. Heuristics 15 and 16 are identical and appear to be optimized versions of Heuristic 2, with hyperparameter tuning evident (e.g., `epsilon`, `perfect_fit_bonus`). These are still ranked lower than 13, which might indicate that the specific parameter values or the overall strategy of 13 is slightly better than the tuned 2. The presence of `torch` and `scipy` imports in 14-16, but not used in the snippet, is odd; they might be remnants of a larger framework or an indication of potential future extensions. The ranking suggests that even tuned versions of simpler hybrid strategies are not as good as the complex one (12) or even the FFD-inspired one (13).\n\n*   **Heuristics 15 & 16 vs 17 & 18:** Heuristics 17 and 18 are identical \"Random Fit\" strategies. They simply assign random priorities to bins that can fit the item. These are the simplest and worst-performing strategies, as they lack any exploitation logic. The ranking clearly places them at the bottom.\n\n*   **Heuristics 17 & 18 vs 19 & 20:** Heuristic 19 aims to combine perfect fits, tight fits, and tie-breaking based on original capacity. Heuristic 20 tries to balance tight fitting with penalties for small residuals and adds exploration. The ranking places 19 and 20 above random fit but below the more sophisticated hybrid strategies. Heuristic 19's tie-breaking logic is a good addition. Heuristic 20's penalty for small residuals is an interesting refinement.\n\n*   **Overall:** The top-performing heuristics (1, 2, 12) are complex, often combining tight-fitting principles with sophisticated exploration strategies (adaptive exploration, multi-objective scoring). Simpler \"Best Fit\" or epsilon-greedy approaches are ranked lower. Pure random strategies are at the bottom. The use of explicit bonuses and penalties for specific fit scenarios (perfect fit, large/small residuals) appears beneficial. The ranking suggests a hierarchy: complex hybrids > refined hybrids/FFD-inspired > basic hybrids/probabilistic > simple exploitation (Best Fit) > random.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, guided exploration, adaptive reward functions, hybrid strategies.\n*   **Advice:** Focus on iterative refinement of multi-objective heuristics, dynamically adjusting exploration/exploitation based on problem state.\n*   **Avoid:** Over-reliance on static or purely greedy approaches; neglecting the interplay between penalizing poor fits and rewarding ideal fits.\n*   **Explanation:** By viewing heuristic design as a continuous learning process, you can better identify optimal trade-offs between thorough exploration and efficient exploitation, leading to more robust and performant solutions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}