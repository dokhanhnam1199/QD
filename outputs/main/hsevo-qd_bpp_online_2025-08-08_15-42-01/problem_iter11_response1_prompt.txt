{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight-fitting (Best Fit) with a penalty for large unused capacity\n    and a bonus for perfect fits, guided by adaptive exploration principles.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity (closer to worst-fit among good fits).\n    # Bonus for perfect fit, penalty for large unused capacity.\n    \n    # Calculate remaining capacity if item fits\n    potential_remain_cap_vals = fitting_bins_capacities - item\n    \n    # Base score: favoring tight fits (smaller remaining capacity)\n    # Use negative remaining capacity to maximize for smaller remainders.\n    scores = -potential_remain_cap_vals\n    \n    # Bonus for perfect fits\n    perfect_fit_mask = np.isclose(potential_remain_cap_vals, 0)\n    scores[perfect_fit_mask] += 10.0 # Significant bonus for perfect fit\n    \n    # Penalty for large unused capacity: Discourage bins that are much larger than needed.\n    # This is a form of guided exploration towards \"good enough\" bins.\n    # We penalize bins where the remaining capacity is significantly larger than a small residual.\n    # A simple penalty could be proportional to the surplus capacity, but we want it to be less\n    # impactful than the tight-fitting score.\n    # Let's add a penalty that decreases the score for bins with large remaining capacity.\n    # This penalty should be smaller than the gains from tight fits.\n    # Penalty factor: For every unit of capacity above a small threshold (e.g., 10% of item size),\n    # we slightly reduce the score.\n    \n    # Define a threshold for \"large unused capacity\"\n    large_capacity_threshold = item * 0.5 # e.g., if remaining capacity is > 50% of item size\n\n    # Calculate penalty: linearly decreasing score for capacity beyond the threshold.\n    # Bins with capacity <= threshold get no penalty.\n    penalty = np.zeros_like(scores)\n    large_surplus_mask = potential_remain_cap_vals > large_capacity_threshold\n    \n    # The penalty is proportional to how much the surplus exceeds the threshold.\n    # We want this penalty to be relatively small compared to the tight-fit scores.\n    # e.g., subtract (surplus - threshold) / max_possible_surplus * small_value\n    max_possible_surplus = np.max(fitting_bins_capacities) # An upper bound on surplus\n    \n    if np.any(large_surplus_mask) and max_possible_surplus > 0:\n        penalty[large_surplus_mask] = (potential_remain_cap_vals[large_surplus_mask] - large_capacity_threshold) / max_possible_surplus * 5.0\n        \n    scores -= penalty\n\n    # Assign the calculated scores to the original bins array\n    priorities[can_fit_mask] = scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A multi-objective priority function for online bin packing.\n    It balances 'tightest fit' with 'future utility' and guided exploration.\n    It explicitly rewards bins with high remaining capacity for future items\n    and penalizes bins that might become \"awkwardly empty\" after packing.\n    Exploration is focused on bins that are good candidates for either\n    tight fit or future utility, with a bias towards the former.\n    \"\"\"\n    epsilon = 0.1  # Probability of biased exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Multi-Objective Scoring ---\n    # 1. Tightest Fit Score: Maximize -remaining_after_fit. Higher score for less remaining space.\n    tightest_fit_scores = -remaining_after_fit\n\n    # 2. Future Utility Score: Maximize remaining capacity. Higher score for more remaining space.\n    # This encourages leaving larger bins for potentially larger future items.\n    future_utility_scores = remaining_after_fit\n\n    # 3. Perfect Fit Bonus: Add a significant bonus for exact matches.\n    perfect_fit_bonus = 1000.0\n    tightest_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    future_utility_scores[np.abs(remaining_after_fit) < 1e-9] = 0 # No future utility for perfect fit\n\n    # 4. Awkward Empty Penalty: Penalize bins that become \"too empty\" after packing,\n    # relative to the item size. This discourages leaving small unusable remainders.\n    # A bin becoming empty by more than twice the item size is penalized.\n    awkward_empty_threshold_ratio = 2.0\n    awkward_empty_penalty_factor = 0.5\n    awkward_mask = remaining_after_fit > (item * awkward_empty_threshold_ratio)\n    if np.any(awkward_mask):\n        # Penalty is proportional to how much it exceeds the threshold, normalized by item size\n        awkward_penalty = (remaining_after_fit[awkward_mask] / item - awkward_empty_threshold_ratio) * awkward_empty_penalty_factor\n        tightest_fit_scores[awkward_mask] -= awkward_penalty\n        future_utility_scores[awkward_mask] -= awkward_penalty # Penalize future utility as well\n\n    # --- Combined Score ---\n    # A weighted sum of tightest fit and future utility.\n    # Weights can be adjusted. Here, we give slightly more weight to tight fit.\n    tight_fit_weight = 0.6\n    future_utility_weight = 0.4\n    \n    # Normalize scores to prevent one objective from dominating due to scale.\n    # Using robust scaling (median and IQR) might be better, but simple min-max is often sufficient.\n    # We will normalize within the valid bins only.\n    \n    min_tf = np.min(tightest_fit_scores)\n    max_tf = np.max(tightest_fit_scores)\n    range_tf = max_tf - min_tf if max_tf != min_tf else 1.0\n    normalized_tight_fit = (tightest_fit_scores - min_tf) / range_tf if range_tf != 0 else np.zeros_like(tightest_fit_scores)\n\n    min_fu = np.min(future_utility_scores)\n    max_fu = np.max(future_utility_scores)\n    range_fu = max_fu - min_fu if max_fu != min_fu else 1.0\n    normalized_future_utility = (future_utility_scores - min_fu) / range_fu if range_fu != 0 else np.zeros_like(future_utility_scores)\n\n    combined_scores = (tight_fit_weight * normalized_tight_fit) + (future_utility_weight * normalized_future_utility)\n\n    # --- Guided Exploration ---\n    # Exploration will be applied to a subset of bins that are \"promising\" based on the combined score.\n    # \"Promising\" bins are those with high combined scores.\n    \n    sorted_indices_combined = np.argsort(combined_scores)[::-1] # Indices sorted by combined score (descending)\n\n    # Exploration candidates: Top K bins by combined score, plus bins that represent\n    # a good compromise (e.g., not perfectly tight, but good future utility, or vice-versa).\n    \n    exploration_candidate_mask = np.zeros_like(combined_scores, dtype=bool)\n    \n    # Select top 30% of bins as initial candidates for exploration\n    num_top_candidates = min(len(combined_scores), max(1, int(len(combined_scores) * 0.3)))\n    exploration_candidate_mask[sorted_indices_combined[:num_top_candidates]] = True\n\n    # Add bins with high future utility but not necessarily top-tier tight fit,\n    # as these might be useful for larger items that might come later.\n    # Threshold: bins with future utility in the top 50% percentile, but not in the top 20% of combined scores.\n    utility_threshold = np.percentile(future_utility_scores, 50)\n    high_utility_mask = future_utility_scores >= utility_threshold\n    \n    # Exclude already selected top candidates to avoid over-representation\n    already_selected_mask = exploration_candidate_mask\n    potential_explore_candidates = high_utility_mask & ~already_selected_mask\n    \n    # Add a portion of these high-utility candidates\n    num_high_utility_to_add = min(np.sum(potential_explore_candidates), max(0, int(len(combined_scores) * 0.1)))\n    \n    if num_high_utility_to_add > 0:\n        high_utility_indices = np.where(potential_explore_candidates)[0]\n        # Randomly select from potential high utility candidates\n        selected_high_utility_indices = np.random.choice(high_utility_indices, num_high_utility_to_add, replace=False)\n        exploration_candidate_mask[selected_high_utility_indices] = True\n\n    # Generate exploration scores (noisy values for exploration candidates)\n    exploration_scores = np.random.rand(len(combined_scores)) * 0.05 # Small random noise\n\n    # --- Decision: Exploit or Explore ---\n    # With probability epsilon, choose exploration score for selected candidates.\n    # Otherwise, use the combined exploitation score.\n    # For bins not selected as candidates, always use the combined score.\n\n    final_priorities_for_valid_bins = np.copy(combined_scores)\n\n    # For candidates, decide whether to explore\n    should_explore_mask = np.random.rand(len(combined_scores)) < epsilon\n    \n    # Apply exploration score ONLY to candidates where exploration is chosen\n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    # Use exploration score for these bins\n    final_priorities_for_valid_bins[apply_exploration_mask] = exploration_scores[apply_exploration_mask]\n\n    priorities[valid_bins_indices] = final_priorities_for_valid_bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 2 (Worst in this comparison): Both heuristics implement a multi-objective scoring system with exploration. Heuristic 1 is marginally better as it defines exploration candidates based on a combination of top scores and percentile-based moderate capacity, making its exploration more guided. Heuristic 2's exploration candidate selection is identical.\n\nComparing Heuristic 1 vs. Heuristic 3: Heuristic 1 uses a more sophisticated multi-objective approach (tightness, waste, future capacity) compared to Heuristic 3's simple tight fit with a perfect fit bonus and basic epsilon-greedy exploration. Heuristic 1's guided exploration is also more nuanced than Heuristic 3's random assignment.\n\nComparing Heuristic 3 vs. Heuristic 5: Heuristic 3 combines tight fitting with random exploration (epsilon-greedy), while Heuristic 5 focuses purely on tight fitting with a perfect fit bonus. Heuristic 3's exploration component, though basic, offers more potential for finding diverse solutions than Heuristic 5's deterministic approach.\n\nComparing Heuristic 5 vs. Heuristic 7: These heuristics are identical, both focusing on tight fitting with a perfect fit bonus.\n\nComparing Heuristic 8 vs. Heuristic 12: Heuristic 8 employs a more refined strategy, balancing tight fitting, perfect fit bonuses, and penalties for large remainders. Heuristic 12 uses a simple inverse capacity for scoring and a binary perfect fit bonus. Heuristic 8's more nuanced scoring and penalty system likely leads to better packing.\n\nComparing Heuristic 12 vs. Heuristic 15: Both use a simple approach where capacity dictates priority. Heuristic 12 assigns a value of 1.0 for perfect fits and `1.0 / remaining_capacity` otherwise, while Heuristic 15 assigns a random value to fitting bins. Heuristic 12's deterministic scoring for good fits is generally more desirable than random assignment.\n\nComparing Heuristic 14 vs. Heuristic 17: Heuristic 14 implements a multi-objective scoring system (tightest fit, future utility, awkward empty penalty) with a guided exploration strategy. Heuristic 17 uses similar concepts but with different weighting and thresholds, and its parameters are tuned. Without knowing the exact performance impact of the specific parameter values in Heuristic 17, it's difficult to definitively rank them, but Heuristic 14's systematic approach to multi-objective scoring and guided exploration makes it a strong contender.\n\nOverall: Heuristics that combine a primary objective (like tight fitting) with secondary objectives (like waste avoidance, future utility) and employ guided exploration strategies (rather than pure random exploration) tend to perform better. The inclusion of specific bonuses for perfect fits and penalties for undesirable states (e.g., large surpluses) also improves heuristic design. Simple random or worst-fit strategies are generally the least effective.\n- \nHere's a redefined approach to self-reflection for designing better optimization heuristics:\n\n*   **Keywords:** Adaptive, multi-objective, guided exploration, performance feedback.\n*   **Advice:** Focus on heuristics that learn from past placements and dynamically adjust their strategy. Integrate feedback mechanisms to refine scoring and exploration parameters based on actual packing outcomes.\n*   **Avoid:** Static rules, brute-force random sampling, or heuristics that don't adapt to problem instance characteristics.\n*   **Explanation:** True self-reflection involves understanding *why* a heuristic performs well or poorly on specific instances, leading to more intelligent adaptation rather than just applying predefined complex rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}