```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines best-fit with a penalty for large remaining capacity and a bonus
    for moderate capacity bins, employing adaptive exploration.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)
    
    can_fit_mask = bins_remain_cap >= item
    
    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]
    num_valid_bins = len(valid_bins_capacities)

    # --- Objective 1: Tightest Fit (Minimize Residual Space) ---
    remaining_after_fit = valid_bins_capacities - item
    # Score: Maximize negative residual space. Higher score means tighter fit.
    tightness_scores = -remaining_after_fit
    # Bonus for perfect fits
    perfect_fit_bonus = 10.0
    tightness_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus

    # --- Objective 2: Penalize Large Residuals ---
    # Penalty proportional to the size of remaining capacity relative to the item.
    # Encourages fuller bins.
    large_remainder_penalty_factor = 0.2
    slack_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor
    slack_penalty_scores = -slack_penalty
    tightness_scores += slack_penalty_scores

    # --- Objective 3: Promote Moderate Capacity (Future Utility) ---
    # Identify bins that, after fitting, retain a moderate amount of space.
    # This space is useful for future items without being excessively large.
    # We define "moderate" relative to the item size itself.
    moderate_capacity_bonus_factor = 0.1
    
    # Consider bins where remaining capacity is between 20% and 80% of the item's size.
    lower_moderate_bound = 0.2 * item
    upper_moderate_bound = 0.8 * item
    
    moderate_capacity_scores = np.zeros(num_valid_bins)
    is_moderate = (remaining_after_fit >= lower_moderate_bound) & (remaining_after_fit <= upper_moderate_bound)
    
    if np.any(is_moderate):
        moderate_bins_remaining = remaining_after_fit[is_moderate]
        # Score based on how close to the midpoint of the moderate range the bin is.
        mid_moderate_range = (lower_moderate_bound + upper_moderate_bound) / 2.0
        distance_from_mid = np.abs(moderate_bins_remaining - mid_moderate_range)
        max_distance = max(mid_moderate_range - lower_moderate_bound, upper_moderate_bound - mid_moderate_range)
        normalized_distance = distance_from_mid / max_distance if max_distance > 0 else np.zeros_like(distance_from_mid)
        moderate_capacity_scores[is_moderate] = (1.0 - normalized_distance) * moderate_capacity_bonus_factor * item

    # --- Combine Objectives ---
    # Weighted sum of scores. Tightness is primary, moderate capacity is secondary.
    weight_tightness = 0.7
    weight_moderate_capacity = 0.3
    combined_scores = (weight_tightness * tightness_scores) + (weight_moderate_capacity * moderate_capacity_scores)

    # --- Adaptive Exploration ---
    # Introduce a small probability of selecting a slightly suboptimal bin
    # to avoid getting stuck in local optima. The exploration probability
    # is influenced by the density of good fits.
    epsilon = 0.05  # Base exploration probability
    
    # If there are many bins that are very close to fitting (tight), reduce exploration.
    # If there are few tight fits, increase exploration.
    tight_fit_threshold = 0.05 * item # Bins with residual < 5% of item size
    num_very_tight_fits = np.sum(remaining_after_fit < tight_fit_threshold)
    tight_fit_ratio = num_very_tight_fits / num_valid_bins if num_valid_bins > 0 else 0
    
    adaptive_epsilon = epsilon
    if tight_fit_ratio < 0.2: # If less than 20% of valid bins are very tight
        adaptive_epsilon = min(epsilon * 1.5, 0.25) # Increase exploration
    elif tight_fit_ratio > 0.6: # If more than 60% are very tight
        adaptive_epsilon = max(epsilon * 0.5, 0.01) # Decrease exploration

    # --- Guided Exploration Strategy ---
    # Instead of random selection, explore among a pool of promising candidates.
    # Candidates include bins with moderate capacity or "good enough" tight fits.
    exploration_candidate_mask = np.zeros(num_valid_bins, dtype=bool)
    
    # Add bins with moderate capacity to exploration candidates
    exploration_candidate_mask[is_moderate] = True
    
    # Add a portion of the tightest bins (but not perfect fits) as exploration candidates.
    # We want to explore slightly less optimal tight fits.
    sorted_indices_tightness = np.argsort(tightness_scores)[::-1] # Descending for best tightness
    num_tight_candidates_to_consider = max(1, int(num_valid_bins * 0.15)) # Top 15% of tightest
    
    # Select from the top tightest, but exclude perfect fits for exploration diversity
    potential_exploration_indices = sorted_indices_tightness[:num_tight_candidates_to_consider]
    potential_exploration_indices = potential_exploration_indices[remaining_after_fit[potential_exploration_indices] > 1e-9]
    
    exploration_candidate_mask[potential_exploration_indices] = True

    # Generate exploration noise, applied probabilistically to candidates.
    # The noise is small to allow for minor deviations.
    exploration_noise_magnitude = 0.05 * item # Scale noise relative to item size
    exploration_noise = np.random.rand(num_valid_bins) * exploration_noise_magnitude
    
    # Apply exploration noise to candidates with probability adaptive_epsilon
    should_explore_mask = np.random.rand(num_valid_bins) < adaptive_epsilon
    apply_exploration_mask = exploration_candidate_mask & should_explore_mask
    
    combined_scores[apply_exploration_mask] += exploration_noise[apply_exploration_mask]
    
    # Assign final priorities
    priorities[valid_bins_indices] = combined_scores
    
    return priorities
```
