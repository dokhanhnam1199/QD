{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines the multi-objective scoring of v0 with the refined tie-breaking\n    and perfect-fit bonus of v15. Introduces guided exploration for better balance.\n    \"\"\"\n    epsilon = 0.05  # Probability for exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Multi-objective Scoring (inspired by v0) ---\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    #    Higher score for smaller remaining_after_fit.\n    tightness_score = -remaining_after_fit\n\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    #    This is complementary to tightness. Less impact compared to tightness.\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    #    This helps in potentially fitting larger items later. Scaled by max original capacity.\n    max_original_cap = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0\n    future_capacity_score = remaining_after_fit / max_original_cap\n\n    # Combine objectives with weights. These weights can be tuned.\n    weight_tightness = 1.0\n    weight_waste = 0.3  # Reduced weight for waste avoidance to emphasize tightness\n    weight_future_capacity = 0.2\n\n    combined_scores = (weight_tightness * tightness_score\n                       - weight_waste * waste_avoidance_score\n                       + weight_future_capacity * future_capacity_score)\n\n    # --- Perfect Fit Bonus and Tie-breaking (inspired by v15) ---\n    # Identify perfect fits and assign a very high score to them.\n    perfect_fit_mask_in_valid = remaining_after_fit == 0\n    perfect_fit_bonus = 1000.0  # High bonus for exact fits\n\n    # For non-perfect fits, use the combined score, but add a tie-breaker\n    # that favors bins with larger original capacity if their combined score is similar.\n    # We can incorporate the original capacity as a secondary sorting key by adding a scaled value.\n    # This is less about a fixed bonus and more about sorting preference for similar scores.\n    tie_breaker_scale = 1e-6\n    exploitation_scores = combined_scores + (valid_bins_capacities * tie_breaker_scale)\n    exploitation_scores[perfect_fit_mask_in_valid] = perfect_fit_bonus # Override with bonus\n\n    # --- Guided Exploration (combining v0 and introducing structure) ---\n    # Identify a set of \"promising\" bins for exploration.\n    # These include the top-scoring bins and those with moderate remaining capacity.\n\n    # Sort bins by the exploitation scores to identify top candidates\n    sorted_indices_exploitation = np.argsort(exploitation_scores)[::-1]\n\n    # Consider top 20% of bins or at least the top 3 bins for exploration\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.2))\n    top_candidate_indices_in_valid = sorted_indices_exploitation[:num_top_bins]\n\n    # Identify bins with moderate remaining capacity (between Q1 and Q3)\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n\n    # Combine indices for exploration candidates (unique indices)\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates (small random perturbations)\n    # Use Gaussian noise for smoother exploration if needed, but uniform is fine for now.\n    exploration_scores_perturbation = np.random.uniform(-0.05, 0.05, size=len(valid_bins_capacities))\n\n    final_scores = np.copy(exploitation_scores)\n\n    # Apply exploration scores with probability epsilon to the identified candidates\n    # Create a mask for the identified exploration candidates within the valid bins\n    exploration_candidate_mask_in_valid = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_candidate_mask_in_valid[exploration_candidate_indices_in_valid] = True\n\n    # Randomly decide for each candidate if we use the exploration score\n    use_exploration_decision = np.random.rand(len(valid_bins_capacities)) < epsilon\n\n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_candidate_mask_in_valid & use_exploration_decision] = \\\n        exploitation_scores[exploration_candidate_mask_in_valid & use_exploration_decision] + \\\n        exploration_scores_perturbation[exploration_candidate_mask_in_valid & use_exploration_decision]\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1 / (bins_remain_cap[i] - item + 1e-9) \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Second Best): They are identical.\nComparing Heuristic 1 vs Heuristic 3: They are identical.\nComparing Heuristic 1 vs Heuristic 4: Heuristic 1 introduces a hybrid approach with exploration (epsilon probability) and guided candidate selection, whereas Heuristic 4 is purely deterministic best-fit with a penalty for large remainders. Heuristic 1's approach is more sophisticated in balancing immediate needs with future possibilities.\nComparing Heuristic 1 vs Heuristic 5: Heuristic 1 uses a probability-based exploration with candidate selection, while Heuristic 5 attempts to integrate exploration by preferring bins with more remaining capacity, but without a clear probabilistic mechanism. Heuristic 1's explicit exploration is likely more effective.\nComparing Heuristic 1 vs Heuristic 6: Heuristic 1 is a hybrid of tightest fit with exploration. Heuristic 6 is multi-objective, incorporating utilization and adaptive exploration (though the adaptive part is simulated). Heuristic 6 attempts a more holistic approach by considering bin utilization.\nComparing Heuristic 1 vs Heuristic 7: Heuristic 1 is a simpler hybrid. Heuristic 7 builds on multi-objective scoring, adaptive exploration probability, and identifies candidates based on both tightness and moderate capacity, making it more nuanced than Heuristic 1.\nComparing Heuristic 7 vs Heuristic 8: Heuristic 8 builds on Heuristic 7's multi-objective scoring but refines it with better tie-breaking and exploration based on candidate selection and perturbations. Heuristic 8 seems to have a more structured approach to balancing objectives.\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 is a complex multi-objective heuristic with exploration. Heuristic 9 is a very simple heuristic that prioritizes bins with small remaining capacity (inverse of remaining capacity) and gives a bonus to perfect fits. Heuristic 8 is significantly more advanced.\nComparing Heuristic 9 vs Heuristic 10: Heuristics 9 and 10 are almost identical, with Heuristic 9 having a slight preference for perfect fits (score of 1.0 vs inverse of remainder) and Heuristic 10 purely using inverse of remainder. Both are simple \"best-fit\" variations.\nComparing Heuristic 10 vs Heuristic 11: Heuristic 10 is a simple inverse remainder score. Heuristic 11 is also best-fit focused but adds a tie-breaker favoring larger original capacity and a specific score for perfect fits. Heuristic 11 is more refined for tie-breaking.\nComparing Heuristic 11 vs Heuristic 12: Heuristic 11 is a best-fit with tie-breaking. Heuristic 12 introduces several tunable parameters for exploration probability, bonuses, penalties, and candidate selection, indicating a more experimental and potentially optimized approach to balancing objectives.\nComparing Heuristic 12 vs Heuristic 13: Heuristic 12 is a complex multi-objective heuristic. Heuristic 13 is a \"Random Fit\" strategy, assigning random priorities to bins that can fit the item. This is a very basic approach compared to Heuristic 12.\nComparing Heuristic 13 vs Heuristic 14: Heuristics 13 and 14 are identical \"Random Fit\" strategies.\nComparing Heuristic 14 vs Heuristic 15: Heuristic 14 is random. Heuristic 15 is a complex multi-objective heuristic with adaptive exploration based on item size variance and capacity utilization gradient, significantly more sophisticated than random.\nComparing Heuristic 15 vs Heuristic 16: Heuristics 15 and 16 are identical.\nComparing Heuristic 16 vs Heuristic 17: Heuristic 16/15 is multi-objective with adaptive exploration based on variance and utilization gradient. Heuristic 17 is also multi-objective, focusing on tightness, future usability (relative to item size), and adaptive exploration for candidates. Heuristic 16/15 seems to have a more defined adaptive exploration mechanism.\nComparing Heuristic 17 vs Heuristic 18: Heuristics 17 and 18 are identical.\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 is multi-objective with adaptive exploration. Heuristic 19 combines tightness, waste avoidance, future utility, and guided exploration with perturbated scores for candidates. Heuristic 19's exploration is more about perturbing scores of selected candidates.\nComparing Heuristic 19 vs Heuristic 20: Heuristics 19 and 20 are identical.\nOverall: The top heuristics (1-8, 15-18) are complex multi-objective strategies that balance tightest fit with some form of future utility or guided exploration. Heuristics 9-11 and 13-14 are simpler best-fit or random strategies. The intermediate heuristics (12, 19-20) attempt variations on multi-objective and exploration. Heuristics 1, 7, 8, 15, 16, 17, 18 appear to represent the most developed ideas, blending multiple objectives and adaptive/guided exploration.\n- \nHere's a redefinition of \"Current self-reflection\" to guide heuristic design, avoiding ineffective approaches:\n\n*   **Keywords:** Multi-objective, adaptive exploration, sophisticated scoring, penalized residuals, rewarded fits.\n*   **Advice:** Design heuristics that dynamically balance multiple objectives (e.g., tightness, future utility, waste) using adaptive exploration strategies that sample promising candidates rather than relying on pure randomness.\n*   **Avoid:** Overly simplistic strategies like pure Best Fit or random assignments. Also avoid neglecting the trade-off between exploration and exploitation.\n*   **Explanation:** Complex, multi-objective heuristics with nuanced scoring (e.g., softmax, explicit penalties/bonuses) and guided exploration are essential for outperforming simpler methods, especially in complex packing problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}