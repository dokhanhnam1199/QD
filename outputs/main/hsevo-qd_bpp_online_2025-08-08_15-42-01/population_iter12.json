[
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a Best Fit strategy with a penalty for large remaining capacity\n    and a bonus for near-perfect fits, aiming for efficient packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins that can fit the item\n    # Use negative remaining capacity to favor tighter fits (Best Fit)\n    # Add a small penalty for larger remaining capacities to avoid extremely sparse bins\n    # Add a bonus for near-perfect fits to exploit efficient packing opportunities\n    priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask] - 0.1 * (bins_remain_cap[can_fit_mask] - item) + 10 * np.exp(-(bins_remain_cap[can_fit_mask] - item)**2 / 0.1)\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Best Fit strategy for the online Bin Packing Problem.\n    Items are prioritized to be placed in bins where they leave the least\n    remaining space, thus aiming to minimize wasted space.\n    \"\"\"\n    # Calculate the remaining space after placing the item in each bin.\n    # Only consider bins where the item can actually fit.\n    possible_fits = bins_remain_cap >= item\n    remaining_space = np.where(possible_fits, bins_remain_cap - item, np.inf)\n\n    # The priority is inversely related to the remaining space.\n    # A smaller remaining space means a higher priority.\n    # We use a small epsilon to avoid division by zero if remaining_space is 0\n    # and to ensure bins with 0 remaining space (perfect fit) get the highest priority.\n    epsilon = 1e-9\n    priorities = np.where(possible_fits, 1 / (remaining_space + epsilon), 0)\n\n    # Ensure that bins where the item cannot fit have a priority of 0.\n    priorities[~possible_fits] = 0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Almost Full Fit: Prioritize bins that will be closest to full after adding the item.\n    # Calculate the remaining capacity after placing the item.\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want bins where potential_remain_cap is as close to zero as possible (but non-negative)\n    # A simple way to achieve this is to maximize the negative of the absolute difference from zero.\n    # However, to encourage fitting rather than just being close, we can also consider\n    # bins that can actually fit the item.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = potential_remain_cap >= 0\n\n    # For bins that can fit, calculate a score based on how full they will become.\n    # A higher score means the bin will be closer to full.\n    # We can use -(potential_remain_cap) as a measure of \"fullness\" after packing.\n    # To avoid prioritizing bins that become \"too full\" or negative capacity,\n    # we only consider valid fits.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = -potential_remain_cap[can_fit_mask]\n\n    # We can add a small bonus for fitting the item at all to ensure\n    # that fitting items is generally preferred over not fitting.\n    # However, the negative of remaining capacity already does this indirectly.\n    # A more sophisticated approach might involve considering the original capacity\n    # to prioritize filling larger bins more.\n\n    # For \"Almost Full Fit\", we want to minimize the remaining capacity.\n    # So, we want to maximize the negative of the remaining capacity.\n    # Let's refine this. We want the remaining capacity to be as close to 0 as possible.\n    # The value `potential_remain_cap` represents this.\n    # We want to maximize this value if it's negative (meaning it's a good fit).\n    # Let's re-evaluate. For Almost Full Fit, we want the resulting remaining capacity\n    # to be as small as possible, but still non-negative.\n    # So, we want to maximize `-(potential_remain_cap)` for valid fits.\n\n    # Consider the difference between the bin's original capacity and the item size.\n    # We want to prioritize bins where this difference is minimized, but non-negative.\n    # The `potential_remain_cap` already captures this.\n    # We want the most \"positive\" value of `-(potential_remain_cap)` for bins that fit.\n\n    # Another perspective: prioritize bins that have the *least* remaining capacity *after* the item is placed.\n    # This directly translates to maximizing `-(potential_remain_cap)`.\n\n    # Let's consider a small perturbation or a \"niceness\" factor.\n    # Perhaps bins that are already somewhat full are preferred.\n    # But for \"Almost Full Fit\", the focus is purely on the state *after* packing.\n\n    # We want to maximize the remaining capacity in a way that favors being close to zero.\n    # The score should be higher for bins that result in less remaining capacity.\n    # So, we want to maximize `-(bins_remain_cap - item)` for valid bins.\n    # This is equivalent to maximizing `item - bins_remain_cap`. This isn't quite right.\n\n    # The goal is to make the bin as \"full\" as possible *after* adding the item.\n    # \"Full\" means having small remaining capacity.\n    # So, we want to minimize `potential_remain_cap`.\n    # To turn this into a maximization problem for priority, we can use `-potential_remain_cap`.\n    # We also need to ensure that bins that *cannot* fit the item get a very low priority.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Let's refine the \"Almost Full Fit\" idea. We want the bin with the smallest remaining capacity after packing.\n    # This means we want to minimize `bins_remain_cap - item`.\n    # For a priority score (higher is better), we can use the negative of this difference.\n    # So, we want to maximize `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n\n    # This score will be highest for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # Example: item=5\n    # bin1_rem=6  -> priority = 5 - 6 = -1\n    # bin2_rem=5  -> priority = 5 - 5 = 0\n    # bin3_rem=10 -> priority = 5 - 10 = -5\n    # bin4_rem=3  -> priority = -inf (cannot fit)\n\n    # The highest score is 0, from the bin that becomes exactly full.\n    # This seems correct for \"Almost Full Fit\".\n\n    # Consider a scenario where there are multiple bins that become exactly full.\n    # The current heuristic doesn't differentiate between them.\n    # For this strategy, simply picking any of them is fine.\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 12.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Softmax-Based Fit strategy for online Bin Packing Problem.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a 'fit' score. We prefer bins with less remaining capacity that can still fit the item.\n    # This encourages using bins more fully before opening new ones.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size remaining.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Apply softmax to convert fit scores into probabilities (priorities)\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Create an output array of the same size as the original bins_remain_cap\n    # and place the calculated priorities in the correct positions.\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities",
    "response_id": 19,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the most remaining capacity.\n    This heuristic attempts to keep bins with less capacity available for smaller items later on,\n    potentially leading to a more efficient packing in the long run.\n    In this priority function, we assign a higher priority score to bins with larger remaining capacities.\n    Specifically, the priority is directly proportional to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.array([bin_cap if bin_cap >= item else -np.inf for bin_cap in bins_remain_cap])\n    return priorities",
    "response_id": 12,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 3.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1 / (bins_remain_cap[i] - item + 1e-9) \n    return priorities",
    "response_id": 15,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities",
    "response_id": 18,
    "tryHS": false,
    "obj": 4.198244914240141,
    "SLOC": 10.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities",
    "response_id": 27,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 6.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a soft penalty for waste, inspired by Softmax-Fit.\n    Prioritizes bins that leave minimal remaining capacity, with a boost for perfect fits.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return output_priorities\n\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    # Calculate a score: inverse of remaining capacity after fit, favoring tighter fits.\n    # Add a small epsilon to prevent division by zero.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Introduce a bonus for perfect fits to strongly prioritize them.\n    perfect_fit_mask = (valid_bins_cap - item) < 1e-9\n    fit_scores[perfect_fit_mask] *= 5.0  # Significant bonus for perfect fits\n\n    # Apply softmax to normalize scores into probabilities/priorities.\n    # This provides a smoother distribution than pure inverse,\n    # while still emphasizing bins with higher fit_scores.\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring tight fits and perfect fits, with a fallback to worst fit.\n\n    Combines the tight-fitting approach of v0 with the explicit bonus for perfect fits\n    and a tie-breaking mechanism for the worst fit among available bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    available_bins_mask = bins_remain_cap >= item\n\n    if not np.any(available_bins_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    # Base priority: inverse of remaining capacity after placing the item (favors tight fits)\n    # Add epsilon to avoid division by zero.\n    priorities[available_bins_mask] = 1.0 / (available_bins_cap - item + 1e-6)\n\n    # Bonus for perfect fits: highest priority if an item exactly fills a bin\n    perfect_fit_mask = np.isclose(bins_remain_cap, item)\n    perfect_fit_available_mask = perfect_fit_mask & available_bins_mask\n    if np.any(perfect_fit_available_mask):\n        priorities[perfect_fit_available_mask] = np.max(priorities[available_bins_mask]) + 1.0\n\n    # Tie-breaking for bins that are not perfect fits:\n    # If there are multiple bins with the same \"tightest fit\" score (and not perfect fits),\n    # we can use a secondary criterion. Here, we'll give a slight bonus to the\n    # bin with the largest remaining capacity among these \"equally tight\" bins,\n    # similar to a worst-fit among the tightest options, to leave more options for later small items.\n    # This logic needs to be applied carefully to avoid overwriting perfect fit bonuses.\n\n    # Find the maximum priority score among non-perfect fits\n    non_perfect_fit_available_mask = available_bins_mask & ~perfect_fit_mask\n    if np.any(non_perfect_fit_available_mask):\n        max_non_perfect_priority = np.max(priorities[non_perfect_fit_available_mask])\n        \n        # Identify bins that have this max non-perfect priority\n        bins_with_max_priority_mask = np.isclose(priorities, max_non_perfect_priority) & non_perfect_fit_available_mask\n\n        if np.any(bins_with_max_priority_mask):\n            # Among those, find the one with the largest remaining capacity\n            capacities_at_max_priority = bins_remain_cap[bins_with_max_priority_mask]\n            worst_fit_among_tight_idx = np.argmax(capacities_at_max_priority)\n            \n            # Get the original index in bins_remain_cap\n            original_indices = np.where(bins_with_max_priority_mask)[0]\n            worst_fit_original_idx = original_indices[worst_fit_among_tight_idx]\n\n            # Apply a slight bonus (less than perfect fit bonus)\n            priorities[worst_fit_original_idx] = max_non_perfect_priority + 0.5\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 3.0115676106900726,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins that result in the tightest fit after packing,\n    with a bonus for perfect fits and a fallback for worst-fit among remaining options.\"\"\"\n\n    # Initialize priorities with a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in valid bins.\n    potential_remain_cap = bins_remain_cap[fit_mask] - item\n\n    # Assign scores:\n    # 1. High priority for perfect fits (remaining capacity = 0).\n    #    We give a bonus score (e.g., 1.0) to indicate preference.\n    perfect_fit_mask = potential_remain_cap == 0\n    priorities[fit_mask][perfect_fit_mask] = 1.0\n\n    # 2. For other valid fits, prioritize bins that leave less remaining capacity.\n    #    This encourages a \"tight\" packing. We use the negative of the remaining capacity.\n    #    The smallest non-negative remaining capacity will get the highest score here.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    priorities[fit_mask][non_perfect_fit_mask] = -potential_remain_cap[non_perfect_fit_mask]\n\n    # 3. Tie-breaking for bins that result in the same remaining capacity (or are perfect fits):\n    #    Among bins with the same resulting remaining capacity, favor the one with the\n    #    largest original remaining capacity (closer to worst-fit among good fits).\n    #    This helps to keep smaller bins available for smaller items.\n    #    We can achieve this by adding a small bonus proportional to the original capacity.\n    #    Add a small epsilon to the priority to break ties.\n    #    For perfect fits, they already have a score of 1.0. We want to differentiate them.\n    #    Let's use the negative of the original remaining capacity as a secondary score.\n    #    This means, for equally good fits, we prefer the one that was originally larger.\n    #    The `item - bins_remain_cap[fit_mask]` was a good starting point for 'tightness'\n    #    but did not handle tie-breaking well.\n\n    # Let's combine the score: Prioritize by minimizing remaining capacity.\n    # A simple way is to maximize `-(remaining_capacity)`.\n    # To differentiate between equally good fits, we can use the original capacity.\n    # If two bins result in the same remaining capacity, the one that started larger\n    # (worst fit among the tightest) is preferred.\n    # So, for the same `-(remaining_capacity)`, we want to maximize the original capacity.\n\n    # Re-calculating for clarity and combined logic:\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacity for fitting bins.\n    potential_remain_cap_vals = bins_remain_cap[fit_mask] - item\n\n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity.\n    # This can be achieved by maximizing `original_capacity - item`.\n    # Or more simply, `item - original_capacity` (smaller is better here, so maximize `-(item - original_capacity)`)\n    # Let's focus on minimizing `potential_remain_cap_vals`.\n    # A good score would be `-(potential_remain_cap_vals)`.\n    # To break ties (same `potential_remain_cap_vals`), we prefer bins that had higher initial capacity.\n    # So, we can add `bins_remain_cap[fit_mask]` as a secondary factor.\n    # This means we want to maximize: `-(potential_remain_cap_vals) + C * bins_remain_cap[fit_mask]`\n    # where C is a small constant to ensure primary sorting is by remaining capacity.\n\n    # Let's use a simpler approach that captures the essence:\n    # Prioritize bins that leave the least remainder.\n    # For ties, pick the one that was originally larger.\n    # Score = -(remaining_capacity) + small_bonus * original_capacity\n\n    # Let's refine the score calculation.\n    # For bins that fit:\n    # Primary goal: Minimize `potential_remain_cap`.\n    # Secondary goal: If `potential_remain_cap` is the same, pick the bin that was originally largest.\n\n    # Score: `item - bins_remain_cap[fit_mask]` works well for tight fits.\n    # The highest score is for `bins_remain_cap[fit_mask] == item`.\n    # If there are multiple such bins, they have the same score.\n    # To break ties, we can add a small value related to the original capacity.\n    # Higher original capacity is preferred for ties in remaining capacity.\n\n    # Let's use the inverse of remaining capacity, but penalize bins that are too large.\n    # The \"perfect fit\" is ideal.\n    # So, let's try a combined score:\n    # - Perfect fit: Highest score (e.g., 100)\n    # - Tight fit (small remaining capacity): Score inversely proportional to remaining capacity.\n    # - Break ties by original capacity: Higher original capacity gets a small bonus.\n\n    # Final approach:\n    # 1. Perfect fits get a significant bonus (e.g., 1000).\n    # 2. Other fits get a score based on inverse of remaining capacity + a small bonus\n    #    for larger original capacity to break ties.\n    #    Score = 1 / (potential_remain_cap + epsilon) + original_capacity * epsilon_small\n    #    where epsilon is for avoiding division by zero and epsilon_small for tie-breaking.\n\n    # Revised attempt focusing on minimizing remaining capacity and then maximizing original capacity for ties.\n    # Score = -(remaining_capacity) + (original_capacity / MaxCapacity) * epsilon_tiebreak\n    # This needs careful scaling.\n\n    # Let's use a scoring that strongly favors perfect fits, then tight fits, and then larger original bins.\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    scores.fill(-np.inf) # Initialize with a very low score\n\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    potential_remain_cap_vals = fitting_bins_capacity - item\n\n    # Perfect fits have the highest priority\n    perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals == 0)[0]\n    if len(perfect_fit_indices_in_fitting) > 0:\n        scores[can_fit_mask][perfect_fit_indices_in_fitting] = 1000.0\n\n    # Tight fits get priority based on negative remaining capacity\n    # Add a small factor to ensure they are ranked below perfect fits but above others.\n    # The secondary criterion: prefer larger original capacity among those with same remainder.\n    # We can add `fitting_bins_capacity` multiplied by a small factor.\n    # Let's combine this into a single score for non-perfect fits.\n    non_perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals > 0)[0]\n    if len(non_perfect_fit_indices_in_fitting) > 0:\n        # Score = -(remaining_capacity) + bonus for larger original capacity\n        # The bonus should be small enough not to override the primary goal of minimizing remainder.\n        # Use a small fraction of max possible capacity as tie-breaker.\n        # Max possible remainder is roughly bin_capacity. So, max of `fitting_bins_capacity` can be used.\n        max_cap_val = np.max(fitting_bins_capacity) if fitting_bins_capacity.size > 0 else 1.0\n        tie_breaker_factor = 1e-6 # A very small factor for tie-breaking\n\n        # Score for non-perfect fits: prioritize less remaining capacity, then more original capacity.\n        scores[can_fit_mask][non_perfect_fit_indices_in_fitting] = \\\n            -(potential_remain_cap_vals[non_perfect_fit_indices_in_fitting]) \\\n            + (fitting_bins_capacity[non_perfect_fit_indices_in_fitting] / max_cap_val) * tie_breaker_factor\n\n    # Ensure perfect fits still have highest priority if they exist\n    # If perfect fits were assigned 1000.0, and non-perfect fits get scores like -0.1 + 0.99 * 1e-6,\n    # this order is maintained.\n\n    # If there are no perfect fits, and multiple bins have the same minimal remainder,\n    # the tie-breaker correctly selects the one with higher original capacity.\n\n    return scores",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tight packing preference with a bonus for perfect fits\n    and a penalty for large wasted space, inspired by heuristic_v1 and heuristic_v18.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Best Fit component - prioritize bins with minimal remaining space.\n    # Add a small epsilon for numerical stability and to ensure positive scores for tight fits.\n    best_fit_score = 1.0 / (remaining_after_fit + 1e-9)\n\n    # Score 2: Penalty for large wasted space.\n    # Scale by the maximum possible original bin capacity to normalize the penalty.\n    # Use a fraction of the maximum possible remaining capacity.\n    max_original_capacity = np.max(bins_remain_cap) if len(bins_remain_cap) > 0 else 1.0\n    large_waste_penalty = - (remaining_after_fit / max_original_capacity) * 5.0\n\n    # Score 3: Bonus for near-perfect fits.\n    # Higher bonus for exact fits, smaller bonus for very small remaining space.\n    near_perfect_threshold = 0.05 * max_original_capacity\n    perfect_fit_bonus = 100.0\n    near_perfect_fit_bonus = 10.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Combine scores: Emphasize tight fits, penalize significant waste, reward perfect fits.\n    # Weights are tuned for a balance.\n    combined_scores = best_fit_score + large_waste_penalty + fit_bonus\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.148384523334677,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": true,
    "obj": 2.572796170721974,
    "SLOC": 28.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a penalty for wasted space with a bonus for achieving near-perfect fits,\n    while also incorporating a slight preference for bins with more remaining capacity\n    to keep options open for larger future items. Exploration is guided by this\n    preference rather than being purely random.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Penalty for wasted space (higher negative score for more waste)\n    # This encourages tighter fits.\n    wasted_space_penalty = -remaining_after_fit * 100.0  # Scale penalty\n\n    # Score 2: Bonus for near-perfect fits.\n    # Reward bins that leave very little space, up to a small tolerance.\n    # Perfect fit bonus is higher than near-perfect fit bonus.\n    near_perfect_fit_threshold = 0.05\n    perfect_fit_bonus = 10.0\n    near_perfect_fit_bonus = 5.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Score 3: Exploration-guided preference for bins with more remaining capacity.\n    # This is a \"soft\" preference, less impactful than tight fitting,\n    # to keep options open for potentially larger items later.\n    # We normalize this to avoid dominating the tight fit score.\n    # Using log to dampen the effect of very large capacities.\n    max_cap_preference = np.log1p(valid_bins_capacities) / np.log1p(np.max(valid_bins_capacities) + 1e-9) * 1.0 # Scaled preference\n\n    # Combine scores: Primarily penalize waste, reward good fits, with a soft exploration bias.\n    # The relative weights can be tuned.\n    combined_scores = wasted_space_penalty + fit_bonus + max_cap_preference\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.10086824335308542, perfect_fit_bonus: float = 0.14134762876801932, large_remainder_penalty_factor: float = 0.0037181076773583456, exploration_noise_scale: float = 0.014738605941995732, exploration_candidate_portion: float = 0.36099361922072437, moderate_capacity_multiplier: float = 1.5506786957399679) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 5.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with guided exploration and waste penalty.\n\n    Prioritizes bins that offer the tightest fit, with a penalty for creating large surpluses.\n    Explores \"good enough\" bins with a probability to promote diversity.\n    \"\"\"\n    epsilon = 0.05\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Primary objective: Tightest fit\n    tightness_score = -remaining_after_fit\n\n    # Secondary objective: Penalize large waste\n    # Scale penalty by item size to be more sensitive to significant leftovers relative to item\n    waste_penalty = (remaining_after_fit / item) * 0.02\n    \n    # Bonus for perfect fit\n    perfect_fit_bonus = np.where(remaining_after_fit == 0, 10.0, 0.0)\n\n    combined_scores = tightness_score - waste_penalty + perfect_fit_bonus\n\n    # Guided Exploration: Select a subset of bins for potential random selection\n    # Consider top-scoring bins and bins with moderate remaining capacity\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.25))\n    top_candidate_indices = sorted_indices_combined[:num_top_bins]\n\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    moderate_candidate_indices = np.where(moderate_capacity_mask)[0]\n    \n    exploration_candidate_indices_in_valid = list(set(top_candidate_indices) | set(moderate_candidate_indices))\n\n    final_scores = np.copy(combined_scores)\n\n    if exploration_candidate_indices_in_valid:\n        exploration_mask_in_valid = np.zeros_like(valid_bins_capacities, dtype=bool)\n        exploration_mask_in_valid[exploration_candidate_indices_in_valid] = True\n        \n        # Apply random exploration score with probability epsilon\n        random_scores = np.random.uniform(-0.5, 0.5, size=len(valid_bins_capacities))\n        \n        use_exploration_prob = np.random.rand(len(valid_bins_capacities)) < epsilon\n        \n        apply_exploration = exploration_mask_in_valid & use_exploration_prob\n        \n        final_scores[apply_exploration] = random_scores[apply_exploration]\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 1.7351416035101808,
    "SLOC": 31.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best-Fit with a bonus for near-perfect fits and a penalty for large remainders.\"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Base priority: favor bins with less remaining capacity (Best Fit)\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Bonus for perfect fits\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.5 # Increased bonus for perfect fit\n\n    # Penalty for bins that leave a large remainder (discourage inefficient packing)\n    large_remainder_mask = (potential_remain_cap > item * 0.75) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 1.0 # Increased penalty for large remainders\n\n    # Small penalty for bins that are tight but not perfect, to slightly favor more room if available\n    # This is implicitly handled by the negative remaining capacity, but can be tuned.\n    # We can also introduce a small bonus for bins that have *some* remaining capacity\n    # but are still a relatively tight fit.\n    tight_but_not_perfect_mask = (potential_remain_cap > 0) & (potential_remain_cap <= item * 0.25)\n    if np.any(tight_but_not_perfect_mask):\n        priorities[can_fit_mask][tight_but_not_perfect_mask] += 0.75 # Moderate bonus for tight, non-perfect fits\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining tight fitting with a penalty for large surpluses\n    and a bonus for perfect fits, with guided exploration.\n    \"\"\"\n    epsilon = 0.08  # Exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Tight Fit - Maximize negative remaining capacity (prioritize minimal remainder)\n    tight_fit_scores = -remaining_after_fit\n\n    # Score 2: Perfect Fit Bonus - Add a bonus for exact fits\n    perfect_fit_bonus = 0.15\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # Score 3: Surplus Penalty - Mild penalty for bins that would have a large surplus\n    # Scaled by item size to make it relative to the item being packed.\n    large_remainder_penalty_factor = 0.002\n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n\n    # Guided Exploration:\n    # Introduce randomness to a subset of \"good enough\" bins.\n    # Candidates are the top-fitting bins and those with moderate remaining capacity.\n    \n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1] # Indices sorted by tight fit score (desc)\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Select top K% of bins for exploration\n    num_top_candidates = max(1, int(len(valid_bins_capacities) * 0.3))\n    exploration_candidate_indices_in_valid.extend(sorted_indices_tight[:num_top_candidates])\n    \n    # Add bins with moderate remaining capacity (e.g., less than twice the item size, but not too small)\n    moderate_capacity_threshold_upper = np.median(valid_bins_capacities) * 1.5 if len(valid_bins_capacities) > 0 else float('inf')\n    moderate_capacity_threshold_lower = item * 1.1 # Avoid bins that are only slightly larger than item\n    \n    moderate_capacity_mask_in_valid = (remaining_after_fit > (item * 0.1)) & \\\n                                      (remaining_after_fit < moderate_capacity_threshold_upper)\n    \n    moderate_capacity_indices_in_valid = np.where(moderate_capacity_mask_in_valid)[0]\n    exploration_candidate_indices_in_valid.extend(moderate_capacity_indices_in_valid)\n    \n    # Remove duplicates and ensure indices are within bounds\n    exploration_candidate_indices_in_valid = np.unique(exploration_candidate_indices_in_valid)\n    exploration_candidate_indices_in_valid = exploration_candidate_indices_in_valid[\n        exploration_candidate_indices_in_valid < len(valid_bins_capacities)\n    ]\n\n    # Generate exploration scores (small random noise)\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.02\n\n    # Combine scores: use exploration score for candidates with probability epsilon, otherwise tight fit score.\n    # For non-candidates, always use the tight fit score.\n    \n    final_scores = np.copy(tight_fit_scores)\n    \n    # Create a mask for the candidate bins within the valid set\n    is_candidate_mask_in_valid = np.zeros(len(valid_bins_capacities), dtype=bool)\n    is_candidate_mask_in_valid[exploration_candidate_indices_in_valid] = True\n    \n    # Decide probabilistically whether to use exploration score for candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply exploration scores to candidates if chosen\n    explore_mask_combined = is_candidate_mask_in_valid & use_exploration_for_candidates\n    final_scores[explore_mask_combined] = exploration_scores[explore_mask_combined]\n\n    # Assign the final scores to the priorities array\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities",
    "response_id": 6,
    "tryHS": true,
    "obj": 35.36098923015558,
    "SLOC": 38.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting (Best Fit) with a penalty for large unused capacity\n    and a bonus for perfect fits, guided by adaptive exploration principles.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity (closer to worst-fit among good fits).\n    # Bonus for perfect fit, penalty for large unused capacity.\n    \n    # Calculate remaining capacity if item fits\n    potential_remain_cap_vals = fitting_bins_capacities - item\n    \n    # Base score: favoring tight fits (smaller remaining capacity)\n    # Use negative remaining capacity to maximize for smaller remainders.\n    scores = -potential_remain_cap_vals\n    \n    # Bonus for perfect fits\n    perfect_fit_mask = np.isclose(potential_remain_cap_vals, 0)\n    scores[perfect_fit_mask] += 10.0 # Significant bonus for perfect fit\n    \n    # Penalty for large unused capacity: Discourage bins that are much larger than needed.\n    # This is a form of guided exploration towards \"good enough\" bins.\n    # We penalize bins where the remaining capacity is significantly larger than a small residual.\n    # A simple penalty could be proportional to the surplus capacity, but we want it to be less\n    # impactful than the tight-fitting score.\n    # Let's add a penalty that decreases the score for bins with large remaining capacity.\n    # This penalty should be smaller than the gains from tight fits.\n    # Penalty factor: For every unit of capacity above a small threshold (e.g., 10% of item size),\n    # we slightly reduce the score.\n    \n    # Define a threshold for \"large unused capacity\"\n    large_capacity_threshold = item * 0.5 # e.g., if remaining capacity is > 50% of item size\n\n    # Calculate penalty: linearly decreasing score for capacity beyond the threshold.\n    # Bins with capacity <= threshold get no penalty.\n    penalty = np.zeros_like(scores)\n    large_surplus_mask = potential_remain_cap_vals > large_capacity_threshold\n    \n    # The penalty is proportional to how much the surplus exceeds the threshold.\n    # We want this penalty to be relatively small compared to the tight-fit scores.\n    # e.g., subtract (surplus - threshold) / max_possible_surplus * small_value\n    max_possible_surplus = np.max(fitting_bins_capacities) # An upper bound on surplus\n    \n    if np.any(large_surplus_mask) and max_possible_surplus > 0:\n        penalty[large_surplus_mask] = (potential_remain_cap_vals[large_surplus_mask] - large_capacity_threshold) / max_possible_surplus * 5.0\n        \n    scores -= penalty\n\n    # Assign the calculated scores to the original bins array\n    priorities[can_fit_mask] = scores\n    \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    epsilon = 0.03\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Multi-objective scoring:\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    \n    tightness_score = -remaining_after_fit\n    \n    # Penalty for large remainders, scaled by item size\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n    \n    # Reward for significant remaining capacity - can be useful for larger items later\n    future_capacity_score = remaining_after_fit / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(remaining_after_fit)\n    \n    # Combine objectives with weights. These weights can be tuned.\n    # We want to strongly favor tightness, moderately avoid waste, and lightly favor future capacity.\n    weight_tightness = 1.0\n    weight_waste = 0.5\n    weight_future_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # Enhanced Exploration:\n    # Instead of random exploration, we can introduce guided exploration.\n    # This means exploring bins that are \"good enough\" but not necessarily the absolute best.\n    # We can define \"good enough\" as bins that fall within a certain percentile of the best fits.\n    \n    # Sort bins by the combined score to identify top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Identify a range of bins to consider for exploration\n    # This could be the top K bins, or bins within a certain score range.\n    # Let's consider bins within the top 30% of scores, or at least the top 3 bins.\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.3))\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Also consider bins that offer a \"balanced\" fit, not too tight, not too empty.\n    # A bin that leaves a moderate amount of space might be more versatile.\n    # Let's consider bins where remaining_after_fit is between a small fraction and a larger fraction of bin capacity.\n    \n    # To define \"moderate\", we can look at the distribution of remaining capacities.\n    # Let's use quartiles for guidance.\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    \n    # Bins with remaining capacity between Q1 and Q3 (inclusive of Q3) are considered moderately remaining.\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    \n    # Combine indices for exploration candidates\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    \n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates\n    # We want these exploration scores to be slightly random but not too high,\n    # to offer a chance for diversity without sacrificing too much performance.\n    exploration_scores = np.random.uniform(-0.1, 0.1, size=len(valid_bins_capacities))\n    \n    # Apply exploration scores:\n    # With probability epsilon, use exploration score for exploration candidates.\n    # Otherwise, use the combined score.\n    # For non-candidates, always use the combined score.\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates\n    exploration_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask[exploration_candidate_indices_in_valid] = True\n    \n    # Decide whether to use exploration score for exploration candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_mask & use_exploration_for_candidates] = exploration_scores[exploration_mask & use_exploration_for_candidates]\n    \n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.0442760271240528,
    "SLOC": 38.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins using a hybrid approach: tightest fit with perfect fit bonus,\n    and guided exploration favoring promising bins, balancing exploitation and exploration.\"\"\"\n\n    epsilon = 0.05  # Probability of exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # --- Exploitation Strategy (Based on Heuristic 15th/17th and 18th/19th) ---\n    # Prioritize perfect fits, then tight fits. Add a small bonus for bins\n    # that are not excessively empty after packing.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Score for tightest fit: maximize negative remaining capacity\n    exploitation_scores = -remaining_after_fit\n    \n    # Bonus for perfect fits (exactly zero remaining capacity)\n    perfect_fit_bonus = 1000.0  # High bonus for exact matches\n    exploitation_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    \n    # Add a small penalty for bins that would have a very large surplus,\n    # encouraging more efficient use of space for the current item.\n    # Scale penalty by item size and bin capacity to make it relative.\n    large_surplus_threshold_ratio = 0.5 # If remaining capacity is more than 50% of item size\n    large_surplus_penalty_factor = 0.1\n    surplus_penalty_mask = remaining_after_fit > (item * large_surplus_threshold_ratio)\n    exploitation_scores[surplus_penalty_mask] -= (remaining_after_fit[surplus_penalty_mask] / item) * large_surplus_penalty_factor\n\n    # --- Exploration Strategy (Guided by Heuristic 1st/6th) ---\n    # Instead of purely random, explore among the 'good enough' bins.\n    # This involves selecting a subset of bins that are either very good fits\n    # or have moderate remaining capacity (potentially useful for future larger items).\n    \n    sorted_indices_exploitation = np.argsort(exploitation_scores)[::-1] # Indices sorted by exploitation score\n    \n    # Candidate selection for exploration:\n    # 1. Top X% of bins based on exploitation score.\n    # 2. Bins with moderate remaining capacity.\n    exploration_candidate_mask = np.zeros_like(exploitation_scores, dtype=bool)\n    \n    num_candidates_from_top = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2))) # Top 20%\n    exploration_candidate_mask[sorted_indices_exploitation[:num_candidates_from_top]] = True\n    \n    # Consider bins that are not too tight, but not too empty.\n    median_capacity = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (remaining_after_fit > item * 0.1) & (remaining_after_fit < median_capacity) # Greater than 10% of item, less than median remaining\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    # Generate random scores for exploration candidates\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Small random noise\n\n    # --- Combine Exploitation and Exploration ---\n    # With probability epsilon, use exploration score for candidates; otherwise, use exploitation.\n    # For non-candidates, always use exploitation score.\n    \n    combined_priorities = np.copy(exploitation_scores)\n    \n    # Decide for each candidate whether to use exploration score\n    use_exploration_decision = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply exploration scores only to candidates selected for exploration AND where exploration is chosen\n    apply_exploration_mask = exploration_candidate_mask & use_exploration_decision\n    combined_priorities[apply_exploration_mask] = exploration_scores[apply_exploration_mask]\n\n    priorities[valid_bins_indices] = combined_priorities\n    \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 1.7251695253290855,
    "SLOC": 30.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the multi-objective scoring of v0 with the refined tie-breaking\n    and perfect-fit bonus of v15. Introduces guided exploration for better balance.\n    \"\"\"\n    epsilon = 0.05  # Probability for exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Multi-objective Scoring (inspired by v0) ---\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    #    Higher score for smaller remaining_after_fit.\n    tightness_score = -remaining_after_fit\n\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    #    This is complementary to tightness. Less impact compared to tightness.\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    #    This helps in potentially fitting larger items later. Scaled by max original capacity.\n    max_original_cap = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0\n    future_capacity_score = remaining_after_fit / max_original_cap\n\n    # Combine objectives with weights. These weights can be tuned.\n    weight_tightness = 1.0\n    weight_waste = 0.3  # Reduced weight for waste avoidance to emphasize tightness\n    weight_future_capacity = 0.2\n\n    combined_scores = (weight_tightness * tightness_score\n                       - weight_waste * waste_avoidance_score\n                       + weight_future_capacity * future_capacity_score)\n\n    # --- Perfect Fit Bonus and Tie-breaking (inspired by v15) ---\n    # Identify perfect fits and assign a very high score to them.\n    perfect_fit_mask_in_valid = remaining_after_fit == 0\n    perfect_fit_bonus = 1000.0  # High bonus for exact fits\n\n    # For non-perfect fits, use the combined score, but add a tie-breaker\n    # that favors bins with larger original capacity if their combined score is similar.\n    # We can incorporate the original capacity as a secondary sorting key by adding a scaled value.\n    # This is less about a fixed bonus and more about sorting preference for similar scores.\n    tie_breaker_scale = 1e-6\n    exploitation_scores = combined_scores + (valid_bins_capacities * tie_breaker_scale)\n    exploitation_scores[perfect_fit_mask_in_valid] = perfect_fit_bonus # Override with bonus\n\n    # --- Guided Exploration (combining v0 and introducing structure) ---\n    # Identify a set of \"promising\" bins for exploration.\n    # These include the top-scoring bins and those with moderate remaining capacity.\n\n    # Sort bins by the exploitation scores to identify top candidates\n    sorted_indices_exploitation = np.argsort(exploitation_scores)[::-1]\n\n    # Consider top 20% of bins or at least the top 3 bins for exploration\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.2))\n    top_candidate_indices_in_valid = sorted_indices_exploitation[:num_top_bins]\n\n    # Identify bins with moderate remaining capacity (between Q1 and Q3)\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n\n    # Combine indices for exploration candidates (unique indices)\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates (small random perturbations)\n    # Use Gaussian noise for smoother exploration if needed, but uniform is fine for now.\n    exploration_scores_perturbation = np.random.uniform(-0.05, 0.05, size=len(valid_bins_capacities))\n\n    final_scores = np.copy(exploitation_scores)\n\n    # Apply exploration scores with probability epsilon to the identified candidates\n    # Create a mask for the identified exploration candidates within the valid bins\n    exploration_candidate_mask_in_valid = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_candidate_mask_in_valid[exploration_candidate_indices_in_valid] = True\n\n    # Randomly decide for each candidate if we use the exploration score\n    use_exploration_decision = np.random.rand(len(valid_bins_capacities)) < epsilon\n\n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_candidate_mask_in_valid & use_exploration_decision] = \\\n        exploitation_scores[exploration_candidate_mask_in_valid & use_exploration_decision] + \\\n        exploration_scores_perturbation[exploration_candidate_mask_in_valid & use_exploration_decision]\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 45.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest-fit preference with a penalty for excessive remaining capacity,\n    and introduces a limited epsilon-greedy exploration for diverse bin selection.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Exploitation: Prioritize bins with minimal remaining capacity (tight fit)\n    # Negative scores ensure higher remaining capacity (smaller value) gets a higher score\n    tight_fit_score = -remaining_after_fit\n    \n    # Penalty for large wasted space: Penalize bins that leave a lot of capacity after fitting.\n    # This encourages using bins more fully. Max value is for normalization.\n    max_remaining_capacity = np.max(bins_remain_cap)\n    if max_remaining_capacity > 0:\n        waste_penalty = -(remaining_after_fit / max_remaining_capacity) * 10.0 # Scaled penalty\n    else:\n        waste_penalty = np.zeros_like(remaining_after_fit)\n\n    # Bonus for near-perfect fits: Reward bins that will be almost full\n    perfect_fit_bonus = 1.0 # A small bonus for bins that will be very tight\n    near_perfect_fit_threshold = 0.05 # Threshold for considering a fit \"near-perfect\"\n    near_perfect_fit_bonus_val = 0.5\n    \n    fit_bonus = np.zeros_like(remaining_after_fit)\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_mask] = near_perfect_fit_bonus_val\n\n    # Combine exploitation scores\n    combined_exploitation_scores = tight_fit_score + waste_penalty + fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins using epsilon-greedy\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Decide whether to use exploration score or exploitation score\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    final_scores = np.where(use_exploration, exploration_scores, combined_exploitation_scores)\n\n    priorities[can_fit_mask] = final_scores\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 2.223773434383721,
    "SLOC": 28.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response2.txt_stdout.txt",
    "code_path": "problem_iter9_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A multi-objective priority function for online Bin Packing.\n    It considers bin tightness, potential for future packing (based on remaining capacity distribution),\n    and introduces an adaptive exploration mechanism favoring bins that are not excessively full or empty.\n    \"\"\"\n    \n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Mask for bins that can fit the current item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n    \n    # --- Objective 1: Tightest Fit (Exploitation) ---\n    # Minimize remaining capacity after packing the item.\n    remaining_after_fit = valid_bins_capacities - item\n    tightness_score = -remaining_after_fit  # Higher score for smaller remaining capacity\n\n    # --- Objective 2: Future Usability (Guided Exploration) ---\n    # Favor bins that, after packing, still have a \"useful\" amount of remaining capacity.\n    # This is relative to the item size itself, aiming to leave space that could accommodate\n    # future items of moderate size. Avoid bins that become too empty.\n    \n    # Define \"useful\" range: between a small fraction and a moderate fraction of the item size.\n    min_useful_surplus_ratio = 0.2 # e.g., at least 20% of item size left\n    max_useful_surplus_ratio = 1.0 # e.g., at most 100% of item size left\n\n    # Calculate surplus relative to item size for bins that are not perfect fits\n    surplus_relative_to_item = np.where(remaining_after_fit > 1e-9, remaining_after_fit / item, 0)\n\n    # Create a score that rewards fitting within the useful surplus range\n    future_usability_score = np.zeros_like(valid_bins_capacities)\n    \n    # Reward bins that leave a moderate surplus\n    moderate_surplus_mask = (surplus_relative_to_item >= min_useful_surplus_ratio) & \\\n                            (surplus_relative_to_item <= max_useful_surplus_ratio)\n    future_usability_score[moderate_surplus_mask] = 1.0 # Base reward for \"good\" surplus\n    \n    # Add a bonus for perfect fits (zero remaining capacity) as they are optimally exploited.\n    perfect_fit_mask = np.abs(remaining_after_fit) < 1e-9\n    future_usability_score[perfect_fit_mask] = 1.5 # Higher reward for perfect fits\n    \n    # Penalize bins that become excessively empty (surplus > max_useful_surplus_ratio)\n    excessively_empty_mask = surplus_relative_to_item > max_useful_surplus_ratio\n    future_usability_score[excessively_empty_mask] = -0.5 # Penalty for leaving too much space\n\n    # --- Objective 3: Adaptive Exploration ---\n    # Introduce a small exploration component. Instead of pure random,\n    # explore among bins that are good candidates based on the other objectives.\n    # We'll use a probability to switch from a combined score to an exploration score.\n    exploration_probability = 0.1 # 10% chance to use exploration score for a candidate\n    \n    # Define candidates for exploration: bins with good tightness or good future usability.\n    # A simple thresholding based on combined scores before adding exploration noise.\n    # Let's combine tightness and usability for initial candidate selection.\n    # Normalize scores to be in a similar range if needed, or use weighted sum.\n    # For simplicity, let's consider bins that are among the top percentile for either metric.\n    \n    # Rank bins by tightness and future usability\n    sorted_indices_tightness = np.argsort(tightness_score)[::-1]\n    sorted_indices_usability = np.argsort(future_usability_score)[::-1]\n    \n    num_candidates = max(1, int(len(valid_bins_capacities) * 0.25)) # Top 25% of bins for consideration\n    \n    exploration_candidate_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    exploration_candidate_mask[sorted_indices_tightness[:num_candidates]] = True\n    exploration_candidate_mask[sorted_indices_usability[:num_candidates]] = True\n    \n    # Generate exploration scores for candidates (small random values to break ties or explore)\n    exploration_scores_for_candidates = np.random.rand(len(valid_bins_capacities)) * 0.05\n    \n    # --- Combining Objectives ---\n    # Weighted sum of exploitation (tightness) and guided exploration (future usability).\n    # We add a slight bias to tightness.\n    weight_tightness = 0.7\n    weight_usability = 0.3\n    \n    combined_exploitation_score = (weight_tightness * tightness_score) + (weight_usability * future_usability_score)\n    \n    # Apply exploration probabilistically\n    final_scores = np.copy(combined_exploitation_score)\n    \n    # Decide for each bin whether to potentially use exploration score\n    use_exploration_roll = np.random.rand(len(valid_bins_capacities)) < exploration_probability\n    \n    # Apply exploration score ONLY to candidates that are selected for exploration AND the roll succeeds\n    apply_exploration_mask = exploration_candidate_mask & use_exploration_roll\n    \n    final_scores[apply_exploration_mask] = exploration_scores_for_candidates[apply_exploration_mask]\n    \n    # Assign the calculated scores back to the original priority array\n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 10.570402871958516,
    "SLOC": 37.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A multi-objective priority function for online bin packing.\n    It balances 'tightest fit' with 'future utility' and guided exploration.\n    It explicitly rewards bins with high remaining capacity for future items\n    and penalizes bins that might become \"awkwardly empty\" after packing.\n    Exploration is focused on bins that are good candidates for either\n    tight fit or future utility, with a bias towards the former.\n    \"\"\"\n    epsilon = 0.1  # Probability of biased exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Multi-Objective Scoring ---\n    # 1. Tightest Fit Score: Maximize -remaining_after_fit. Higher score for less remaining space.\n    tightest_fit_scores = -remaining_after_fit\n\n    # 2. Future Utility Score: Maximize remaining capacity. Higher score for more remaining space.\n    # This encourages leaving larger bins for potentially larger future items.\n    future_utility_scores = remaining_after_fit\n\n    # 3. Perfect Fit Bonus: Add a significant bonus for exact matches.\n    perfect_fit_bonus = 1000.0\n    tightest_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    future_utility_scores[np.abs(remaining_after_fit) < 1e-9] = 0 # No future utility for perfect fit\n\n    # 4. Awkward Empty Penalty: Penalize bins that become \"too empty\" after packing,\n    # relative to the item size. This discourages leaving small unusable remainders.\n    # A bin becoming empty by more than twice the item size is penalized.\n    awkward_empty_threshold_ratio = 2.0\n    awkward_empty_penalty_factor = 0.5\n    awkward_mask = remaining_after_fit > (item * awkward_empty_threshold_ratio)\n    if np.any(awkward_mask):\n        # Penalty is proportional to how much it exceeds the threshold, normalized by item size\n        awkward_penalty = (remaining_after_fit[awkward_mask] / item - awkward_empty_threshold_ratio) * awkward_empty_penalty_factor\n        tightest_fit_scores[awkward_mask] -= awkward_penalty\n        future_utility_scores[awkward_mask] -= awkward_penalty # Penalize future utility as well\n\n    # --- Combined Score ---\n    # A weighted sum of tightest fit and future utility.\n    # Weights can be adjusted. Here, we give slightly more weight to tight fit.\n    tight_fit_weight = 0.6\n    future_utility_weight = 0.4\n    \n    # Normalize scores to prevent one objective from dominating due to scale.\n    # Using robust scaling (median and IQR) might be better, but simple min-max is often sufficient.\n    # We will normalize within the valid bins only.\n    \n    min_tf = np.min(tightest_fit_scores)\n    max_tf = np.max(tightest_fit_scores)\n    range_tf = max_tf - min_tf if max_tf != min_tf else 1.0\n    normalized_tight_fit = (tightest_fit_scores - min_tf) / range_tf if range_tf != 0 else np.zeros_like(tightest_fit_scores)\n\n    min_fu = np.min(future_utility_scores)\n    max_fu = np.max(future_utility_scores)\n    range_fu = max_fu - min_fu if max_fu != min_fu else 1.0\n    normalized_future_utility = (future_utility_scores - min_fu) / range_fu if range_fu != 0 else np.zeros_like(future_utility_scores)\n\n    combined_scores = (tight_fit_weight * normalized_tight_fit) + (future_utility_weight * normalized_future_utility)\n\n    # --- Guided Exploration ---\n    # Exploration will be applied to a subset of bins that are \"promising\" based on the combined score.\n    # \"Promising\" bins are those with high combined scores.\n    \n    sorted_indices_combined = np.argsort(combined_scores)[::-1] # Indices sorted by combined score (descending)\n\n    # Exploration candidates: Top K bins by combined score, plus bins that represent\n    # a good compromise (e.g., not perfectly tight, but good future utility, or vice-versa).\n    \n    exploration_candidate_mask = np.zeros_like(combined_scores, dtype=bool)\n    \n    # Select top 30% of bins as initial candidates for exploration\n    num_top_candidates = min(len(combined_scores), max(1, int(len(combined_scores) * 0.3)))\n    exploration_candidate_mask[sorted_indices_combined[:num_top_candidates]] = True\n\n    # Add bins with high future utility but not necessarily top-tier tight fit,\n    # as these might be useful for larger items that might come later.\n    # Threshold: bins with future utility in the top 50% percentile, but not in the top 20% of combined scores.\n    utility_threshold = np.percentile(future_utility_scores, 50)\n    high_utility_mask = future_utility_scores >= utility_threshold\n    \n    # Exclude already selected top candidates to avoid over-representation\n    already_selected_mask = exploration_candidate_mask\n    potential_explore_candidates = high_utility_mask & ~already_selected_mask\n    \n    # Add a portion of these high-utility candidates\n    num_high_utility_to_add = min(np.sum(potential_explore_candidates), max(0, int(len(combined_scores) * 0.1)))\n    \n    if num_high_utility_to_add > 0:\n        high_utility_indices = np.where(potential_explore_candidates)[0]\n        # Randomly select from potential high utility candidates\n        selected_high_utility_indices = np.random.choice(high_utility_indices, num_high_utility_to_add, replace=False)\n        exploration_candidate_mask[selected_high_utility_indices] = True\n\n    # Generate exploration scores (noisy values for exploration candidates)\n    exploration_scores = np.random.rand(len(combined_scores)) * 0.05 # Small random noise\n\n    # --- Decision: Exploit or Explore ---\n    # With probability epsilon, choose exploration score for selected candidates.\n    # Otherwise, use the combined exploitation score.\n    # For bins not selected as candidates, always use the combined score.\n\n    final_priorities_for_valid_bins = np.copy(combined_scores)\n\n    # For candidates, decide whether to explore\n    should_explore_mask = np.random.rand(len(combined_scores)) < epsilon\n    \n    # Apply exploration score ONLY to candidates where exploration is chosen\n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    # Use exploration score for these bins\n    final_priorities_for_valid_bins[apply_exploration_mask] = exploration_scores[apply_exploration_mask]\n\n    priorities[valid_bins_indices] = final_priorities_for_valid_bins\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.487435181491823,
    "SLOC": 52.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for significant waste and a bonus for future utility,\n    using guided exploration for diversity.\n    \"\"\"\n    epsilon = 0.05  # Probability for exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # --- Multi-objective Scoring ---\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    tightness_score = -remaining_after_fit\n    \n    # 2. Waste avoidance score: Penalize bins that leave a large surplus relative to the item size.\n    # This encourages using bins that are not excessively large for the item.\n    waste_penalty_factor = 0.01\n    # Use max(1, item) to prevent division by zero and handle very small items gracefully.\n    waste_avoidance_score = (remaining_after_fit / np.maximum(1.0, item)) * waste_penalty_factor\n    \n    # 3. Future capacity utility: Reward bins that, after fitting, still offer substantial capacity.\n    # This can be useful for packing larger items later. Normalize by max possible bin capacity for scale.\n    max_bin_capacity = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 1.0\n    future_capacity_score = remaining_after_fit / np.maximum(max_bin_capacity, 1e-9) # Avoid division by zero\n    \n    # Combine objectives with tunable weights\n    weight_tightness = 1.0\n    weight_waste = 0.6  # Slightly more emphasis on avoiding waste than previous example\n    weight_future_capacity = 0.3 # Slightly more emphasis on future utility\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # --- Guided Exploration ---\n    # Identify candidate bins for exploration:\n    # a) Top-scoring bins: Bins that are generally good fits.\n    # b) Bins with moderate remaining capacity: Offer a balance between tight fit and future utility.\n\n    # Sort by combined score to find top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    # Consider top N bins or top X percentile, whichever is more\n    num_top_bins = max(2, int(len(valid_bins_capacities) * 0.25)) # Top 25% or at least 2 bins\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Identify bins with moderate remaining capacity (e.g., between 25th and 75th percentile)\n    if len(valid_bins_capacities) > 1:\n        q1_rem = np.percentile(remaining_after_fit, 25)\n        q3_rem = np.percentile(remaining_after_fit, 75)\n        # Ensure moderate capacity is not too small or too large\n        moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n        moderate_capacity_indices_in_valid = np.where(moderate_capacity_mask)[0]\n    else:\n        moderate_capacity_indices_in_valid = np.array([], dtype=int)\n\n    # Combine unique indices for exploration candidates\n    exploration_candidate_indices_in_valid_set = set(top_candidate_indices_in_valid)\n    exploration_candidate_indices_in_valid_set.update(moderate_capacity_indices_in_valid)\n    exploration_candidate_indices_in_valid = list(exploration_candidate_indices_in_valid_set)\n    \n    # Assign slightly perturbed scores to exploration candidates\n    exploration_scores = np.random.uniform(-0.15, 0.15, size=len(valid_bins_capacities)) # Wider exploration range\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates within the valid subset\n    exploration_mask_for_valid = np.zeros(len(valid_bins_capacities), dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask_for_valid[exploration_candidate_indices_in_valid] = True\n    \n    # Determine which candidates will use exploration scores based on epsilon\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply exploration scores only to selected candidates\n    indices_to_perturb = np.where(exploration_mask_for_valid & use_exploration_for_candidates)[0]\n    final_scores[indices_to_perturb] = exploration_scores[indices_to_perturb]\n    \n    # Map the final scores back to the original bins array\n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 73.28480255285203,
    "SLOC": 43.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a bonus for perfect fits and an epsilon-greedy\n    strategy that prioritizes bins with moderate remaining capacity for exploration.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a bonus for perfect fits.\n    remaining_after_fit = valid_bins_capacities - item\n    tight_fit_scores = -remaining_after_fit\n    perfect_fit_bonus = 1.0  # Higher bonus for exact fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Select a subset of bins for exploration.\n    # Prefer bins with moderate remaining capacity (not too full, not too empty)\n    # to allow for future packing flexibility.\n    exploration_candidates_mask = np.logical_and(remaining_after_fit > 1e-9, remaining_after_fit < np.median(remaining_after_fit[remaining_after_fit > 1e-9] if np.any(remaining_after_fit > 1e-9) else [0]))\n    \n    exploration_scores = np.zeros_like(valid_bins_capacities)\n    if np.any(exploration_candidates_mask):\n        exploration_scores[exploration_candidates_mask] = np.random.rand(np.sum(exploration_candidates_mask))\n    else: # If no moderate bins, explore any fitting bin randomly\n        exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration_mask = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration_mask, exploration_scores, tight_fit_scores)\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 2.303550059832483,
    "SLOC": 22.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more adaptive and multi-objective priority function for online Bin Packing.\n    It considers:\n    1. Tightest fit (minimizing remaining capacity).\n    2. Perfect fit bonus.\n    3. Bin utilization efficiency (how much of the bin is used by the current item).\n    4. A penalty for bins that are too empty relative to the item size.\n    5. Adaptive exploration: prioritizes bins that have previously been \"good\" for items\n       of similar size, guided by a decaying memory of past performance.\n    \"\"\"\n\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # --- Core Scoring Components ---\n\n    # 1. Tightest Fit: Maximize the negative difference (remaining capacity after fit)\n    remaining_after_fit = valid_bins_capacities - item\n    tightest_fit_score = -remaining_after_fit\n\n    # 2. Perfect Fit Bonus\n    perfect_fit_bonus = 1000.0\n    perfect_fit_mask = np.abs(remaining_after_fit) < 1e-9\n    exploitation_scores = tightest_fit_score\n    exploitation_scores[perfect_fit_mask] += perfect_fit_bonus\n\n    # 3. Bin Utilization Efficiency: Reward bins that are well-utilized by the current item.\n    # This prevents picking a very large bin for a small item if a tighter fit exists.\n    # Score is proportional to item_size / bin_capacity.\n    utilization_score = item / (bins_remain_cap[can_fit_mask] + 1e-9) # Add epsilon to avoid division by zero\n    utilization_score_weight = 0.5 # Weight for utilization score\n    exploitation_scores += utilization_score * utilization_score_weight\n\n    # 4. Penalty for overly empty bins (relative to item size):\n    # Avoid bins where the item would occupy a very small fraction of its capacity.\n    # This is an inverse of utilization for very small items.\n    overly_empty_penalty_threshold = 0.1 # If remaining capacity > 10% of bin capacity, apply penalty\n    overly_empty_penalty_factor = 0.8  # Penalty strength\n\n    # Calculate penalty based on remaining capacity relative to the *original* bin capacity.\n    # We want to penalize bins that are large and leave a lot of space *after* the item is placed.\n    # A bin that is already almost full and has little remaining capacity after fitting is good.\n    # We are penalizing bins that are very large and the item is small relative to them.\n    # Here, we look at the residual capacity *after* fitting the item, relative to the *original* bin capacity.\n    # If residual_capacity / original_bin_capacity is large, it means the bin was much larger than needed.\n    overly_empty_mask = (remaining_after_fit / (valid_bins_capacities + 1e-9)) > overly_empty_penalty_threshold\n    overly_empty_penalty = (remaining_after_fit / (valid_bins_capacities + 1e-9)) * overly_empty_penalty_factor\n    exploitation_scores[overly_empty_mask] -= overly_empty_penalty[overly_empty_mask]\n\n\n    # --- Adaptive Exploration Component (Decaying Memory) ---\n    # This part requires state (memory of past performance).\n    # For demonstration, we'll simulate a simple memory. In a real system,\n    # this memory would be updated after each packing decision.\n\n    # Assume `memory_scores` is a numpy array of shape `bins_remain_cap.shape`\n    # where `memory_scores[i]` represents how \"good\" bin `i` has been historically\n    # for items of similar size. A higher score means it was a good fit.\n    # For this function, we'll simulate it.\n    # Let's assume a global state or pass it as an argument in a real scenario.\n    # For this example, we'll just generate a placeholder.\n    # A real implementation would involve a dictionary or a more complex data structure\n    # to store scores based on item sizes.\n\n    # Placeholder for adaptive exploration score. In a real system, this would\n    # be derived from past packing outcomes.\n    # For example, if we sorted bins by their current `exploitation_scores`,\n    # we might have a `past_performance` array for these bins.\n    # We'll assume a slight bias towards bins that were good in the past.\n\n    # Simulate a hypothetical past performance score for the valid bins\n    # Higher means historically good for similar items\n    simulated_past_performance = np.random.rand(len(valid_bins_capacities)) * 0.2 # Small random contribution\n\n    # Combine exploitation score with a scaled adaptive exploration score.\n    # We add a small random noise to the exploitation scores to break ties and\n    # introduce some exploration-like behavior naturally.\n    noise_level = 0.01\n    final_scores = exploitation_scores + (simulated_past_performance * noise_level)\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more adaptive and multi-objective priority function for online Bin Packing.\n    This version incorporates feedback from past packing performance and dynamically\n    adjusts exploration based on bin utilization patterns.\n    \"\"\"\n    \n    epsilon = 0.05  # Base probability of exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n    \n    num_valid_bins = len(valid_bins_capacities)\n    \n    # --- Objective 1: Minimizing Wasted Space (Exploitation - Tightest Fit) ---\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Score for tightest fit: maximize negative remaining capacity\n    # A high score means very little remaining space after fitting.\n    tightness_scores = -remaining_after_fit\n    \n    # Perfect fit bonus\n    perfect_fit_bonus = 100.0  # Reduced bonus to balance with other objectives\n    tightness_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    \n    # Penalty for excessive slack: penalize bins that leave a lot of unused space\n    # relative to the item size. This encourages filling bins more efficiently.\n    slack_penalty_factor = 0.5\n    slack_scores = -(remaining_after_fit / item) * slack_penalty_factor\n    slack_scores[remaining_after_fit < 0] = -np.inf # Ensure only valid fits get scores\n    tightness_scores += slack_scores\n\n    # --- Objective 2: Promoting Future Fit (Exploration - Moderate Capacity) ---\n    # Identify bins that have moderate remaining capacity, which might be useful\n    # for larger items that come later. We want to avoid completely filling bins\n    # or leaving them too empty.\n    \n    # Consider bins with remaining capacity that's not too small and not too large.\n    # A \"good\" remaining capacity might be between 20% of the item size and a\n    # certain fraction of the *original* bin capacity (or a global average capacity).\n    # For simplicity here, let's use a heuristic based on item size and current remaining space.\n    \n    # A bin is \"promising\" if its remaining capacity is at least X% of the item's size\n    # and not excessively large (e.g., less than half of its *current* remaining capacity).\n    # The idea is to keep some moderate space available.\n    promising_capacity_bonus_factor = 0.1\n    \n    # Calculate a score based on how \"balanced\" the remaining capacity is.\n    # This favors bins where remaining capacity is a reasonable fraction of the bin's *current* capacity.\n    # Normalize remaining capacity by the *original* capacity of the bin it could fit into.\n    # Assuming a fixed bin capacity 'B' would be better, but without it, we use current.\n    # For this implementation, let's consider remaining_after_fit itself.\n    # Bins with remaining_after_fit between 0.2 * item and 0.6 * (some typical bin capacity proxy like median valid bin capacity)\n    \n    # Use a proxy for \"ideal\" remaining capacity \u2013 perhaps the median remaining space among fitting bins.\n    median_remaining_space_proxy = np.median(valid_bins_capacities) # Proxy for original bin capacity\n    \n    # Score based on how close remaining_after_fit is to a 'moderately useful' value.\n    # Let's define 'moderately useful' as being between 0.2 * item and 0.5 * median_remaining_space_proxy.\n    moderate_remaining_scores = np.zeros(num_valid_bins)\n    \n    lower_bound_moderate = 0.2 * item\n    upper_bound_moderate = 0.5 * median_remaining_space_proxy\n    \n    # Create masks for promising capacity\n    is_moderate_capacity = (remaining_after_fit >= lower_bound_moderate) & (remaining_after_fit <= upper_bound_moderate)\n    \n    # Assign a bonus to bins falling into this moderate range.\n    # The bonus is higher if it's closer to the 'ideal' middle of this range.\n    if np.any(is_moderate_capacity):\n        moderate_bins_remaining = remaining_after_fit[is_moderate_capacity]\n        # Calculate score based on distance from the midpoint of the moderate range\n        mid_moderate_range = (lower_bound_moderate + upper_bound_moderate) / 2.0\n        distance_from_mid = np.abs(moderate_bins_remaining - mid_moderate_range)\n        # Normalize distance so smaller distance gives higher score (less penalty)\n        max_distance = max(mid_moderate_range - lower_bound_moderate, upper_bound_moderate - mid_moderate_range)\n        normalized_distance = distance_from_mid / max_distance if max_distance > 0 else np.zeros_like(distance_from_mid)\n        \n        moderate_remaining_scores[is_moderate_capacity] = (1.0 - normalized_distance) * promising_capacity_bonus_factor * item\n\n    # --- Adaptive Exploration Probability ---\n    # Adjust epsilon based on how \"challenging\" the current state is.\n    # If many bins are very full or very empty, we might explore more to find a good fit.\n    # If there are many \"perfect\" or \"tight\" fits, exploration might be less critical.\n    \n    # Heuristic: If the proportion of bins with very little remaining capacity is high,\n    # increase exploration to find a slightly less tight fit that might be better for future items.\n    # Or, if there are very few good fits available.\n    \n    # Calculate a \"difficulty\" metric\n    # Difficulty can be related to the variance of remaining capacities, or the scarcity of good fits.\n    # Let's use the scarcity of tight fits (e.g., remaining_after_fit < 0.1 * item).\n    num_tight_fits = np.sum(remaining_after_fit < 0.1 * item)\n    tight_fit_ratio = num_tight_fits / num_valid_bins if num_valid_bins > 0 else 0\n    \n    # If tight fits are rare (<20% of valid bins), slightly increase exploration.\n    adaptive_epsilon = epsilon\n    if tight_fit_ratio < 0.2:\n        adaptive_epsilon = min(epsilon * 1.5, 0.2) # Cap exploration increase\n    \n    # If there are many bins with very large remaining capacity (e.g., > 0.8 * original capacity proxy),\n    # maybe reduce exploration slightly as there are plenty of options.\n    num_large_remaining = np.sum(remaining_after_fit > 0.8 * median_remaining_space_proxy)\n    large_remaining_ratio = num_large_remaining / num_valid_bins if num_valid_bins > 0 else 0\n    \n    if large_remaining_ratio > 0.5:\n        adaptive_epsilon = max(epsilon * 0.7, 0.01) # Cap exploration decrease\n\n    # --- Combining Objectives ---\n    # Weighted sum of objectives. The weights can be dynamic, but fixed here for simplicity.\n    # Weight for tightness: higher, as it's the primary goal.\n    # Weight for moderate capacity: lower, as it's secondary/exploratory.\n    weight_tightness = 0.8\n    weight_moderate_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_scores) + (weight_moderate_capacity * moderate_remaining_scores)\n    \n    # --- Guided Exploration ---\n    # Instead of purely random exploration, we explore among bins that are \"good enough\"\n    # but not necessarily the absolute best according to the combined score.\n    # We sample from a subset of bins, potentially those with moderate capacity or decent tightness.\n    \n    # Create a candidate pool for exploration:\n    # 1. Bins with moderate remaining capacity.\n    # 2. Bins that are reasonably tight but not perfect fits.\n    \n    exploration_candidate_mask = np.zeros(num_valid_bins, dtype=bool)\n    \n    # Add bins with moderate capacity to exploration candidates\n    exploration_candidate_mask[is_moderate_capacity] = True\n    \n    # Add a portion of the \"tightest\" bins (but not perfect fits) as exploration candidates\n    # We want to explore slightly less optimal tight fits.\n    sorted_indices_tightness = np.argsort(tightness_scores)[::-1] # Descending for best tightness\n    num_tight_candidates = min(num_valid_bins, max(1, int(num_valid_bins * 0.15))) # Top 15% tightest bins\n    \n    # Exclude perfect fits from this exploration subset to avoid over-sampling them\n    tight_candidates_indices = sorted_indices_tightness[:num_tight_candidates]\n    tight_candidates_are_perfect = np.abs(remaining_after_fit[tight_candidates_indices]) < 1e-9\n    valid_tight_candidates_indices = tight_candidates_indices[~tight_candidates_are_perfect]\n    \n    exploration_candidate_mask[valid_tight_candidates_indices] = True\n    \n    # Generate random exploration scores for selected candidates\n    # These scores are added to the combined scores with a certain probability (adaptive_epsilon)\n    exploration_noise = np.random.rand(num_valid_bins) * 0.05 # Small random noise to break ties and enable exploration\n    \n    # Apply exploration noise to selected candidates with probability adaptive_epsilon\n    should_explore_mask = np.random.rand(num_valid_bins) < adaptive_epsilon\n    \n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    # Add exploration noise to the combined scores for selected bins\n    combined_scores[apply_exploration_mask] += exploration_noise[apply_exploration_mask]\n    \n    # Final priorities are the combined scores\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 56.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins using an adaptive multi-objective approach:\n    Balances tightest fit, perfect fit bonus, and a novel \"capacity utilization gradient\"\n    to favor bins that are likely to accommodate future items efficiently.\n    Includes a mechanism for dynamically adjusting exploration based on item size variance.\n    \"\"\"\n\n    epsilon_base = 0.1  # Base probability of exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # --- Objective 1: Tightest Fit & Perfect Fit ---\n    remaining_after_fit = valid_bins_capacities - item\n    tight_fit_score = -remaining_after_fit\n    perfect_fit_bonus = 1000.0\n    tight_fit_score[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # --- Objective 2: Capacity Utilization Gradient ---\n    # Favor bins that, after packing the current item, leave a remaining capacity\n    # that is a \"good\" fit for a range of potential future item sizes.\n    # We can approximate this by looking at the distribution of remaining capacities.\n    # A bin that leaves capacity close to the median remaining capacity (among eligible bins)\n    # might be good for future medium-sized items.\n\n    # Calculate variance of item sizes encountered so far (requires external state, simulating here)\n    # In a real implementation, this would be a class member or passed parameter.\n    # For this example, we'll use a placeholder.\n    # Assuming historical item sizes were [0.2, 0.5, 0.1, 0.8, 0.3]\n    # Current item is 'item'. Let's assume a mix of small and large items are common.\n    # A simpler proxy: use variance of *current* remaining capacities to infer future item patterns.\n    current_remaining_variance = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # If variance is high, explore more. If low, exploit tighter fits.\n    exploration_boost_factor = min(1.0, np.sqrt(current_remaining_variance) / 0.5) # Scale boost by variance, capped\n\n    # Create a score based on how \"useful\" the remaining capacity is.\n    # We want remaining capacity that is not too small (useless) and not too large (wasteful).\n    # A simple approach: penalize extreme remaining capacities.\n    # Let's consider remaining capacities relative to the *maximum possible remaining capacity*\n    # or median remaining capacity of eligible bins.\n    if len(valid_bins_capacities) > 1:\n        median_remaining_eligible = np.median(remaining_after_fit)\n        # Score is higher if remaining_after_fit is close to median_remaining_eligible\n        utilization_score = -np.abs(remaining_after_fit - median_remaining_eligible)\n    else:\n        utilization_score = np.zeros_like(remaining_after_fit) # No comparison possible\n\n    # Normalize and scale utilization score\n    if np.ptp(utilization_score) > 1e-9: # Avoid division by zero if all scores are same\n        utilization_score = (utilization_score - np.min(utilization_score)) / np.ptp(utilization_score)\n    else:\n        utilization_score = np.zeros_like(utilization_score)\n\n    # Weighting the objectives. Tight fit is primary, utilization is secondary.\n    # The weight of utilization can be influenced by exploration tendency.\n    utilization_weight = 0.1 + 0.2 * exploration_boost_factor # Additive boost based on variance\n    combined_exploitation_scores = tight_fit_score + utilization_score * utilization_weight\n\n    # --- Adaptive Exploration ---\n    # Explore more aggressively if the current item is small relative to bin capacity,\n    # or if there's high variance in available capacities, suggesting uncertainty.\n    # The exploration probability should be dynamic.\n    # Let's define \"small item\" as less than 25% of average available capacity.\n    avg_available_capacity = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    is_small_item = (item / avg_available_capacity < 0.25) if avg_available_capacity > 0 else False\n\n    # Dynamic epsilon: higher if item is small, or if variance is high\n    epsilon = epsilon_base + 0.1 * (exploration_boost_factor) + (0.15 if is_small_item else 0)\n    epsilon = min(epsilon, 0.8) # Cap exploration probability\n\n    # Select candidates for exploration:\n    # 1. Top-performing bins based on combined_exploitation_scores.\n    # 2. Bins that have moderate remaining capacity (less than half of the item's size, but not perfect fit).\n    sorted_indices_exploitation = np.argsort(combined_exploitation_scores)[::-1]\n\n    exploration_candidate_mask = np.zeros_like(combined_exploitation_scores, dtype=bool)\n    num_top_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.15))) # Top 15%\n    exploration_candidate_mask[sorted_indices_exploitation[:num_top_candidates]] = True\n\n    moderate_remaining_mask = (remaining_after_fit > item * 0.1) & (remaining_after_fit < item * 0.5)\n    exploration_candidate_mask[moderate_remaining_mask] = True\n\n    # Generate exploration scores (random, but biased by candidates)\n    exploration_scores_raw = np.random.rand(len(valid_bins_capacities))\n\n    # Apply exploration scores only to selected candidates with a certain probability\n    final_scores = np.copy(combined_exploitation_scores)\n    exploration_decision_mask = np.random.rand(len(valid_bins_capacities)) < epsilon\n\n    # For bins that are candidates AND we decide to explore: use the exploration score\n    apply_exploration_mask = exploration_candidate_mask & exploration_decision_mask\n    final_scores[apply_exploration_mask] = np.maximum(final_scores[apply_exploration_mask], exploration_scores_raw[apply_exploration_mask] * 0.05) # Scale exploration noise\n\n    # For bins that are candidates but we DON'T explore: stick to exploitation score.\n    # For bins that are NOT candidates: stick to exploitation score.\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 5.634224172317511,
    "SLOC": 42.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A more adaptive and multi-objective priority function for Online Bin Packing.\n    It prioritizes bins based on a weighted combination of several factors:\n    1. Tightest Fit: Minimizes remaining capacity after packing.\n    2. Perfect Fit Bonus: Rewards exact matches.\n    3. Space Utilization: Rewards bins that would have a low surplus relative to their original capacity.\n    4. Promising Capacity: Identifies bins that might be good for future larger items.\n    5. Adaptive Exploration: Selects a subset of potentially good bins and perturbs their scores slightly\n       to encourage exploration of less obvious but potentially good options.\n    \"\"\"\n\n    epsilon_exploration_rate = 0.1  # Base probability for exploration\n    perfect_fit_bonus = 1000.0\n    surplus_penalty_weight = 0.5\n    promising_capacity_threshold = 0.6  # Bins with at least this fraction of original capacity remaining might be promising\n    exploration_perturbation_scale = 0.05 # Scale of random noise for exploration\n\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Score Component 1: Tightest Fit ---\n    # Maximize the negative remaining capacity (minimizes positive remaining capacity)\n    tightest_fit_scores = -remaining_after_fit\n\n    # --- Score Component 2: Perfect Fit Bonus ---\n    perfect_fit_mask = np.abs(remaining_after_fit) < 1e-9\n    perfect_fit_scores = np.zeros_like(tightest_fit_scores)\n    perfect_fit_scores[perfect_fit_mask] = perfect_fit_bonus\n\n    # --- Score Component 3: Space Utilization (Penalize large surpluses) ---\n    # Penalize bins where remaining capacity is a large fraction of the bin's original capacity\n    # Here, we use the original capacity of the bin *before* the current item was considered.\n    # To do this, we need the original capacities of the bins that *can* fit the item.\n    # We'll approximate this by assuming the initial capacity was slightly larger than current remaining.\n    # A better approach would be to store original capacities or a history.\n    # For this heuristic, we'll use `valid_bins_capacities` as a proxy for \"capacity that could hold item\"\n    # and penalize if `remaining_after_fit` is large *relative to the capacity that can hold the item*.\n    # This encourages using bins that are \"just big enough\".\n    large_surplus_threshold_ratio = 0.7  # If remaining capacity is more than 70% of the capacity that can fit the item\n    large_surplus_penalty_factor = 0.2\n    surplus_penalty_mask = remaining_after_fit > (valid_bins_capacities * large_surplus_threshold_ratio)\n    # The penalty is proportional to how much *more* empty space there is than needed.\n    surplus_penalty = (remaining_after_fit[surplus_penalty_mask] - (valid_bins_capacities[surplus_penalty_mask] * large_surplus_threshold_ratio)) / valid_bins_capacities[surplus_penalty_mask] * large_surplus_penalty_factor\n    space_utilization_scores = np.zeros_like(tightest_fit_scores)\n    space_utilization_scores[surplus_penalty_mask] = -surplus_penalty\n\n    # --- Score Component 4: Promising Capacity (Favor bins with decent remaining space) ---\n    # Favor bins that have a substantial amount of capacity left, potentially useful for future items.\n    # This is a bit counter-intuitive to tightest-fit, so we'll give it a moderate score boost.\n    # The idea is to not always pick the absolute tightest if it leaves *too little* space.\n    # We'll favor bins where remaining capacity is between a small buffer and a larger portion.\n    # Let's define \"promising\" as having remaining capacity between `item * 1.1` and `original_capacity * promising_capacity_threshold`.\n    # Using `valid_bins_capacities` as original capacity for this step.\n    min_promising_lower_bound = item * 1.1 # At least a small buffer beyond the item size\n    promising_capacity_mask = (remaining_after_fit >= min_promising_lower_bound) & (remaining_after_fit < valid_bins_capacities * promising_capacity_threshold)\n    promising_capacity_scores = np.zeros_like(tightest_fit_scores)\n    promising_capacity_scores[promising_capacity_mask] = 0.5 # A moderate boost\n\n    # --- Combine Scores ---\n    # Weighted sum of the components. Weights can be tuned.\n    # We want tightest fit and perfect fit to dominate, then utilization, then promising.\n    exploitation_scores = (\n        tightest_fit_scores * 1.0 +\n        perfect_fit_scores * 1.0 +\n        space_utilization_scores * 0.8 +\n        promising_capacity_scores * 0.5\n    )\n\n    # --- Adaptive Exploration ---\n    # Introduce a chance to explore among a subset of \"good\" bins.\n    # \"Good\" bins are those that are not the absolute worst fits.\n    sorted_indices_exploitation = np.argsort(exploitation_scores)[::-1] # Indices sorted by overall exploitation score\n\n    # Select candidates for exploration: Top N bins by exploitation score, and bins that are \"reasonably good\" fits.\n    num_candidates_from_top = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.15))) # Top 15%\n    exploration_candidate_mask = np.zeros_like(exploitation_scores, dtype=bool)\n    exploration_candidate_mask[sorted_indices_exploitation[:num_candidates_from_top]] = True\n\n    # Also consider bins that are not extremely tight but not too loose either (moderately filled)\n    # Let's define moderate as having remaining capacity between `item * 1.2` and `valid_bins_capacities * 0.8`\n    moderate_fit_lower_bound = item * 1.2\n    moderate_fit_upper_bound = valid_bins_capacities * 0.8\n    moderate_fit_mask = (remaining_after_fit >= moderate_fit_lower_bound) & (remaining_after_fit <= moderate_fit_upper_bound)\n    exploration_candidate_mask[moderate_fit_mask] = True\n\n    # Generate small random perturbations for exploration candidates\n    exploration_perturbations = (np.random.rand(len(valid_bins_capacities)) - 0.5) * exploration_perturbation_scale\n\n    # Determine which candidates will actually be perturbed based on epsilon\n    num_candidates_to_perturb = int(np.sum(exploration_candidate_mask) * epsilon_exploration_rate)\n    indices_to_perturb_candidates = np.where(exploration_candidate_mask)[0]\n    if num_candidates_to_perturb > 0 and len(indices_to_perturb_candidates) > 0:\n        perturb_indices_in_candidates = np.random.choice(len(indices_to_perturb_candidates), num_candidates_to_perturb, replace=False)\n        actual_perturb_mask = np.zeros_like(exploitation_scores, dtype=bool)\n        actual_perturb_mask[indices_to_perturb_candidates[perturb_indices_in_candidates]] = True\n        \n        combined_priorities = np.copy(exploitation_scores)\n        combined_priorities[actual_perturb_mask] += exploration_perturbations[actual_perturb_mask]\n    else:\n        combined_priorities = exploitation_scores\n\n    priorities[valid_bins_indices] = combined_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 54.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  }
]