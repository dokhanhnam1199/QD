{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Softmax-Based Fit strategy for online Bin Packing Problem.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a 'fit' score. We prefer bins with less remaining capacity that can still fit the item.\n    # This encourages using bins more fully before opening new ones.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size remaining.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Apply softmax to convert fit scores into probabilities (priorities)\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Create an output array of the same size as the original bins_remain_cap\n    # and place the calculated priorities in the correct positions.\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** These heuristics are identical. The ranking suggests a potential issue with the evaluation or the ranking itself. However, assuming they represent a valid point in the ranking, their complexity and explicit handling of \"exploration candidates\" (top K fits and moderate capacity bins) is a notable design choice. The \"surplus penalty\" is a nuanced addition to pure tight-fitting.\n\n*   **Heuristics 2 vs 3 & 4:** Heuristics 3 and 4 are identical and simpler than Heuristic 2. They implement a basic epsilon-greedy approach (random choice vs. tight fit). Heuristic 2's \"adaptive exploration\" is more sophisticated, trying to guide exploration towards \"good enough\" bins rather than purely random ones. The penalty for large surpluses in Heuristic 2 also adds a layer of strategic thinking. The simpler epsilon-greedy in 3 & 4 is less likely to discover complex packing patterns.\n\n*   **Heuristics 3 & 4 vs 5 & 6:** Heuristics 5 and 6 are also simpler than 2 but introduce different mechanisms. Heuristic 6 is a pure \"Best Fit\" (minimizing remaining space). Heuristic 5 uses a softmax approach, turning \"fit scores\" into probabilities. This is an interesting probabilistic exploration. Comparing 3/4 (epsilon-greedy) to 5 (softmax) and 6 (pure best fit), the epsilon-greedy approach (3/4) is more direct in its exploration strategy. Best Fit (6) is purely exploitative, while Softmax (5) introduces a probabilistic distribution over good fits. The ranking suggests that these more direct or probabilistic approaches are less effective than the hybrid ones.\n\n*   **Heuristics 6 vs 7 & 8 & 9:** Heuristics 7, 8, and 9 are similar, focusing on tight fits and bonuses for perfect fits. Heuristic 6 (pure Best Fit) is simpler. Heuristics 7-9 add specific bonuses and sometimes penalties. Heuristic 8 & 9 are identical. The ranking indicates that adding these specific bonuses (like for perfect fits) and penalties (like for large remainders) improves performance over pure Best Fit. Heuristic 7 is more basic than 8/9.\n\n*   **Heuristics 7 vs 10 & 11:** Heuristic 10 is identical to 5. Heuristic 11 uses \"Almost Full Fit\" by maximizing `item - bins_remain_cap`, which is equivalent to minimizing `bins_remain_cap - item` (Best Fit). It does not explicitly add bonuses or penalties. The ranking suggests that the more nuanced approaches (like those in 7-9) are better than this direct Best Fit implementation.\n\n*   **Heuristics 11 vs 12:** Heuristic 12 is the most complex, combining penalties for wasted space, bonuses for near-perfect fits, and a \"soft\" preference for bins with more capacity (exploration-guided). Its higher ranking suggests that this multi-objective optimization within the heuristic is effective. Heuristic 11 is a simpler Best Fit variant.\n\n*   **Heuristics 12 vs 13:** Heuristic 13 is an FFD-inspired heuristic, prioritizing tight fits with a small penalty for slight oversights. It's simpler than 12. The ranking places 12 significantly higher, implying its combined strategy is superior.\n\n*   **Heuristics 13 vs 14 & 15 & 16:** Heuristic 14 is a simple Best Fit with explicit probabilities, but the loop implementation is inefficient. Heuristics 15 and 16 are identical and appear to be optimized versions of Heuristic 2, with hyperparameter tuning evident (e.g., `epsilon`, `perfect_fit_bonus`). These are still ranked lower than 13, which might indicate that the specific parameter values or the overall strategy of 13 is slightly better than the tuned 2. The presence of `torch` and `scipy` imports in 14-16, but not used in the snippet, is odd; they might be remnants of a larger framework or an indication of potential future extensions. The ranking suggests that even tuned versions of simpler hybrid strategies are not as good as the complex one (12) or even the FFD-inspired one (13).\n\n*   **Heuristics 15 & 16 vs 17 & 18:** Heuristics 17 and 18 are identical \"Random Fit\" strategies. They simply assign random priorities to bins that can fit the item. These are the simplest and worst-performing strategies, as they lack any exploitation logic. The ranking clearly places them at the bottom.\n\n*   **Heuristics 17 & 18 vs 19 & 20:** Heuristic 19 aims to combine perfect fits, tight fits, and tie-breaking based on original capacity. Heuristic 20 tries to balance tight fitting with penalties for small residuals and adds exploration. The ranking places 19 and 20 above random fit but below the more sophisticated hybrid strategies. Heuristic 19's tie-breaking logic is a good addition. Heuristic 20's penalty for small residuals is an interesting refinement.\n\n*   **Overall:** The top-performing heuristics (1, 2, 12) are complex, often combining tight-fitting principles with sophisticated exploration strategies (adaptive exploration, multi-objective scoring). Simpler \"Best Fit\" or epsilon-greedy approaches are ranked lower. Pure random strategies are at the bottom. The use of explicit bonuses and penalties for specific fit scenarios (perfect fit, large/small residuals) appears beneficial. The ranking suggests a hierarchy: complex hybrids > refined hybrids/FFD-inspired > basic hybrids/probabilistic > simple exploitation (Best Fit) > random.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, guided exploration, adaptive reward functions, hybrid strategies.\n*   **Advice:** Focus on iterative refinement of multi-objective heuristics, dynamically adjusting exploration/exploitation based on problem state.\n*   **Avoid:** Over-reliance on static or purely greedy approaches; neglecting the interplay between penalizing poor fits and rewarding ideal fits.\n*   **Explanation:** By viewing heuristic design as a continuous learning process, you can better identify optimal trade-offs between thorough exploration and efficient exploitation, leading to more robust and performant solutions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}