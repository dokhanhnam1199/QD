{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tightest fit with a penalty for significant waste and a bonus for future utility,\n    using guided exploration for diversity.\n    \"\"\"\n    epsilon = 0.05  # Probability for exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # --- Multi-objective Scoring ---\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    tightness_score = -remaining_after_fit\n    \n    # 2. Waste avoidance score: Penalize bins that leave a large surplus relative to the item size.\n    # This encourages using bins that are not excessively large for the item.\n    waste_penalty_factor = 0.01\n    # Use max(1, item) to prevent division by zero and handle very small items gracefully.\n    waste_avoidance_score = (remaining_after_fit / np.maximum(1.0, item)) * waste_penalty_factor\n    \n    # 3. Future capacity utility: Reward bins that, after fitting, still offer substantial capacity.\n    # This can be useful for packing larger items later. Normalize by max possible bin capacity for scale.\n    max_bin_capacity = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 1.0\n    future_capacity_score = remaining_after_fit / np.maximum(max_bin_capacity, 1e-9) # Avoid division by zero\n    \n    # Combine objectives with tunable weights\n    weight_tightness = 1.0\n    weight_waste = 0.6  # Slightly more emphasis on avoiding waste than previous example\n    weight_future_capacity = 0.3 # Slightly more emphasis on future utility\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # --- Guided Exploration ---\n    # Identify candidate bins for exploration:\n    # a) Top-scoring bins: Bins that are generally good fits.\n    # b) Bins with moderate remaining capacity: Offer a balance between tight fit and future utility.\n\n    # Sort by combined score to find top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    # Consider top N bins or top X percentile, whichever is more\n    num_top_bins = max(2, int(len(valid_bins_capacities) * 0.25)) # Top 25% or at least 2 bins\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Identify bins with moderate remaining capacity (e.g., between 25th and 75th percentile)\n    if len(valid_bins_capacities) > 1:\n        q1_rem = np.percentile(remaining_after_fit, 25)\n        q3_rem = np.percentile(remaining_after_fit, 75)\n        # Ensure moderate capacity is not too small or too large\n        moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n        moderate_capacity_indices_in_valid = np.where(moderate_capacity_mask)[0]\n    else:\n        moderate_capacity_indices_in_valid = np.array([], dtype=int)\n\n    # Combine unique indices for exploration candidates\n    exploration_candidate_indices_in_valid_set = set(top_candidate_indices_in_valid)\n    exploration_candidate_indices_in_valid_set.update(moderate_capacity_indices_in_valid)\n    exploration_candidate_indices_in_valid = list(exploration_candidate_indices_in_valid_set)\n    \n    # Assign slightly perturbed scores to exploration candidates\n    exploration_scores = np.random.uniform(-0.15, 0.15, size=len(valid_bins_capacities)) # Wider exploration range\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates within the valid subset\n    exploration_mask_for_valid = np.zeros(len(valid_bins_capacities), dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask_for_valid[exploration_candidate_indices_in_valid] = True\n    \n    # Determine which candidates will use exploration scores based on epsilon\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply exploration scores only to selected candidates\n    indices_to_perturb = np.where(exploration_mask_for_valid & use_exploration_for_candidates)[0]\n    final_scores[indices_to_perturb] = exploration_scores[indices_to_perturb]\n    \n    # Map the final scores back to the original bins array\n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Second Best): They are identical.\nComparing Heuristic 1 vs Heuristic 3: They are identical.\nComparing Heuristic 1 vs Heuristic 4: Heuristic 1 introduces a hybrid approach with exploration (epsilon probability) and guided candidate selection, whereas Heuristic 4 is purely deterministic best-fit with a penalty for large remainders. Heuristic 1's approach is more sophisticated in balancing immediate needs with future possibilities.\nComparing Heuristic 1 vs Heuristic 5: Heuristic 1 uses a probability-based exploration with candidate selection, while Heuristic 5 attempts to integrate exploration by preferring bins with more remaining capacity, but without a clear probabilistic mechanism. Heuristic 1's explicit exploration is likely more effective.\nComparing Heuristic 1 vs Heuristic 6: Heuristic 1 is a hybrid of tightest fit with exploration. Heuristic 6 is multi-objective, incorporating utilization and adaptive exploration (though the adaptive part is simulated). Heuristic 6 attempts a more holistic approach by considering bin utilization.\nComparing Heuristic 1 vs Heuristic 7: Heuristic 1 is a simpler hybrid. Heuristic 7 builds on multi-objective scoring, adaptive exploration probability, and identifies candidates based on both tightness and moderate capacity, making it more nuanced than Heuristic 1.\nComparing Heuristic 7 vs Heuristic 8: Heuristic 8 builds on Heuristic 7's multi-objective scoring but refines it with better tie-breaking and exploration based on candidate selection and perturbations. Heuristic 8 seems to have a more structured approach to balancing objectives.\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 is a complex multi-objective heuristic with exploration. Heuristic 9 is a very simple heuristic that prioritizes bins with small remaining capacity (inverse of remaining capacity) and gives a bonus to perfect fits. Heuristic 8 is significantly more advanced.\nComparing Heuristic 9 vs Heuristic 10: Heuristics 9 and 10 are almost identical, with Heuristic 9 having a slight preference for perfect fits (score of 1.0 vs inverse of remainder) and Heuristic 10 purely using inverse of remainder. Both are simple \"best-fit\" variations.\nComparing Heuristic 10 vs Heuristic 11: Heuristic 10 is a simple inverse remainder score. Heuristic 11 is also best-fit focused but adds a tie-breaker favoring larger original capacity and a specific score for perfect fits. Heuristic 11 is more refined for tie-breaking.\nComparing Heuristic 11 vs Heuristic 12: Heuristic 11 is a best-fit with tie-breaking. Heuristic 12 introduces several tunable parameters for exploration probability, bonuses, penalties, and candidate selection, indicating a more experimental and potentially optimized approach to balancing objectives.\nComparing Heuristic 12 vs Heuristic 13: Heuristic 12 is a complex multi-objective heuristic. Heuristic 13 is a \"Random Fit\" strategy, assigning random priorities to bins that can fit the item. This is a very basic approach compared to Heuristic 12.\nComparing Heuristic 13 vs Heuristic 14: Heuristics 13 and 14 are identical \"Random Fit\" strategies.\nComparing Heuristic 14 vs Heuristic 15: Heuristic 14 is random. Heuristic 15 is a complex multi-objective heuristic with adaptive exploration based on item size variance and capacity utilization gradient, significantly more sophisticated than random.\nComparing Heuristic 15 vs Heuristic 16: Heuristics 15 and 16 are identical.\nComparing Heuristic 16 vs Heuristic 17: Heuristic 16/15 is multi-objective with adaptive exploration based on variance and utilization gradient. Heuristic 17 is also multi-objective, focusing on tightness, future usability (relative to item size), and adaptive exploration for candidates. Heuristic 16/15 seems to have a more defined adaptive exploration mechanism.\nComparing Heuristic 17 vs Heuristic 18: Heuristics 17 and 18 are identical.\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 is multi-objective with adaptive exploration. Heuristic 19 combines tightness, waste avoidance, future utility, and guided exploration with perturbated scores for candidates. Heuristic 19's exploration is more about perturbing scores of selected candidates.\nComparing Heuristic 19 vs Heuristic 20: Heuristics 19 and 20 are identical.\nOverall: The top heuristics (1-8, 15-18) are complex multi-objective strategies that balance tightest fit with some form of future utility or guided exploration. Heuristics 9-11 and 13-14 are simpler best-fit or random strategies. The intermediate heuristics (12, 19-20) attempt variations on multi-objective and exploration. Heuristics 1, 7, 8, 15, 16, 17, 18 appear to represent the most developed ideas, blending multiple objectives and adaptive/guided exploration.\n- \nHere's a redefinition of \"Current self-reflection\" to guide heuristic design, avoiding ineffective approaches:\n\n*   **Keywords:** Multi-objective, adaptive exploration, sophisticated scoring, penalized residuals, rewarded fits.\n*   **Advice:** Design heuristics that dynamically balance multiple objectives (e.g., tightness, future utility, waste) using adaptive exploration strategies that sample promising candidates rather than relying on pure randomness.\n*   **Avoid:** Overly simplistic strategies like pure Best Fit or random assignments. Also avoid neglecting the trade-off between exploration and exploitation.\n*   **Explanation:** Complex, multi-objective heuristics with nuanced scoring (e.g., softmax, explicit penalties/bonuses) and guided exploration are essential for outperforming simpler methods, especially in complex packing problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}