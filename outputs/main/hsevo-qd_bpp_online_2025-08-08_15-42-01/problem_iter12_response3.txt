```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins using an adaptive multi-objective approach:
    Balances tightest fit, perfect fit bonus, and a novel "capacity utilization gradient"
    to favor bins that are likely to accommodate future items efficiently.
    Includes a mechanism for dynamically adjusting exploration based on item size variance.
    """

    epsilon_base = 0.1  # Base probability of exploration
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)

    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]

    # --- Objective 1: Tightest Fit & Perfect Fit ---
    remaining_after_fit = valid_bins_capacities - item
    tight_fit_score = -remaining_after_fit
    perfect_fit_bonus = 1000.0
    tight_fit_score[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus

    # --- Objective 2: Capacity Utilization Gradient ---
    # Favor bins that, after packing the current item, leave a remaining capacity
    # that is a "good" fit for a range of potential future item sizes.
    # We can approximate this by looking at the distribution of remaining capacities.
    # A bin that leaves capacity close to the median remaining capacity (among eligible bins)
    # might be good for future medium-sized items.

    # Calculate variance of item sizes encountered so far (requires external state, simulating here)
    # In a real implementation, this would be a class member or passed parameter.
    # For this example, we'll use a placeholder.
    # Assuming historical item sizes were [0.2, 0.5, 0.1, 0.8, 0.3]
    # Current item is 'item'. Let's assume a mix of small and large items are common.
    # A simpler proxy: use variance of *current* remaining capacities to infer future item patterns.
    current_remaining_variance = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0
    # If variance is high, explore more. If low, exploit tighter fits.
    exploration_boost_factor = min(1.0, np.sqrt(current_remaining_variance) / 0.5) # Scale boost by variance, capped

    # Create a score based on how "useful" the remaining capacity is.
    # We want remaining capacity that is not too small (useless) and not too large (wasteful).
    # A simple approach: penalize extreme remaining capacities.
    # Let's consider remaining capacities relative to the *maximum possible remaining capacity*
    # or median remaining capacity of eligible bins.
    if len(valid_bins_capacities) > 1:
        median_remaining_eligible = np.median(remaining_after_fit)
        # Score is higher if remaining_after_fit is close to median_remaining_eligible
        utilization_score = -np.abs(remaining_after_fit - median_remaining_eligible)
    else:
        utilization_score = np.zeros_like(remaining_after_fit) # No comparison possible

    # Normalize and scale utilization score
    if np.ptp(utilization_score) > 1e-9: # Avoid division by zero if all scores are same
        utilization_score = (utilization_score - np.min(utilization_score)) / np.ptp(utilization_score)
    else:
        utilization_score = np.zeros_like(utilization_score)

    # Weighting the objectives. Tight fit is primary, utilization is secondary.
    # The weight of utilization can be influenced by exploration tendency.
    utilization_weight = 0.1 + 0.2 * exploration_boost_factor # Additive boost based on variance
    combined_exploitation_scores = tight_fit_score + utilization_score * utilization_weight

    # --- Adaptive Exploration ---
    # Explore more aggressively if the current item is small relative to bin capacity,
    # or if there's high variance in available capacities, suggesting uncertainty.
    # The exploration probability should be dynamic.
    # Let's define "small item" as less than 25% of average available capacity.
    avg_available_capacity = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0
    is_small_item = (item / avg_available_capacity < 0.25) if avg_available_capacity > 0 else False

    # Dynamic epsilon: higher if item is small, or if variance is high
    epsilon = epsilon_base + 0.1 * (exploration_boost_factor) + (0.15 if is_small_item else 0)
    epsilon = min(epsilon, 0.8) # Cap exploration probability

    # Select candidates for exploration:
    # 1. Top-performing bins based on combined_exploitation_scores.
    # 2. Bins that have moderate remaining capacity (less than half of the item's size, but not perfect fit).
    sorted_indices_exploitation = np.argsort(combined_exploitation_scores)[::-1]

    exploration_candidate_mask = np.zeros_like(combined_exploitation_scores, dtype=bool)
    num_top_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.15))) # Top 15%
    exploration_candidate_mask[sorted_indices_exploitation[:num_top_candidates]] = True

    moderate_remaining_mask = (remaining_after_fit > item * 0.1) & (remaining_after_fit < item * 0.5)
    exploration_candidate_mask[moderate_remaining_mask] = True

    # Generate exploration scores (random, but biased by candidates)
    exploration_scores_raw = np.random.rand(len(valid_bins_capacities))

    # Apply exploration scores only to selected candidates with a certain probability
    final_scores = np.copy(combined_exploitation_scores)
    exploration_decision_mask = np.random.rand(len(valid_bins_capacities)) < epsilon

    # For bins that are candidates AND we decide to explore: use the exploration score
    apply_exploration_mask = exploration_candidate_mask & exploration_decision_mask
    final_scores[apply_exploration_mask] = np.maximum(final_scores[apply_exploration_mask], exploration_scores_raw[apply_exploration_mask] * 0.05) # Scale exploration noise

    # For bins that are candidates but we DON'T explore: stick to exploitation score.
    # For bins that are NOT candidates: stick to exploitation score.

    priorities[valid_bins_indices] = final_scores

    return priorities
```
