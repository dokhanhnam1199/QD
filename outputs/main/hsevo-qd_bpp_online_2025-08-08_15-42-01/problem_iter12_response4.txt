```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A more adaptive and multi-objective priority function for Online Bin Packing.
    It prioritizes bins based on a weighted combination of several factors:
    1. Tightest Fit: Minimizes remaining capacity after packing.
    2. Perfect Fit Bonus: Rewards exact matches.
    3. Space Utilization: Rewards bins that would have a low surplus relative to their original capacity.
    4. Promising Capacity: Identifies bins that might be good for future larger items.
    5. Adaptive Exploration: Selects a subset of potentially good bins and perturbs their scores slightly
       to encourage exploration of less obvious but potentially good options.
    """

    epsilon_exploration_rate = 0.1  # Base probability for exploration
    perfect_fit_bonus = 1000.0
    surplus_penalty_weight = 0.5
    promising_capacity_threshold = 0.6  # Bins with at least this fraction of original capacity remaining might be promising
    exploration_perturbation_scale = 0.05 # Scale of random noise for exploration

    priorities = np.full_like(bins_remain_cap, -np.inf)
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]

    # Calculate remaining capacity after fitting the item
    remaining_after_fit = valid_bins_capacities - item

    # --- Score Component 1: Tightest Fit ---
    # Maximize the negative remaining capacity (minimizes positive remaining capacity)
    tightest_fit_scores = -remaining_after_fit

    # --- Score Component 2: Perfect Fit Bonus ---
    perfect_fit_mask = np.abs(remaining_after_fit) < 1e-9
    perfect_fit_scores = np.zeros_like(tightest_fit_scores)
    perfect_fit_scores[perfect_fit_mask] = perfect_fit_bonus

    # --- Score Component 3: Space Utilization (Penalize large surpluses) ---
    # Penalize bins where remaining capacity is a large fraction of the bin's original capacity
    # Here, we use the original capacity of the bin *before* the current item was considered.
    # To do this, we need the original capacities of the bins that *can* fit the item.
    # We'll approximate this by assuming the initial capacity was slightly larger than current remaining.
    # A better approach would be to store original capacities or a history.
    # For this heuristic, we'll use `valid_bins_capacities` as a proxy for "capacity that could hold item"
    # and penalize if `remaining_after_fit` is large *relative to the capacity that can hold the item*.
    # This encourages using bins that are "just big enough".
    large_surplus_threshold_ratio = 0.7  # If remaining capacity is more than 70% of the capacity that can fit the item
    large_surplus_penalty_factor = 0.2
    surplus_penalty_mask = remaining_after_fit > (valid_bins_capacities * large_surplus_threshold_ratio)
    # The penalty is proportional to how much *more* empty space there is than needed.
    surplus_penalty = (remaining_after_fit[surplus_penalty_mask] - (valid_bins_capacities[surplus_penalty_mask] * large_surplus_threshold_ratio)) / valid_bins_capacities[surplus_penalty_mask] * large_surplus_penalty_factor
    space_utilization_scores = np.zeros_like(tightest_fit_scores)
    space_utilization_scores[surplus_penalty_mask] = -surplus_penalty

    # --- Score Component 4: Promising Capacity (Favor bins with decent remaining space) ---
    # Favor bins that have a substantial amount of capacity left, potentially useful for future items.
    # This is a bit counter-intuitive to tightest-fit, so we'll give it a moderate score boost.
    # The idea is to not always pick the absolute tightest if it leaves *too little* space.
    # We'll favor bins where remaining capacity is between a small buffer and a larger portion.
    # Let's define "promising" as having remaining capacity between `item * 1.1` and `original_capacity * promising_capacity_threshold`.
    # Using `valid_bins_capacities` as original capacity for this step.
    min_promising_lower_bound = item * 1.1 # At least a small buffer beyond the item size
    promising_capacity_mask = (remaining_after_fit >= min_promising_lower_bound) & (remaining_after_fit < valid_bins_capacities * promising_capacity_threshold)
    promising_capacity_scores = np.zeros_like(tightest_fit_scores)
    promising_capacity_scores[promising_capacity_mask] = 0.5 # A moderate boost

    # --- Combine Scores ---
    # Weighted sum of the components. Weights can be tuned.
    # We want tightest fit and perfect fit to dominate, then utilization, then promising.
    exploitation_scores = (
        tightest_fit_scores * 1.0 +
        perfect_fit_scores * 1.0 +
        space_utilization_scores * 0.8 +
        promising_capacity_scores * 0.5
    )

    # --- Adaptive Exploration ---
    # Introduce a chance to explore among a subset of "good" bins.
    # "Good" bins are those that are not the absolute worst fits.
    sorted_indices_exploitation = np.argsort(exploitation_scores)[::-1] # Indices sorted by overall exploitation score

    # Select candidates for exploration: Top N bins by exploitation score, and bins that are "reasonably good" fits.
    num_candidates_from_top = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.15))) # Top 15%
    exploration_candidate_mask = np.zeros_like(exploitation_scores, dtype=bool)
    exploration_candidate_mask[sorted_indices_exploitation[:num_candidates_from_top]] = True

    # Also consider bins that are not extremely tight but not too loose either (moderately filled)
    # Let's define moderate as having remaining capacity between `item * 1.2` and `valid_bins_capacities * 0.8`
    moderate_fit_lower_bound = item * 1.2
    moderate_fit_upper_bound = valid_bins_capacities * 0.8
    moderate_fit_mask = (remaining_after_fit >= moderate_fit_lower_bound) & (remaining_after_fit <= moderate_fit_upper_bound)
    exploration_candidate_mask[moderate_fit_mask] = True

    # Generate small random perturbations for exploration candidates
    exploration_perturbations = (np.random.rand(len(valid_bins_capacities)) - 0.5) * exploration_perturbation_scale

    # Determine which candidates will actually be perturbed based on epsilon
    num_candidates_to_perturb = int(np.sum(exploration_candidate_mask) * epsilon_exploration_rate)
    indices_to_perturb_candidates = np.where(exploration_candidate_mask)[0]
    if num_candidates_to_perturb > 0 and len(indices_to_perturb_candidates) > 0:
        perturb_indices_in_candidates = np.random.choice(len(indices_to_perturb_candidates), num_candidates_to_perturb, replace=False)
        actual_perturb_mask = np.zeros_like(exploitation_scores, dtype=bool)
        actual_perturb_mask[indices_to_perturb_candidates[perturb_indices_in_candidates]] = True
        
        combined_priorities = np.copy(exploitation_scores)
        combined_priorities[actual_perturb_mask] += exploration_perturbations[actual_perturb_mask]
    else:
        combined_priorities = exploitation_scores

    priorities[valid_bins_indices] = combined_priorities

    return priorities
```
