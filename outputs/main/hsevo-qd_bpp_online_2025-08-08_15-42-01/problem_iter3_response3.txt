```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines a penalty for wasted space with a bonus for achieving near-perfect fits,
    while also incorporating a slight preference for bins with more remaining capacity
    to keep options open for larger future items. Exploration is guided by this
    preference rather than being purely random.
    """
    priorities = np.full_like(bins_remain_cap, -np.inf)
    can_fit_mask = bins_remain_cap >= item

    if np.sum(can_fit_mask) == 0:
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]

    # Calculate remaining capacity after fitting the item
    remaining_after_fit = valid_bins_capacities - item

    # Score 1: Penalty for wasted space (higher negative score for more waste)
    # This encourages tighter fits.
    wasted_space_penalty = -remaining_after_fit * 100.0  # Scale penalty

    # Score 2: Bonus for near-perfect fits.
    # Reward bins that leave very little space, up to a small tolerance.
    # Perfect fit bonus is higher than near-perfect fit bonus.
    near_perfect_fit_threshold = 0.05
    perfect_fit_bonus = 10.0
    near_perfect_fit_bonus = 5.0

    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)
    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)

    fit_bonus = np.zeros_like(remaining_after_fit)
    fit_bonus[perfect_fit_mask] = perfect_fit_bonus
    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus

    # Score 3: Exploration-guided preference for bins with more remaining capacity.
    # This is a "soft" preference, less impactful than tight fitting,
    # to keep options open for potentially larger items later.
    # We normalize this to avoid dominating the tight fit score.
    # Using log to dampen the effect of very large capacities.
    max_cap_preference = np.log1p(valid_bins_capacities) / np.log1p(np.max(valid_bins_capacities) + 1e-9) * 1.0 # Scaled preference

    # Combine scores: Primarily penalize waste, reward good fits, with a soft exploration bias.
    # The relative weights can be tuned.
    combined_scores = wasted_space_penalty + fit_bonus + max_cap_preference

    priorities[can_fit_mask] = combined_scores

    return priorities
```
