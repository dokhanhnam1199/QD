{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Prioritizes bins that result in the tightest fit after packing,\n    with a bonus for perfect fits and a fallback for worst-fit among remaining options.\"\"\"\n\n    # Initialize priorities with a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in valid bins.\n    potential_remain_cap = bins_remain_cap[fit_mask] - item\n\n    # Assign scores:\n    # 1. High priority for perfect fits (remaining capacity = 0).\n    #    We give a bonus score (e.g., 1.0) to indicate preference.\n    perfect_fit_mask = potential_remain_cap == 0\n    priorities[fit_mask][perfect_fit_mask] = 1.0\n\n    # 2. For other valid fits, prioritize bins that leave less remaining capacity.\n    #    This encourages a \"tight\" packing. We use the negative of the remaining capacity.\n    #    The smallest non-negative remaining capacity will get the highest score here.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    priorities[fit_mask][non_perfect_fit_mask] = -potential_remain_cap[non_perfect_fit_mask]\n\n    # 3. Tie-breaking for bins that result in the same remaining capacity (or are perfect fits):\n    #    Among bins with the same resulting remaining capacity, favor the one with the\n    #    largest original remaining capacity (closer to worst-fit among good fits).\n    #    This helps to keep smaller bins available for smaller items.\n    #    We can achieve this by adding a small bonus proportional to the original capacity.\n    #    Add a small epsilon to the priority to break ties.\n    #    For perfect fits, they already have a score of 1.0. We want to differentiate them.\n    #    Let's use the negative of the original remaining capacity as a secondary score.\n    #    This means, for equally good fits, we prefer the one that was originally larger.\n    #    The `item - bins_remain_cap[fit_mask]` was a good starting point for 'tightness'\n    #    but did not handle tie-breaking well.\n\n    # Let's combine the score: Prioritize by minimizing remaining capacity.\n    # A simple way is to maximize `-(remaining_capacity)`.\n    # To differentiate between equally good fits, we can use the original capacity.\n    # If two bins result in the same remaining capacity, the one that started larger\n    # (worst fit among the tightest) is preferred.\n    # So, for the same `-(remaining_capacity)`, we want to maximize the original capacity.\n\n    # Re-calculating for clarity and combined logic:\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacity for fitting bins.\n    potential_remain_cap_vals = bins_remain_cap[fit_mask] - item\n\n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity.\n    # This can be achieved by maximizing `original_capacity - item`.\n    # Or more simply, `item - original_capacity` (smaller is better here, so maximize `-(item - original_capacity)`)\n    # Let's focus on minimizing `potential_remain_cap_vals`.\n    # A good score would be `-(potential_remain_cap_vals)`.\n    # To break ties (same `potential_remain_cap_vals`), we prefer bins that had higher initial capacity.\n    # So, we can add `bins_remain_cap[fit_mask]` as a secondary factor.\n    # This means we want to maximize: `-(potential_remain_cap_vals) + C * bins_remain_cap[fit_mask]`\n    # where C is a small constant to ensure primary sorting is by remaining capacity.\n\n    # Let's use a simpler approach that captures the essence:\n    # Prioritize bins that leave the least remainder.\n    # For ties, pick the one that was originally larger.\n    # Score = -(remaining_capacity) + small_bonus * original_capacity\n\n    # Let's refine the score calculation.\n    # For bins that fit:\n    # Primary goal: Minimize `potential_remain_cap`.\n    # Secondary goal: If `potential_remain_cap` is the same, pick the bin that was originally largest.\n\n    # Score: `item - bins_remain_cap[fit_mask]` works well for tight fits.\n    # The highest score is for `bins_remain_cap[fit_mask] == item`.\n    # If there are multiple such bins, they have the same score.\n    # To break ties, we can add a small value related to the original capacity.\n    # Higher original capacity is preferred for ties in remaining capacity.\n\n    # Let's use the inverse of remaining capacity, but penalize bins that are too large.\n    # The \"perfect fit\" is ideal.\n    # So, let's try a combined score:\n    # - Perfect fit: Highest score (e.g., 100)\n    # - Tight fit (small remaining capacity): Score inversely proportional to remaining capacity.\n    # - Break ties by original capacity: Higher original capacity gets a small bonus.\n\n    # Final approach:\n    # 1. Perfect fits get a significant bonus (e.g., 1000).\n    # 2. Other fits get a score based on inverse of remaining capacity + a small bonus\n    #    for larger original capacity to break ties.\n    #    Score = 1 / (potential_remain_cap + epsilon) + original_capacity * epsilon_small\n    #    where epsilon is for avoiding division by zero and epsilon_small for tie-breaking.\n\n    # Revised attempt focusing on minimizing remaining capacity and then maximizing original capacity for ties.\n    # Score = -(remaining_capacity) + (original_capacity / MaxCapacity) * epsilon_tiebreak\n    # This needs careful scaling.\n\n    # Let's use a scoring that strongly favors perfect fits, then tight fits, and then larger original bins.\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    scores.fill(-np.inf) # Initialize with a very low score\n\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    potential_remain_cap_vals = fitting_bins_capacity - item\n\n    # Perfect fits have the highest priority\n    perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals == 0)[0]\n    if len(perfect_fit_indices_in_fitting) > 0:\n        scores[can_fit_mask][perfect_fit_indices_in_fitting] = 1000.0\n\n    # Tight fits get priority based on negative remaining capacity\n    # Add a small factor to ensure they are ranked below perfect fits but above others.\n    # The secondary criterion: prefer larger original capacity among those with same remainder.\n    # We can add `fitting_bins_capacity` multiplied by a small factor.\n    # Let's combine this into a single score for non-perfect fits.\n    non_perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals > 0)[0]\n    if len(non_perfect_fit_indices_in_fitting) > 0:\n        # Score = -(remaining_capacity) + bonus for larger original capacity\n        # The bonus should be small enough not to override the primary goal of minimizing remainder.\n        # Use a small fraction of max possible capacity as tie-breaker.\n        # Max possible remainder is roughly bin_capacity. So, max of `fitting_bins_capacity` can be used.\n        max_cap_val = np.max(fitting_bins_capacity) if fitting_bins_capacity.size > 0 else 1.0\n        tie_breaker_factor = 1e-6 # A very small factor for tie-breaking\n\n        # Score for non-perfect fits: prioritize less remaining capacity, then more original capacity.\n        scores[can_fit_mask][non_perfect_fit_indices_in_fitting] = \\\n            -(potential_remain_cap_vals[non_perfect_fit_indices_in_fitting]) \\\n            + (fitting_bins_capacity[non_perfect_fit_indices_in_fitting] / max_cap_val) * tie_breaker_factor\n\n    # Ensure perfect fits still have highest priority if they exist\n    # If perfect fits were assigned 1000.0, and non-perfect fits get scores like -0.1 + 0.99 * 1e-6,\n    # this order is maintained.\n\n    # If there are no perfect fits, and multiple bins have the same minimal remainder,\n    # the tie-breaker correctly selects the one with higher original capacity.\n\n    return scores\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** These heuristics are identical. The ranking suggests a potential issue with the evaluation or the ranking itself. However, assuming they represent a valid point in the ranking, their complexity and explicit handling of \"exploration candidates\" (top K fits and moderate capacity bins) is a notable design choice. The \"surplus penalty\" is a nuanced addition to pure tight-fitting.\n\n*   **Heuristics 2 vs 3 & 4:** Heuristics 3 and 4 are identical and simpler than Heuristic 2. They implement a basic epsilon-greedy approach (random choice vs. tight fit). Heuristic 2's \"adaptive exploration\" is more sophisticated, trying to guide exploration towards \"good enough\" bins rather than purely random ones. The penalty for large surpluses in Heuristic 2 also adds a layer of strategic thinking. The simpler epsilon-greedy in 3 & 4 is less likely to discover complex packing patterns.\n\n*   **Heuristics 3 & 4 vs 5 & 6:** Heuristics 5 and 6 are also simpler than 2 but introduce different mechanisms. Heuristic 6 is a pure \"Best Fit\" (minimizing remaining space). Heuristic 5 uses a softmax approach, turning \"fit scores\" into probabilities. This is an interesting probabilistic exploration. Comparing 3/4 (epsilon-greedy) to 5 (softmax) and 6 (pure best fit), the epsilon-greedy approach (3/4) is more direct in its exploration strategy. Best Fit (6) is purely exploitative, while Softmax (5) introduces a probabilistic distribution over good fits. The ranking suggests that these more direct or probabilistic approaches are less effective than the hybrid ones.\n\n*   **Heuristics 6 vs 7 & 8 & 9:** Heuristics 7, 8, and 9 are similar, focusing on tight fits and bonuses for perfect fits. Heuristic 6 (pure Best Fit) is simpler. Heuristics 7-9 add specific bonuses and sometimes penalties. Heuristic 8 & 9 are identical. The ranking indicates that adding these specific bonuses (like for perfect fits) and penalties (like for large remainders) improves performance over pure Best Fit. Heuristic 7 is more basic than 8/9.\n\n*   **Heuristics 7 vs 10 & 11:** Heuristic 10 is identical to 5. Heuristic 11 uses \"Almost Full Fit\" by maximizing `item - bins_remain_cap`, which is equivalent to minimizing `bins_remain_cap - item` (Best Fit). It does not explicitly add bonuses or penalties. The ranking suggests that the more nuanced approaches (like those in 7-9) are better than this direct Best Fit implementation.\n\n*   **Heuristics 11 vs 12:** Heuristic 12 is the most complex, combining penalties for wasted space, bonuses for near-perfect fits, and a \"soft\" preference for bins with more capacity (exploration-guided). Its higher ranking suggests that this multi-objective optimization within the heuristic is effective. Heuristic 11 is a simpler Best Fit variant.\n\n*   **Heuristics 12 vs 13:** Heuristic 13 is an FFD-inspired heuristic, prioritizing tight fits with a small penalty for slight oversights. It's simpler than 12. The ranking places 12 significantly higher, implying its combined strategy is superior.\n\n*   **Heuristics 13 vs 14 & 15 & 16:** Heuristic 14 is a simple Best Fit with explicit probabilities, but the loop implementation is inefficient. Heuristics 15 and 16 are identical and appear to be optimized versions of Heuristic 2, with hyperparameter tuning evident (e.g., `epsilon`, `perfect_fit_bonus`). These are still ranked lower than 13, which might indicate that the specific parameter values or the overall strategy of 13 is slightly better than the tuned 2. The presence of `torch` and `scipy` imports in 14-16, but not used in the snippet, is odd; they might be remnants of a larger framework or an indication of potential future extensions. The ranking suggests that even tuned versions of simpler hybrid strategies are not as good as the complex one (12) or even the FFD-inspired one (13).\n\n*   **Heuristics 15 & 16 vs 17 & 18:** Heuristics 17 and 18 are identical \"Random Fit\" strategies. They simply assign random priorities to bins that can fit the item. These are the simplest and worst-performing strategies, as they lack any exploitation logic. The ranking clearly places them at the bottom.\n\n*   **Heuristics 17 & 18 vs 19 & 20:** Heuristic 19 aims to combine perfect fits, tight fits, and tie-breaking based on original capacity. Heuristic 20 tries to balance tight fitting with penalties for small residuals and adds exploration. The ranking places 19 and 20 above random fit but below the more sophisticated hybrid strategies. Heuristic 19's tie-breaking logic is a good addition. Heuristic 20's penalty for small residuals is an interesting refinement.\n\n*   **Overall:** The top-performing heuristics (1, 2, 12) are complex, often combining tight-fitting principles with sophisticated exploration strategies (adaptive exploration, multi-objective scoring). Simpler \"Best Fit\" or epsilon-greedy approaches are ranked lower. Pure random strategies are at the bottom. The use of explicit bonuses and penalties for specific fit scenarios (perfect fit, large/small residuals) appears beneficial. The ranking suggests a hierarchy: complex hybrids > refined hybrids/FFD-inspired > basic hybrids/probabilistic > simple exploitation (Best Fit) > random.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, guided exploration, adaptive reward functions, hybrid strategies.\n*   **Advice:** Focus on iterative refinement of multi-objective heuristics, dynamically adjusting exploration/exploitation based on problem state.\n*   **Avoid:** Over-reliance on static or purely greedy approaches; neglecting the interplay between penalizing poor fits and rewarding ideal fits.\n*   **Explanation:** By viewing heuristic design as a continuous learning process, you can better identify optimal trade-offs between thorough exploration and efficient exploitation, leading to more robust and performant solutions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}