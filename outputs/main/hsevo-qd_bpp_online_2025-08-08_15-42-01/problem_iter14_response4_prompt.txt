{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    A more adaptive and multi-objective priority function for online Bin Packing.\n    This version incorporates feedback from past packing performance and dynamically\n    adjusts exploration based on bin utilization patterns.\n    \"\"\"\n    \n    epsilon = 0.05  # Base probability of exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n    \n    num_valid_bins = len(valid_bins_capacities)\n    \n    # --- Objective 1: Minimizing Wasted Space (Exploitation - Tightest Fit) ---\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Score for tightest fit: maximize negative remaining capacity\n    # A high score means very little remaining space after fitting.\n    tightness_scores = -remaining_after_fit\n    \n    # Perfect fit bonus\n    perfect_fit_bonus = 100.0  # Reduced bonus to balance with other objectives\n    tightness_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    \n    # Penalty for excessive slack: penalize bins that leave a lot of unused space\n    # relative to the item size. This encourages filling bins more efficiently.\n    slack_penalty_factor = 0.5\n    slack_scores = -(remaining_after_fit / item) * slack_penalty_factor\n    slack_scores[remaining_after_fit < 0] = -np.inf # Ensure only valid fits get scores\n    tightness_scores += slack_scores\n\n    # --- Objective 2: Promoting Future Fit (Exploration - Moderate Capacity) ---\n    # Identify bins that have moderate remaining capacity, which might be useful\n    # for larger items that come later. We want to avoid completely filling bins\n    # or leaving them too empty.\n    \n    # Consider bins with remaining capacity that's not too small and not too large.\n    # A \"good\" remaining capacity might be between 20% of the item size and a\n    # certain fraction of the *original* bin capacity (or a global average capacity).\n    # For simplicity here, let's use a heuristic based on item size and current remaining space.\n    \n    # A bin is \"promising\" if its remaining capacity is at least X% of the item's size\n    # and not excessively large (e.g., less than half of its *current* remaining capacity).\n    # The idea is to keep some moderate space available.\n    promising_capacity_bonus_factor = 0.1\n    \n    # Calculate a score based on how \"balanced\" the remaining capacity is.\n    # This favors bins where remaining capacity is a reasonable fraction of the bin's *current* capacity.\n    # Normalize remaining capacity by the *original* capacity of the bin it could fit into.\n    # Assuming a fixed bin capacity 'B' would be better, but without it, we use current.\n    # For this implementation, let's consider remaining_after_fit itself.\n    # Bins with remaining_after_fit between 0.2 * item and 0.6 * (some typical bin capacity proxy like median valid bin capacity)\n    \n    # Use a proxy for \"ideal\" remaining capacity \u2013 perhaps the median remaining space among fitting bins.\n    median_remaining_space_proxy = np.median(valid_bins_capacities) # Proxy for original bin capacity\n    \n    # Score based on how close remaining_after_fit is to a 'moderately useful' value.\n    # Let's define 'moderately useful' as being between 0.2 * item and 0.5 * median_remaining_space_proxy.\n    moderate_remaining_scores = np.zeros(num_valid_bins)\n    \n    lower_bound_moderate = 0.2 * item\n    upper_bound_moderate = 0.5 * median_remaining_space_proxy\n    \n    # Create masks for promising capacity\n    is_moderate_capacity = (remaining_after_fit >= lower_bound_moderate) & (remaining_after_fit <= upper_bound_moderate)\n    \n    # Assign a bonus to bins falling into this moderate range.\n    # The bonus is higher if it's closer to the 'ideal' middle of this range.\n    if np.any(is_moderate_capacity):\n        moderate_bins_remaining = remaining_after_fit[is_moderate_capacity]\n        # Calculate score based on distance from the midpoint of the moderate range\n        mid_moderate_range = (lower_bound_moderate + upper_bound_moderate) / 2.0\n        distance_from_mid = np.abs(moderate_bins_remaining - mid_moderate_range)\n        # Normalize distance so smaller distance gives higher score (less penalty)\n        max_distance = max(mid_moderate_range - lower_bound_moderate, upper_bound_moderate - mid_moderate_range)\n        normalized_distance = distance_from_mid / max_distance if max_distance > 0 else np.zeros_like(distance_from_mid)\n        \n        moderate_remaining_scores[is_moderate_capacity] = (1.0 - normalized_distance) * promising_capacity_bonus_factor * item\n\n    # --- Adaptive Exploration Probability ---\n    # Adjust epsilon based on how \"challenging\" the current state is.\n    # If many bins are very full or very empty, we might explore more to find a good fit.\n    # If there are many \"perfect\" or \"tight\" fits, exploration might be less critical.\n    \n    # Heuristic: If the proportion of bins with very little remaining capacity is high,\n    # increase exploration to find a slightly less tight fit that might be better for future items.\n    # Or, if there are very few good fits available.\n    \n    # Calculate a \"difficulty\" metric\n    # Difficulty can be related to the variance of remaining capacities, or the scarcity of good fits.\n    # Let's use the scarcity of tight fits (e.g., remaining_after_fit < 0.1 * item).\n    num_tight_fits = np.sum(remaining_after_fit < 0.1 * item)\n    tight_fit_ratio = num_tight_fits / num_valid_bins if num_valid_bins > 0 else 0\n    \n    # If tight fits are rare (<20% of valid bins), slightly increase exploration.\n    adaptive_epsilon = epsilon\n    if tight_fit_ratio < 0.2:\n        adaptive_epsilon = min(epsilon * 1.5, 0.2) # Cap exploration increase\n    \n    # If there are many bins with very large remaining capacity (e.g., > 0.8 * original capacity proxy),\n    # maybe reduce exploration slightly as there are plenty of options.\n    num_large_remaining = np.sum(remaining_after_fit > 0.8 * median_remaining_space_proxy)\n    large_remaining_ratio = num_large_remaining / num_valid_bins if num_valid_bins > 0 else 0\n    \n    if large_remaining_ratio > 0.5:\n        adaptive_epsilon = max(epsilon * 0.7, 0.01) # Cap exploration decrease\n\n    # --- Combining Objectives ---\n    # Weighted sum of objectives. The weights can be dynamic, but fixed here for simplicity.\n    # Weight for tightness: higher, as it's the primary goal.\n    # Weight for moderate capacity: lower, as it's secondary/exploratory.\n    weight_tightness = 0.8\n    weight_moderate_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_scores) + (weight_moderate_capacity * moderate_remaining_scores)\n    \n    # --- Guided Exploration ---\n    # Instead of purely random exploration, we explore among bins that are \"good enough\"\n    # but not necessarily the absolute best according to the combined score.\n    # We sample from a subset of bins, potentially those with moderate capacity or decent tightness.\n    \n    # Create a candidate pool for exploration:\n    # 1. Bins with moderate remaining capacity.\n    # 2. Bins that are reasonably tight but not perfect fits.\n    \n    exploration_candidate_mask = np.zeros(num_valid_bins, dtype=bool)\n    \n    # Add bins with moderate capacity to exploration candidates\n    exploration_candidate_mask[is_moderate_capacity] = True\n    \n    # Add a portion of the \"tightest\" bins (but not perfect fits) as exploration candidates\n    # We want to explore slightly less optimal tight fits.\n    sorted_indices_tightness = np.argsort(tightness_scores)[::-1] # Descending for best tightness\n    num_tight_candidates = min(num_valid_bins, max(1, int(num_valid_bins * 0.15))) # Top 15% tightest bins\n    \n    # Exclude perfect fits from this exploration subset to avoid over-sampling them\n    tight_candidates_indices = sorted_indices_tightness[:num_tight_candidates]\n    tight_candidates_are_perfect = np.abs(remaining_after_fit[tight_candidates_indices]) < 1e-9\n    valid_tight_candidates_indices = tight_candidates_indices[~tight_candidates_are_perfect]\n    \n    exploration_candidate_mask[valid_tight_candidates_indices] = True\n    \n    # Generate random exploration scores for selected candidates\n    # These scores are added to the combined scores with a certain probability (adaptive_epsilon)\n    exploration_noise = np.random.rand(num_valid_bins) * 0.05 # Small random noise to break ties and enable exploration\n    \n    # Apply exploration noise to selected candidates with probability adaptive_epsilon\n    should_explore_mask = np.random.rand(num_valid_bins) < adaptive_epsilon\n    \n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    # Add exploration noise to the combined scores for selected bins\n    combined_scores[apply_exploration_mask] += exploration_noise[apply_exploration_mask]\n    \n    # Final priorities are the combined scores\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Second Best): They are identical.\nComparing Heuristic 1 vs Heuristic 3: They are identical.\nComparing Heuristic 1 vs Heuristic 4: Heuristic 1 introduces a hybrid approach with exploration (epsilon probability) and guided candidate selection, whereas Heuristic 4 is purely deterministic best-fit with a penalty for large remainders. Heuristic 1's approach is more sophisticated in balancing immediate needs with future possibilities.\nComparing Heuristic 1 vs Heuristic 5: Heuristic 1 uses a probability-based exploration with candidate selection, while Heuristic 5 attempts to integrate exploration by preferring bins with more remaining capacity, but without a clear probabilistic mechanism. Heuristic 1's explicit exploration is likely more effective.\nComparing Heuristic 1 vs Heuristic 6: Heuristic 1 is a hybrid of tightest fit with exploration. Heuristic 6 is multi-objective, incorporating utilization and adaptive exploration (though the adaptive part is simulated). Heuristic 6 attempts a more holistic approach by considering bin utilization.\nComparing Heuristic 1 vs Heuristic 7: Heuristic 1 is a simpler hybrid. Heuristic 7 builds on multi-objective scoring, adaptive exploration probability, and identifies candidates based on both tightness and moderate capacity, making it more nuanced than Heuristic 1.\nComparing Heuristic 7 vs Heuristic 8: Heuristic 8 builds on Heuristic 7's multi-objective scoring but refines it with better tie-breaking and exploration based on candidate selection and perturbations. Heuristic 8 seems to have a more structured approach to balancing objectives.\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 is a complex multi-objective heuristic with exploration. Heuristic 9 is a very simple heuristic that prioritizes bins with small remaining capacity (inverse of remaining capacity) and gives a bonus to perfect fits. Heuristic 8 is significantly more advanced.\nComparing Heuristic 9 vs Heuristic 10: Heuristics 9 and 10 are almost identical, with Heuristic 9 having a slight preference for perfect fits (score of 1.0 vs inverse of remainder) and Heuristic 10 purely using inverse of remainder. Both are simple \"best-fit\" variations.\nComparing Heuristic 10 vs Heuristic 11: Heuristic 10 is a simple inverse remainder score. Heuristic 11 is also best-fit focused but adds a tie-breaker favoring larger original capacity and a specific score for perfect fits. Heuristic 11 is more refined for tie-breaking.\nComparing Heuristic 11 vs Heuristic 12: Heuristic 11 is a best-fit with tie-breaking. Heuristic 12 introduces several tunable parameters for exploration probability, bonuses, penalties, and candidate selection, indicating a more experimental and potentially optimized approach to balancing objectives.\nComparing Heuristic 12 vs Heuristic 13: Heuristic 12 is a complex multi-objective heuristic. Heuristic 13 is a \"Random Fit\" strategy, assigning random priorities to bins that can fit the item. This is a very basic approach compared to Heuristic 12.\nComparing Heuristic 13 vs Heuristic 14: Heuristics 13 and 14 are identical \"Random Fit\" strategies.\nComparing Heuristic 14 vs Heuristic 15: Heuristic 14 is random. Heuristic 15 is a complex multi-objective heuristic with adaptive exploration based on item size variance and capacity utilization gradient, significantly more sophisticated than random.\nComparing Heuristic 15 vs Heuristic 16: Heuristics 15 and 16 are identical.\nComparing Heuristic 16 vs Heuristic 17: Heuristic 16/15 is multi-objective with adaptive exploration based on variance and utilization gradient. Heuristic 17 is also multi-objective, focusing on tightness, future usability (relative to item size), and adaptive exploration for candidates. Heuristic 16/15 seems to have a more defined adaptive exploration mechanism.\nComparing Heuristic 17 vs Heuristic 18: Heuristics 17 and 18 are identical.\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 is multi-objective with adaptive exploration. Heuristic 19 combines tightness, waste avoidance, future utility, and guided exploration with perturbated scores for candidates. Heuristic 19's exploration is more about perturbing scores of selected candidates.\nComparing Heuristic 19 vs Heuristic 20: Heuristics 19 and 20 are identical.\nOverall: The top heuristics (1-8, 15-18) are complex multi-objective strategies that balance tightest fit with some form of future utility or guided exploration. Heuristics 9-11 and 13-14 are simpler best-fit or random strategies. The intermediate heuristics (12, 19-20) attempt variations on multi-objective and exploration. Heuristics 1, 7, 8, 15, 16, 17, 18 appear to represent the most developed ideas, blending multiple objectives and adaptive/guided exploration.\n- \nHere's a redefinition of \"Current self-reflection\" to guide heuristic design, avoiding ineffective approaches:\n\n*   **Keywords:** Multi-objective, adaptive exploration, sophisticated scoring, penalized residuals, rewarded fits.\n*   **Advice:** Design heuristics that dynamically balance multiple objectives (e.g., tightness, future utility, waste) using adaptive exploration strategies that sample promising candidates rather than relying on pure randomness.\n*   **Avoid:** Overly simplistic strategies like pure Best Fit or random assignments. Also avoid neglecting the trade-off between exploration and exploitation.\n*   **Explanation:** Complex, multi-objective heuristics with nuanced scoring (e.g., softmax, explicit penalties/bonuses) and guided exploration are essential for outperforming simpler methods, especially in complex packing problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}