{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    epsilon = 0.03\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Multi-objective scoring:\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    \n    tightness_score = -remaining_after_fit\n    \n    # Penalty for large remainders, scaled by item size\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n    \n    # Reward for significant remaining capacity - can be useful for larger items later\n    future_capacity_score = remaining_after_fit / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(remaining_after_fit)\n    \n    # Combine objectives with weights. These weights can be tuned.\n    # We want to strongly favor tightness, moderately avoid waste, and lightly favor future capacity.\n    weight_tightness = 1.0\n    weight_waste = 0.5\n    weight_future_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # Enhanced Exploration:\n    # Instead of random exploration, we can introduce guided exploration.\n    # This means exploring bins that are \"good enough\" but not necessarily the absolute best.\n    # We can define \"good enough\" as bins that fall within a certain percentile of the best fits.\n    \n    # Sort bins by the combined score to identify top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Identify a range of bins to consider for exploration\n    # This could be the top K bins, or bins within a certain score range.\n    # Let's consider bins within the top 30% of scores, or at least the top 3 bins.\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.3))\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Also consider bins that offer a \"balanced\" fit, not too tight, not too empty.\n    # A bin that leaves a moderate amount of space might be more versatile.\n    # Let's consider bins where remaining_after_fit is between a small fraction and a larger fraction of bin capacity.\n    \n    # To define \"moderate\", we can look at the distribution of remaining capacities.\n    # Let's use quartiles for guidance.\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    \n    # Bins with remaining capacity between Q1 and Q3 (inclusive of Q3) are considered moderately remaining.\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    \n    # Combine indices for exploration candidates\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    \n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates\n    # We want these exploration scores to be slightly random but not too high,\n    # to offer a chance for diversity without sacrificing too much performance.\n    exploration_scores = np.random.uniform(-0.1, 0.1, size=len(valid_bins_capacities))\n    \n    # Apply exploration scores:\n    # With probability epsilon, use exploration score for exploration candidates.\n    # Otherwise, use the combined score.\n    # For non-candidates, always use the combined score.\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates\n    exploration_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask[exploration_candidate_indices_in_valid] = True\n    \n    # Decide whether to use exploration score for exploration candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_mask & use_exploration_for_candidates] = exploration_scores[exploration_mask & use_exploration_for_candidates]\n    \n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.03  # Further reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Multi-objective approach: balance several criteria\n    # 1. Tightness (minimize remaining capacity)\n    # 2. Space Utilization (maximize used capacity)\n    # 3. Future Item Suitability (prioritize bins that can fit subsequent items well)\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Score 1: Tightness (higher is better, i.e., less remaining capacity)\n    # Invert the remaining capacity, so smaller positive remaining capacity is better.\n    tightness_score = -remaining_after_fit \n    \n    # Bonus for perfect fits\n    perfect_fit_bonus = 0.2\n    tightness_score[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # Score 2: Space Utilization (maximize the use of existing bin capacity)\n    # This encourages filling bins more fully, even if not a perfect fit.\n    # We consider the *current* remaining capacity as a proxy for how \"full\" the bin is.\n    # Higher remaining capacity means less utilized. So, we want to penalize bins with high remaining capacity.\n    # We use the original remaining capacity before fitting the item.\n    # Using current remaining capacity as it directly reflects the bin's state.\n    space_utilization_score = valid_bins_capacities \n    \n    # Score 3: Future Item Suitability (Adaptive Penalty for large remainders)\n    # Penalize bins that leave a large residual capacity relative to the bin's current capacity.\n    # This encourages leaving \"medium-sized\" gaps rather than very large ones,\n    # which might be better suited for specific intermediate-sized items later.\n    # We scale the penalty by the *current* remaining capacity.\n    future_suitability_penalty_factor = 0.005\n    future_suitability_score = -(valid_bins_capacities / item) * future_suitability_penalty_factor\n\n    # Combine scores with weighted sum. Weights can be tuned.\n    # Weight for tightness: emphasizes finding tight fits.\n    # Weight for utilization: ensures bins are utilized well.\n    # Weight for future suitability: tries to leave useful residual capacities.\n    combined_exploitation_score = 1.0 * tightness_score + 0.5 * space_utilization_score + 0.8 * future_suitability_score\n\n    # Guided Exploration:\n    # Instead of purely random, we explore bins that are \"good enough\" or represent diverse states.\n    # The exploration should slightly perturb the exploitation scores.\n    \n    # Define a threshold for \"good enough\" bins.\n    # We consider bins that are in the top X percentile of the combined exploitation score.\n    sorted_indices_exploitation = np.argsort(combined_exploitation_score)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(combined_exploitation_score, dtype=bool)\n    \n    # Select a small percentage of the best bins for exploration.\n    num_top_candidates = max(1, int(len(valid_bins_capacities) * 0.15))\n    exploration_candidate_mask[sorted_indices_exploitation[:num_top_candidates]] = True\n    \n    # Additionally, include bins that are \"average\" in terms of remaining capacity.\n    # This helps explore bins that are neither too tight nor too empty.\n    sorted_indices_by_remaining = np.argsort(valid_bins_capacities)\n    median_idx = len(valid_bins_capacities) // 2\n    # Include the bin closest to the median remaining capacity.\n    exploration_candidate_mask[sorted_indices_by_remaining[median_idx]] = True\n    # Include the bin with the smallest remaining capacity (if not already in top candidates)\n    exploration_candidate_mask[sorted_indices_by_remaining[0]] = True\n    \n\n    exploration_noise = np.random.randn(len(valid_bins_capacities)) * 0.005 # Gaussian noise\n\n    # Apply exploration noise with probability epsilon only to the selected candidate bins\n    # The noise is added to the *original* exploitation score for these candidates.\n    \n    final_scores = np.copy(combined_exploitation_score)\n    \n    # For candidate bins, with probability epsilon, apply the exploration noise\n    # This means we explore the decision for these bins.\n    should_explore_mask = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # We only apply exploration to the candidates that are selected for exploration\n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    final_scores[apply_exploration_mask] = combined_exploitation_score[apply_exploration_mask] + exploration_noise[apply_exploration_mask]\n\n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a Best Fit strategy with a penalty for large remaining capacity\n    and a bonus for near-perfect fits, aiming for efficient packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins that can fit the item\n    # Use negative remaining capacity to favor tighter fits (Best Fit)\n    # Add a small penalty for larger remaining capacities to avoid extremely sparse bins\n    # Add a bonus for near-perfect fits to exploit efficient packing opportunities\n    priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask] - 0.1 * (bins_remain_cap[can_fit_mask] - item) + 10 * np.exp(-(bins_remain_cap[can_fit_mask] - item)**2 / 0.1)\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a soft penalty for waste, inspired by Softmax-Fit.\n    Prioritizes bins that leave minimal remaining capacity, with a boost for perfect fits.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return output_priorities\n\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    # Calculate a score: inverse of remaining capacity after fit, favoring tighter fits.\n    # Add a small epsilon to prevent division by zero.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Introduce a bonus for perfect fits to strongly prioritize them.\n    perfect_fit_mask = (valid_bins_cap - item) < 1e-9\n    fit_scores[perfect_fit_mask] *= 5.0  # Significant bonus for perfect fits\n\n    # Apply softmax to normalize scores into probabilities/priorities.\n    # This provides a smoother distribution than pure inverse,\n    # while still emphasizing bins with higher fit_scores.\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Almost Full Fit: Prioritize bins that will be closest to full after adding the item.\n    # Calculate the remaining capacity after placing the item.\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want bins where potential_remain_cap is as close to zero as possible (but non-negative)\n    # A simple way to achieve this is to maximize the negative of the absolute difference from zero.\n    # However, to encourage fitting rather than just being close, we can also consider\n    # bins that can actually fit the item.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = potential_remain_cap >= 0\n\n    # For bins that can fit, calculate a score based on how full they will become.\n    # A higher score means the bin will be closer to full.\n    # We can use -(potential_remain_cap) as a measure of \"fullness\" after packing.\n    # To avoid prioritizing bins that become \"too full\" or negative capacity,\n    # we only consider valid fits.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = -potential_remain_cap[can_fit_mask]\n\n    # We can add a small bonus for fitting the item at all to ensure\n    # that fitting items is generally preferred over not fitting.\n    # However, the negative of remaining capacity already does this indirectly.\n    # A more sophisticated approach might involve considering the original capacity\n    # to prioritize filling larger bins more.\n\n    # For \"Almost Full Fit\", we want to minimize the remaining capacity.\n    # So, we want to maximize the negative of the remaining capacity.\n    # Let's refine this. We want the remaining capacity to be as close to 0 as possible.\n    # The value `potential_remain_cap` represents this.\n    # We want to maximize this value if it's negative (meaning it's a good fit).\n    # Let's re-evaluate. For Almost Full Fit, we want the resulting remaining capacity\n    # to be as small as possible, but still non-negative.\n    # So, we want to maximize `-(potential_remain_cap)` for valid fits.\n\n    # Consider the difference between the bin's original capacity and the item size.\n    # We want to prioritize bins where this difference is minimized, but non-negative.\n    # The `potential_remain_cap` already captures this.\n    # We want the most \"positive\" value of `-(potential_remain_cap)` for bins that fit.\n\n    # Another perspective: prioritize bins that have the *least* remaining capacity *after* the item is placed.\n    # This directly translates to maximizing `-(potential_remain_cap)`.\n\n    # Let's consider a small perturbation or a \"niceness\" factor.\n    # Perhaps bins that are already somewhat full are preferred.\n    # But for \"Almost Full Fit\", the focus is purely on the state *after* packing.\n\n    # We want to maximize the remaining capacity in a way that favors being close to zero.\n    # The score should be higher for bins that result in less remaining capacity.\n    # So, we want to maximize `-(bins_remain_cap - item)` for valid bins.\n    # This is equivalent to maximizing `item - bins_remain_cap`. This isn't quite right.\n\n    # The goal is to make the bin as \"full\" as possible *after* adding the item.\n    # \"Full\" means having small remaining capacity.\n    # So, we want to minimize `potential_remain_cap`.\n    # To turn this into a maximization problem for priority, we can use `-potential_remain_cap`.\n    # We also need to ensure that bins that *cannot* fit the item get a very low priority.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Let's refine the \"Almost Full Fit\" idea. We want the bin with the smallest remaining capacity after packing.\n    # This means we want to minimize `bins_remain_cap - item`.\n    # For a priority score (higher is better), we can use the negative of this difference.\n    # So, we want to maximize `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n\n    # This score will be highest for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # Example: item=5\n    # bin1_rem=6  -> priority = 5 - 6 = -1\n    # bin2_rem=5  -> priority = 5 - 5 = 0\n    # bin3_rem=10 -> priority = 5 - 10 = -5\n    # bin4_rem=3  -> priority = -inf (cannot fit)\n\n    # The highest score is 0, from the bin that becomes exactly full.\n    # This seems correct for \"Almost Full Fit\".\n\n    # Consider a scenario where there are multiple bins that become exactly full.\n    # The current heuristic doesn't differentiate between them.\n    # For this strategy, simply picking any of them is fine.\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1 / (bins_remain_cap[i] - item + 1e-9) \n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins that result in the tightest fit after packing,\n    with a bonus for perfect fits and a fallback for worst-fit among remaining options.\"\"\"\n\n    # Initialize priorities with a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in valid bins.\n    potential_remain_cap = bins_remain_cap[fit_mask] - item\n\n    # Assign scores:\n    # 1. High priority for perfect fits (remaining capacity = 0).\n    #    We give a bonus score (e.g., 1.0) to indicate preference.\n    perfect_fit_mask = potential_remain_cap == 0\n    priorities[fit_mask][perfect_fit_mask] = 1.0\n\n    # 2. For other valid fits, prioritize bins that leave less remaining capacity.\n    #    This encourages a \"tight\" packing. We use the negative of the remaining capacity.\n    #    The smallest non-negative remaining capacity will get the highest score here.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    priorities[fit_mask][non_perfect_fit_mask] = -potential_remain_cap[non_perfect_fit_mask]\n\n    # 3. Tie-breaking for bins that result in the same remaining capacity (or are perfect fits):\n    #    Among bins with the same resulting remaining capacity, favor the one with the\n    #    largest original remaining capacity (closer to worst-fit among good fits).\n    #    This helps to keep smaller bins available for smaller items.\n    #    We can achieve this by adding a small bonus proportional to the original capacity.\n    #    Add a small epsilon to the priority to break ties.\n    #    For perfect fits, they already have a score of 1.0. We want to differentiate them.\n    #    Let's use the negative of the original remaining capacity as a secondary score.\n    #    This means, for equally good fits, we prefer the one that was originally larger.\n    #    The `item - bins_remain_cap[fit_mask]` was a good starting point for 'tightness'\n    #    but did not handle tie-breaking well.\n\n    # Let's combine the score: Prioritize by minimizing remaining capacity.\n    # A simple way is to maximize `-(remaining_capacity)`.\n    # To differentiate between equally good fits, we can use the original capacity.\n    # If two bins result in the same remaining capacity, the one that started larger\n    # (worst fit among the tightest) is preferred.\n    # So, for the same `-(remaining_capacity)`, we want to maximize the original capacity.\n\n    # Re-calculating for clarity and combined logic:\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacity for fitting bins.\n    potential_remain_cap_vals = bins_remain_cap[fit_mask] - item\n\n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity.\n    # This can be achieved by maximizing `original_capacity - item`.\n    # Or more simply, `item - original_capacity` (smaller is better here, so maximize `-(item - original_capacity)`)\n    # Let's focus on minimizing `potential_remain_cap_vals`.\n    # A good score would be `-(potential_remain_cap_vals)`.\n    # To break ties (same `potential_remain_cap_vals`), we prefer bins that had higher initial capacity.\n    # So, we can add `bins_remain_cap[fit_mask]` as a secondary factor.\n    # This means we want to maximize: `-(potential_remain_cap_vals) + C * bins_remain_cap[fit_mask]`\n    # where C is a small constant to ensure primary sorting is by remaining capacity.\n\n    # Let's use a simpler approach that captures the essence:\n    # Prioritize bins that leave the least remainder.\n    # For ties, pick the one that was originally larger.\n    # Score = -(remaining_capacity) + small_bonus * original_capacity\n\n    # Let's refine the score calculation.\n    # For bins that fit:\n    # Primary goal: Minimize `potential_remain_cap`.\n    # Secondary goal: If `potential_remain_cap` is the same, pick the bin that was originally largest.\n\n    # Score: `item - bins_remain_cap[fit_mask]` works well for tight fits.\n    # The highest score is for `bins_remain_cap[fit_mask] == item`.\n    # If there are multiple such bins, they have the same score.\n    # To break ties, we can add a small value related to the original capacity.\n    # Higher original capacity is preferred for ties in remaining capacity.\n\n    # Let's use the inverse of remaining capacity, but penalize bins that are too large.\n    # The \"perfect fit\" is ideal.\n    # So, let's try a combined score:\n    # - Perfect fit: Highest score (e.g., 100)\n    # - Tight fit (small remaining capacity): Score inversely proportional to remaining capacity.\n    # - Break ties by original capacity: Higher original capacity gets a small bonus.\n\n    # Final approach:\n    # 1. Perfect fits get a significant bonus (e.g., 1000).\n    # 2. Other fits get a score based on inverse of remaining capacity + a small bonus\n    #    for larger original capacity to break ties.\n    #    Score = 1 / (potential_remain_cap + epsilon) + original_capacity * epsilon_small\n    #    where epsilon is for avoiding division by zero and epsilon_small for tie-breaking.\n\n    # Revised attempt focusing on minimizing remaining capacity and then maximizing original capacity for ties.\n    # Score = -(remaining_capacity) + (original_capacity / MaxCapacity) * epsilon_tiebreak\n    # This needs careful scaling.\n\n    # Let's use a scoring that strongly favors perfect fits, then tight fits, and then larger original bins.\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    scores.fill(-np.inf) # Initialize with a very low score\n\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    potential_remain_cap_vals = fitting_bins_capacity - item\n\n    # Perfect fits have the highest priority\n    perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals == 0)[0]\n    if len(perfect_fit_indices_in_fitting) > 0:\n        scores[can_fit_mask][perfect_fit_indices_in_fitting] = 1000.0\n\n    # Tight fits get priority based on negative remaining capacity\n    # Add a small factor to ensure they are ranked below perfect fits but above others.\n    # The secondary criterion: prefer larger original capacity among those with same remainder.\n    # We can add `fitting_bins_capacity` multiplied by a small factor.\n    # Let's combine this into a single score for non-perfect fits.\n    non_perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals > 0)[0]\n    if len(non_perfect_fit_indices_in_fitting) > 0:\n        # Score = -(remaining_capacity) + bonus for larger original capacity\n        # The bonus should be small enough not to override the primary goal of minimizing remainder.\n        # Use a small fraction of max possible capacity as tie-breaker.\n        # Max possible remainder is roughly bin_capacity. So, max of `fitting_bins_capacity` can be used.\n        max_cap_val = np.max(fitting_bins_capacity) if fitting_bins_capacity.size > 0 else 1.0\n        tie_breaker_factor = 1e-6 # A very small factor for tie-breaking\n\n        # Score for non-perfect fits: prioritize less remaining capacity, then more original capacity.\n        scores[can_fit_mask][non_perfect_fit_indices_in_fitting] = \\\n            -(potential_remain_cap_vals[non_perfect_fit_indices_in_fitting]) \\\n            + (fitting_bins_capacity[non_perfect_fit_indices_in_fitting] / max_cap_val) * tie_breaker_factor\n\n    # Ensure perfect fits still have highest priority if they exist\n    # If perfect fits were assigned 1000.0, and non-perfect fits get scores like -0.1 + 0.99 * 1e-6,\n    # this order is maintained.\n\n    # If there are no perfect fits, and multiple bins have the same minimal remainder,\n    # the tie-breaker correctly selects the one with higher original capacity.\n\n    return scores\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins that result in the tightest fit after packing,\n    with a bonus for perfect fits and a fallback for worst-fit among remaining options.\"\"\"\n\n    # Initialize priorities with a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in valid bins.\n    potential_remain_cap = bins_remain_cap[fit_mask] - item\n\n    # Assign scores:\n    # 1. High priority for perfect fits (remaining capacity = 0).\n    #    We give a bonus score (e.g., 1.0) to indicate preference.\n    perfect_fit_mask = potential_remain_cap == 0\n    priorities[fit_mask][perfect_fit_mask] = 1.0\n\n    # 2. For other valid fits, prioritize bins that leave less remaining capacity.\n    #    This encourages a \"tight\" packing. We use the negative of the remaining capacity.\n    #    The smallest non-negative remaining capacity will get the highest score here.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    priorities[fit_mask][non_perfect_fit_mask] = -potential_remain_cap[non_perfect_fit_mask]\n\n    # 3. Tie-breaking for bins that result in the same remaining capacity (or are perfect fits):\n    #    Among bins with the same resulting remaining capacity, favor the one with the\n    #    largest original remaining capacity (closer to worst-fit among good fits).\n    #    This helps to keep smaller bins available for smaller items.\n    #    We can achieve this by adding a small bonus proportional to the original capacity.\n    #    Add a small epsilon to the priority to break ties.\n    #    For perfect fits, they already have a score of 1.0. We want to differentiate them.\n    #    Let's use the negative of the original remaining capacity as a secondary score.\n    #    This means, for equally good fits, we prefer the one that was originally larger.\n    #    The `item - bins_remain_cap[fit_mask]` was a good starting point for 'tightness'\n    #    but did not handle tie-breaking well.\n\n    # Let's combine the score: Prioritize by minimizing remaining capacity.\n    # A simple way is to maximize `-(remaining_capacity)`.\n    # To differentiate between equally good fits, we can use the original capacity.\n    # If two bins result in the same remaining capacity, the one that started larger\n    # (worst fit among the tightest) is preferred.\n    # So, for the same `-(remaining_capacity)`, we want to maximize the original capacity.\n\n    # Re-calculating for clarity and combined logic:\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacity for fitting bins.\n    potential_remain_cap_vals = bins_remain_cap[fit_mask] - item\n\n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity.\n    # This can be achieved by maximizing `original_capacity - item`.\n    # Or more simply, `item - original_capacity` (smaller is better here, so maximize `-(item - original_capacity)`)\n    # Let's focus on minimizing `potential_remain_cap_vals`.\n    # A good score would be `-(potential_remain_cap_vals)`.\n    # To break ties (same `potential_remain_cap_vals`), we prefer bins that had higher initial capacity.\n    # So, we can add `bins_remain_cap[fit_mask]` as a secondary factor.\n    # This means we want to maximize: `-(potential_remain_cap_vals) + C * bins_remain_cap[fit_mask]`\n    # where C is a small constant to ensure primary sorting is by remaining capacity.\n\n    # Let's use a simpler approach that captures the essence:\n    # Prioritize bins that leave the least remainder.\n    # For ties, pick the one that was originally larger.\n    # Score = -(remaining_capacity) + small_bonus * original_capacity\n\n    # Let's refine the score calculation.\n    # For bins that fit:\n    # Primary goal: Minimize `potential_remain_cap`.\n    # Secondary goal: If `potential_remain_cap` is the same, pick the bin that was originally largest.\n\n    # Score: `item - bins_remain_cap[fit_mask]` works well for tight fits.\n    # The highest score is for `bins_remain_cap[fit_mask] == item`.\n    # If there are multiple such bins, they have the same score.\n    # To break ties, we can add a small value related to the original capacity.\n    # Higher original capacity is preferred for ties in remaining capacity.\n\n    # Let's use the inverse of remaining capacity, but penalize bins that are too large.\n    # The \"perfect fit\" is ideal.\n    # So, let's try a combined score:\n    # - Perfect fit: Highest score (e.g., 100)\n    # - Tight fit (small remaining capacity): Score inversely proportional to remaining capacity.\n    # - Break ties by original capacity: Higher original capacity gets a small bonus.\n\n    # Final approach:\n    # 1. Perfect fits get a significant bonus (e.g., 1000).\n    # 2. Other fits get a score based on inverse of remaining capacity + a small bonus\n    #    for larger original capacity to break ties.\n    #    Score = 1 / (potential_remain_cap + epsilon) + original_capacity * epsilon_small\n    #    where epsilon is for avoiding division by zero and epsilon_small for tie-breaking.\n\n    # Revised attempt focusing on minimizing remaining capacity and then maximizing original capacity for ties.\n    # Score = -(remaining_capacity) + (original_capacity / MaxCapacity) * epsilon_tiebreak\n    # This needs careful scaling.\n\n    # Let's use a scoring that strongly favors perfect fits, then tight fits, and then larger original bins.\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    scores.fill(-np.inf) # Initialize with a very low score\n\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    potential_remain_cap_vals = fitting_bins_capacity - item\n\n    # Perfect fits have the highest priority\n    perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals == 0)[0]\n    if len(perfect_fit_indices_in_fitting) > 0:\n        scores[can_fit_mask][perfect_fit_indices_in_fitting] = 1000.0\n\n    # Tight fits get priority based on negative remaining capacity\n    # Add a small factor to ensure they are ranked below perfect fits but above others.\n    # The secondary criterion: prefer larger original capacity among those with same remainder.\n    # We can add `fitting_bins_capacity` multiplied by a small factor.\n    # Let's combine this into a single score for non-perfect fits.\n    non_perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals > 0)[0]\n    if len(non_perfect_fit_indices_in_fitting) > 0:\n        # Score = -(remaining_capacity) + bonus for larger original capacity\n        # The bonus should be small enough not to override the primary goal of minimizing remainder.\n        # Use a small fraction of max possible capacity as tie-breaker.\n        # Max possible remainder is roughly bin_capacity. So, max of `fitting_bins_capacity` can be used.\n        max_cap_val = np.max(fitting_bins_capacity) if fitting_bins_capacity.size > 0 else 1.0\n        tie_breaker_factor = 1e-6 # A very small factor for tie-breaking\n\n        # Score for non-perfect fits: prioritize less remaining capacity, then more original capacity.\n        scores[can_fit_mask][non_perfect_fit_indices_in_fitting] = \\\n            -(potential_remain_cap_vals[non_perfect_fit_indices_in_fitting]) \\\n            + (fitting_bins_capacity[non_perfect_fit_indices_in_fitting] / max_cap_val) * tie_breaker_factor\n\n    # Ensure perfect fits still have highest priority if they exist\n    # If perfect fits were assigned 1000.0, and non-perfect fits get scores like -0.1 + 0.99 * 1e-6,\n    # this order is maintained.\n\n    # If there are no perfect fits, and multiple bins have the same minimal remainder,\n    # the tie-breaker correctly selects the one with higher original capacity.\n\n    return scores\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tight packing preference with a penalty for large wasted space,\n    and a bonus for near-perfect fits, aiming for efficient and strategic packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Best Fit component - prioritize bins with minimal remaining space after fitting.\n    # This is essentially prioritizing the smallest positive `remaining_after_fit`.\n    # We use a negative value so that smaller remaining_after_fit gets a higher score.\n    # Adding a small epsilon to avoid division by zero for perfect fits, then scaling.\n    best_fit_score = -100.0 / (remaining_after_fit + 1e-9)\n\n    # Score 2: Penalty for large wasted space - penalize bins that leave a lot of capacity.\n    # This discourages using a bin that is only slightly occupied.\n    # Normalize remaining capacity to bound the penalty.\n    max_residual = np.max(bins_remain_cap) # Max possible remaining capacity before fitting\n    large_waste_penalty = - (remaining_after_fit / max_residual) * 50.0 if max_residual > 0 else 0.0\n\n    # Score 3: Bonus for near-perfect fits - reward bins that leave very little space.\n    near_perfect_fit_threshold = 0.05\n    perfect_fit_bonus = 20.0\n    near_perfect_fit_bonus = 10.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Combine scores: Prioritize tight fits (Best Fit), penalize large waste, and reward near-perfect fits.\n    # The weights are empirical and can be tuned.\n    combined_scores = best_fit_score + large_waste_penalty + fit_bonus\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit's tight packing preference with a penalty for large wasted space,\n    and a bonus for near-perfect fits, aiming for efficient and strategic packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Best Fit component - prioritize bins with minimal remaining space after fitting.\n    # This is essentially prioritizing the smallest positive `remaining_after_fit`.\n    # We use a negative value so that smaller remaining_after_fit gets a higher score.\n    # Adding a small epsilon to avoid division by zero for perfect fits, then scaling.\n    best_fit_score = -100.0 / (remaining_after_fit + 1e-9)\n\n    # Score 2: Penalty for large wasted space - penalize bins that leave a lot of capacity.\n    # This discourages using a bin that is only slightly occupied.\n    # Normalize remaining capacity to bound the penalty.\n    max_residual = np.max(bins_remain_cap) # Max possible remaining capacity before fitting\n    large_waste_penalty = - (remaining_after_fit / max_residual) * 50.0 if max_residual > 0 else 0.0\n\n    # Score 3: Bonus for near-perfect fits - reward bins that leave very little space.\n    near_perfect_fit_threshold = 0.05\n    perfect_fit_bonus = 20.0\n    near_perfect_fit_bonus = 10.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Combine scores: Prioritize tight fits (Best Fit), penalize large waste, and reward near-perfect fits.\n    # The weights are empirical and can be tuned.\n    combined_scores = best_fit_score + large_waste_penalty + fit_bonus\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the most remaining capacity.\n    This heuristic attempts to keep bins with less capacity available for smaller items later on,\n    potentially leading to a more efficient packing in the long run.\n    In this priority function, we assign a higher priority score to bins with larger remaining capacities.\n    Specifically, the priority is directly proportional to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.array([bin_cap if bin_cap >= item else -np.inf for bin_cap in bins_remain_cap])\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}