{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins based on a weighted combination of tight-fitting potential\n    and a penalty for leaving very small, unusable remaining capacities.\n    Includes a small exploration factor for bins that are not perfect fits.\n    \"\"\"\n    epsilon = 0.05  # Reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Calculate scores for valid bins\n    # Score is primarily based on how tightly the item fits (minimizing remaining capacity)\n    # We invert the remaining capacity after fitting to get a \"tightness\" score.\n    # Higher score means tighter fit.\n    tight_fit_scores = valid_bins_capacities - item\n\n    # Penalize very small remaining capacities that might be \"wasted space\"\n    # This encourages filling bins more completely when possible.\n    # We use a small value for remaining capacities close to zero to avoid large penalties for perfect fits.\n    small_residual_penalty = np.maximum(0, 0.1 - tight_fit_scores) * 1000  # Heavier penalty for unusable space\n    \n    # Combine tight fit score with penalty.\n    # We want to maximize tight_fit_scores (minimize remaining capacity)\n    # and minimize small_residual_penalty.\n    # So, score = tight_fit_scores - small_residual_penalty\n    exploitation_scores = tight_fit_scores - small_residual_penalty\n\n    # Add a small bonus for perfect fits to explicitly favor them.\n    perfect_fit_bonus = 1e-3\n    exploitation_scores[np.abs(tight_fit_scores) < 1e-9] += perfect_fit_bonus\n\n    # Exploration: For bins that are not perfect fits, introduce a small random component\n    # to explore alternatives that might lead to better overall packing.\n    # We apply exploration only to a subset of non-perfect-fit bins.\n    exploration_mask = np.abs(tight_fit_scores) >= 1e-9\n    \n    if np.sum(exploration_mask) > 0:\n        exploration_scores_part = np.random.rand(np.sum(exploration_mask)) * 0.1 # Small random exploration score\n        \n        # Apply exploration scores probabilistically to a subset of non-perfect fit bins\n        exploration_indices_in_valid = np.where(exploration_mask)[0]\n        exploration_selection_prob = np.random.rand(len(exploration_indices_in_valid)) < epsilon\n\n        selected_exploration_indices_in_valid = exploration_indices_in_valid[exploration_selection_prob]\n        \n        exploitation_scores[selected_exploration_indices_in_valid] += exploration_scores_part[np.where(exploration_selection_prob)[0]]\n\n    priorities[can_fit_mask] = exploitation_scores\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** These heuristics are identical. The ranking suggests a potential issue with the evaluation or the ranking itself. However, assuming they represent a valid point in the ranking, their complexity and explicit handling of \"exploration candidates\" (top K fits and moderate capacity bins) is a notable design choice. The \"surplus penalty\" is a nuanced addition to pure tight-fitting.\n\n*   **Heuristics 2 vs 3 & 4:** Heuristics 3 and 4 are identical and simpler than Heuristic 2. They implement a basic epsilon-greedy approach (random choice vs. tight fit). Heuristic 2's \"adaptive exploration\" is more sophisticated, trying to guide exploration towards \"good enough\" bins rather than purely random ones. The penalty for large surpluses in Heuristic 2 also adds a layer of strategic thinking. The simpler epsilon-greedy in 3 & 4 is less likely to discover complex packing patterns.\n\n*   **Heuristics 3 & 4 vs 5 & 6:** Heuristics 5 and 6 are also simpler than 2 but introduce different mechanisms. Heuristic 6 is a pure \"Best Fit\" (minimizing remaining space). Heuristic 5 uses a softmax approach, turning \"fit scores\" into probabilities. This is an interesting probabilistic exploration. Comparing 3/4 (epsilon-greedy) to 5 (softmax) and 6 (pure best fit), the epsilon-greedy approach (3/4) is more direct in its exploration strategy. Best Fit (6) is purely exploitative, while Softmax (5) introduces a probabilistic distribution over good fits. The ranking suggests that these more direct or probabilistic approaches are less effective than the hybrid ones.\n\n*   **Heuristics 6 vs 7 & 8 & 9:** Heuristics 7, 8, and 9 are similar, focusing on tight fits and bonuses for perfect fits. Heuristic 6 (pure Best Fit) is simpler. Heuristics 7-9 add specific bonuses and sometimes penalties. Heuristic 8 & 9 are identical. The ranking indicates that adding these specific bonuses (like for perfect fits) and penalties (like for large remainders) improves performance over pure Best Fit. Heuristic 7 is more basic than 8/9.\n\n*   **Heuristics 7 vs 10 & 11:** Heuristic 10 is identical to 5. Heuristic 11 uses \"Almost Full Fit\" by maximizing `item - bins_remain_cap`, which is equivalent to minimizing `bins_remain_cap - item` (Best Fit). It does not explicitly add bonuses or penalties. The ranking suggests that the more nuanced approaches (like those in 7-9) are better than this direct Best Fit implementation.\n\n*   **Heuristics 11 vs 12:** Heuristic 12 is the most complex, combining penalties for wasted space, bonuses for near-perfect fits, and a \"soft\" preference for bins with more capacity (exploration-guided). Its higher ranking suggests that this multi-objective optimization within the heuristic is effective. Heuristic 11 is a simpler Best Fit variant.\n\n*   **Heuristics 12 vs 13:** Heuristic 13 is an FFD-inspired heuristic, prioritizing tight fits with a small penalty for slight oversights. It's simpler than 12. The ranking places 12 significantly higher, implying its combined strategy is superior.\n\n*   **Heuristics 13 vs 14 & 15 & 16:** Heuristic 14 is a simple Best Fit with explicit probabilities, but the loop implementation is inefficient. Heuristics 15 and 16 are identical and appear to be optimized versions of Heuristic 2, with hyperparameter tuning evident (e.g., `epsilon`, `perfect_fit_bonus`). These are still ranked lower than 13, which might indicate that the specific parameter values or the overall strategy of 13 is slightly better than the tuned 2. The presence of `torch` and `scipy` imports in 14-16, but not used in the snippet, is odd; they might be remnants of a larger framework or an indication of potential future extensions. The ranking suggests that even tuned versions of simpler hybrid strategies are not as good as the complex one (12) or even the FFD-inspired one (13).\n\n*   **Heuristics 15 & 16 vs 17 & 18:** Heuristics 17 and 18 are identical \"Random Fit\" strategies. They simply assign random priorities to bins that can fit the item. These are the simplest and worst-performing strategies, as they lack any exploitation logic. The ranking clearly places them at the bottom.\n\n*   **Heuristics 17 & 18 vs 19 & 20:** Heuristic 19 aims to combine perfect fits, tight fits, and tie-breaking based on original capacity. Heuristic 20 tries to balance tight fitting with penalties for small residuals and adds exploration. The ranking places 19 and 20 above random fit but below the more sophisticated hybrid strategies. Heuristic 19's tie-breaking logic is a good addition. Heuristic 20's penalty for small residuals is an interesting refinement.\n\n*   **Overall:** The top-performing heuristics (1, 2, 12) are complex, often combining tight-fitting principles with sophisticated exploration strategies (adaptive exploration, multi-objective scoring). Simpler \"Best Fit\" or epsilon-greedy approaches are ranked lower. Pure random strategies are at the bottom. The use of explicit bonuses and penalties for specific fit scenarios (perfect fit, large/small residuals) appears beneficial. The ranking suggests a hierarchy: complex hybrids > refined hybrids/FFD-inspired > basic hybrids/probabilistic > simple exploitation (Best Fit) > random.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective optimization, guided exploration, adaptive reward functions, hybrid strategies.\n*   **Advice:** Focus on iterative refinement of multi-objective heuristics, dynamically adjusting exploration/exploitation based on problem state.\n*   **Avoid:** Over-reliance on static or purely greedy approaches; neglecting the interplay between penalizing poor fits and rewarding ideal fits.\n*   **Explanation:** By viewing heuristic design as a continuous learning process, you can better identify optimal trade-offs between thorough exploration and efficient exploitation, leading to more robust and performant solutions.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}