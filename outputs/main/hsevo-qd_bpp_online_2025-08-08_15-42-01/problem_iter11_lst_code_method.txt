{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    epsilon = 0.03\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Multi-objective scoring:\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    \n    tightness_score = -remaining_after_fit\n    \n    # Penalty for large remainders, scaled by item size\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n    \n    # Reward for significant remaining capacity - can be useful for larger items later\n    future_capacity_score = remaining_after_fit / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(remaining_after_fit)\n    \n    # Combine objectives with weights. These weights can be tuned.\n    # We want to strongly favor tightness, moderately avoid waste, and lightly favor future capacity.\n    weight_tightness = 1.0\n    weight_waste = 0.5\n    weight_future_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # Enhanced Exploration:\n    # Instead of random exploration, we can introduce guided exploration.\n    # This means exploring bins that are \"good enough\" but not necessarily the absolute best.\n    # We can define \"good enough\" as bins that fall within a certain percentile of the best fits.\n    \n    # Sort bins by the combined score to identify top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Identify a range of bins to consider for exploration\n    # This could be the top K bins, or bins within a certain score range.\n    # Let's consider bins within the top 30% of scores, or at least the top 3 bins.\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.3))\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Also consider bins that offer a \"balanced\" fit, not too tight, not too empty.\n    # A bin that leaves a moderate amount of space might be more versatile.\n    # Let's consider bins where remaining_after_fit is between a small fraction and a larger fraction of bin capacity.\n    \n    # To define \"moderate\", we can look at the distribution of remaining capacities.\n    # Let's use quartiles for guidance.\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    \n    # Bins with remaining capacity between Q1 and Q3 (inclusive of Q3) are considered moderately remaining.\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    \n    # Combine indices for exploration candidates\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    \n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates\n    # We want these exploration scores to be slightly random but not too high,\n    # to offer a chance for diversity without sacrificing too much performance.\n    exploration_scores = np.random.uniform(-0.1, 0.1, size=len(valid_bins_capacities))\n    \n    # Apply exploration scores:\n    # With probability epsilon, use exploration score for exploration candidates.\n    # Otherwise, use the combined score.\n    # For non-candidates, always use the combined score.\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates\n    exploration_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask[exploration_candidate_indices_in_valid] = True\n    \n    # Decide whether to use exploration score for exploration candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_mask & use_exploration_for_candidates] = exploration_scores[exploration_mask & use_exploration_for_candidates]\n    \n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \n    epsilon = 0.03\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Multi-objective scoring:\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    \n    tightness_score = -remaining_after_fit\n    \n    # Penalty for large remainders, scaled by item size\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n    \n    # Reward for significant remaining capacity - can be useful for larger items later\n    future_capacity_score = remaining_after_fit / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(remaining_after_fit)\n    \n    # Combine objectives with weights. These weights can be tuned.\n    # We want to strongly favor tightness, moderately avoid waste, and lightly favor future capacity.\n    weight_tightness = 1.0\n    weight_waste = 0.5\n    weight_future_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # Enhanced Exploration:\n    # Instead of random exploration, we can introduce guided exploration.\n    # This means exploring bins that are \"good enough\" but not necessarily the absolute best.\n    # We can define \"good enough\" as bins that fall within a certain percentile of the best fits.\n    \n    # Sort bins by the combined score to identify top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Identify a range of bins to consider for exploration\n    # This could be the top K bins, or bins within a certain score range.\n    # Let's consider bins within the top 30% of scores, or at least the top 3 bins.\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.3))\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Also consider bins that offer a \"balanced\" fit, not too tight, not too empty.\n    # A bin that leaves a moderate amount of space might be more versatile.\n    # Let's consider bins where remaining_after_fit is between a small fraction and a larger fraction of bin capacity.\n    \n    # To define \"moderate\", we can look at the distribution of remaining capacities.\n    # Let's use quartiles for guidance.\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    \n    # Bins with remaining capacity between Q1 and Q3 (inclusive of Q3) are considered moderately remaining.\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    \n    # Combine indices for exploration candidates\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    \n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates\n    # We want these exploration scores to be slightly random but not too high,\n    # to offer a chance for diversity without sacrificing too much performance.\n    exploration_scores = np.random.uniform(-0.1, 0.1, size=len(valid_bins_capacities))\n    \n    # Apply exploration scores:\n    # With probability epsilon, use exploration score for exploration candidates.\n    # Otherwise, use the combined score.\n    # For non-candidates, always use the combined score.\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates\n    exploration_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask[exploration_candidate_indices_in_valid] = True\n    \n    # Decide whether to use exploration score for exploration candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_mask & use_exploration_for_candidates] = exploration_scores[exploration_mask & use_exploration_for_candidates]\n    \n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting (Best Fit) with a penalty for large unused capacity\n    and a bonus for perfect fits, guided by adaptive exploration principles.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity (closer to worst-fit among good fits).\n    # Bonus for perfect fit, penalty for large unused capacity.\n    \n    # Calculate remaining capacity if item fits\n    potential_remain_cap_vals = fitting_bins_capacities - item\n    \n    # Base score: favoring tight fits (smaller remaining capacity)\n    # Use negative remaining capacity to maximize for smaller remainders.\n    scores = -potential_remain_cap_vals\n    \n    # Bonus for perfect fits\n    perfect_fit_mask = np.isclose(potential_remain_cap_vals, 0)\n    scores[perfect_fit_mask] += 10.0 # Significant bonus for perfect fit\n    \n    # Penalty for large unused capacity: Discourage bins that are much larger than needed.\n    # This is a form of guided exploration towards \"good enough\" bins.\n    # We penalize bins where the remaining capacity is significantly larger than a small residual.\n    # A simple penalty could be proportional to the surplus capacity, but we want it to be less\n    # impactful than the tight-fitting score.\n    # Let's add a penalty that decreases the score for bins with large remaining capacity.\n    # This penalty should be smaller than the gains from tight fits.\n    # Penalty factor: For every unit of capacity above a small threshold (e.g., 10% of item size),\n    # we slightly reduce the score.\n    \n    # Define a threshold for \"large unused capacity\"\n    large_capacity_threshold = item * 0.5 # e.g., if remaining capacity is > 50% of item size\n\n    # Calculate penalty: linearly decreasing score for capacity beyond the threshold.\n    # Bins with capacity <= threshold get no penalty.\n    penalty = np.zeros_like(scores)\n    large_surplus_mask = potential_remain_cap_vals > large_capacity_threshold\n    \n    # The penalty is proportional to how much the surplus exceeds the threshold.\n    # We want this penalty to be relatively small compared to the tight-fit scores.\n    # e.g., subtract (surplus - threshold) / max_possible_surplus * small_value\n    max_possible_surplus = np.max(fitting_bins_capacities) # An upper bound on surplus\n    \n    if np.any(large_surplus_mask) and max_possible_surplus > 0:\n        penalty[large_surplus_mask] = (potential_remain_cap_vals[large_surplus_mask] - large_capacity_threshold) / max_possible_surplus * 5.0\n        \n    scores -= penalty\n\n    # Assign the calculated scores to the original bins array\n    priorities[can_fit_mask] = scores\n    \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a soft penalty for waste, inspired by Softmax-Fit.\n    Prioritizes bins that leave minimal remaining capacity, with a boost for perfect fits.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return output_priorities\n\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    # Calculate a score: inverse of remaining capacity after fit, favoring tighter fits.\n    # Add a small epsilon to prevent division by zero.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Introduce a bonus for perfect fits to strongly prioritize them.\n    perfect_fit_mask = (valid_bins_cap - item) < 1e-9\n    fit_scores[perfect_fit_mask] *= 5.0  # Significant bonus for perfect fits\n\n    # Apply softmax to normalize scores into probabilities/priorities.\n    # This provides a smoother distribution than pure inverse,\n    # while still emphasizing bins with higher fit_scores.\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a Best Fit strategy with a penalty for large remaining capacity\n    and a bonus for near-perfect fits, aiming for efficient packing.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    # Prioritize bins that can fit the item\n    # Use negative remaining capacity to favor tighter fits (Best Fit)\n    # Add a small penalty for larger remaining capacities to avoid extremely sparse bins\n    # Add a bonus for near-perfect fits to exploit efficient packing opportunities\n    priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask] - 0.1 * (bins_remain_cap[can_fit_mask] - item) + 10 * np.exp(-(bins_remain_cap[can_fit_mask] - item)**2 / 0.1)\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    A multi-objective priority function for online bin packing.\n    It balances 'tightest fit' with 'future utility' and guided exploration.\n    It explicitly rewards bins with high remaining capacity for future items\n    and penalizes bins that might become \"awkwardly empty\" after packing.\n    Exploration is focused on bins that are good candidates for either\n    tight fit or future utility, with a bias towards the former.\n    \"\"\"\n    epsilon = 0.1  # Probability of biased exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    remaining_after_fit = valid_bins_capacities - item\n\n    # --- Multi-Objective Scoring ---\n    # 1. Tightest Fit Score: Maximize -remaining_after_fit. Higher score for less remaining space.\n    tightest_fit_scores = -remaining_after_fit\n\n    # 2. Future Utility Score: Maximize remaining capacity. Higher score for more remaining space.\n    # This encourages leaving larger bins for potentially larger future items.\n    future_utility_scores = remaining_after_fit\n\n    # 3. Perfect Fit Bonus: Add a significant bonus for exact matches.\n    perfect_fit_bonus = 1000.0\n    tightest_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n    future_utility_scores[np.abs(remaining_after_fit) < 1e-9] = 0 # No future utility for perfect fit\n\n    # 4. Awkward Empty Penalty: Penalize bins that become \"too empty\" after packing,\n    # relative to the item size. This discourages leaving small unusable remainders.\n    # A bin becoming empty by more than twice the item size is penalized.\n    awkward_empty_threshold_ratio = 2.0\n    awkward_empty_penalty_factor = 0.5\n    awkward_mask = remaining_after_fit > (item * awkward_empty_threshold_ratio)\n    if np.any(awkward_mask):\n        # Penalty is proportional to how much it exceeds the threshold, normalized by item size\n        awkward_penalty = (remaining_after_fit[awkward_mask] / item - awkward_empty_threshold_ratio) * awkward_empty_penalty_factor\n        tightest_fit_scores[awkward_mask] -= awkward_penalty\n        future_utility_scores[awkward_mask] -= awkward_penalty # Penalize future utility as well\n\n    # --- Combined Score ---\n    # A weighted sum of tightest fit and future utility.\n    # Weights can be adjusted. Here, we give slightly more weight to tight fit.\n    tight_fit_weight = 0.6\n    future_utility_weight = 0.4\n    \n    # Normalize scores to prevent one objective from dominating due to scale.\n    # Using robust scaling (median and IQR) might be better, but simple min-max is often sufficient.\n    # We will normalize within the valid bins only.\n    \n    min_tf = np.min(tightest_fit_scores)\n    max_tf = np.max(tightest_fit_scores)\n    range_tf = max_tf - min_tf if max_tf != min_tf else 1.0\n    normalized_tight_fit = (tightest_fit_scores - min_tf) / range_tf if range_tf != 0 else np.zeros_like(tightest_fit_scores)\n\n    min_fu = np.min(future_utility_scores)\n    max_fu = np.max(future_utility_scores)\n    range_fu = max_fu - min_fu if max_fu != min_fu else 1.0\n    normalized_future_utility = (future_utility_scores - min_fu) / range_fu if range_fu != 0 else np.zeros_like(future_utility_scores)\n\n    combined_scores = (tight_fit_weight * normalized_tight_fit) + (future_utility_weight * normalized_future_utility)\n\n    # --- Guided Exploration ---\n    # Exploration will be applied to a subset of bins that are \"promising\" based on the combined score.\n    # \"Promising\" bins are those with high combined scores.\n    \n    sorted_indices_combined = np.argsort(combined_scores)[::-1] # Indices sorted by combined score (descending)\n\n    # Exploration candidates: Top K bins by combined score, plus bins that represent\n    # a good compromise (e.g., not perfectly tight, but good future utility, or vice-versa).\n    \n    exploration_candidate_mask = np.zeros_like(combined_scores, dtype=bool)\n    \n    # Select top 30% of bins as initial candidates for exploration\n    num_top_candidates = min(len(combined_scores), max(1, int(len(combined_scores) * 0.3)))\n    exploration_candidate_mask[sorted_indices_combined[:num_top_candidates]] = True\n\n    # Add bins with high future utility but not necessarily top-tier tight fit,\n    # as these might be useful for larger items that might come later.\n    # Threshold: bins with future utility in the top 50% percentile, but not in the top 20% of combined scores.\n    utility_threshold = np.percentile(future_utility_scores, 50)\n    high_utility_mask = future_utility_scores >= utility_threshold\n    \n    # Exclude already selected top candidates to avoid over-representation\n    already_selected_mask = exploration_candidate_mask\n    potential_explore_candidates = high_utility_mask & ~already_selected_mask\n    \n    # Add a portion of these high-utility candidates\n    num_high_utility_to_add = min(np.sum(potential_explore_candidates), max(0, int(len(combined_scores) * 0.1)))\n    \n    if num_high_utility_to_add > 0:\n        high_utility_indices = np.where(potential_explore_candidates)[0]\n        # Randomly select from potential high utility candidates\n        selected_high_utility_indices = np.random.choice(high_utility_indices, num_high_utility_to_add, replace=False)\n        exploration_candidate_mask[selected_high_utility_indices] = True\n\n    # Generate exploration scores (noisy values for exploration candidates)\n    exploration_scores = np.random.rand(len(combined_scores)) * 0.05 # Small random noise\n\n    # --- Decision: Exploit or Explore ---\n    # With probability epsilon, choose exploration score for selected candidates.\n    # Otherwise, use the combined exploitation score.\n    # For bins not selected as candidates, always use the combined score.\n\n    final_priorities_for_valid_bins = np.copy(combined_scores)\n\n    # For candidates, decide whether to explore\n    should_explore_mask = np.random.rand(len(combined_scores)) < epsilon\n    \n    # Apply exploration score ONLY to candidates where exploration is chosen\n    apply_exploration_mask = exploration_candidate_mask & should_explore_mask\n    \n    # Use exploration score for these bins\n    final_priorities_for_valid_bins[apply_exploration_mask] = exploration_scores[apply_exploration_mask]\n\n    priorities[valid_bins_indices] = final_priorities_for_valid_bins\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.10086824335308542, perfect_fit_bonus: float = 0.14134762876801932, large_remainder_penalty_factor: float = 0.0037181076773583456, exploration_noise_scale: float = 0.014738605941995732, exploration_candidate_portion: float = 0.36099361922072437, moderate_capacity_multiplier: float = 1.5506786957399679) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by combining tight fitting with a penalty for large surpluses\n    and a bonus for perfect fits, with guided exploration.\n    \"\"\"\n    epsilon = 0.08  # Exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Tight Fit - Maximize negative remaining capacity (prioritize minimal remainder)\n    tight_fit_scores = -remaining_after_fit\n\n    # Score 2: Perfect Fit Bonus - Add a bonus for exact fits\n    perfect_fit_bonus = 0.15\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # Score 3: Surplus Penalty - Mild penalty for bins that would have a large surplus\n    # Scaled by item size to make it relative to the item being packed.\n    large_remainder_penalty_factor = 0.002\n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n\n    # Guided Exploration:\n    # Introduce randomness to a subset of \"good enough\" bins.\n    # Candidates are the top-fitting bins and those with moderate remaining capacity.\n    \n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1] # Indices sorted by tight fit score (desc)\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Select top K% of bins for exploration\n    num_top_candidates = max(1, int(len(valid_bins_capacities) * 0.3))\n    exploration_candidate_indices_in_valid.extend(sorted_indices_tight[:num_top_candidates])\n    \n    # Add bins with moderate remaining capacity (e.g., less than twice the item size, but not too small)\n    moderate_capacity_threshold_upper = np.median(valid_bins_capacities) * 1.5 if len(valid_bins_capacities) > 0 else float('inf')\n    moderate_capacity_threshold_lower = item * 1.1 # Avoid bins that are only slightly larger than item\n    \n    moderate_capacity_mask_in_valid = (remaining_after_fit > (item * 0.1)) & \\\n                                      (remaining_after_fit < moderate_capacity_threshold_upper)\n    \n    moderate_capacity_indices_in_valid = np.where(moderate_capacity_mask_in_valid)[0]\n    exploration_candidate_indices_in_valid.extend(moderate_capacity_indices_in_valid)\n    \n    # Remove duplicates and ensure indices are within bounds\n    exploration_candidate_indices_in_valid = np.unique(exploration_candidate_indices_in_valid)\n    exploration_candidate_indices_in_valid = exploration_candidate_indices_in_valid[\n        exploration_candidate_indices_in_valid < len(valid_bins_capacities)\n    ]\n\n    # Generate exploration scores (small random noise)\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.02\n\n    # Combine scores: use exploration score for candidates with probability epsilon, otherwise tight fit score.\n    # For non-candidates, always use the tight fit score.\n    \n    final_scores = np.copy(tight_fit_scores)\n    \n    # Create a mask for the candidate bins within the valid set\n    is_candidate_mask_in_valid = np.zeros(len(valid_bins_capacities), dtype=bool)\n    is_candidate_mask_in_valid[exploration_candidate_indices_in_valid] = True\n    \n    # Decide probabilistically whether to use exploration score for candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply exploration scores to candidates if chosen\n    explore_mask_combined = is_candidate_mask_in_valid & use_exploration_for_candidates\n    final_scores[explore_mask_combined] = exploration_scores[explore_mask_combined]\n\n    # Assign the final scores to the priorities array\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the most remaining capacity.\n    This heuristic attempts to keep bins with less capacity available for smaller items later on,\n    potentially leading to a more efficient packing in the long run.\n    In this priority function, we assign a higher priority score to bins with larger remaining capacities.\n    Specifically, the priority is directly proportional to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.array([bin_cap if bin_cap >= item else -np.inf for bin_cap in bins_remain_cap])\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the most remaining capacity.\n    This heuristic attempts to keep bins with less capacity available for smaller items later on,\n    potentially leading to a more efficient packing in the long run.\n    In this priority function, we assign a higher priority score to bins with larger remaining capacities.\n    Specifically, the priority is directly proportional to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.array([bin_cap if bin_cap >= item else -np.inf for bin_cap in bins_remain_cap])\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}