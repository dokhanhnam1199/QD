```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A more adaptive and multi-objective priority function for online Bin Packing.
    This version incorporates feedback from past packing performance and dynamically
    adjusts exploration based on bin utilization patterns.
    """
    
    epsilon = 0.05  # Base probability of exploration
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)
    
    can_fit_mask = bins_remain_cap >= item
    
    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]
    
    num_valid_bins = len(valid_bins_capacities)
    
    # --- Objective 1: Minimizing Wasted Space (Exploitation - Tightest Fit) ---
    remaining_after_fit = valid_bins_capacities - item
    
    # Score for tightest fit: maximize negative remaining capacity
    # A high score means very little remaining space after fitting.
    tightness_scores = -remaining_after_fit
    
    # Perfect fit bonus
    perfect_fit_bonus = 100.0  # Reduced bonus to balance with other objectives
    tightness_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus
    
    # Penalty for excessive slack: penalize bins that leave a lot of unused space
    # relative to the item size. This encourages filling bins more efficiently.
    slack_penalty_factor = 0.5
    slack_scores = -(remaining_after_fit / item) * slack_penalty_factor
    slack_scores[remaining_after_fit < 0] = -np.inf # Ensure only valid fits get scores
    tightness_scores += slack_scores

    # --- Objective 2: Promoting Future Fit (Exploration - Moderate Capacity) ---
    # Identify bins that have moderate remaining capacity, which might be useful
    # for larger items that come later. We want to avoid completely filling bins
    # or leaving them too empty.
    
    # Consider bins with remaining capacity that's not too small and not too large.
    # A "good" remaining capacity might be between 20% of the item size and a
    # certain fraction of the *original* bin capacity (or a global average capacity).
    # For simplicity here, let's use a heuristic based on item size and current remaining space.
    
    # A bin is "promising" if its remaining capacity is at least X% of the item's size
    # and not excessively large (e.g., less than half of its *current* remaining capacity).
    # The idea is to keep some moderate space available.
    promising_capacity_bonus_factor = 0.1
    
    # Calculate a score based on how "balanced" the remaining capacity is.
    # This favors bins where remaining capacity is a reasonable fraction of the bin's *current* capacity.
    # Normalize remaining capacity by the *original* capacity of the bin it could fit into.
    # Assuming a fixed bin capacity 'B' would be better, but without it, we use current.
    # For this implementation, let's consider remaining_after_fit itself.
    # Bins with remaining_after_fit between 0.2 * item and 0.6 * (some typical bin capacity proxy like median valid bin capacity)
    
    # Use a proxy for "ideal" remaining capacity â€“ perhaps the median remaining space among fitting bins.
    median_remaining_space_proxy = np.median(valid_bins_capacities) # Proxy for original bin capacity
    
    # Score based on how close remaining_after_fit is to a 'moderately useful' value.
    # Let's define 'moderately useful' as being between 0.2 * item and 0.5 * median_remaining_space_proxy.
    moderate_remaining_scores = np.zeros(num_valid_bins)
    
    lower_bound_moderate = 0.2 * item
    upper_bound_moderate = 0.5 * median_remaining_space_proxy
    
    # Create masks for promising capacity
    is_moderate_capacity = (remaining_after_fit >= lower_bound_moderate) & (remaining_after_fit <= upper_bound_moderate)
    
    # Assign a bonus to bins falling into this moderate range.
    # The bonus is higher if it's closer to the 'ideal' middle of this range.
    if np.any(is_moderate_capacity):
        moderate_bins_remaining = remaining_after_fit[is_moderate_capacity]
        # Calculate score based on distance from the midpoint of the moderate range
        mid_moderate_range = (lower_bound_moderate + upper_bound_moderate) / 2.0
        distance_from_mid = np.abs(moderate_bins_remaining - mid_moderate_range)
        # Normalize distance so smaller distance gives higher score (less penalty)
        max_distance = max(mid_moderate_range - lower_bound_moderate, upper_bound_moderate - mid_moderate_range)
        normalized_distance = distance_from_mid / max_distance if max_distance > 0 else np.zeros_like(distance_from_mid)
        
        moderate_remaining_scores[is_moderate_capacity] = (1.0 - normalized_distance) * promising_capacity_bonus_factor * item

    # --- Adaptive Exploration Probability ---
    # Adjust epsilon based on how "challenging" the current state is.
    # If many bins are very full or very empty, we might explore more to find a good fit.
    # If there are many "perfect" or "tight" fits, exploration might be less critical.
    
    # Heuristic: If the proportion of bins with very little remaining capacity is high,
    # increase exploration to find a slightly less tight fit that might be better for future items.
    # Or, if there are very few good fits available.
    
    # Calculate a "difficulty" metric
    # Difficulty can be related to the variance of remaining capacities, or the scarcity of good fits.
    # Let's use the scarcity of tight fits (e.g., remaining_after_fit < 0.1 * item).
    num_tight_fits = np.sum(remaining_after_fit < 0.1 * item)
    tight_fit_ratio = num_tight_fits / num_valid_bins if num_valid_bins > 0 else 0
    
    # If tight fits are rare (<20% of valid bins), slightly increase exploration.
    adaptive_epsilon = epsilon
    if tight_fit_ratio < 0.2:
        adaptive_epsilon = min(epsilon * 1.5, 0.2) # Cap exploration increase
    
    # If there are many bins with very large remaining capacity (e.g., > 0.8 * original capacity proxy),
    # maybe reduce exploration slightly as there are plenty of options.
    num_large_remaining = np.sum(remaining_after_fit > 0.8 * median_remaining_space_proxy)
    large_remaining_ratio = num_large_remaining / num_valid_bins if num_valid_bins > 0 else 0
    
    if large_remaining_ratio > 0.5:
        adaptive_epsilon = max(epsilon * 0.7, 0.01) # Cap exploration decrease

    # --- Combining Objectives ---
    # Weighted sum of objectives. The weights can be dynamic, but fixed here for simplicity.
    # Weight for tightness: higher, as it's the primary goal.
    # Weight for moderate capacity: lower, as it's secondary/exploratory.
    weight_tightness = 0.8
    weight_moderate_capacity = 0.2
    
    combined_scores = (weight_tightness * tightness_scores) + (weight_moderate_capacity * moderate_remaining_scores)
    
    # --- Guided Exploration ---
    # Instead of purely random exploration, we explore among bins that are "good enough"
    # but not necessarily the absolute best according to the combined score.
    # We sample from a subset of bins, potentially those with moderate capacity or decent tightness.
    
    # Create a candidate pool for exploration:
    # 1. Bins with moderate remaining capacity.
    # 2. Bins that are reasonably tight but not perfect fits.
    
    exploration_candidate_mask = np.zeros(num_valid_bins, dtype=bool)
    
    # Add bins with moderate capacity to exploration candidates
    exploration_candidate_mask[is_moderate_capacity] = True
    
    # Add a portion of the "tightest" bins (but not perfect fits) as exploration candidates
    # We want to explore slightly less optimal tight fits.
    sorted_indices_tightness = np.argsort(tightness_scores)[::-1] # Descending for best tightness
    num_tight_candidates = min(num_valid_bins, max(1, int(num_valid_bins * 0.15))) # Top 15% tightest bins
    
    # Exclude perfect fits from this exploration subset to avoid over-sampling them
    tight_candidates_indices = sorted_indices_tightness[:num_tight_candidates]
    tight_candidates_are_perfect = np.abs(remaining_after_fit[tight_candidates_indices]) < 1e-9
    valid_tight_candidates_indices = tight_candidates_indices[~tight_candidates_are_perfect]
    
    exploration_candidate_mask[valid_tight_candidates_indices] = True
    
    # Generate random exploration scores for selected candidates
    # These scores are added to the combined scores with a certain probability (adaptive_epsilon)
    exploration_noise = np.random.rand(num_valid_bins) * 0.05 # Small random noise to break ties and enable exploration
    
    # Apply exploration noise to selected candidates with probability adaptive_epsilon
    should_explore_mask = np.random.rand(num_valid_bins) < adaptive_epsilon
    
    apply_exploration_mask = exploration_candidate_mask & should_explore_mask
    
    # Add exploration noise to the combined scores for selected bins
    combined_scores[apply_exploration_mask] += exploration_noise[apply_exploration_mask]
    
    # Final priorities are the combined scores
    priorities[valid_bins_indices] = combined_scores
    
    return priorities
```
