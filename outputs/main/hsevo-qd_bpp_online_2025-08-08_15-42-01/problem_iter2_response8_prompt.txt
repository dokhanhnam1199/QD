{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Almost Full Fit: Prioritize bins that will be closest to full after adding the item.\n    # Calculate the remaining capacity after placing the item.\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want bins where potential_remain_cap is as close to zero as possible (but non-negative)\n    # A simple way to achieve this is to maximize the negative of the absolute difference from zero.\n    # However, to encourage fitting rather than just being close, we can also consider\n    # bins that can actually fit the item.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = potential_remain_cap >= 0\n\n    # For bins that can fit, calculate a score based on how full they will become.\n    # A higher score means the bin will be closer to full.\n    # We can use -(potential_remain_cap) as a measure of \"fullness\" after packing.\n    # To avoid prioritizing bins that become \"too full\" or negative capacity,\n    # we only consider valid fits.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = -potential_remain_cap[can_fit_mask]\n\n    # We can add a small bonus for fitting the item at all to ensure\n    # that fitting items is generally preferred over not fitting.\n    # However, the negative of remaining capacity already does this indirectly.\n    # A more sophisticated approach might involve considering the original capacity\n    # to prioritize filling larger bins more.\n\n    # For \"Almost Full Fit\", we want to minimize the remaining capacity.\n    # So, we want to maximize the negative of the remaining capacity.\n    # Let's refine this. We want the remaining capacity to be as close to 0 as possible.\n    # The value `potential_remain_cap` represents this.\n    # We want to maximize this value if it's negative (meaning it's a good fit).\n    # Let's re-evaluate. For Almost Full Fit, we want the resulting remaining capacity\n    # to be as small as possible, but still non-negative.\n    # So, we want to maximize `-(potential_remain_cap)` for valid fits.\n\n    # Consider the difference between the bin's original capacity and the item size.\n    # We want to prioritize bins where this difference is minimized, but non-negative.\n    # The `potential_remain_cap` already captures this.\n    # We want the most \"positive\" value of `-(potential_remain_cap)` for bins that fit.\n\n    # Another perspective: prioritize bins that have the *least* remaining capacity *after* the item is placed.\n    # This directly translates to maximizing `-(potential_remain_cap)`.\n\n    # Let's consider a small perturbation or a \"niceness\" factor.\n    # Perhaps bins that are already somewhat full are preferred.\n    # But for \"Almost Full Fit\", the focus is purely on the state *after* packing.\n\n    # We want to maximize the remaining capacity in a way that favors being close to zero.\n    # The score should be higher for bins that result in less remaining capacity.\n    # So, we want to maximize `-(bins_remain_cap - item)` for valid bins.\n    # This is equivalent to maximizing `item - bins_remain_cap`. This isn't quite right.\n\n    # The goal is to make the bin as \"full\" as possible *after* adding the item.\n    # \"Full\" means having small remaining capacity.\n    # So, we want to minimize `potential_remain_cap`.\n    # To turn this into a maximization problem for priority, we can use `-potential_remain_cap`.\n    # We also need to ensure that bins that *cannot* fit the item get a very low priority.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Let's refine the \"Almost Full Fit\" idea. We want the bin with the smallest remaining capacity after packing.\n    # This means we want to minimize `bins_remain_cap - item`.\n    # For a priority score (higher is better), we can use the negative of this difference.\n    # So, we want to maximize `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n\n    # This score will be highest for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # Example: item=5\n    # bin1_rem=6  -> priority = 5 - 6 = -1\n    # bin2_rem=5  -> priority = 5 - 5 = 0\n    # bin3_rem=10 -> priority = 5 - 10 = -5\n    # bin4_rem=3  -> priority = -inf (cannot fit)\n\n    # The highest score is 0, from the bin that becomes exactly full.\n    # This seems correct for \"Almost Full Fit\".\n\n    # Consider a scenario where there are multiple bins that become exactly full.\n    # The current heuristic doesn't differentiate between them.\n    # For this strategy, simply picking any of them is fine.\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1st (Almost Full Fit) vs. Heuristic 16th (Random Fit), the former focuses on a specific fitting strategy (minimizing remaining capacity after placement), while the latter uses random assignment for valid bins. Heuristic 1st is likely better as it's deterministic and aims for efficiency.\n\nComparing Heuristic 2nd (Softmax-Based Fit) vs. Heuristic 11th (Inverse of Remaining Capacity with special case for perfect fit), both aim to prioritize bins with less remaining capacity. Heuristic 2nd uses softmax to normalize these scores into probabilities, which might offer smoother transitions and better exploration of options. Heuristic 11th is simpler but the special case for perfect fit is a good addition.\n\nComparing Heuristic 7th (FFD-inspired with penalty) vs. Heuristic 20th (Worst Fit), Heuristic 7th's approach of prioritizing bins with least remaining capacity is generally preferred in bin packing for tighter fits, whereas Worst Fit prioritizes largest remaining capacity which can lead to fragmentation.\n\nComparing Heuristic 10th and Heuristic 14th (both seem to be identical, \"Best Fit with tie-breaking for perfect fit and worst fit\"), they explicitly handle perfect fits with a bonus and provide a slight bonus for worst fits among available bins, which is more sophisticated than simple inverse remaining capacity.\n\nComparing Heuristic 17th, 18th, and 19th (all identical, Epsilon-Greedy) vs. Heuristic 11th/2nd, the epsilon-greedy approach introduces exploration, which can be beneficial for discovering better packing strategies over time, but it adds complexity and randomness.\n\nOverall: The heuristics that aim for a \"tight fit\" (minimizing remaining capacity after placement), like 1st, 5th, 6th, 7th, 8th, 9th, 10th, 11th, 12th, 13th, 14th, 15th, 17th, 18th, 19th, generally perform better than random or worst-fit strategies. Among these, those with explicit tie-breaking (like 10th/14th) or more nuanced scoring (like softmax in 2nd) show potential for better performance. Epsilon-greedy strategies offer exploration but introduce randomness.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Bin Packing, Tight Packing, Perfect Fits, Exploration, Scoring, Adaptability.\n*   **Advice:** Focus on *maximizing utilization* and *exploring beneficial deviations* from greedy approaches. Quantify the value of \"perfect fits\" and incorporate adaptive scoring.\n*   **Avoid:** Relying solely on simple \"fill first\" logic; ignoring the long-term impact of placement choices; implementing exploration without a clear performance target.\n*   **Explanation:** Current success hinges on packing densely. Effective heuristics should explicitly reward this (e.g., bonus for tight fits) and consider intelligent exploration to find even better packings than pure greedy, rather than random exploration which often degrades performance.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}