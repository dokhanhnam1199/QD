```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A multi-objective priority function for online Bin Packing.
    It considers:
    1. Tightest Fit: Minimizing remaining capacity after packing the item.
    2. Future Usability: Prioritizing bins that will still have significant capacity
       after packing the current item, potentially for larger future items.
    3. Novelty/Exploration: Encouraging the use of less frequently used bins
       that are still viable, to avoid getting stuck in local optima.
    """

    epsilon = 0.1  # Probability of exploring a novel bin
    priorities = np.full_like(bins_remain_cap, -np.inf)

    can_fit_mask = bins_remain_cap >= item
    
    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]

    # --- Objective 1: Tightest Fit ---
    remaining_after_fit = valid_bins_capacities - item
    tightest_fit_scores = -remaining_after_fit

    # Bonus for perfect fits
    perfect_fit_bonus = 1000.0
    tightest_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus

    # --- Objective 2: Future Usability ---
    # Reward bins that have substantial remaining capacity, but not excessively so.
    # This aims to keep some larger bins open.
    # We'll use a soft penalty for very large remaining capacities to avoid
    # completely ignoring bins that might be slightly less tight but much more useful later.
    future_usability_scores = np.zeros_like(valid_bins_capacities)
    
    # Define a "good" remaining capacity range. Bins with capacity between
    # item size and a fraction of total capacity are considered good.
    # We use a Gaussian-like weighting centered around a "moderately large" remaining capacity.
    # This encourages bins that can still hold a decent number of future items.
    
    # Scale remaining capacity relative to the item size. A value of 1 means the bin can fit exactly one more item of the same size.
    relative_remaining = remaining_after_fit / item 
    
    # A simple approach: reward remaining capacity between 1x and 5x the item size.
    # Bins with remaining capacity very close to the item size (tight fits) or extremely large capacities are less rewarded here.
    
    # Gaussian-like reward: peak around remaining capacity = item * 3, decaying for smaller and larger.
    mu = 3.0  # Center of the reward (i.e., remaining capacity is 3 times the item size)
    sigma = 2.0 # Spread of the reward
    future_usability_scores = np.exp(-0.5 * ((relative_remaining - mu) / sigma)**2)
    
    # Add a small linear component for very tight fits, but not perfect ones.
    # This is a subtle encouragement to fill bins almost full if not perfectly.
    # small_tight_bonus_threshold = 0.1
    # small_tight_bonus_factor = 0.1
    # small_tight_mask = (remaining_after_fit > 1e-9) & (remaining_after_fit < item * small_tight_bonus_threshold)
    # future_usability_scores[small_tight_mask] += small_tight_bonus_factor * (1 - remaining_after_fit[small_tight_mask] / (item * small_tight_bonus_threshold))

    # --- Objective 3: Novelty/Exploration ---
    # Identify "novel" bins. A bin is novel if it hasn't been chosen recently or if it's
    # a type of bin (based on remaining capacity) that hasn't been explored much.
    # For simplicity here, we'll consider bins that are not "obvious" choices (e.g., not the absolute tightest).
    
    # We'll use a small random component modulated by a factor that favors bins that
    # are not among the top-k tightest fits.
    
    sorted_indices_tightness = np.argsort(tightest_fit_scores)[::-1] # Indices sorted by tightest fit score (descending)
    
    # Identify bins that are "good enough" but not necessarily the absolute best fits.
    # Let's consider bins not in the top 20% of tightest fits as candidates for novelty exploration.
    num_top_tight = max(1, int(len(valid_bins_capacities) * 0.2))
    is_not_top_tight = np.ones(len(valid_bins_capacities), dtype=bool)
    is_not_top_tight[sorted_indices_tightness[:num_top_tight]] = False
    
    novelty_scores = np.zeros_like(valid_bins_capacities)
    novelty_scores[is_not_top_tight] = np.random.rand(np.sum(is_not_top_tight)) * 0.05 # Small random score for less obvious choices

    # --- Combine Objectives ---
    # We'll use a weighted sum. The weights can be dynamic or fixed.
    # Let's use fixed weights for now, prioritizing tightest fit, then future usability, then novelty.
    
    weight_tightness = 0.6
    weight_future_usability = 0.3
    weight_novelty = 0.1

    # Normalize scores to be in a comparable range (e.g., 0 to 1) before weighted sum.
    # This is a crucial step for multi-objective optimization.
    
    # Tightness scores range from very negative to positive (due to bonus). We'll shift and scale.
    min_tight = np.min(tightest_fit_scores)
    max_tight = np.max(tightest_fit_scores)
    normalized_tightness = (tightest_fit_scores - min_tight) / (max_tight - min_tight + 1e-9)

    # Future usability scores are between 0 and 1 (from exp function).
    normalized_future_usability = future_usability_scores

    # Novelty scores are between 0 and 0.05.
    min_novelty = np.min(novelty_scores)
    max_novelty = np.max(novelty_scores)
    normalized_novelty = (novelty_scores - min_novelty) / (max_novelty - min_novelty + 1e-9) if max_novelty > min_novelty else np.zeros_like(novelty_scores)

    # Combine weighted scores
    combined_scores = (weight_tightness * normalized_tightness +
                       weight_future_usability * normalized_future_usability +
                       weight_novelty * normalized_novelty)

    # --- Exploration vs. Exploitation Decision ---
    # With probability epsilon, we might pick a bin based purely on novelty (or a subset of novel bins).
    # Otherwise, we pick based on the combined score.
    
    exploration_choice_prob = 0.2 # Probability of picking an "exploration" bin if available
    
    final_priorities = np.copy(combined_scores)
    
    # Identify potential exploration candidates (bins that are not top-tier for tightness)
    exploration_candidates_mask = is_not_top_tight
    
    # If we decide to explore (with probability epsilon), and there are exploration candidates,
    # we can select one of them with a higher probability or bias.
    if np.random.rand() < epsilon and np.any(exploration_candidates_mask):
        # Create a separate priority list for exploration candidates
        exploration_priorities_candidates = np.zeros_like(valid_bins_capacities)
        
        # Favor exploration candidates that have decent future usability
        exploration_priorities_candidates[exploration_candidates_mask] = normalized_future_usability[exploration_candidates_mask] * 0.5 + \
                                                                         normalized_novelty[exploration_candidates_mask] * 0.5
        
        # Add a small random factor to exploration choices
        exploration_priorities_candidates += np.random.rand(len(valid_bins_capacities)) * 0.02

        # Select the bin with the highest exploration priority among candidates
        exploration_chosen_idx_in_valid = np.argmax(exploration_priorities_candidates)
        
        # Overwrite the final priority for the chosen exploration bin
        # We give it a very high priority temporarily to ensure it's picked if exploration is triggered.
        final_priorities = np.full_like(combined_scores, -np.inf) # Reset to low values
        final_priorities[exploration_chosen_idx_in_valid] = 1e9 # Make it the highest priority

    else:
        # If not exploring, use the combined scores as final priorities
        final_priorities = combined_scores
        
        # Ensure that bins chosen for exploration in the combined score still have a good chance
        # by not resetting their values to -inf. They already have their combined score.
        pass

    # Assign the computed priorities back to the original bins array
    priorities[valid_bins_indices] = final_priorities
    
    return priorities
```
