{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a soft penalty for waste, inspired by Softmax-Fit.\n    Prioritizes bins that leave minimal remaining capacity, with a boost for perfect fits.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(valid_bins_mask):\n        return output_priorities\n\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    # Calculate a score: inverse of remaining capacity after fit, favoring tighter fits.\n    # Add a small epsilon to prevent division by zero.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Introduce a bonus for perfect fits to strongly prioritize them.\n    perfect_fit_mask = (valid_bins_cap - item) < 1e-9\n    fit_scores[perfect_fit_mask] *= 5.0  # Significant bonus for perfect fits\n\n    # Apply softmax to normalize scores into probabilities/priorities.\n    # This provides a smoother distribution than pure inverse,\n    # while still emphasizing bins with higher fit_scores.\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 2 (Worst in this comparison): Both heuristics implement a multi-objective scoring system with exploration. Heuristic 1 is marginally better as it defines exploration candidates based on a combination of top scores and percentile-based moderate capacity, making its exploration more guided. Heuristic 2's exploration candidate selection is identical.\n\nComparing Heuristic 1 vs. Heuristic 3: Heuristic 1 uses a more sophisticated multi-objective approach (tightness, waste, future capacity) compared to Heuristic 3's simple tight fit with a perfect fit bonus and basic epsilon-greedy exploration. Heuristic 1's guided exploration is also more nuanced than Heuristic 3's random assignment.\n\nComparing Heuristic 3 vs. Heuristic 5: Heuristic 3 combines tight fitting with random exploration (epsilon-greedy), while Heuristic 5 focuses purely on tight fitting with a perfect fit bonus. Heuristic 3's exploration component, though basic, offers more potential for finding diverse solutions than Heuristic 5's deterministic approach.\n\nComparing Heuristic 5 vs. Heuristic 7: These heuristics are identical, both focusing on tight fitting with a perfect fit bonus.\n\nComparing Heuristic 8 vs. Heuristic 12: Heuristic 8 employs a more refined strategy, balancing tight fitting, perfect fit bonuses, and penalties for large remainders. Heuristic 12 uses a simple inverse capacity for scoring and a binary perfect fit bonus. Heuristic 8's more nuanced scoring and penalty system likely leads to better packing.\n\nComparing Heuristic 12 vs. Heuristic 15: Both use a simple approach where capacity dictates priority. Heuristic 12 assigns a value of 1.0 for perfect fits and `1.0 / remaining_capacity` otherwise, while Heuristic 15 assigns a random value to fitting bins. Heuristic 12's deterministic scoring for good fits is generally more desirable than random assignment.\n\nComparing Heuristic 14 vs. Heuristic 17: Heuristic 14 implements a multi-objective scoring system (tightest fit, future utility, awkward empty penalty) with a guided exploration strategy. Heuristic 17 uses similar concepts but with different weighting and thresholds, and its parameters are tuned. Without knowing the exact performance impact of the specific parameter values in Heuristic 17, it's difficult to definitively rank them, but Heuristic 14's systematic approach to multi-objective scoring and guided exploration makes it a strong contender.\n\nOverall: Heuristics that combine a primary objective (like tight fitting) with secondary objectives (like waste avoidance, future utility) and employ guided exploration strategies (rather than pure random exploration) tend to perform better. The inclusion of specific bonuses for perfect fits and penalties for undesirable states (e.g., large surpluses) also improves heuristic design. Simple random or worst-fit strategies are generally the least effective.\n- \nHere's a redefined approach to self-reflection for designing better optimization heuristics:\n\n*   **Keywords:** Adaptive, multi-objective, guided exploration, performance feedback.\n*   **Advice:** Focus on heuristics that learn from past placements and dynamically adjust their strategy. Integrate feedback mechanisms to refine scoring and exploration parameters based on actual packing outcomes.\n*   **Avoid:** Static rules, brute-force random sampling, or heuristics that don't adapt to problem instance characteristics.\n*   **Explanation:** True self-reflection involves understanding *why* a heuristic performs well or poorly on specific instances, leading to more intelligent adaptation rather than just applying predefined complex rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}