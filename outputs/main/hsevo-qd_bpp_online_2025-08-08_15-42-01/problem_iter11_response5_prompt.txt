{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \n    epsilon = 0.03\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    remaining_after_fit = valid_bins_capacities - item\n    \n    # Multi-objective scoring:\n    # 1. Tightness score: Prioritize bins that leave minimum remaining capacity.\n    # 2. Waste avoidance score: Penalize bins that would have a large surplus.\n    # 3. Future capacity score: Reward bins that still have substantial capacity after packing.\n    \n    tightness_score = -remaining_after_fit\n    \n    # Penalty for large remainders, scaled by item size\n    waste_penalty_factor = 0.005\n    waste_avoidance_score = (remaining_after_fit / item) * waste_penalty_factor\n    \n    # Reward for significant remaining capacity - can be useful for larger items later\n    future_capacity_score = remaining_after_fit / np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else np.zeros_like(remaining_after_fit)\n    \n    # Combine objectives with weights. These weights can be tuned.\n    # We want to strongly favor tightness, moderately avoid waste, and lightly favor future capacity.\n    weight_tightness = 1.0\n    weight_waste = 0.5\n    weight_future_capacity = 0.2\n    \n    combined_scores = (weight_tightness * tightness_score - \n                       weight_waste * waste_avoidance_score + \n                       weight_future_capacity * future_capacity_score)\n\n    # Enhanced Exploration:\n    # Instead of random exploration, we can introduce guided exploration.\n    # This means exploring bins that are \"good enough\" but not necessarily the absolute best.\n    # We can define \"good enough\" as bins that fall within a certain percentile of the best fits.\n    \n    # Sort bins by the combined score to identify top candidates\n    sorted_indices_combined = np.argsort(combined_scores)[::-1]\n    \n    exploration_candidate_indices_in_valid = []\n    \n    # Identify a range of bins to consider for exploration\n    # This could be the top K bins, or bins within a certain score range.\n    # Let's consider bins within the top 30% of scores, or at least the top 3 bins.\n    num_top_bins = max(3, int(len(valid_bins_capacities) * 0.3))\n    top_candidate_indices_in_valid = sorted_indices_combined[:num_top_bins]\n    \n    # Also consider bins that offer a \"balanced\" fit, not too tight, not too empty.\n    # A bin that leaves a moderate amount of space might be more versatile.\n    # Let's consider bins where remaining_after_fit is between a small fraction and a larger fraction of bin capacity.\n    \n    # To define \"moderate\", we can look at the distribution of remaining capacities.\n    # Let's use quartiles for guidance.\n    q1_rem = np.percentile(remaining_after_fit, 25)\n    q3_rem = np.percentile(remaining_after_fit, 75)\n    \n    # Bins with remaining capacity between Q1 and Q3 (inclusive of Q3) are considered moderately remaining.\n    moderate_capacity_mask = (remaining_after_fit >= q1_rem) & (remaining_after_fit <= q3_rem)\n    \n    # Combine indices for exploration candidates\n    all_candidate_indices_in_valid = set(top_candidate_indices_in_valid)\n    all_candidate_indices_in_valid.update(np.where(moderate_capacity_mask)[0])\n    \n    exploration_candidate_indices_in_valid = list(all_candidate_indices_in_valid)\n\n    # Generate exploration scores for these candidates\n    # We want these exploration scores to be slightly random but not too high,\n    # to offer a chance for diversity without sacrificing too much performance.\n    exploration_scores = np.random.uniform(-0.1, 0.1, size=len(valid_bins_capacities))\n    \n    # Apply exploration scores:\n    # With probability epsilon, use exploration score for exploration candidates.\n    # Otherwise, use the combined score.\n    # For non-candidates, always use the combined score.\n    \n    final_scores = np.copy(combined_scores)\n    \n    # Create a mask for the identified exploration candidates\n    exploration_mask = np.zeros_like(valid_bins_capacities, dtype=bool)\n    if exploration_candidate_indices_in_valid:\n        exploration_mask[exploration_candidate_indices_in_valid] = True\n    \n    # Decide whether to use exploration score for exploration candidates\n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    # Apply the exploration scores only to the chosen candidates\n    final_scores[exploration_mask & use_exploration_for_candidates] = exploration_scores[exploration_mask & use_exploration_for_candidates]\n    \n    priorities[valid_bins_indices] = final_scores\n    \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy aims to place the current item into the bin that has the most remaining capacity.\n    This heuristic attempts to keep bins with less capacity available for smaller items later on,\n    potentially leading to a more efficient packing in the long run.\n    In this priority function, we assign a higher priority score to bins with larger remaining capacities.\n    Specifically, the priority is directly proportional to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.array([bin_cap if bin_cap >= item else -np.inf for bin_cap in bins_remain_cap])\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs. Heuristic 2 (Worst in this comparison): Both heuristics implement a multi-objective scoring system with exploration. Heuristic 1 is marginally better as it defines exploration candidates based on a combination of top scores and percentile-based moderate capacity, making its exploration more guided. Heuristic 2's exploration candidate selection is identical.\n\nComparing Heuristic 1 vs. Heuristic 3: Heuristic 1 uses a more sophisticated multi-objective approach (tightness, waste, future capacity) compared to Heuristic 3's simple tight fit with a perfect fit bonus and basic epsilon-greedy exploration. Heuristic 1's guided exploration is also more nuanced than Heuristic 3's random assignment.\n\nComparing Heuristic 3 vs. Heuristic 5: Heuristic 3 combines tight fitting with random exploration (epsilon-greedy), while Heuristic 5 focuses purely on tight fitting with a perfect fit bonus. Heuristic 3's exploration component, though basic, offers more potential for finding diverse solutions than Heuristic 5's deterministic approach.\n\nComparing Heuristic 5 vs. Heuristic 7: These heuristics are identical, both focusing on tight fitting with a perfect fit bonus.\n\nComparing Heuristic 8 vs. Heuristic 12: Heuristic 8 employs a more refined strategy, balancing tight fitting, perfect fit bonuses, and penalties for large remainders. Heuristic 12 uses a simple inverse capacity for scoring and a binary perfect fit bonus. Heuristic 8's more nuanced scoring and penalty system likely leads to better packing.\n\nComparing Heuristic 12 vs. Heuristic 15: Both use a simple approach where capacity dictates priority. Heuristic 12 assigns a value of 1.0 for perfect fits and `1.0 / remaining_capacity` otherwise, while Heuristic 15 assigns a random value to fitting bins. Heuristic 12's deterministic scoring for good fits is generally more desirable than random assignment.\n\nComparing Heuristic 14 vs. Heuristic 17: Heuristic 14 implements a multi-objective scoring system (tightest fit, future utility, awkward empty penalty) with a guided exploration strategy. Heuristic 17 uses similar concepts but with different weighting and thresholds, and its parameters are tuned. Without knowing the exact performance impact of the specific parameter values in Heuristic 17, it's difficult to definitively rank them, but Heuristic 14's systematic approach to multi-objective scoring and guided exploration makes it a strong contender.\n\nOverall: Heuristics that combine a primary objective (like tight fitting) with secondary objectives (like waste avoidance, future utility) and employ guided exploration strategies (rather than pure random exploration) tend to perform better. The inclusion of specific bonuses for perfect fits and penalties for undesirable states (e.g., large surpluses) also improves heuristic design. Simple random or worst-fit strategies are generally the least effective.\n- \nHere's a redefined approach to self-reflection for designing better optimization heuristics:\n\n*   **Keywords:** Adaptive, multi-objective, guided exploration, performance feedback.\n*   **Advice:** Focus on heuristics that learn from past placements and dynamically adjust their strategy. Integrate feedback mechanisms to refine scoring and exploration parameters based on actual packing outcomes.\n*   **Avoid:** Static rules, brute-force random sampling, or heuristics that don't adapt to problem instance characteristics.\n*   **Explanation:** True self-reflection involves understanding *why* a heuristic performs well or poorly on specific instances, leading to more intelligent adaptation rather than just applying predefined complex rules.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}