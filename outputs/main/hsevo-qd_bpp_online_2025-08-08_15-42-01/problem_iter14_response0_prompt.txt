{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes bins using an adaptive multi-objective approach:\n    Balances tightest fit, perfect fit bonus, and a novel \"capacity utilization gradient\"\n    to favor bins that are likely to accommodate future items efficiently.\n    Includes a mechanism for dynamically adjusting exploration based on item size variance.\n    \"\"\"\n\n    epsilon_base = 0.1  # Base probability of exploration\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # --- Objective 1: Tightest Fit & Perfect Fit ---\n    remaining_after_fit = valid_bins_capacities - item\n    tight_fit_score = -remaining_after_fit\n    perfect_fit_bonus = 1000.0\n    tight_fit_score[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # --- Objective 2: Capacity Utilization Gradient ---\n    # Favor bins that, after packing the current item, leave a remaining capacity\n    # that is a \"good\" fit for a range of potential future item sizes.\n    # We can approximate this by looking at the distribution of remaining capacities.\n    # A bin that leaves capacity close to the median remaining capacity (among eligible bins)\n    # might be good for future medium-sized items.\n\n    # Calculate variance of item sizes encountered so far (requires external state, simulating here)\n    # In a real implementation, this would be a class member or passed parameter.\n    # For this example, we'll use a placeholder.\n    # Assuming historical item sizes were [0.2, 0.5, 0.1, 0.8, 0.3]\n    # Current item is 'item'. Let's assume a mix of small and large items are common.\n    # A simpler proxy: use variance of *current* remaining capacities to infer future item patterns.\n    current_remaining_variance = np.var(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    # If variance is high, explore more. If low, exploit tighter fits.\n    exploration_boost_factor = min(1.0, np.sqrt(current_remaining_variance) / 0.5) # Scale boost by variance, capped\n\n    # Create a score based on how \"useful\" the remaining capacity is.\n    # We want remaining capacity that is not too small (useless) and not too large (wasteful).\n    # A simple approach: penalize extreme remaining capacities.\n    # Let's consider remaining capacities relative to the *maximum possible remaining capacity*\n    # or median remaining capacity of eligible bins.\n    if len(valid_bins_capacities) > 1:\n        median_remaining_eligible = np.median(remaining_after_fit)\n        # Score is higher if remaining_after_fit is close to median_remaining_eligible\n        utilization_score = -np.abs(remaining_after_fit - median_remaining_eligible)\n    else:\n        utilization_score = np.zeros_like(remaining_after_fit) # No comparison possible\n\n    # Normalize and scale utilization score\n    if np.ptp(utilization_score) > 1e-9: # Avoid division by zero if all scores are same\n        utilization_score = (utilization_score - np.min(utilization_score)) / np.ptp(utilization_score)\n    else:\n        utilization_score = np.zeros_like(utilization_score)\n\n    # Weighting the objectives. Tight fit is primary, utilization is secondary.\n    # The weight of utilization can be influenced by exploration tendency.\n    utilization_weight = 0.1 + 0.2 * exploration_boost_factor # Additive boost based on variance\n    combined_exploitation_scores = tight_fit_score + utilization_score * utilization_weight\n\n    # --- Adaptive Exploration ---\n    # Explore more aggressively if the current item is small relative to bin capacity,\n    # or if there's high variance in available capacities, suggesting uncertainty.\n    # The exploration probability should be dynamic.\n    # Let's define \"small item\" as less than 25% of average available capacity.\n    avg_available_capacity = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 1.0\n    is_small_item = (item / avg_available_capacity < 0.25) if avg_available_capacity > 0 else False\n\n    # Dynamic epsilon: higher if item is small, or if variance is high\n    epsilon = epsilon_base + 0.1 * (exploration_boost_factor) + (0.15 if is_small_item else 0)\n    epsilon = min(epsilon, 0.8) # Cap exploration probability\n\n    # Select candidates for exploration:\n    # 1. Top-performing bins based on combined_exploitation_scores.\n    # 2. Bins that have moderate remaining capacity (less than half of the item's size, but not perfect fit).\n    sorted_indices_exploitation = np.argsort(combined_exploitation_scores)[::-1]\n\n    exploration_candidate_mask = np.zeros_like(combined_exploitation_scores, dtype=bool)\n    num_top_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.15))) # Top 15%\n    exploration_candidate_mask[sorted_indices_exploitation[:num_top_candidates]] = True\n\n    moderate_remaining_mask = (remaining_after_fit > item * 0.1) & (remaining_after_fit < item * 0.5)\n    exploration_candidate_mask[moderate_remaining_mask] = True\n\n    # Generate exploration scores (random, but biased by candidates)\n    exploration_scores_raw = np.random.rand(len(valid_bins_capacities))\n\n    # Apply exploration scores only to selected candidates with a certain probability\n    final_scores = np.copy(combined_exploitation_scores)\n    exploration_decision_mask = np.random.rand(len(valid_bins_capacities)) < epsilon\n\n    # For bins that are candidates AND we decide to explore: use the exploration score\n    apply_exploration_mask = exploration_candidate_mask & exploration_decision_mask\n    final_scores[apply_exploration_mask] = np.maximum(final_scores[apply_exploration_mask], exploration_scores_raw[apply_exploration_mask] * 0.05) # Scale exploration noise\n\n    # For bins that are candidates but we DON'T explore: stick to exploitation score.\n    # For bins that are NOT candidates: stick to exploitation score.\n\n    priorities[valid_bins_indices] = final_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Best) vs Heuristic 2 (Second Best): They are identical.\nComparing Heuristic 1 vs Heuristic 3: They are identical.\nComparing Heuristic 1 vs Heuristic 4: Heuristic 1 introduces a hybrid approach with exploration (epsilon probability) and guided candidate selection, whereas Heuristic 4 is purely deterministic best-fit with a penalty for large remainders. Heuristic 1's approach is more sophisticated in balancing immediate needs with future possibilities.\nComparing Heuristic 1 vs Heuristic 5: Heuristic 1 uses a probability-based exploration with candidate selection, while Heuristic 5 attempts to integrate exploration by preferring bins with more remaining capacity, but without a clear probabilistic mechanism. Heuristic 1's explicit exploration is likely more effective.\nComparing Heuristic 1 vs Heuristic 6: Heuristic 1 is a hybrid of tightest fit with exploration. Heuristic 6 is multi-objective, incorporating utilization and adaptive exploration (though the adaptive part is simulated). Heuristic 6 attempts a more holistic approach by considering bin utilization.\nComparing Heuristic 1 vs Heuristic 7: Heuristic 1 is a simpler hybrid. Heuristic 7 builds on multi-objective scoring, adaptive exploration probability, and identifies candidates based on both tightness and moderate capacity, making it more nuanced than Heuristic 1.\nComparing Heuristic 7 vs Heuristic 8: Heuristic 8 builds on Heuristic 7's multi-objective scoring but refines it with better tie-breaking and exploration based on candidate selection and perturbations. Heuristic 8 seems to have a more structured approach to balancing objectives.\nComparing Heuristic 8 vs Heuristic 9: Heuristic 8 is a complex multi-objective heuristic with exploration. Heuristic 9 is a very simple heuristic that prioritizes bins with small remaining capacity (inverse of remaining capacity) and gives a bonus to perfect fits. Heuristic 8 is significantly more advanced.\nComparing Heuristic 9 vs Heuristic 10: Heuristics 9 and 10 are almost identical, with Heuristic 9 having a slight preference for perfect fits (score of 1.0 vs inverse of remainder) and Heuristic 10 purely using inverse of remainder. Both are simple \"best-fit\" variations.\nComparing Heuristic 10 vs Heuristic 11: Heuristic 10 is a simple inverse remainder score. Heuristic 11 is also best-fit focused but adds a tie-breaker favoring larger original capacity and a specific score for perfect fits. Heuristic 11 is more refined for tie-breaking.\nComparing Heuristic 11 vs Heuristic 12: Heuristic 11 is a best-fit with tie-breaking. Heuristic 12 introduces several tunable parameters for exploration probability, bonuses, penalties, and candidate selection, indicating a more experimental and potentially optimized approach to balancing objectives.\nComparing Heuristic 12 vs Heuristic 13: Heuristic 12 is a complex multi-objective heuristic. Heuristic 13 is a \"Random Fit\" strategy, assigning random priorities to bins that can fit the item. This is a very basic approach compared to Heuristic 12.\nComparing Heuristic 13 vs Heuristic 14: Heuristics 13 and 14 are identical \"Random Fit\" strategies.\nComparing Heuristic 14 vs Heuristic 15: Heuristic 14 is random. Heuristic 15 is a complex multi-objective heuristic with adaptive exploration based on item size variance and capacity utilization gradient, significantly more sophisticated than random.\nComparing Heuristic 15 vs Heuristic 16: Heuristics 15 and 16 are identical.\nComparing Heuristic 16 vs Heuristic 17: Heuristic 16/15 is multi-objective with adaptive exploration based on variance and utilization gradient. Heuristic 17 is also multi-objective, focusing on tightness, future usability (relative to item size), and adaptive exploration for candidates. Heuristic 16/15 seems to have a more defined adaptive exploration mechanism.\nComparing Heuristic 17 vs Heuristic 18: Heuristics 17 and 18 are identical.\nComparing Heuristic 18 vs Heuristic 19: Heuristic 18 is multi-objective with adaptive exploration. Heuristic 19 combines tightness, waste avoidance, future utility, and guided exploration with perturbated scores for candidates. Heuristic 19's exploration is more about perturbing scores of selected candidates.\nComparing Heuristic 19 vs Heuristic 20: Heuristics 19 and 20 are identical.\nOverall: The top heuristics (1-8, 15-18) are complex multi-objective strategies that balance tightest fit with some form of future utility or guided exploration. Heuristics 9-11 and 13-14 are simpler best-fit or random strategies. The intermediate heuristics (12, 19-20) attempt variations on multi-objective and exploration. Heuristics 1, 7, 8, 15, 16, 17, 18 appear to represent the most developed ideas, blending multiple objectives and adaptive/guided exploration.\n- \nHere's a redefinition of \"Current self-reflection\" to guide heuristic design, avoiding ineffective approaches:\n\n*   **Keywords:** Multi-objective, adaptive exploration, sophisticated scoring, penalized residuals, rewarded fits.\n*   **Advice:** Design heuristics that dynamically balance multiple objectives (e.g., tightness, future utility, waste) using adaptive exploration strategies that sample promising candidates rather than relying on pure randomness.\n*   **Avoid:** Overly simplistic strategies like pure Best Fit or random assignments. Also avoid neglecting the trade-off between exploration and exploitation.\n*   **Explanation:** Complex, multi-objective heuristics with nuanced scoring (e.g., softmax, explicit penalties/bonuses) and guided exploration are essential for outperforming simpler methods, especially in complex packing problems.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}