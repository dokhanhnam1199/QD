{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    epsilon = 0.05  # Slightly reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Enhanced Exploitation:\n    # 1. Tight fit: Prioritize bins that leave minimum remaining capacity.\n    # 2. Perfect fit bonus: A higher bonus for exact fits to minimize waste.\n    # 3. Surplus penalty: A mild penalty for bins that would have a large surplus\n    #    after packing, as these might be better saved for larger items.\n    \n    remaining_after_fit = valid_bins_capacities - item\n    \n    tight_fit_scores = -remaining_after_fit\n    \n    perfect_fit_bonus = 0.1  # Increased bonus for perfect fits\n    tight_fit_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus\n\n    # A gentle penalty for large remainders, scaled by the item size\n    # to make it more relevant.\n    large_remainder_penalty_factor = 0.001 \n    surplus_penalty = (remaining_after_fit / item) * large_remainder_penalty_factor\n    tight_fit_scores -= surplus_penalty\n    \n    # Adaptive Exploration:\n    # Instead of purely random exploration, we can explore bins that are \"good enough\"\n    # but not necessarily the absolute best (according to tight fit).\n    # This can be done by introducing a small random perturbation to the scores\n    # of a subset of bins, or by giving a chance to bins that are not the tightest.\n    \n    # Let's use a strategy where we explore bins that are among the top K tightest fits,\n    # or bins that have a moderate remaining capacity.\n    \n    # Sort bins by tight fit score to identify top candidates\n    sorted_indices_tight = np.argsort(tight_fit_scores)[::-1]\n    \n    exploration_candidate_mask = np.zeros_like(tight_fit_scores, dtype=bool)\n    \n    # Select a portion of the best fitting bins for potential exploration\n    num_explore_candidates = min(len(valid_bins_capacities), max(1, int(len(valid_bins_capacities) * 0.2)))\n    exploration_candidate_mask[sorted_indices_tight[:num_explore_candidates]] = True\n    \n    # Additionally, include some bins that have a moderate amount of remaining capacity\n    # This might represent bins that are not tightly packed but could be useful later.\n    moderate_capacity_threshold = np.median(valid_bins_capacities)\n    moderate_capacity_mask = (valid_bins_capacities > item) & (valid_bins_capacities < moderate_capacity_threshold * 2) # bins that are not too tight, not too empty\n    exploration_candidate_mask[moderate_capacity_mask] = True\n\n    exploration_scores = np.random.rand(len(valid_bins_capacities)) * 0.01 # Smaller random noise for exploration\n    \n    # Combine: With probability epsilon, choose exploration score for candidate bins,\n    # otherwise use the tight fit score. For non-candidate bins, always use tight fit.\n    \n    use_exploration_for_candidates = np.random.rand(len(valid_bins_capacities)) < epsilon\n    \n    combined_scores = np.copy(tight_fit_scores)\n    \n    # Apply exploration scores only to the identified exploration candidates\n    combined_scores[exploration_candidate_mask & use_exploration_for_candidates] = exploration_scores[exploration_candidate_mask & use_exploration_for_candidates]\n\n    priorities[valid_bins_indices] = combined_scores\n    \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines FFD-inspired tight fitting with an epsilon-greedy exploration strategy\n    to balance exploitation of good fits and discovery of potentially better packings.\n    \"\"\"\n    epsilon = 0.1\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    \n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    # Exploitation: Prioritize bins with least remaining capacity after fitting (tight fit)\n    # Add a small bonus for perfect fits, similar to FFD's goal of minimizing waste.\n    tight_fit_scores = -(valid_bins_capacities - item)\n    perfect_fit_bonus = 1e-6 # Small bonus for bins that will be exactly filled\n    tight_fit_scores[valid_bins_capacities - item < 1e-9] += perfect_fit_bonus\n\n    # Exploration: Random scores for a subset of valid bins to explore options\n    exploration_scores = np.random.rand(len(valid_bins_capacities))\n\n    # Combine exploitation and exploration using epsilon-greedy\n    use_exploration = np.random.rand(len(valid_bins_capacities)) < epsilon\n    combined_scores = np.where(use_exploration, exploration_scores, tight_fit_scores)\n\n    priorities[can_fit_mask] = combined_scores\n    \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Softmax-Based Fit strategy for online Bin Packing Problem.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a 'fit' score. We prefer bins with less remaining capacity that can still fit the item.\n    # This encourages using bins more fully before opening new ones.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size remaining.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Apply softmax to convert fit scores into probabilities (priorities)\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Create an output array of the same size as the original bins_remain_cap\n    # and place the calculated priorities in the correct positions.\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements the Best Fit strategy for the online Bin Packing Problem.\n    Items are prioritized to be placed in bins where they leave the least\n    remaining space, thus aiming to minimize wasted space.\n    \"\"\"\n    # Calculate the remaining space after placing the item in each bin.\n    # Only consider bins where the item can actually fit.\n    possible_fits = bins_remain_cap >= item\n    remaining_space = np.where(possible_fits, bins_remain_cap - item, np.inf)\n\n    # The priority is inversely related to the remaining space.\n    # A smaller remaining space means a higher priority.\n    # We use a small epsilon to avoid division by zero if remaining_space is 0\n    # and to ensure bins with 0 remaining space (perfect fit) get the highest priority.\n    epsilon = 1e-9\n    priorities = np.where(possible_fits, 1 / (remaining_space + epsilon), 0)\n\n    # Ensure that bins where the item cannot fit have a priority of 0.\n    priorities[~possible_fits] = 0\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins for tightest fit, with a bonus for perfect fits.\n\n    Combines the 'Almost Full Fit' idea with a bonus for bins that become exactly full,\n    and a small penalty for bins that leave significant residual space,\n    while ensuring unfillable bins receive the lowest priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate scores for bins that can fit the item\n    remaining_capacities_after_fit = bins_remain_cap[fit_mask] - item\n\n    # Prioritize bins that result in smallest remaining capacity (tightest fit)\n    # Use the negative of remaining capacity to turn minimization into maximization\n    # Add a small value to ensure negative remaining capacities are prioritized\n    # over positive ones.\n    scores = -remaining_capacities_after_fit\n\n    # Add a bonus for perfect fits (remaining capacity is zero)\n    # This encourages using bins that are exactly filled.\n    perfect_fit_bonus = 1.0\n    scores[remaining_capacities_after_fit == 0] += perfect_fit_bonus\n\n    # Assign the calculated scores to the valid bins\n    priorities[fit_mask] = scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Assigns priorities to bins based on a hybrid Best-Fit and penalty strategy.\n    Prioritizes bins that result in minimal remaining capacity (tightest fit),\n    giving a significant bonus for perfect fits and a slight penalty for\n    bins that become too full.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate potential remaining capacity for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    potential_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Assign base priority: favor bins with less remaining capacity after packing\n    # Maximizing -(potential_remain_cap) is equivalent to minimizing potential_remain_cap\n    priorities[can_fit_mask] = -potential_remain_cap\n\n    # Apply bonus for perfect fits (remaining capacity is zero)\n    perfect_fit_mask = (potential_remain_cap == 0)\n    if np.any(perfect_fit_mask):\n        priorities[can_fit_mask][perfect_fit_mask] += 1.0  # Bonus for perfect fit\n\n    # Apply a small penalty for bins that would become \"too full\" (negative remaining capacity, though handled by mask)\n    # Or a slightly reduced priority for bins that have a lot of leftover space.\n    # This can be implicitly handled by the negative remaining capacity scoring,\n    # but we can also make it more explicit if needed.\n    # For now, the negative remaining capacity score already penalizes larger leftovers.\n\n    # Consider a penalty for bins that *can* fit but leave a very large remainder.\n    # This discourages placing small items in very large bins if a tighter fit exists.\n    large_remainder_mask = (potential_remain_cap > item * 0.5) & ~perfect_fit_mask\n    if np.any(large_remainder_mask):\n        priorities[can_fit_mask][large_remainder_mask] -= 0.5 # Small penalty for large remainders\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Softmax-Based Fit strategy for online Bin Packing Problem.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin.\n    \"\"\"\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_cap = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    # Calculate a 'fit' score. We prefer bins with less remaining capacity that can still fit the item.\n    # This encourages using bins more fully before opening new ones.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size remaining.\n    fit_scores = 1.0 / (valid_bins_cap - item + 1e-9)\n\n    # Apply softmax to convert fit scores into probabilities (priorities)\n    exp_scores = np.exp(fit_scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    # Create an output array of the same size as the original bins_remain_cap\n    # and place the calculated priorities in the correct positions.\n    output_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    output_priorities[valid_bins_mask] = priorities\n\n    return output_priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Almost Full Fit: Prioritize bins that will be closest to full after adding the item.\n    # Calculate the remaining capacity after placing the item.\n    potential_remain_cap = bins_remain_cap - item\n\n    # We want bins where potential_remain_cap is as close to zero as possible (but non-negative)\n    # A simple way to achieve this is to maximize the negative of the absolute difference from zero.\n    # However, to encourage fitting rather than just being close, we can also consider\n    # bins that can actually fit the item.\n\n    # Create a mask for bins that can fit the item\n    can_fit_mask = potential_remain_cap >= 0\n\n    # For bins that can fit, calculate a score based on how full they will become.\n    # A higher score means the bin will be closer to full.\n    # We can use -(potential_remain_cap) as a measure of \"fullness\" after packing.\n    # To avoid prioritizing bins that become \"too full\" or negative capacity,\n    # we only consider valid fits.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    priorities[can_fit_mask] = -potential_remain_cap[can_fit_mask]\n\n    # We can add a small bonus for fitting the item at all to ensure\n    # that fitting items is generally preferred over not fitting.\n    # However, the negative of remaining capacity already does this indirectly.\n    # A more sophisticated approach might involve considering the original capacity\n    # to prioritize filling larger bins more.\n\n    # For \"Almost Full Fit\", we want to minimize the remaining capacity.\n    # So, we want to maximize the negative of the remaining capacity.\n    # Let's refine this. We want the remaining capacity to be as close to 0 as possible.\n    # The value `potential_remain_cap` represents this.\n    # We want to maximize this value if it's negative (meaning it's a good fit).\n    # Let's re-evaluate. For Almost Full Fit, we want the resulting remaining capacity\n    # to be as small as possible, but still non-negative.\n    # So, we want to maximize `-(potential_remain_cap)` for valid fits.\n\n    # Consider the difference between the bin's original capacity and the item size.\n    # We want to prioritize bins where this difference is minimized, but non-negative.\n    # The `potential_remain_cap` already captures this.\n    # We want the most \"positive\" value of `-(potential_remain_cap)` for bins that fit.\n\n    # Another perspective: prioritize bins that have the *least* remaining capacity *after* the item is placed.\n    # This directly translates to maximizing `-(potential_remain_cap)`.\n\n    # Let's consider a small perturbation or a \"niceness\" factor.\n    # Perhaps bins that are already somewhat full are preferred.\n    # But for \"Almost Full Fit\", the focus is purely on the state *after* packing.\n\n    # We want to maximize the remaining capacity in a way that favors being close to zero.\n    # The score should be higher for bins that result in less remaining capacity.\n    # So, we want to maximize `-(bins_remain_cap - item)` for valid bins.\n    # This is equivalent to maximizing `item - bins_remain_cap`. This isn't quite right.\n\n    # The goal is to make the bin as \"full\" as possible *after* adding the item.\n    # \"Full\" means having small remaining capacity.\n    # So, we want to minimize `potential_remain_cap`.\n    # To turn this into a maximization problem for priority, we can use `-potential_remain_cap`.\n    # We also need to ensure that bins that *cannot* fit the item get a very low priority.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    # Let's refine the \"Almost Full Fit\" idea. We want the bin with the smallest remaining capacity after packing.\n    # This means we want to minimize `bins_remain_cap - item`.\n    # For a priority score (higher is better), we can use the negative of this difference.\n    # So, we want to maximize `-(bins_remain_cap - item)` which is `item - bins_remain_cap`.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = item - bins_remain_cap[fit_mask]\n\n    # This score will be highest for bins where `bins_remain_cap` is just slightly larger than `item`.\n    # Example: item=5\n    # bin1_rem=6  -> priority = 5 - 6 = -1\n    # bin2_rem=5  -> priority = 5 - 5 = 0\n    # bin3_rem=10 -> priority = 5 - 10 = -5\n    # bin4_rem=3  -> priority = -inf (cannot fit)\n\n    # The highest score is 0, from the bin that becomes exactly full.\n    # This seems correct for \"Almost Full Fit\".\n\n    # Consider a scenario where there are multiple bins that become exactly full.\n    # The current heuristic doesn't differentiate between them.\n    # For this strategy, simply picking any of them is fine.\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a penalty for wasted space with a bonus for achieving near-perfect fits,\n    while also incorporating a slight preference for bins with more remaining capacity\n    to keep options open for larger future items. Exploration is guided by this\n    preference rather than being purely random.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Calculate remaining capacity after fitting the item\n    remaining_after_fit = valid_bins_capacities - item\n\n    # Score 1: Penalty for wasted space (higher negative score for more waste)\n    # This encourages tighter fits.\n    wasted_space_penalty = -remaining_after_fit * 100.0  # Scale penalty\n\n    # Score 2: Bonus for near-perfect fits.\n    # Reward bins that leave very little space, up to a small tolerance.\n    # Perfect fit bonus is higher than near-perfect fit bonus.\n    near_perfect_fit_threshold = 0.05\n    perfect_fit_bonus = 10.0\n    near_perfect_fit_bonus = 5.0\n\n    perfect_fit_mask = np.isclose(remaining_after_fit, 0.0, atol=1e-9)\n    near_perfect_fit_mask = (remaining_after_fit > 0) & (remaining_after_fit <= near_perfect_fit_threshold)\n\n    fit_bonus = np.zeros_like(remaining_after_fit)\n    fit_bonus[perfect_fit_mask] = perfect_fit_bonus\n    fit_bonus[near_perfect_fit_mask] = near_perfect_fit_bonus\n\n    # Score 3: Exploration-guided preference for bins with more remaining capacity.\n    # This is a \"soft\" preference, less impactful than tight fitting,\n    # to keep options open for potentially larger items later.\n    # We normalize this to avoid dominating the tight fit score.\n    # Using log to dampen the effect of very large capacities.\n    max_cap_preference = np.log1p(valid_bins_capacities) / np.log1p(np.max(valid_bins_capacities) + 1e-9) * 1.0 # Scaled preference\n\n    # Combine scores: Primarily penalize waste, reward good fits, with a soft exploration bias.\n    # The relative weights can be tuned.\n    combined_scores = wasted_space_penalty + fit_bonus + max_cap_preference\n\n    priorities[can_fit_mask] = combined_scores\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    First Fit Decreasing (FFD) inspired priority function.\n    It prioritizes bins that can accommodate the item and have the least remaining capacity,\n    effectively trying to fill bins as much as possible.\n    A small penalty is added to bins that can fit the item to favor them slightly.\n    Bins that cannot fit the item receive a very low priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    priorities[can_fit_mask] = -bins_remain_cap[can_fit_mask] + 1e-6 * (bins_remain_cap[can_fit_mask] - item)\n    \n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_after_fit = bins_remain_cap[i] - item\n            if remaining_after_fit == 0:\n                priorities[i] = 1.0\n            else:\n                priorities[i] = 1.0 / (remaining_after_fit + 1e-9)\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.10086824335308542, perfect_fit_bonus: float = 0.14134762876801932, large_remainder_penalty_factor: float = 0.0037181076773583456, exploration_noise_scale: float = 0.014738605941995732, exploration_candidate_portion: float = 0.36099361922072437, moderate_capacity_multiplier: float = 1.5506786957399679) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.10086824335308542, perfect_fit_bonus: float = 0.14134762876801932, large_remainder_penalty_factor: float = 0.0037181076773583456, exploration_noise_scale: float = 0.014738605941995732, exploration_candidate_portion: float = 0.36099361922072437, moderate_capacity_multiplier: float = 1.5506786957399679) -> np.ndarray:\n    \"\"\"\n    Calculates priorities for placing an item into bins based on remaining capacity.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array of remaining capacities for each bin.\n        epsilon: The probability of choosing an exploration score for candidate bins.\n        perfect_fit_bonus: An additional score added to bins with near-perfect fits.\n        large_remainder_penalty_factor: A scaling factor for penalizing large surpluses.\n        exploration_noise_scale: The maximum value of random noise added to exploration scores.\n        exploration_candidate_portion: The portion of the best-fitting bins to consider for exploration.\n        moderate_capacity_multiplier: A multiplier to define the upper bound for moderate capacity bins.\n\n    Returns:\n        A numpy array of priority scores for each bin.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Random Fit strategy.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n    if len(fitting_bins_indices) > 0:\n        priorities[fitting_bins_indices] = np.random.rand(len(fitting_bins_indices))\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins that result in the tightest fit after packing,\n    with a bonus for perfect fits and a fallback for worst-fit among remaining options.\"\"\"\n\n    # Initialize priorities with a very low value for bins that cannot fit the item.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity after placing the item in valid bins.\n    potential_remain_cap = bins_remain_cap[fit_mask] - item\n\n    # Assign scores:\n    # 1. High priority for perfect fits (remaining capacity = 0).\n    #    We give a bonus score (e.g., 1.0) to indicate preference.\n    perfect_fit_mask = potential_remain_cap == 0\n    priorities[fit_mask][perfect_fit_mask] = 1.0\n\n    # 2. For other valid fits, prioritize bins that leave less remaining capacity.\n    #    This encourages a \"tight\" packing. We use the negative of the remaining capacity.\n    #    The smallest non-negative remaining capacity will get the highest score here.\n    non_perfect_fit_mask = ~perfect_fit_mask\n    priorities[fit_mask][non_perfect_fit_mask] = -potential_remain_cap[non_perfect_fit_mask]\n\n    # 3. Tie-breaking for bins that result in the same remaining capacity (or are perfect fits):\n    #    Among bins with the same resulting remaining capacity, favor the one with the\n    #    largest original remaining capacity (closer to worst-fit among good fits).\n    #    This helps to keep smaller bins available for smaller items.\n    #    We can achieve this by adding a small bonus proportional to the original capacity.\n    #    Add a small epsilon to the priority to break ties.\n    #    For perfect fits, they already have a score of 1.0. We want to differentiate them.\n    #    Let's use the negative of the original remaining capacity as a secondary score.\n    #    This means, for equally good fits, we prefer the one that was originally larger.\n    #    The `item - bins_remain_cap[fit_mask]` was a good starting point for 'tightness'\n    #    but did not handle tie-breaking well.\n\n    # Let's combine the score: Prioritize by minimizing remaining capacity.\n    # A simple way is to maximize `-(remaining_capacity)`.\n    # To differentiate between equally good fits, we can use the original capacity.\n    # If two bins result in the same remaining capacity, the one that started larger\n    # (worst fit among the tightest) is preferred.\n    # So, for the same `-(remaining_capacity)`, we want to maximize the original capacity.\n\n    # Re-calculating for clarity and combined logic:\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n    fit_mask = bins_remain_cap >= item\n\n    # Calculate potential remaining capacity for fitting bins.\n    potential_remain_cap_vals = bins_remain_cap[fit_mask] - item\n\n    # Score: Primarily, minimize remaining capacity. Maximize -(remaining_capacity).\n    # Secondary: For ties in remaining capacity, prefer larger original capacity.\n    # This can be achieved by maximizing `original_capacity - item`.\n    # Or more simply, `item - original_capacity` (smaller is better here, so maximize `-(item - original_capacity)`)\n    # Let's focus on minimizing `potential_remain_cap_vals`.\n    # A good score would be `-(potential_remain_cap_vals)`.\n    # To break ties (same `potential_remain_cap_vals`), we prefer bins that had higher initial capacity.\n    # So, we can add `bins_remain_cap[fit_mask]` as a secondary factor.\n    # This means we want to maximize: `-(potential_remain_cap_vals) + C * bins_remain_cap[fit_mask]`\n    # where C is a small constant to ensure primary sorting is by remaining capacity.\n\n    # Let's use a simpler approach that captures the essence:\n    # Prioritize bins that leave the least remainder.\n    # For ties, pick the one that was originally larger.\n    # Score = -(remaining_capacity) + small_bonus * original_capacity\n\n    # Let's refine the score calculation.\n    # For bins that fit:\n    # Primary goal: Minimize `potential_remain_cap`.\n    # Secondary goal: If `potential_remain_cap` is the same, pick the bin that was originally largest.\n\n    # Score: `item - bins_remain_cap[fit_mask]` works well for tight fits.\n    # The highest score is for `bins_remain_cap[fit_mask] == item`.\n    # If there are multiple such bins, they have the same score.\n    # To break ties, we can add a small value related to the original capacity.\n    # Higher original capacity is preferred for ties in remaining capacity.\n\n    # Let's use the inverse of remaining capacity, but penalize bins that are too large.\n    # The \"perfect fit\" is ideal.\n    # So, let's try a combined score:\n    # - Perfect fit: Highest score (e.g., 100)\n    # - Tight fit (small remaining capacity): Score inversely proportional to remaining capacity.\n    # - Break ties by original capacity: Higher original capacity gets a small bonus.\n\n    # Final approach:\n    # 1. Perfect fits get a significant bonus (e.g., 1000).\n    # 2. Other fits get a score based on inverse of remaining capacity + a small bonus\n    #    for larger original capacity to break ties.\n    #    Score = 1 / (potential_remain_cap + epsilon) + original_capacity * epsilon_small\n    #    where epsilon is for avoiding division by zero and epsilon_small for tie-breaking.\n\n    # Revised attempt focusing on minimizing remaining capacity and then maximizing original capacity for ties.\n    # Score = -(remaining_capacity) + (original_capacity / MaxCapacity) * epsilon_tiebreak\n    # This needs careful scaling.\n\n    # Let's use a scoring that strongly favors perfect fits, then tight fits, and then larger original bins.\n    scores = np.zeros_like(bins_remain_cap, dtype=float)\n    scores.fill(-np.inf) # Initialize with a very low score\n\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacity = bins_remain_cap[can_fit_mask]\n    potential_remain_cap_vals = fitting_bins_capacity - item\n\n    # Perfect fits have the highest priority\n    perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals == 0)[0]\n    if len(perfect_fit_indices_in_fitting) > 0:\n        scores[can_fit_mask][perfect_fit_indices_in_fitting] = 1000.0\n\n    # Tight fits get priority based on negative remaining capacity\n    # Add a small factor to ensure they are ranked below perfect fits but above others.\n    # The secondary criterion: prefer larger original capacity among those with same remainder.\n    # We can add `fitting_bins_capacity` multiplied by a small factor.\n    # Let's combine this into a single score for non-perfect fits.\n    non_perfect_fit_indices_in_fitting = np.where(potential_remain_cap_vals > 0)[0]\n    if len(non_perfect_fit_indices_in_fitting) > 0:\n        # Score = -(remaining_capacity) + bonus for larger original capacity\n        # The bonus should be small enough not to override the primary goal of minimizing remainder.\n        # Use a small fraction of max possible capacity as tie-breaker.\n        # Max possible remainder is roughly bin_capacity. So, max of `fitting_bins_capacity` can be used.\n        max_cap_val = np.max(fitting_bins_capacity) if fitting_bins_capacity.size > 0 else 1.0\n        tie_breaker_factor = 1e-6 # A very small factor for tie-breaking\n\n        # Score for non-perfect fits: prioritize less remaining capacity, then more original capacity.\n        scores[can_fit_mask][non_perfect_fit_indices_in_fitting] = \\\n            -(potential_remain_cap_vals[non_perfect_fit_indices_in_fitting]) \\\n            + (fitting_bins_capacity[non_perfect_fit_indices_in_fitting] / max_cap_val) * tie_breaker_factor\n\n    # Ensure perfect fits still have highest priority if they exist\n    # If perfect fits were assigned 1000.0, and non-perfect fits get scores like -0.1 + 0.99 * 1e-6,\n    # this order is maintained.\n\n    # If there are no perfect fits, and multiple bins have the same minimal remainder,\n    # the tie-breaker correctly selects the one with higher original capacity.\n\n    return scores\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins based on a weighted combination of tight-fitting potential\n    and a penalty for leaving very small, unusable remaining capacities.\n    Includes a small exploration factor for bins that are not perfect fits.\n    \"\"\"\n    epsilon = 0.05  # Reduced exploration probability\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.sum(can_fit_mask) == 0:\n        return priorities\n\n    valid_bins_capacities = bins_remain_cap[can_fit_mask]\n    valid_bins_indices = np.where(can_fit_mask)[0]\n\n    # Calculate scores for valid bins\n    # Score is primarily based on how tightly the item fits (minimizing remaining capacity)\n    # We invert the remaining capacity after fitting to get a \"tightness\" score.\n    # Higher score means tighter fit.\n    tight_fit_scores = valid_bins_capacities - item\n\n    # Penalize very small remaining capacities that might be \"wasted space\"\n    # This encourages filling bins more completely when possible.\n    # We use a small value for remaining capacities close to zero to avoid large penalties for perfect fits.\n    small_residual_penalty = np.maximum(0, 0.1 - tight_fit_scores) * 1000  # Heavier penalty for unusable space\n    \n    # Combine tight fit score with penalty.\n    # We want to maximize tight_fit_scores (minimize remaining capacity)\n    # and minimize small_residual_penalty.\n    # So, score = tight_fit_scores - small_residual_penalty\n    exploitation_scores = tight_fit_scores - small_residual_penalty\n\n    # Add a small bonus for perfect fits to explicitly favor them.\n    perfect_fit_bonus = 1e-3\n    exploitation_scores[np.abs(tight_fit_scores) < 1e-9] += perfect_fit_bonus\n\n    # Exploration: For bins that are not perfect fits, introduce a small random component\n    # to explore alternatives that might lead to better overall packing.\n    # We apply exploration only to a subset of non-perfect-fit bins.\n    exploration_mask = np.abs(tight_fit_scores) >= 1e-9\n    \n    if np.sum(exploration_mask) > 0:\n        exploration_scores_part = np.random.rand(np.sum(exploration_mask)) * 0.1 # Small random exploration score\n        \n        # Apply exploration scores probabilistically to a subset of non-perfect fit bins\n        exploration_indices_in_valid = np.where(exploration_mask)[0]\n        exploration_selection_prob = np.random.rand(len(exploration_indices_in_valid)) < epsilon\n\n        selected_exploration_indices_in_valid = exploration_indices_in_valid[exploration_selection_prob]\n        \n        exploitation_scores[selected_exploration_indices_in_valid] += exploration_scores_part[np.where(exploration_selection_prob)[0]]\n\n    priorities[can_fit_mask] = exploitation_scores\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}