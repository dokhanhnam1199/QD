```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Prioritizes bins using a multi-objective approach:
    - Primary: Tightest fit (minimizing remaining capacity).
    - Secondary: Reward bins with remaining capacity suitable for future medium-sized items.
    - Tertiary: Penalize bins that would lead to excessive wasted space after packing.
    - Exploration: Adaptively explore among the top-scoring bins, biased by current performance.
    """

    epsilon = 0.1  # Probability of exploring a candidate bin
    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=np.float64)

    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]

    # --- Multi-objective Scoring ---
    remaining_after_fit = valid_bins_capacities - item

    # Objective 1: Tightest Fit (minimize remaining capacity)
    # We want to maximize the negative of remaining capacity.
    tightness_score = -remaining_after_fit

    # Objective 2: Reward for future utility (medium remaining capacity)
    # Encourage bins that will have a moderate amount of space left.
    # This is a Gaussian-like bonus centered around a 'good' residual capacity.
    # Let's define 'good' residual capacity as roughly 0.5 * item_size.
    target_residual = item * 0.5
    # Using a difference of squares to create a peak at target_residual, decaying away.
    # Scale by 1/item^2 to make it more robust to item size variations.
    future_utility_score = -((remaining_after_fit - target_residual)**2) / (item**2 + 1e-9)
    # Add a small constant to ensure this is not too dominant over tightness for small items.
    future_utility_score *= 0.5 # Weight this objective

    # Objective 3: Penalize excessive waste
    # If remaining capacity is much larger than the item, it might be a poor fit for *this* item.
    # We penalize bins where remaining_after_fit is significantly larger than item.
    # This is a linear penalty for being too "loose".
    # Threshold: if remaining capacity is more than 2x the item size.
    excessive_waste_threshold_ratio = 2.0
    excessive_waste_penalty_factor = 0.2
    excessive_waste_mask = remaining_after_fit > (item * excessive_waste_threshold_ratio)
    excessive_waste_score = np.zeros_like(remaining_after_fit)
    excessive_waste_score[excessive_waste_mask] = -(remaining_after_fit[excessive_waste_mask] / item) * excessive_waste_penalty_factor

    # Combine objectives using a weighted sum.
    # Perfect fits (remaining_after_fit == 0) should have the highest score.
    # Add a large bonus for perfect fits to ensure they are prioritized.
    perfect_fit_bonus = 1000.0
    combined_exploitation_scores = tightness_score + future_utility_score + excessive_waste_score
    combined_exploitation_scores[np.abs(remaining_after_fit) < 1e-9] += perfect_fit_bonus

    # --- Adaptive Exploration Strategy ---
    # Instead of random exploration, explore among the top candidates.
    # Define "candidates" as bins that are either good fits or offer future utility.

    # Sort bins by their combined exploitation score
    sorted_indices_exploitation = np.argsort(combined_exploitation_scores)[::-1]

    # Identify candidate bins for exploration.
    # Candidates are the top-performing bins (e.g., top 25%)
    num_candidates = max(1, int(len(valid_bins_capacities) * 0.25))
    candidate_indices_in_valid = sorted_indices_exploitation[:num_candidates]

    # Generate exploration scores for these candidates.
    # These scores are a small random perturbation, allowing for some search.
    # The perturbation is scaled by the bin's current exploitation score to bias exploration
    # towards already promising bins.
    exploration_scores_raw = np.random.randn(len(valid_bins_capacities)) * 0.1 # Base noise
    exploration_scores_biased = exploration_scores_raw + combined_exploitation_scores * 0.05 # Bias towards good bins
    
    # Apply a softmax-like function to convert scores into probabilities
    # This helps in sampling without outright discarding less optimal, but still viable options.
    # We only apply this to the candidates.
    candidate_exploration_scores = np.full(len(valid_bins_capacities), -np.inf)
    candidate_exploration_scores[candidate_indices_in_valid] = exploration_scores_biased[candidate_indices_in_valid]

    # Decide probabilistically whether to use the exploration score for candidates.
    # For candidates, we use exploration score with probability epsilon, else exploitation.
    # For non-candidates, we always use exploitation score.
    
    final_priorities = np.copy(combined_exploitation_scores)
    
    # For candidate bins, decide probabilistically if we use their exploration score
    use_exploration_for_candidate = np.random.rand(len(valid_bins_capacities)) < epsilon
    
    # Apply exploration score if it's a candidate AND we chose to explore for this bin
    should_apply_exploration = use_exploration_for_candidate & (candidate_exploration_scores != -np.inf)
    
    # Here, instead of just replacing, we can blend.
    # For bins where exploration is chosen, we take a weighted average.
    # If exploration is NOT chosen for a candidate, we stick to the exploitation score.
    # For non-candidates, we always use the exploitation score.

    # Let's simplify: for candidates, if exploration is chosen, use the exploration score. Otherwise, use exploitation.
    # For non-candidates, always use exploitation.
    
    final_priorities[should_apply_exploration] = candidate_exploration_scores[should_apply_exploration]
    
    # Assign the computed priorities to the original bins array
    priorities[valid_bins_indices] = final_priorities

    return priorities
```
