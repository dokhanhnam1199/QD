```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines tightest fit with a penalty for excessive remaining capacity and
    a bonus for perfect fits, guided by an adaptive exploration bias towards
    bins with good current fit and historical performance.
    """

    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    valid_bins_capacities = bins_remain_cap[can_fit_mask]
    valid_bins_indices = np.where(can_fit_mask)[0]

    # --- Core Scoring Components (Exploitation) ---

    # 1. Tightest Fit: Maximize negative difference (remaining capacity after fit)
    remaining_after_fit = valid_bins_capacities - item
    tightest_fit_score = -remaining_after_fit

    # 2. Perfect Fit Bonus: High reward for exactly filling a bin
    perfect_fit_bonus = 1000.0
    perfect_fit_mask = np.abs(remaining_after_fit) < 1e-9
    exploitation_scores = tightest_fit_score
    exploitation_scores[perfect_fit_mask] += perfect_fit_bonus

    # 3. Penalty for Overly Empty Bins: Penalize bins that become excessively empty
    # This is a penalty if the remaining capacity is a large fraction of the *original* bin capacity.
    # We use a threshold relative to the current capacity after fit.
    overly_empty_penalty_factor = 0.5
    # Penalize if remaining capacity is more than 50% of the current capacity (after fitting)
    # This targets bins that were much larger than needed.
    overly_empty_mask = (remaining_after_fit / (valid_bins_capacities + 1e-9)) > 0.5
    overly_empty_penalty = (remaining_after_fit / (valid_bins_capacities + 1e-9)) * overly_empty_penalty_factor
    exploitation_scores[overly_empty_mask] -= overly_empty_penalty[overly_empty_mask]
    
    # --- Adaptive Exploration Component ---
    # Simulate a decaying memory or historical performance score for exploration.
    # Higher scores indicate bins that were historically good for similar items.
    # For demonstration, we'll use a small random positive bias.
    # In a real system, this would be updated based on past packing outcomes.
    simulated_historical_bias = np.random.rand(len(valid_bins_capacities)) * 0.1 # Small exploration bias
    
    # Combine exploitation scores with a scaled exploration bias.
    # The scaling factor controls the influence of the exploration bias.
    exploration_influence = 0.1
    final_scores = exploitation_scores + (simulated_historical_bias * exploration_influence)

    priorities[valid_bins_indices] = final_scores

    return priorities
```
