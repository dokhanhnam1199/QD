```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Priority function for online Bin Packing.

    Returns a score for each bin; higher score means more preferred.
    Infeasible bins receive -inf.
    The scoring combines a sigmoid bias towards tight fits, an exponential
    penalty for waste, a boost for exact fits, and a tiny tie‑breaker.
    Scores are min‑max normalised to [0, 1] for comparability.
    """
    # Ensure a NumPy array of floats
    caps = np.asarray(bins_remain_cap, dtype=float)

    # Remaining capacity after placing the item
    leftover = caps - item

    # Feasibility mask: only bins that can accommodate the item
    feasible = leftover >= 0

    # Initialise all scores to -inf (infeasible)
    scores = np.full_like(caps, -np.inf, dtype=float)

    # If no feasible bins exist, return early
    if not np.any(feasible):
        return scores

    # ----------------------------------------------------------------------
    # Adaptive steepness based on current leftover distribution
    # ----------------------------------------------------------------------
    median_leftover = np.median(leftover[feasible])
    base_alpha = 5.0            # base steepness for the sigmoid
    eps = 1e-12
    # Larger steepness when median leftover is small (tight‑fit regime)
    alpha = base_alpha / (median_leftover + eps)
    # Clamp to avoid extreme exponentials
    alpha = np.clip(alpha, 0.1, 100.0)

    # Use the same parameter for the exponential waste penalty
    beta = alpha

    # ----------------------------------------------------------------------
    # Sigmoid component (tight‑fit bias)
    # ----------------------------------------------------------------------
    max_exp = 50.0  # clip exponent to keep np.exp stable
    exp_arg = np.clip(alpha * leftover[feasible], 0.0, max_exp)
    # 2/(1+exp(alpha * leftover)) yields 1 at leftover=0 and decays smoothly
    sigmoid = 2.0 / (1.0 + np.exp(exp_arg))

    # ----------------------------------------------------------------------
    # Exponential waste penalty
    # ----------------------------------------------------------------------
    pen_exp_arg = np.clip(beta * leftover[feasible], 0.0, max_exp)
    penalty = np.exp(pen_exp_arg)

    # Combine components; lambda balances penalty strength (set to 1.0)
    lam = 1.0
    raw = sigmoid - lam * penalty

    # ----------------------------------------------------------------------
    # Boost exact fits
    # ----------------------------------------------------------------------
    exact_fit_tol = 1e-9
    exact_fit_bonus = 10.0
    exact_fit_mask = leftover[feasible] <= exact_fit_tol
    raw[exact_fit_mask] += exact_fit_bonus

    # ----------------------------------------------------------------------
    # Normalise to [0, 1]
    # ----------------------------------------------------------------------
    min_raw = raw.min()
    max_raw = raw.max()
    if max_raw > min_raw:
        norm_raw = (raw - min_raw) / (max_raw - min_raw)
    else:
        # All values equal (e.g., all exact fits); give them zero before tie‑break
        norm_raw = np.zeros_like(raw)

    # ----------------------------------------------------------------------
    # Tiny tie‑breaker preferring lower‑index bins
    # ----------------------------------------------------------------------
    epsilon = 1e-12
    feasible_indices = np.nonzero(feasible)[0]  # original indices of feasible bins
    norm_raw = norm_raw - epsilon * feasible_indices.astype(float)

    # Populate the final scores array
    scores[feasible] = norm_raw

    return scores
```
