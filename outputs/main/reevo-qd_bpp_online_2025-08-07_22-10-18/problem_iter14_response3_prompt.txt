{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Adaptive priority for online bin packing.\n\n    - Scores only feasible bins (remaining capacity >= item).\n    - Uses a sharp sigmoid centred at a small residual threshold to\n      boost exact or near\u2011exact fits.\n    - Penalises waste exponentially based on the normalised leftover.\n    - Adds a decaying epsilon random perturbation for exploration.\n    - Deterministic tie\u2011breaker via a tiny bias favouring lower indices.\n    - Infeasible bins receive -inf.\n\n    Parameters\n    ----------\n    item : float\n        Size of the incoming item.\n    bins_remain_cap : np.ndarray\n        1\u2011D array of remaining capacities of the currently open bins.\n\n    Returns\n    -------\n    np.ndarray\n        Priority scores (higher = more desirable). Shape matches\n        `bins_remain_cap`.\n    \"\"\"\n    caps = np.asarray(bins_remain_cap, dtype=float)\n    if caps.size == 0:\n        return np.array([], dtype=float)\n\n    # Residual capacity after placing the item in each bin.\n    residual = caps - item\n    feasible = residual >= 0\n\n    # If nothing fits, penalise all bins.\n    if not np.any(feasible):\n        return np.full_like(caps, -np.inf, dtype=float)\n\n    # Estimate bin capacity (assume at least one empty bin exists).\n    bin_capacity = np.max(caps)\n    if bin_capacity <= 0:\n        # Fallback: use the largest possible capacity inferred from residuals.\n        bin_capacity = np.max(caps + residual)\n    if bin_capacity <= 0:\n        bin_capacity = 1.0  # safety fallback\n\n    # Normalised residual (relative waste).\n    norm_res = residual / bin_capacity\n\n    # Adaptive sigmoid steepness based on spread of feasible residuals.\n    feasible_res = residual[feasible]\n    spread = np.std(feasible_res)\n    if spread <= 0.0:\n        spread = np.ptp(feasible_res)  # range as fallback\n    if spread <= 0.0:\n        spread = 1.0\n\n    base_steepness = 10.0  # tunable: larger \u2192 sharper transition\n    slope = base_steepness / spread\n\n    # Threshold for a \u201ctight\u201d fit (e.g., 5\u202f% of bin capacity).\n    tight_threshold = 0.05 * bin_capacity\n\n    # Sharp sigmoid: high score for residual << tight_threshold, low otherwise.\n    # sigmoid(x) = 1 / (1 + exp(slope * (x - tight_threshold)))\n    exp_arg = slope * (residual - tight_threshold)\n    exp_arg = np.clip(exp_arg, -700, 700)  # avoid overflow\n    sigmoid_score = 1.0 / (1.0 + np.exp(exp_arg))\n\n    # Exponential waste penalty: larger residual \u2192 stronger penalty.\n    alpha = 5.0  # tunable\n    waste_penalty = np.exp(alpha * norm_res)  # >= 1\n    base_score = sigmoid_score / waste_penalty\n\n    # Infeasible bins get -inf.\n    scores = np.where(feasible, base_score, -np.inf)\n\n    # --- Decaying epsilon exploration ---\n    # Initialise static call counter.\n    if not hasattr(priority_v2, \"_call_count\"):\n        priority_v2._call_count = 0\n    priority_v2._call_count += 1\n\n    # Epsilon decays as 1/sqrt(call_count) (adjustable schedule).\n    epsilon0 = 0.01\n    epsilon = epsilon0 / np.sqrt(priority_v2._call_count)\n\n    # Add small random perturbation to feasible scores.\n    noise = np.random.rand(caps.shape[0]) * epsilon\n    scores = np.where(feasible, scores + noise, scores)\n\n    # --- Deterministic tie\u2011breaker (tiny bias for lower index) ---\n    tie_bias = -np.arange(caps.shape[0]) * 1e-12\n    scores = np.where(scores > -np.inf, scores + tie_bias, scores)\n\n    return scores\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Priority function for online Bin Packing.\n\n    Returns a score for each bin; higher score means more preferred.\n    Infeasible bins receive -inf.\n    The scoring combines a sigmoid bias towards tight fits, an exponential\n    penalty for waste, a boost for exact fits, and a tiny tie\u2011breaker.\n    Scores are min\u2011max normalised to [0, 1] for comparability.\n    \"\"\"\n    # Ensure a NumPy array of floats\n    caps = np.asarray(bins_remain_cap, dtype=float)\n\n    # Remaining capacity after placing the item\n    leftover = caps - item\n\n    # Feasibility mask: only bins that can accommodate the item\n    feasible = leftover >= 0\n\n    # Initialise all scores to -inf (infeasible)\n    scores = np.full_like(caps, -np.inf, dtype=float)\n\n    # If no feasible bins exist, return early\n    if not np.any(feasible):\n        return scores\n\n    # ----------------------------------------------------------------------\n    # Adaptive steepness based on current leftover distribution\n    # ----------------------------------------------------------------------\n    median_leftover = np.median(leftover[feasible])\n    base_alpha = 5.0            # base steepness for the sigmoid\n    eps = 1e-12\n    # Larger steepness when median leftover is small (tight\u2011fit regime)\n    alpha = base_alpha / (median_leftover + eps)\n    # Clamp to avoid extreme exponentials\n    alpha = np.clip(alpha, 0.1, 100.0)\n\n    # Use the same parameter for the exponential waste penalty\n    beta = alpha\n\n    # ----------------------------------------------------------------------\n    # Sigmoid component (tight\u2011fit bias)\n    # ----------------------------------------------------------------------\n    max_exp = 50.0  # clip exponent to keep np.exp stable\n    exp_arg = np.clip(alpha * leftover[feasible], 0.0, max_exp)\n    # 2/(1+exp(alpha * leftover)) yields 1 at leftover=0 and decays smoothly\n    sigmoid = 2.0 / (1.0 + np.exp(exp_arg))\n\n    # ----------------------------------------------------------------------\n    # Exponential waste penalty\n    # ----------------------------------------------------------------------\n    pen_exp_arg = np.clip(beta * leftover[feasible], 0.0, max_exp)\n    penalty = np.exp(pen_exp_arg)\n\n    # Combine components; lambda balances penalty strength (set to 1.0)\n    lam = 1.0\n    raw = sigmoid - lam * penalty\n\n    # ----------------------------------------------------------------------\n    # Boost exact fits\n    # ----------------------------------------------------------------------\n    exact_fit_tol = 1e-9\n    exact_fit_bonus = 10.0\n    exact_fit_mask = leftover[feasible] <= exact_fit_tol\n    raw[exact_fit_mask] += exact_fit_bonus\n\n    # ----------------------------------------------------------------------\n    # Normalise to [0, 1]\n    # ----------------------------------------------------------------------\n    min_raw = raw.min()\n    max_raw = raw.max()\n    if max_raw > min_raw:\n        norm_raw = (raw - min_raw) / (max_raw - min_raw)\n    else:\n        # All values equal (e.g., all exact fits); give them zero before tie\u2011break\n        norm_raw = np.zeros_like(raw)\n\n    # ----------------------------------------------------------------------\n    # Tiny tie\u2011breaker preferring lower\u2011index bins\n    # ----------------------------------------------------------------------\n    epsilon = 1e-12\n    feasible_indices = np.nonzero(feasible)[0]  # original indices of feasible bins\n    norm_raw = norm_raw - epsilon * feasible_indices.astype(float)\n\n    # Populate the final scores array\n    scores[feasible] = norm_raw\n\n    return scores\n\n[Reflection]\nAdapt steepness via median leftover, cap exponentials, reward exact fits, normalise, avoid per\u2011call randomness, deterministic tie\u2011break.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}