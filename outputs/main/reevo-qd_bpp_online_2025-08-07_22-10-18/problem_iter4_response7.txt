```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Priority function for online Bin Packing.

    Parameters
    ----------
    item : float
        Size of the incoming item.
    bins_remain_cap : np.ndarray
        Array of the remaining capacities of the open bins.

    Returns
    -------
    np.ndarray
        Priority scores for each bin. Higher scores indicate a more
        desirable bin. Bins that cannot accommodate the item are given
        a score of -inf.

    Scoring scheme
    --------------
    1.  Compute the remaining capacity after placing the item.
    2.  If a bin is infeasible (remaining < 0), its score is -inf.
    3.  For feasible bins:
          - A sigmoid term 2/(1+exp(α * leftover)) that rewards tight
            fits and decays smoothly as leftover increases.
          - An exponential penalty exp(β * leftover) that strongly
            penalises waste.
          - An exact‑fit bonus added for bins where leftover ≈ 0.
    4.  The raw score is the difference between the sigmoid term and
        the weighted penalty.
    5.  Scores are min‑max normalised to [0, 1] for comparability.
    6.  A tiny tie‑breaker proportional to the bin index biases the
        selection toward lower indices when scores are equal.

    The steepness parameters α and β are adapted to the current
    distribution of leftover capacities using the median leftover
    of the feasible bins.  This keeps the scoring sensitive in both
    tight‑fit and waste‑heavy regimes.
    """
    caps = np.asarray(bins_remain_cap, dtype=float)
    leftover = caps - item
    feasible = leftover >= 0

    scores = np.full_like(caps, -np.inf, dtype=float)

    if not np.any(feasible):
        return scores

    # Parameters
    base_alpha = 5.0          # base steepness for the sigmoid
    eps = 1e-12
    max_exp = 50.0            # to avoid overflow in exp

    median_leftover = np.median(leftover[feasible])
    alpha = base_alpha / (median_leftover + eps)
    alpha = np.clip(alpha, 0.1, 20.0)

    beta = alpha

    # Sigmoid component (tight‑fit bias)
    exp_arg = np.clip(alpha * leftover[feasible], 0.0, max_exp)
    sigmoid = 2.0 / (1.0 + np.exp(exp_arg))

    # Exponential waste penalty
    pen_exp_arg = np.clip(beta * leftover[feasible], 0.0, max_exp)
    penalty = np.exp(pen_exp_arg)

    lam = 1.0  # weight of the penalty
    raw = sigmoid - lam * penalty

    # Boost exact fits
    exact_fit_tol = 1e-9
    exact_fit_mask = leftover[feasible] <= exact_fit_tol
    exact_fit_bonus = 10.0
    raw[exact_fit_mask] += exact_fit_bonus

    # Normalise to [0, 1]
    min_raw = raw.min()
    max_raw = raw.max()
    if max_raw > min_raw:
        norm_raw = (raw - min_raw) / (max_raw - min_raw)
    else:
        norm_raw = np.zeros_like(raw)

    # Tie‑breaker: lower index preferred
    epsilon = 1e-12
    feasible_indices = np.nonzero(feasible)[0]
    norm_raw -= epsilon * feasible_indices.astype(float)

    scores[feasible] = norm_raw

    return scores
```
