{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\nCurrent heuristics:\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines inverse distance, node degree penalty, and adaptive normalization.\"\"\"\n\n    zero_replacement_value = np.inf # use inf to discourage zero distance\n    std_replacement_value = 1e-6\n    nan_replacement_value = 0.0\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\nNow, think outside the box write a mutated function `heuristics_v2` better than current version.\nYou can use some hints below:\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine self-reflection for designing better heuristics, avoiding the pitfalls of generic advice.\n\n*   **Keywords:** Multifactorial analysis, Adaptive parameters, Edge case robustness, Search space focusing.\n\n*   **Advice:** Design heuristics by integrating diverse factors (distance, node features, problem-specific attributes) with adaptive parameters tuned by feedback during the search.\n\n*   **Avoid:** Overly simplistic \"consider normalization\" or \"handle edge cases\" without specifying *how* or *why* within the specific heuristic context.\n\n*   **Explanation:** Effective self-reflection involves critically evaluating *how* design choices (factor combinations, parameter settings) impact heuristic performance across different problem instances and adapting those choices accordingly.\n\n\nOutput code only and enclose your code with Python code block: ```python ... ```.\nI'm going to tip $999K for a better solution!"}