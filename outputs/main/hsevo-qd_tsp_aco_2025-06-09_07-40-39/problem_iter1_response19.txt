```python
import numpy as np

def heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:
    """
    Enhancements from v1:

    1.  Inverse Distance:  As before, shorter distances are initially preferred.  However, we'll scale it such that the *shortest* distance has maximal prior influence (1.0)
    2.  Normalization and Softmax: Applying a softmax function encourages diversity in exploration and avoids premature convergence on local optima.  This is particularly helpful in larger TSP instances.

    """

    # Handle potential division by zero (infinite distance)
    safe_distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)

    # Inverse of distance, scaled to [0, 1] where 1 is the shortest distance
    inverse_distances = 1 / safe_distance_matrix
    min_inv_dist = np.min(inverse_distances[np.isfinite(inverse_distances)])
    max_inv_dist = np.max(inverse_distances[np.isfinite(inverse_distances)])


    if np.isinf(max_inv_dist): # If all distances are infinite
        normalized_inverse_distances = np.zeros_like(distance_matrix)
    else:
        normalized_inverse_distances = (inverse_distances - min_inv_dist) / (max_inv_dist - min_inv_dist) # Scale to [0,1]
        normalized_inverse_distances = np.nan_to_num(normalized_inverse_distances, nan=0.0, posinf=0.0, neginf=0.0)
    # Apply Softmax row-wise to get probabilities.  High temperature for exploratory behavior.
    row_max = np.max(normalized_inverse_distances, axis=1, keepdims=True)  # Subtract maximum for numerical stability
    exp_values = np.exp((normalized_inverse_distances - row_max) * 2.0) # Higher temp

    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)


    return probabilities
```
