{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse distance, node degree penalty, and adaptive normalization.\"\"\"\n\n    zero_replacement_value = np.inf # use inf to discourage zero distance\n    std_replacement_value = 1e-6\n    nan_replacement_value = 0.0\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse distance, node degree penalty, and adaptive normalization.\"\"\"\n\n    zero_replacement_value = np.inf # use inf to discourage zero distance\n    std_replacement_value = 1e-6\n    nan_replacement_value = 0.0\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"Combines inverse distance, node degree penalty, and adaptive normalization.\"\"\"\n\n    zero_replacement_value = np.inf # use inf to discourage zero distance\n    std_replacement_value = 1e-6\n    nan_replacement_value = 0.0\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 4th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, zero_replacement_value: float = 52.12723074772252, std_replacement_value: float = 0.00028301997060490544, nan_replacement_value: float = 0.039947844979879576) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 5th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, zero_replacement_value: float = 52.12723074772252, std_replacement_value: float = 0.00028301997060490544, nan_replacement_value: float = 0.039947844979879576) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, zero_replacement_value: float = 52.12723074772252, std_replacement_value: float = 0.00028301997060490544, nan_replacement_value: float = 0.039947844979879576) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 7th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, zero_replacement_value: float = 52.12723074772252, std_replacement_value: float = 0.00028301997060490544, nan_replacement_value: float = 0.039947844979879576) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 8th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a heuristic for the Traveling Salesman Problem (TSP) based on:\n    1. Inverse distance: Closer cities are more likely to be connected.\n    2. Node Degree Preference: Penalizes high-degree nodes (prevents local optima saturation).\n    3. Variance based normalization to create reasonable probability.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n                                         between cities. distance_matrix[i][j] is the\n                                         distance between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                      representing the heuristic scores for each edge.  Higher\n                      scores indicate a more promising edge.\n    \"\"\"\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance (more or less what we did before)\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty - encourages exploration and avoids quickly settling\n    #    on a suboptimal solution. This penalizes edges connected to nodes that\n    #    already have many connections in the current (incomplete) tour.\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n\n    #Calculate sum of values associated to node. \n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                # Penalize connections to nodes with high \"strength\" from all connections\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    # Standardize each row and column to values between 0 and 1.\n    # To do this, take exponential. Scale exp by deviation from the mean.\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    # Avoid division by zero standard deviation\n    row_std = np.where(row_std == 0, 1e-6, row_std)\n    col_std = np.where(col_std == 0, 1e-6, col_std)\n    \n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n    \n    #Scale by product of standard deviations to normalize.\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    #Make sure to set any nan or inf values to a reasonable number.\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 9th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a heuristic for the Traveling Salesman Problem (TSP) based on:\n    1. Inverse distance: Closer cities are more likely to be connected.\n    2. Node Degree Preference: Penalizes high-degree nodes (prevents local optima saturation).\n    3. Variance based normalization to create reasonable probability.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n                                         between cities. distance_matrix[i][j] is the\n                                         distance between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                      representing the heuristic scores for each edge.  Higher\n                      scores indicate a more promising edge.\n    \"\"\"\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance (more or less what we did before)\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty - encourages exploration and avoids quickly settling\n    #    on a suboptimal solution. This penalizes edges connected to nodes that\n    #    already have many connections in the current (incomplete) tour.\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n\n    #Calculate sum of values associated to node. \n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                # Penalize connections to nodes with high \"strength\" from all connections\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    # Standardize each row and column to values between 0 and 1.\n    # To do this, take exponential. Scale exp by deviation from the mean.\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    # Avoid division by zero standard deviation\n    row_std = np.where(row_std == 0, 1e-6, row_std)\n    col_std = np.where(col_std == 0, 1e-6, col_std)\n    \n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n    \n    #Scale by product of standard deviations to normalize.\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    #Make sure to set any nan or inf values to a reasonable number.\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 10th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a heuristic for the Traveling Salesman Problem (TSP) based on:\n    1. Inverse distance: Closer cities are more likely to be connected.\n    2. Node Degree Preference: Penalizes high-degree nodes (prevents local optima saturation).\n    3. Variance based normalization to create reasonable probability.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n                                         between cities. distance_matrix[i][j] is the\n                                         distance between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                      representing the heuristic scores for each edge.  Higher\n                      scores indicate a more promising edge.\n    \"\"\"\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance (more or less what we did before)\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty - encourages exploration and avoids quickly settling\n    #    on a suboptimal solution. This penalizes edges connected to nodes that\n    #    already have many connections in the current (incomplete) tour.\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n\n    #Calculate sum of values associated to node. \n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                # Penalize connections to nodes with high \"strength\" from all connections\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    # Standardize each row and column to values between 0 and 1.\n    # To do this, take exponential. Scale exp by deviation from the mean.\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    # Avoid division by zero standard deviation\n    row_std = np.where(row_std == 0, 1e-6, row_std)\n    col_std = np.where(col_std == 0, 1e-6, col_std)\n    \n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n    \n    #Scale by product of standard deviations to normalize.\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    #Make sure to set any nan or inf values to a reasonable number.\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 11th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a heuristic for the Traveling Salesman Problem (TSP) based on:\n    1. Inverse distance: Closer cities are more likely to be connected.\n    2. Node Degree Preference: Penalizes high-degree nodes (prevents local optima saturation).\n    3. Variance based normalization to create reasonable probability.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n                                         between cities. distance_matrix[i][j] is the\n                                         distance between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                      representing the heuristic scores for each edge.  Higher\n                      scores indicate a more promising edge.\n    \"\"\"\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance (more or less what we did before)\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty - encourages exploration and avoids quickly settling\n    #    on a suboptimal solution. This penalizes edges connected to nodes that\n    #    already have many connections in the current (incomplete) tour.\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n\n    #Calculate sum of values associated to node. \n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                # Penalize connections to nodes with high \"strength\" from all connections\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    # Standardize each row and column to values between 0 and 1.\n    # To do this, take exponential. Scale exp by deviation from the mean.\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    # Avoid division by zero standard deviation\n    row_std = np.where(row_std == 0, 1e-6, row_std)\n    col_std = np.where(col_std == 0, 1e-6, col_std)\n    \n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n    \n    #Scale by product of standard deviations to normalize.\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    #Make sure to set any nan or inf values to a reasonable number.\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 12th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a heuristic for the Traveling Salesman Problem (TSP) based on:\n    1. Inverse distance: Closer cities are more likely to be connected.\n    2. Node Degree Preference: Penalizes high-degree nodes (prevents local optima saturation).\n    3. Variance based normalization to create reasonable probability.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance\n                                         between cities. distance_matrix[i][j] is the\n                                         distance between city i and city j.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                      representing the heuristic scores for each edge.  Higher\n                      scores indicate a more promising edge.\n    \"\"\"\n\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance (more or less what we did before)\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty - encourages exploration and avoids quickly settling\n    #    on a suboptimal solution. This penalizes edges connected to nodes that\n    #    already have many connections in the current (incomplete) tour.\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n\n    #Calculate sum of values associated to node. \n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                # Penalize connections to nodes with high \"strength\" from all connections\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    # Standardize each row and column to values between 0 and 1.\n    # To do this, take exponential. Scale exp by deviation from the mean.\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    # Avoid division by zero standard deviation\n    row_std = np.where(row_std == 0, 1e-6, row_std)\n    col_std = np.where(col_std == 0, 1e-6, col_std)\n    \n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n    \n    #Scale by product of standard deviations to normalize.\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    #Make sure to set any nan or inf values to a reasonable number.\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n\n    return final_heuristic_matrix\n\n[Heuristics 13th]\nimport numpy as np\n\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpening_factor = 2.0  # Adjust for desired sharpness\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, 1e-9, row_sum)\n    col_sum = np.where(col_sum == 0, 1e-9, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n[Heuristics 14th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n[Heuristics 15th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n[Heuristics 16th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, sharpening_factor: float = 2.6630944696060506, zero_replacement: float = 3.2874197298051493e-07) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n[Heuristics 17th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, sharpening_factor: float = 2.6630944696060506, zero_replacement: float = 3.2874197298051493e-07) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n[Heuristics 18th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, sharpening_factor: float = 2.6630944696060506, zero_replacement: float = 3.2874197298051493e-07) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n[Heuristics 19th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, sharpening_factor: float = 2.6630944696060506, zero_replacement: float = 3.2874197298051493e-07) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n[Heuristics 20th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef heuristics_v2(distance_matrix: np.ndarray, sharpening_factor: float = 2.6630944696060506, zero_replacement: float = 3.2874197298051493e-07) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}