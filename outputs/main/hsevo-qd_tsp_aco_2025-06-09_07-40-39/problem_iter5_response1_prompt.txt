{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n    return 1 / distance_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic incorporates inverse distance, node degree penalty, and variance-based normalization with safeguards against division by zero and NaN/Inf values, while the worst heuristic only uses inverse distance.\n\nComparing (2nd best) vs (second worst), heuristics (19th) is the same with (20th).\n\nComparing (1st) vs (2nd), we see the top two are exactly the same. Comparing (3rd) vs (4th) ...Comparing (5th) vs (1st), all of them are the same.\n\nComparing (6th) vs (1st), the 6th version adds parameters `zero_replacement_value`, `std_replacement_value`, and `nan_replacement_value`, allowing more control over handling edge cases. This is an improvement. Comparing (7th) vs (6th)...Comparing (10th) vs (6th), all of them are the same.\n\nComparing (11th) vs (6th), the 11th is simplified by removing parameters.\n\nComparing (16th) vs (11th), we see the 16th version introduces sparsification and edge sharpening based on node mean heuristic values, followed by normalization across rows and columns using sums instead of standard deviations. This aims to emphasize promising edges and reduce noise.\n\nComparing (17th) vs (16th)...Comparing (18th) vs (16th), all of them are the same.\n\nOverall: The better heuristics incorporate more sophisticated techniques like node degree penalty, variance-based normalization, and edge sharpening/sparsification to improve the quality of the heuristic matrix. The worst heuristics rely only on the inverse distance, which is a very basic approach.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine self-reflection for designing better heuristics, avoiding the pitfalls of generic advice.\n\n*   **Keywords:** Multifactorial analysis, Adaptive parameters, Edge case robustness, Search space focusing.\n\n*   **Advice:** Design heuristics by integrating diverse factors (distance, node features, problem-specific attributes) with adaptive parameters tuned by feedback during the search.\n\n*   **Avoid:** Overly simplistic \"consider normalization\" or \"handle edge cases\" without specifying *how* or *why* within the specific heuristic context.\n\n*   **Explanation:** Effective self-reflection involves critically evaluating *how* design choices (factor combinations, parameter settings) impact heuristic performance across different problem instances and adapting those choices accordingly.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}