{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n### Better code\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse distance and node degree penalty for TSP.\n    Normalizes heuristic scores to represent edge promisingness.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, zero_replacement_value, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Normalization using variance across each edges\n    row_mean = np.mean(heuristic_matrix, axis=1, keepdims=True)\n    row_std = np.std(heuristic_matrix, axis=1, keepdims=True)\n\n    col_mean = np.mean(heuristic_matrix, axis=0, keepdims=True)\n    col_std = np.std(heuristic_matrix, axis=0, keepdims=True)\n\n    row_std = np.where(row_std == 0, std_replacement_value, row_std)\n    col_std = np.where(col_std == 0, std_replacement_value, col_std)\n\n    row_normalized = np.exp((heuristic_matrix - row_mean) / row_std)\n    col_normalized = np.exp((heuristic_matrix - col_mean) / col_std)\n\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=nan_replacement_value, posinf=nan_replacement_value, neginf=nan_replacement_value)\n\n\n    return final_heuristic_matrix\n\n### Worse code\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Enhanced heuristics for TSP, combining inverse distance,\n    node degree penalty, and sparsity. Normalizes and sharpens\n    heuristic scores to represent edge promisingness more effectively.\n    \"\"\"\n    # Avoid division by zero\n    distance_matrix = np.where(distance_matrix == 0, np.inf, distance_matrix)\n\n    # 1. Inverse distance\n    heuristic_matrix = 1 / distance_matrix\n\n    # 2. Node degree penalty\n    num_nodes = distance_matrix.shape[0]\n    degree_penalty = np.zeros_like(distance_matrix)\n    node_strengths = np.sum(heuristic_matrix, axis=0)\n\n    for i in range(num_nodes):\n        for j in range(num_nodes):\n            if i != j:\n                degree_penalty[i, j] = 1 / (node_strengths[i] + node_strengths[j])\n\n    # Combine inverse distance and node degree penalty\n    heuristic_matrix = heuristic_matrix * degree_penalty\n\n    # 3. Sparsification and Edge Sharpening\n    # Calculate the mean heuristic value for each node\n    node_mean_heuristic = np.mean(heuristic_matrix, axis=1, keepdims=True)\n\n    # Sharpen edges by emphasizing those significantly above the node's mean\n    sharpened_heuristic = np.where(heuristic_matrix > sharpening_factor * node_mean_heuristic,\n                                  heuristic_matrix,\n                                  0.0)  # Sparsify by setting others to zero\n\n    # 4. Normalization across rows and columns\n    row_sum = np.sum(sharpened_heuristic, axis=1, keepdims=True)\n    col_sum = np.sum(sharpened_heuristic, axis=0, keepdims=True)\n\n    # Avoid division by zero\n    row_sum = np.where(row_sum == 0, zero_replacement, row_sum)\n    col_sum = np.where(col_sum == 0, zero_replacement, col_sum)\n\n    row_normalized = sharpened_heuristic / row_sum\n    col_normalized = sharpened_heuristic / col_sum\n\n    # Combine row and column normalized values\n    final_heuristic_matrix = row_normalized * col_normalized\n\n    #Zero out diagonals (no self-loops)\n    np.fill_diagonal(final_heuristic_matrix, 0)\n    final_heuristic_matrix = np.nan_to_num(final_heuristic_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return final_heuristic_matrix\n\n### Analyze & experience\n- Comparing (1st) vs (14th), we see the best heuristic incorporates inverse distance, node degree penalty, and variance-based normalization, while the worst only uses inverse distance. (8th) vs (15th) reinforces this. Comparing (1st) vs (2nd), we see the top 3 heuristics are functionally identical. (4th) vs (5th) and (6th) vs (7th) also show identical functionality, merely differing in added (unused) imports. (16th) vs (14th), the 16th introduces node degree penalty and sparsification, edge sharpening. Comparing (8th) vs (14th), the 8th heuristic incorporates node degree penalty and variance based normalization. Comparing (13th) vs (14th), the 13th includes node degree penalty, and sparsification with edge sharpening. Comparing (16th) vs (14th), we see that the 16th includes node degree penalty, sparsification and edge sharpening. Comparing (16th) vs (13th), the 16th has exposed parameters. Comparing (13th) vs (8th), the 13th has edge sharpening and sparsification, normalization. Overall: Better heuristics combine inverse distance with node degree penalty and normalization techniques. Sparsification and edge sharpening may further enhance performance.\n- - Try combining various factors to determine how promising it is to select an edge.\n- Try sparsifying the matrix by setting unpromising elements to zero.\nOkay, let's refine our self-reflection to design better heuristics, aiming for actionable insights:\n\n*   **Keywords:** Iterative refinement, multi-factor integration, robustness, computational efficiency, parameter tuning, search space focusing.\n\n*   **Advice:** Systematically experiment with combining diverse features (distance, degree, etc.). Quantify the impact of each addition on both solution quality and runtime. Explicitly design for edge cases and numerical stability.\n\n*   **Avoid:** Premature optimization, relying solely on intuition, neglecting computational cost analysis.\n\n*   **Explanation:** This approach encourages data-driven design, balancing complexity and efficiency, while ensuring generalizability through parameter tuning and careful consideration of edge cases.\n\n\nYour task is to write an improved function `heuristics_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}