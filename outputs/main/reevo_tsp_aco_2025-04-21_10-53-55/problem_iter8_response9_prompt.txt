{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n[Worse code]\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for the Traveling Salesman Problem (TSP) that combines\n    multiple factors to estimate the \"promise\" of including each edge in a solution.\n    This version focuses on neighborhood density, implicitly avoids hub nodes, and\n    normalizes heuristics for robust sampling.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where element (i, j) represents\n                                      the distance between city i and city j.  It is\n                                      assumed that the diagonal elements are zero or very large\n                                      to discourage self-loops.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n                      (i, j) indicates the desirability of including the edge (i, j) in\n                      the TSP tour. Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n\n    # 1. Inverse distance: Shorter distances are generally more desirable.\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Add a small constant to avoid division by zero\n\n    # 2. Neighborhood density:  Encourage edges connecting to less dense areas.\n    #    Calculate the average distance to the k nearest neighbors for each city.\n    #    Cities in sparser areas will have larger average nearest neighbor distances.\n    k = min(5, n - 1)  # Consider up to the 5th nearest neighbor, or fewer if n is small\n    avg_neighbor_distances = np.zeros(n)\n    for i in range(n):\n        distances = distance_matrix[i, :]\n        nearest_neighbor_distances = np.partition(distances, k + 1)[1:k + 1]  # Exclude self-loop (distance=0)\n        avg_neighbor_distances[i] = np.mean(nearest_neighbor_distances)\n\n    neighbor_density_factor = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            neighbor_density_factor[i, j] = avg_neighbor_distances[i] + avg_neighbor_distances[j]\n\n    # 3. Implicit Hub Avoidance: Penalize edges if either node has many close neighbors.\n    hub_penalty = np.zeros_like(distance_matrix)\n    degree_threshold = np.median(distance_matrix[distance_matrix > 0])  # Dynamic Threshold\n    for i in range(n):\n        for j in range(n):\n            degree_i = np.sum(distance_matrix[i, :] < degree_threshold)\n            degree_j = np.sum(distance_matrix[j, :] < degree_threshold)\n            hub_penalty[i, j] = (degree_i + degree_j)\n\n    # 4. Combination and scaling: Combine the factors with adjusted weights\n    heuristic_matrix = inverse_distance * (1 + 0.5 * neighbor_density_factor) / (\n                1 + 0.01 * hub_penalty ** 2)  # Adjust Weights; Increased Hub Penalty\n\n    # 5. Sparsification: Zero out edges that are very unlikely to be in the optimal tour\n    #    based on a threshold derived from the mean of the heuristic matrix. This promotes\n    #    exploration of more focused search spaces.\n\n    # Adaptive sparsification based on a percentage of top edges\n    num_edges_to_keep = int(0.5 * n * (n - 1))  # Keep approximately half of possible edges\n    flattened_heuristic = heuristic_matrix.flatten()\n    top_indices = np.argpartition(flattened_heuristic, -num_edges_to_keep)[-num_edges_to_keep:]\n    threshold = np.min(flattened_heuristic[top_indices]) #Threshold is minimum of kept edges\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # 6. Normalize the matrix to have values between 0 and 1.\n    max_val = np.max(heuristic_matrix)\n    if max_val > 0:\n        heuristic_matrix = heuristic_matrix / max_val\n\n    return heuristic_matrix\n\n[Better code]\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using nearest neighbors, local density,\n    geometric means, adaptive sparsification, and row-wise normalization.\n\n    Args:\n        distance_matrix (np.ndarray): A 2D numpy array representing the distance matrix.\n\n    Returns:\n        np.ndarray: A 2D numpy array of the same shape as distance_matrix,\n                     representing the heuristic values (prior indicators).\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # Calculate nearest neighbors for each node.\n    nearest_neighbors = []\n    for i in range(n):\n        distances = distance_matrix[i].copy()\n        distances[i] = np.inf  # Exclude self-loop\n        nearest_neighbors.append(np.argsort(distances))\n\n    # Calculate a centrality measure (degree of connectivity).\n    degree_centrality = np.sum(1 / (distance_matrix + 1e-9), axis=0)\n    degree_centrality = degree_centrality / np.max(degree_centrality)  # Normalize\n\n    # Inverse distance factor\n    inverse_distance = 1 / (distance_matrix + 1e-9)\n    inverse_distance = inverse_distance / np.max(inverse_distance)  # Normalize\n\n    # Local Density Estimation\n    local_density = np.zeros(n)\n    for i in range(n):\n        neighbor_count = np.sum(distance_matrix[i] < np.median(distance_matrix[i]))  # Number of neighbors within median distance\n        local_density[i] = neighbor_count / (n - 1)\n\n    # Combine factors: inverse distance, nearest neighbor rank, and centrality.\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Rank of j in i's nearest neighbors and vice versa\n                rank_i = np.where(nearest_neighbors[i] == j)[0][0]\n                rank_j = np.where(nearest_neighbors[j] == i)[0][0]\n                rank_factor = 1 / (rank_i + rank_j + 1)\n\n                centrality_factor = (degree_centrality[i] + degree_centrality[j])\n\n                # Adjust centrality factor based on local density\n                adjusted_centrality = centrality_factor * (1 + local_density[i] + local_density[j]) / 2\n\n                # Geometric Mean for combining factors\n                heuristic_matrix[i, j] = (inverse_distance[i, j] * rank_factor * adjusted_centrality)**(1/3)\n\n    # Adaptive Sparsification\n    quantile_level = 0.75  # Adjust as needed\n    flat_heuristic = heuristic_matrix[heuristic_matrix > 0]\n\n    if len(flat_heuristic) > 0:  # avoid potential error\n        threshold = np.quantile(flat_heuristic, quantile_level)  # only consider elements > 0\n        heuristic_matrix[heuristic_matrix < threshold] = 0  # sparsify\n\n    # Row-wise normalization to avoid zero rows after sparsification\n    for i in range(n):\n        row_sum = np.sum(heuristic_matrix[i])\n        if row_sum > 0:\n            heuristic_matrix[i] = heuristic_matrix[i] / row_sum\n\n    return heuristic_matrix\n\n[Reflection]\nBetter heuristics: use rank-based measures, geometric means, adaptive sparsification, and row-wise normalization.\n\n\n[Improved code]\nPlease write an improved function `heuristics_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}