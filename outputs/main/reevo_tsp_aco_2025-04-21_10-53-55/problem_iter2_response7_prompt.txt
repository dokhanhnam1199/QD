{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n[Worse code]\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristics function for TSP using a combination of factors and sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix representing the distances between cities.\n\n    Returns:\n        np.ndarray: Prior indicators of how promising it is to include each edge in a solution.\n                     The return is of the same shape as the input, with values between 0 and 1.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance: Shorter distances are more promising.\n    inverse_distance = 1 / (distance_matrix + 1e-6)  # Adding a small constant to avoid division by zero\n\n    # 2. Node degree consideration: Favor edges connected to nodes with fewer connections\n    degree = np.sum(distance_matrix > 0, axis=0) -1 # exclude self loops\n    degree_matrix_i = np.tile(degree, (n, 1))\n    degree_matrix_j = np.tile(degree[:, np.newaxis], (1, n))\n    degree_factor = 1 / (degree_matrix_i + degree_matrix_j + 1e-6)\n\n    # 3. Combination of factors: Combining inverse distance and degree consideration\n    heuristics_matrix = inverse_distance * degree_factor\n\n    # 4. Normalization to [0, 1]\n    max_val = np.max(heuristics_matrix)\n    heuristics_matrix = heuristics_matrix / max_val if max_val > 0 else heuristics_matrix\n\n\n    # 5. Sparsification: Setting less promising elements to zero\n    threshold = np.mean(heuristics_matrix[heuristics_matrix > 0]) * 0.2 # dynamically adjust threshold\n    heuristics_matrix[heuristics_matrix < threshold] = 0\n\n    # Remove self-loops\n    np.fill_diagonal(heuristics_matrix, 0)\n    return heuristics_matrix\n\n[Better code]\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for the Traveling Salesman Problem (TSP) that combines\n    multiple factors to estimate the \"promise\" of including each edge in a solution.\n    This version attempts to be more sophisticated than a simple inverse distance.\n\n    Args:\n        distance_matrix (np.ndarray): A square matrix where element (i, j) represents\n                                      the distance between city i and city j.  It is\n                                      assumed that the diagonal elements are zero or very large\n                                      to discourage self-loops.\n\n    Returns:\n        np.ndarray: A matrix of the same shape as distance_matrix, where each element\n                      (i, j) indicates the desirability of including the edge (i, j) in\n                      the TSP tour. Higher values indicate more promising edges.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n\n    # 1. Inverse distance: Shorter distances are generally more desirable.\n    inverse_distance = 1.0 / (distance_matrix + 1e-9)  # Add a small constant to avoid division by zero\n\n    # 2. Nearest neighbor influence:  Edges connecting to cities with few nearby neighbors are more crucial.\n    #    For each city, find the distance to its k-th nearest neighbor. A smaller k-th nearest\n    #    neighbor distance indicates a denser local neighborhood. Edges connected to less dense\n    #    areas might be more important to ensure connectivity.\n\n    k = min(5, n - 1)  # Consider up to the 5th nearest neighbor, or fewer if n is small\n    nearest_neighbor_distances = np.zeros(n)\n    for i in range(n):\n        distances = distance_matrix[i, :]\n        nearest_neighbor_distances[i] = np.partition(distances, k + 1)[k + 1]\n\n    neighbor_influence = np.zeros_like(distance_matrix)\n    for i in range(n):\n        for j in range(n):\n            neighbor_influence[i, j] = nearest_neighbor_distances[i] + nearest_neighbor_distances[j]\n\n    # 3. Combination and scaling: Combine the factors.  Experiment with different weights.\n    heuristic_matrix = inverse_distance * (1 + neighbor_influence)\n\n    # 4. Sparsification: Zero out edges that are very unlikely to be in the optimal tour\n    #    based on a threshold derived from the mean of the heuristic matrix. This promotes\n    #    exploration of more focused search spaces.\n\n    threshold = np.mean(heuristic_matrix) * 0.2  # Tune this threshold multiplier\n    heuristic_matrix[heuristic_matrix < threshold] = 0\n\n    # Normalize the matrix to have values between 0 and 1. This isn't strictly necessary\n    # but might be helpful for some algorithms that use it.  Be aware this can also zero-out\n    # very small heuristic values due to floating-point issues, so exercise caution if your\n    # algorithm relies on edges with very weak probability remaining non-zero.\n    max_val = np.max(heuristic_matrix)\n    if max_val > 0: #Avoid division by zero.\n        heuristic_matrix = heuristic_matrix / max_val\n\n    return heuristic_matrix\n\n[Reflection]\nConsider local neighborhood density, tune combination weights, and be cautious about normalization side effects.\n\n\n[Improved code]\nPlease write an improved function `heuristics_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}