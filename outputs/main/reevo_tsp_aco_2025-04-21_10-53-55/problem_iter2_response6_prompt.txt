{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a heuristics function for Solving Traveling Salesman Problem (TSP) via stochastic solution sampling following \"heuristics\". TSP requires finding the shortest path that visits all given nodes and returns to the starting node.\nThe `heuristics` function takes as input a distance matrix, and returns prior indicators of how promising it is to include each edge in a solution. The return is of the same shape as the input.\n\n\n[Worse code]\ndef heuristics_v0(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using a combination of factors and sparsification.\n\n    Args:\n        distance_matrix (np.ndarray): A distance matrix representing the distances between cities.\n\n    Returns:\n        np.ndarray: A heuristic matrix indicating the desirability of including each edge in a tour.\n    \"\"\"\n    n = distance_matrix.shape[0]\n    heuristic_matrix = np.zeros_like(distance_matrix, dtype=float)\n\n    # 1. Inverse distance: Shorter edges are generally better.\n    inverse_distance = 1 / (distance_matrix + 1e-9)  # Adding a small value to avoid division by zero.\n\n    # 2. Node degree desirability:  Penalize connecting to nodes that already have many close neighbors.  This encourages exploration.\n    node_degree_desirability = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        distances_to_i = distance_matrix[i, :]\n        sorted_indices = np.argsort(distances_to_i)\n        closest_neighbors = sorted_indices[1:min(4, n)]  # Consider top 3 closest neighbors (excluding itself)\n        for j in range(n):\n             if i != j:\n                 neighbor_count = 0\n                 for neighbor in closest_neighbors:\n                   if j != i and np.any(np.argsort(distance_matrix[j,:])[1:min(4,n)] == i):\n                       neighbor_count += 1\n                 node_degree_desirability[i,j] = 1 / (neighbor_count +1 ) # avoid div by zero.\n    #3. Global Average Distance: Favor edges shorter than average distance\n    avg_dist = np.mean(distance_matrix[distance_matrix > 0])\n\n    # Combine the factors:\n    heuristic_matrix = inverse_distance * node_degree_desirability\n\n    # Adjust based on average distance\n    heuristic_matrix[distance_matrix > avg_dist] *= 0.2 # heavily penalize long edges\n\n    # 4. Sparsification: Keep only the top K promising edges for each node to reduce complexity\n    K = min(5, n - 1)  # Keep at most 5 edges per node\n    for i in range(n):\n        row = heuristic_matrix[i, :]\n        indices = np.argsort(row)[::-1]  # Sort in descending order of heuristic value\n        keep_indices = indices[:K]\n\n        #Zero out all but the top K edges:\n        zero_indices = np.setdiff1d(np.arange(n), keep_indices)\n        heuristic_matrix[i,zero_indices] = 0\n        heuristic_matrix[i,i] = 0 #zero out self loop\n\n    return heuristic_matrix\n\n[Better code]\ndef heuristics_v1(distance_matrix: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Heuristic function for TSP using stochastic solution sampling.\n\n    This version combines multiple factors to estimate edge promise and sparsifies the matrix.\n\n    Args:\n        distance_matrix (np.ndarray): Distance matrix of the TSP problem.\n\n    Returns:\n        np.ndarray: Prior indicators (heuristics) of edge promise.\n    \"\"\"\n\n    n = distance_matrix.shape[0]\n    heuristics = np.zeros_like(distance_matrix)\n\n    # 1. Inverse distance (basic heuristic)\n    inverse_distance = 1 / (distance_matrix + 1e-9)  # Add a small constant to avoid division by zero\n\n    # 2. Nearest neighbor heuristic: reward edges to nearest neighbors\n    nearest_neighbors = np.argsort(distance_matrix, axis=1)\n    nearest_neighbor_bonus = np.zeros_like(distance_matrix, dtype=float)\n    for i in range(n):\n        for j in range(1, min(4, n)):  # Bonus for the 3 nearest neighbors (excluding self)\n            neighbor_index = nearest_neighbors[i, j]\n            nearest_neighbor_bonus[i, neighbor_index] = 1.0 / j #closer, larger bonus\n\n    # 3. Minimum Spanning Tree approximation: edges that participate MSTs are important\n\n    # Calculate probabilities based on these heuristics:\n    heuristics = inverse_distance + nearest_neighbor_bonus\n\n    # 4. Sparsification: Remove edges unlikely to be in the optimal solution\n    # Only keep the top k edges per row/column based on the combined heuristic\n\n    k = min(5, n -1)  # Number of edges to keep per node (excluding self)\n\n    for i in range(n):\n        # Zero out all entries except the k largest in each row\n        row = heuristics[i, :].copy()\n        indices_to_keep = np.argsort(row)[-k:]  # Indices of the k largest elements\n        heuristics[i, :] = 0\n        heuristics[i, indices_to_keep] = row[indices_to_keep]\n\n        # Zero out the diagonal element, if not done.\n        heuristics[i, i] = 0\n\n    # Normalize\n    max_val = np.max(heuristics)\n    if max_val > 0:\n        heuristics = heuristics / max_val\n\n    return heuristics\n\n[Reflection]\nPrioritize nearest neighbors and MST approximation. Normalize heuristics after combination.\n\n\n[Improved code]\nPlease write an improved function `heuristics_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}