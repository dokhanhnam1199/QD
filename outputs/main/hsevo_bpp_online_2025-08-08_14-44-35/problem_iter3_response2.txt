```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a hybrid approach
    combining First Fit Decreasing (FFD) intuition with a focus on minimizing wasted space.

    This heuristic prioritizes bins that can fit the item and have a remaining capacity
    that is "close" to the item size, aiming to reduce fragmentation. It also incorporates
    a penalty for bins that are too large relative to the item, discouraging the use
     of oversized bins for small items.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Bins that can fit the item
    can_fit_mask = bins_remain_cap >= item
    
    # For bins that can fit the item, calculate a priority score.
    # We want to prioritize bins where the remaining capacity is *just* enough or slightly more.
    # A bin with remaining capacity `r` after packing the item `i` will have `r = bins_remain_cap - item`.
    # We want to maximize `1 / (r + epsilon)` for tighter packing.
    # Additionally, we penalize bins that are excessively large compared to the item.
    
    epsilon = 1e-9
    
    # Calculate 'tightness' score: higher for smaller remaining capacity after packing
    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)
    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]
    remaining_after_packing = fitting_bins_remain_cap - item
    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)
    
    # Calculate 'oversize' penalty: penalize bins that are much larger than the item.
    # This encourages using bins that are closer in size to the item if possible.
    # We use a logarithmic scale to dampen the effect of very large bins.
    oversize_penalty = np.zeros_like(bins_remain_cap, dtype=float)
    oversize_factor = bins_remain_cap[can_fit_mask] / (item + epsilon)
    # Penalize if oversize_factor is significantly greater than 1.
    # A threshold of 2 means a bin is twice the item size. We can adjust this.
    oversize_threshold = 2.0
    penalty_strength = 0.5 # Tune this parameter
    
    oversize_penalty[can_fit_mask] = np.maximum(0, 1 - penalty_strength * np.log(oversize_factor / oversize_threshold + 1))
    
    # Combine scores. Prioritize tightness, but also penalize significant oversizing.
    # A weighted sum can be used, or a multiplicative approach.
    # Here we'll use a multiplicative approach where oversizing reduces the priority.
    
    combined_priority = tightness_score * oversize_penalty
    
    # Normalize priorities to have a max of 1.0
    max_priority = np.max(combined_priority)
    if max_priority > 0:
        priorities[can_fit_mask] = combined_priority / max_priority
    
    return priorities
```
