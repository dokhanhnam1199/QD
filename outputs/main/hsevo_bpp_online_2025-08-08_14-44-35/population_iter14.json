[
  {
    "stdout_filepath": "problem_iter14_response0.txt_stdout.txt",
    "code_path": "problem_iter14_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a dynamic utilization bonus and adaptive exploration.\n    Prioritizes bins that minimize waste, are less utilized, and explores promising options.\n    \"\"\"\n\n    epsilon = 1e-9\n    exploration_prob = 0.10  # Slightly reduced exploration for more exploitation focus\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    valid_bin_indices = np.where(can_fit_mask)[0]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Score is inversely proportional to remaining capacity after packing.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Dynamic Utilization Bonus ---\n    # Reward bins that are less utilized (more empty space).\n    # Bonus is higher for bins with significantly larger remaining capacities.\n    # This encourages spreading items but also considers bins that are mostly empty.\n    # Using a logistic function to smoothly penalize larger remaining capacities.\n    utilization_bonus = 0.5 * (1.0 - 1.0 / (1.0 + np.exp(-(valid_bins_remain_cap - np.mean(valid_bins_remain_cap)) / np.std(valid_bins_remain_cap + epsilon))))\n\n    # Combine exploitation scores: multiplicative approach balances both objectives.\n    combined_exploitation_scores = tight_fit_scores * (1.0 + utilization_bonus)\n\n    # Normalize exploitation scores to be between 0 and 1 for consistent weighting.\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Adaptive Exploration (Epsilon-Greedy Variant) ---\n    # With a probability `exploration_prob`, select a random fitting bin.\n    # Otherwise, select based on the exploitation scores.\n    exploration_mask = np.random.rand(len(valid_bins_remain_cap)) < exploration_prob\n    \n    # For exploration, assign a uniform high score to randomly chosen bins.\n    # For exploitation, use the calculated combined scores.\n    final_scores_unnormalized = np.zeros_like(normalized_exploitation_scores)\n    \n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    if num_fitting_bins > 0:\n        # Randomly select a subset of bins for exploration boost\n        num_explore_bins = max(1, int(np.floor(exploration_prob * num_fitting_bins))) # At least one bin if possible\n        explore_indices_in_valid = np.random.choice(num_fitting_bins, size=num_explore_bins, replace=False)\n        \n        # Assign a high exploration score to these randomly chosen bins\n        final_scores_unnormalized[explore_indices_in_valid] = 1.0\n        \n        # For bins not selected for explicit exploration boost, use their exploitation scores.\n        # Add a small constant to exploitation scores to ensure they are still considered.\n        exploitation_indices_in_valid = np.where(~np.isin(np.arange(num_fitting_bins), explore_indices_in_valid))[0]\n        final_scores_unnormalized[exploitation_indices_in_valid] = normalized_exploitation_scores[exploitation_indices_in_valid] * 0.8 # Slightly reduce exploitation to favor exploration boost if scores are close\n\n\n    # Ensure all priorities are non-negative.\n    final_scores_unnormalized[final_scores_unnormalized < 0] = 0\n\n    # Normalize the final scores to sum to 1 for the bins that can fit the item.\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    elif num_fitting_bins > 0:\n        # Fallback to uniform probability if all scores are zero or negative sum\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 5.893498205025941,
    "SLOC": 34.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response1.txt_stdout.txt",
    "code_path": "problem_iter14_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a weighted utilization bonus and adaptive exploration.\n    Prioritizes bins that minimize slack after packing, with a bonus for utilized bins.\n    Exploration strategy boosts less utilized, fitting bins to encourage diversity.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    utilization_weight = 0.3\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    fitting_indices = np.where(can_fit_mask)[0]\n\n    # 1. Tightest Fit: Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Utilization Bonus: Prefer bins that are already partially filled.\n    # Score is inverse of remaining capacity before packing.\n    utilization_score = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Normalize utilization score to be between 0 and 1 relative to fitting bins\n    max_utilization_score = np.max(utilization_score)\n    if max_utilization_score > epsilon:\n        normalized_utilization_score = utilization_score / max_utilization_score\n    else:\n        normalized_utilization_score = np.zeros_like(utilization_score)\n\n    # Combine Tightness and Utilization (Exploitation)\n    exploitation_score = tightness_score + utilization_weight * normalized_utilization_score\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(exploitation_score)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_score)\n\n    # 3. Adaptive Exploration: Boost scores of some less utilized fitting bins.\n    exploration_boost = np.zeros_like(normalized_exploitation_scores)\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    \n    # Identify bins that are \"less utilized\" among the fitting ones (higher utilization score)\n    # We want to boost bins that have been used more, but are still \"good\" fits.\n    # A simple way is to select from bins with utilization scores in the lower half (meaning more capacity used)\n    # but still providing a decent fit.\n    \n    # Let's sort by utilization score (descending) and pick some from the top.\n    # More simply, randomly pick a subset and boost them.\n    \n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins)))\n    explore_indices_local = np.random.choice(num_fitting_bins, size=min(num_to_explore, num_fitting_bins), replace=False)\n    \n    # Assign a modest boost to these selected bins.\n    # The boost should be enough to make them competitive but not necessarily dominate.\n    # Let's give them a score that is slightly above average of the normalized exploitation scores.\n    mean_exploitation_score = np.mean(normalized_exploitation_scores) if num_fitting_bins > 0 else 0\n    exploration_boost[explore_indices_local] = mean_exploitation_score * 1.2 \n\n    # Final Scores: Combine exploitation and exploration\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_boost\n\n    # Final Normalization to ensure the highest priority is 1.0\n    max_final_score = np.max(final_scores_unnormalized)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / max_final_score\n    else:\n        # Fallback if all scores are zero\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins if num_fitting_bins > 0 else 0.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 3.6398085360989234,
    "SLOC": 37.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response2.txt_stdout.txt",
    "code_path": "problem_iter14_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fitting preference with an adaptive exploration strategy\n    that balances exploitation and exploration more effectively.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.1  # Base exploration rate\n    alpha = 0.5    # Weight for utilization bonus\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Score: Tightness + Utilization Bonus ---\n\n    # Tightness: Prioritize bins with less remaining capacity after packing\n    tightness_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_scores[can_fit_mask] = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Utilization Bonus: Reward bins that are already more full (less remaining capacity)\n    utilization_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    utilization_scores[can_fit_mask] = 1.0 / (fitting_bins_remain_cap + 1e-9)\n\n    # Combine scores: weighted sum\n    combined_exploitation_score = tightness_scores + alpha * utilization_scores\n\n    # Normalize exploitation scores for fitting bins to [0, 1]\n    normalized_exploitation_scores = np.zeros_like(combined_exploitation_score, dtype=float)\n    if np.any(can_fit_mask):\n        fitting_scores = combined_exploitation_score[can_fit_mask]\n        max_score = np.max(fitting_scores)\n        if max_score > 0:\n            normalized_exploitation_scores[can_fit_mask] = fitting_scores / max_score\n\n    # --- Adaptive Exploration: Epsilon-Greedy with Targeted Exploration ---\n    # With probability epsilon, choose a random fitting bin.\n    # Otherwise, use the exploitation scores.\n    # We can also add a slight uniform boost to all fitting bins to encourage\n    # exploration without completely overriding good exploitation choices.\n\n    exploration_boost = epsilon / (np.sum(can_fit_mask) + 1e-9)\n    exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    exploration_scores[can_fit_mask] = exploration_boost\n\n    # Final priorities: blend normalized exploitation and exploration boost\n    priorities[can_fit_mask] = normalized_exploitation_scores[can_fit_mask] + exploration_scores[can_fit_mask]\n\n    # Ensure no negative priorities and re-normalize if necessary\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 29.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response3.txt_stdout.txt",
    "code_path": "problem_iter14_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with an adaptive fill bonus and a boosted exploration strategy.\n    Prioritizes bins that are both tight fits and moderately utilized, encouraging exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation: Combined Tightness and Fill Bonus ---\n    # Tightness: Score based on how much capacity is left after packing. Higher is better.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    # Inverse of remaining capacity for tightness, add epsilon to avoid division by zero\n    tightness_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # Fill Bonus: Score based on current bin utilization. Higher for more utilized bins.\n    # Using inverse of current remaining capacity as a proxy for utilization.\n    fill_bonus = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Combined exploitation score: Product of tightness and fill bonus.\n    # A bin that is tight *and* already somewhat full gets a higher score.\n    # Added 1.0 to fill_bonus to ensure tightness score isn't zeroed out by small fill_bonus.\n    combined_exploitation_score = tightness_score * (1.0 + 0.5 * fill_bonus) # Tunable weight 0.5\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_score)\n    if max_exploitation_score > 0:\n        normalized_exploitation_scores = combined_exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_score)\n\n    # --- Exploration: Boost promising but not top candidates ---\n    # This strategy aims to explore bins that are not the absolute best but are still good.\n    # It boosts the score of bins that are \"close\" to the top exploitation score.\n    exploration_weight = 0.2 # Base exploration weight\n    num_fitting_bins = np.sum(can_fit_mask)\n\n    # If there are many options, slightly increase exploration. If few, decrease.\n    if num_fitting_bins > 6:\n        exploration_weight *= 1.1\n    elif num_fitting_bins < 3:\n        exploration_weight *= 0.9\n\n    # Calculate a threshold for \"close to top\" (e.g., within 20% of the max score)\n    if max_exploitation_score > 0:\n        threshold = max_exploitation_score * 0.8\n        # Bins that are below the threshold but still good candidates for exploration\n        exploration_candidates_mask = (combined_exploitation_score >= threshold) & \\\n                                      (combined_exploitation_score < max_exploitation_score)\n        num_exploration_candidates = np.sum(exploration_candidates_mask)\n\n        if num_exploration_candidates > 0:\n            # Boost these candidates\n            boost_amount = exploration_weight / num_exploration_candidates\n            exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n            exploration_scores[can_fit_mask][exploration_candidates_mask] = boost_amount\n        else:\n            # If no bins fall in the \"close to top\" range, give a small uniform boost\n            # to all fitting bins as a fallback exploration.\n            exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n            exploration_scores[can_fit_mask] = exploration_weight / num_fitting_bins\n    else:\n        # If all exploitation scores are zero (e.g., all fitting bins have identical scores)\n        # provide a uniform exploration boost to all fitting bins.\n        exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n        exploration_scores[can_fit_mask] = exploration_weight / num_fitting_bins\n\n\n    # --- Final Priority: Sum of normalized exploitation and exploration scores ---\n    priorities[can_fit_mask] = normalized_exploitation_scores + exploration_scores\n\n    # Ensure no negative priorities and normalize the final output to [0, 1]\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 43.0,
    "cyclomatic_complexity": 8.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response4.txt_stdout.txt",
    "code_path": "problem_iter14_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a dynamically adjusted exploration bias\n    towards less utilized bins to balance exploitation and exploration.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.20  # Increased exploration for better diversity\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    valid_bin_indices = np.where(can_fit_mask)[0]\n    num_fitting_bins = len(valid_bins_remain_cap)\n\n    # --- Core Heuristic: Tightest Fit ---\n    # Prioritize bins that minimize remaining capacity after packing.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Adaptive Utilization Score ---\n    # Prefer bins that are less utilized (higher remaining capacity).\n    # This acts as a soft bias against overly full bins.\n    # Normalize by the max possible remaining capacity to get a relative fill level.\n    # High score means less utilized.\n    utilization_scores = valid_bins_remain_cap / np.max(bins_remain_cap + epsilon)\n\n    # --- Combined Exploitation Score ---\n    # Weighted sum: Tight fit is primary, utilization is secondary.\n    # Give more weight to tight fit as it directly minimizes waste.\n    combined_exploitation_scores = 0.7 * tight_fit_scores + 0.3 * utilization_scores\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Dynamic Exploration ---\n    # With a certain probability, introduce randomness.\n    # Bias exploration towards bins that are less utilized (lower utilization_scores).\n    random_exploration_scores = np.random.rand(num_fitting_bins)\n\n    # Scale random scores: Boost scores for less utilized bins (higher utilization_scores).\n    # The boost factor should be higher for bins with lower utilization_scores.\n    # We want to explore bins that are \"average\" or less utilized.\n    # Invert the utilization score for boosting: higher value means more under-utilized.\n    under_utilization_factor = 1.0 / (1.0 - utilization_scores + epsilon)\n    \n    # Combine random score with the under-utilization factor.\n    # Higher under_utilization_factor means higher boost for exploration.\n    scaled_random_scores = random_exploration_scores * under_utilization_factor\n\n    # Normalize these scaled random scores to [0, 1]\n    max_scaled_random_score = np.max(scaled_random_scores)\n    if max_scaled_random_score > epsilon:\n        normalized_exploration_scores = scaled_random_scores / max_scaled_random_score\n    else:\n        normalized_exploration_scores = np.zeros_like(scaled_random_scores)\n\n    # --- Final Priority Calculation ---\n    # Blend exploitation and exploration scores using the exploration probability.\n    alpha = exploration_prob\n    final_scores_unnormalized = (1 - alpha) * normalized_exploitation_scores + alpha * normalized_exploration_scores\n\n    # Normalize the final blended scores to ensure they are in a [0, 1] range.\n    max_final_score = np.max(final_scores_unnormalized)\n    if max_final_score > epsilon:\n        priorities[valid_bin_indices] = final_scores_unnormalized / max_final_score\n    else:\n        # Fallback if all scores are zero or near-zero\n        priorities[valid_bin_indices] = final_scores_unnormalized\n\n    # Ensure priorities are non-negative.\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 83.60590347028321,
    "SLOC": 36.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response5.txt_stdout.txt",
    "code_path": "problem_iter14_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and utilization bonus for exploitation,\n    with an epsilon-greedy strategy that slightly boosts all suitable bins for exploration.\n    \"\"\"\n    epsilon = 0.05  # Probability of triggering exploration\n    exploration_boost = 0.1 # Small boost for exploration phase\n    epsilon_for_division = 1e-9 # Small value to prevent division by zero\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploitation Component ---\n    fitting_bins_remain_cap = bins_remain_cap[suitable_bins_indices]\n\n    # 1. Tightest Fit Score: Lower remaining capacity after packing is better.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_scores = 1.0 / (remaining_after_packing + epsilon_for_division)\n\n    # 2. Utilization Score: Higher initial remaining capacity is worse (less utilized).\n    # We want to penalize bins with very large remaining capacities.\n    # So, higher score for smaller initial remaining capacity.\n    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon_for_division)\n\n    # Combine exploitation scores with weights\n    # Giving slightly more weight to tightness as it's a primary BPP goal.\n    combined_exploitation_scores = 0.6 * tightness_scores + 0.4 * utilization_scores\n\n    # Normalize exploitation scores to a [0, 1] range based on their current values\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon_for_division:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Exploration Component ---\n    if np.random.rand() < epsilon:\n        # Exploration phase: Boost all suitable bins\n        # Add a small, constant boost to all fitting bins' normalized scores\n        exploration_scores = normalized_exploitation_scores + exploration_boost\n        # Ensure scores don't exceed a reasonable upper bound if boost is too large\n        exploration_scores = np.clip(exploration_scores, 0, 1.0 + exploration_boost)\n        \n        # Use these exploration scores as the final priorities\n        final_priorities_unnormalized = exploration_scores\n    else:\n        # Exploitation phase: Use the combined exploitation scores\n        final_priorities_unnormalized = normalized_exploitation_scores\n\n    # --- Final Priority Calculation ---\n    # Assign priorities to the original bins\n    priorities[suitable_bins_indices] = final_priorities_unnormalized\n\n    # Normalize priorities so they sum to 1, ensuring a valid probability distribution\n    sum_priorities = np.sum(priorities)\n    if sum_priorities > epsilon_for_division:\n        priorities /= sum_priorities\n    else:\n        # If all scores somehow end up zero, distribute probability equally among suitable bins\n        if suitable_bins_indices.size > 0:\n            priorities[suitable_bins_indices] = 1.0 / suitable_bins_indices.size\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 33.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response6.txt_stdout.txt",
    "code_path": "problem_iter14_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a weighted utilization bonus and scaled exploration.\n    Prioritizes bins that minimize waste and are more utilized, with exploration\n    favoring less utilized bins based on a dynamically adjusted scaling.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Probability for exploration component\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    valid_bin_indices = np.where(can_fit_mask)[0]\n    num_fitting_bins = len(valid_bins_remain_cap)\n\n    # --- Core Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Score is inverse of remaining space after packing.\n    # Higher score for less remaining capacity.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Utilization Bonus: Score based on how \"full\" the bin is (lower remaining capacity).\n    # We use a weighted bonus to control its influence.\n    utilization_bonus_weight = 0.5  # Weight for the utilization bonus\n    # Score is higher for bins with less remaining capacity.\n    utilization_scores = 1.0 / (valid_bins_remain_cap + epsilon)\n    \n    # Combine exploitation scores: weighted sum of tightness and utilization.\n    # This approach allows balancing the preference for tight fits with preference for utilized bins.\n    combined_exploitation_scores = tight_fit_scores + utilization_bonus_weight * utilization_scores\n\n    # Normalize exploitation scores to be between 0 and 1 for consistent mixing.\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Dynamic Epsilon-Greedy Exploration ---\n    # Generate random scores for exploration, but scale them to favor less utilized bins.\n    random_exploration_scores = np.random.rand(num_fitting_bins)\n    \n    # Scale random scores: boost scores for less utilized bins (low utilization_scores).\n    # A higher boost_factor makes exploration more likely for less utilized bins.\n    boost_factor = 1.5  # Control how much less utilized bins are favored in exploration\n    # The term `(1.0 - utilization_scores)` increases as utilization_scores decreases (i.e., bin is emptier).\n    scaled_random_scores = random_exploration_scores * (1.0 + boost_factor * (1.0 - utilization_scores))\n    \n    # Normalize exploration scores to be in a comparable range [0, 1].\n    max_scaled_random_score = np.max(scaled_random_scores)\n    if max_scaled_random_score > epsilon:\n        normalized_exploration_scores = scaled_random_scores / max_scaled_random_score\n    else:\n        normalized_exploration_scores = np.zeros_like(scaled_random_scores)\n\n    # Blend the deterministic (exploitation) scores with the exploration scores.\n    # The `exploration_prob` determines the weight for exploration.\n    alpha = exploration_prob\n    final_scores = (1 - alpha) * normalized_exploitation_scores + alpha * normalized_exploration_scores\n\n    # Ensure all priorities are non-negative and re-normalize the final scores.\n    final_scores[final_scores < 0] = 0\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        # Fallback to uniform probability if all scores are zero or near-zero.\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins if num_fitting_bins > 0 else 0\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.028719585161557,
    "SLOC": 37.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response7.txt_stdout.txt",
    "code_path": "problem_iter14_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightness) with a utilization bonus and adaptive exploration.\n    Prioritizes bins that are tightly fit, somewhat utilized, and less frequently chosen.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.1  # Exploration probability\n\n    can_fit_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # --- Exploitation: Combine Tightness and Utilization ---\n    fitting_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Tightness score: Lower remaining capacity after packing is better.\n    # We want to maximize this score, so we invert the remaining capacity.\n    remaining_after_packing = fitting_bins_capacities - item\n    \n    # Add a small epsilon to avoid division by zero if a bin is perfectly filled\n    # and to give a slight preference to bins that are not perfectly filled.\n    tightness_score = 1.0 / (remaining_after_packing + 1e-6)\n\n    # Utilization bonus: Reward bins that are already partially filled.\n    # A higher current fill level (lower remaining capacity) gets a higher bonus.\n    # We invert the current remaining capacity.\n    utilization_bonus = 1.0 / (fitting_bins_capacities + 1e-6)\n\n    # Combine scores: Weighted sum of tightness and utilization.\n    # We normalize the components first to ensure they are on a similar scale.\n    \n    # Normalize tightness: Higher score is better, scale to [0, 1]\n    if np.max(tightness_score) > 0:\n        normalized_tightness = tightness_score / np.max(tightness_score)\n    else:\n        normalized_tightness = np.zeros_like(tightness_score)\n\n    # Normalize utilization bonus: Higher score is better, scale to [0, 1]\n    if np.max(utilization_bonus) > 0:\n        normalized_utilization = utilization_bonus / np.max(utilization_bonus)\n    else:\n        normalized_utilization = np.zeros_like(utilization_bonus)\n        \n    # Combine using weights. Let's give slightly more weight to tightness.\n    exploitation_scores = 0.6 * normalized_tightness + 0.4 * normalized_utilization\n\n    # --- Exploration: Epsilon-Greedy ---\n    exploration_scores = np.zeros_like(suitable_bins_indices, dtype=float)\n    \n    if np.random.rand() < epsilon:\n        # Choose a random suitable bin\n        chosen_bin_idx_in_suitable = np.random.choice(len(suitable_bins_indices))\n        exploration_scores[chosen_bin_idx_in_suitable] = 1.0\n    else:\n        # If not exploring randomly, assign priorities based on exploitation scores\n        exploration_scores = exploitation_scores\n        \n    # Combine exploration and exploitation: add a small uniform exploration boost\n    # to all fitting bins to ensure they are considered.\n    # A simple way is to add a constant value to all, then normalize.\n    # Alternative: let exploration scores override exploitation if random choice is made.\n    \n    # Let's use a blended approach:\n    # Base priorities are exploitation scores. If random choice made, override with uniform high priority.\n    if np.random.rand() < epsilon:\n        final_scores = np.zeros_like(suitable_bins_indices, dtype=float)\n        # Give a uniform high score to a randomly chosen bin\n        random_choice_idx = np.random.randint(len(suitable_bins_indices))\n        final_scores[random_choice_idx] = 1.0\n    else:\n        # Otherwise, use the exploitation scores\n        final_scores = exploitation_scores\n\n    # Assign these final scores to the overall priorities array\n    priorities[suitable_bins_indices] = final_scores\n\n    # Normalize the final priorities to be in [0, 1] range\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities /= max_priority\n        \n    # Ensure no bin that can't fit gets a priority\n    priorities[~can_fit_mask] = 0.0\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.2580773833266905,
    "SLOC": 38.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response8.txt_stdout.txt",
    "code_path": "problem_iter14_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit preference with adaptive fill-level bonus and\n    a principled exploration strategy, balancing exploitation and discovery.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exploitation Component: Tightness + Fill Bonus ---\n\n    # Tightness: Favors bins that leave minimal slack after packing.\n    # Higher score for smaller remaining capacity.\n    tightness_score = 1.0 / (fitting_bins_remain_cap - item + epsilon)\n\n    # Fill Level Bonus: Rewards bins that are already more utilized.\n    # Calculated as 1 / (current remaining capacity). Higher score for less remaining capacity.\n    fill_bonus = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Combined exploitation score: Multiplies tightness by a factor related to fill bonus.\n    # This emphasizes bins that are both tight and already somewhat full.\n    # The (1 + 0.3 * fill_bonus) acts as a modulating factor.\n    combined_exploitation_score = tightness_score * (1.0 + 0.3 * fill_bonus)\n\n    # Normalize exploitation scores to [0, 1] for consistent contribution\n    max_exploitation_score = np.max(combined_exploitation_score)\n    normalized_exploitation_scores = (combined_exploitation_score / max_exploitation_score) if max_exploitation_score > 0 else np.zeros_like(combined_exploitation_score)\n\n    # --- Exploration Component: Adaptive Epsilon-Greedy ---\n    # We use an epsilon-greedy approach, where epsilon is dynamically adjusted.\n    # A higher epsilon encourages more exploration, especially when there are many fitting bins.\n\n    num_fitting_bins = np.sum(can_fit_mask)\n    base_exploration_prob = 0.15\n\n    # Increase exploration if many options exist, decrease if few, to balance search\n    if num_fitting_bins > 5:\n        exploration_prob = min(0.3, base_exploration_prob * (1 + (num_fitting_bins - 5) * 0.05)) # Cap exploration\n    elif num_fitting_bins < 3:\n        exploration_prob = max(0.05, base_exploration_prob * (1 - (3 - num_fitting_bins) * 0.1)) # Min exploration\n    else:\n        exploration_prob = base_exploration_prob\n\n    # Apply exploration: With probability exploration_prob, pick a random fitting bin.\n    # This is achieved by adding a small, uniform exploration bonus to all fitting bins,\n    # and then during selection, we can use this probability to decide if we deviate.\n    # For simplicity in priority calculation, we add a small fixed exploration score\n    # scaled by the probability.\n    exploration_score_component = exploration_prob / num_fitting_bins\n\n    # --- Final Priority Calculation ---\n    # Blend normalized exploitation scores with the exploration component.\n    # Exploration gives a baseline chance to any fitting bin.\n    priorities[can_fit_mask] = normalized_exploitation_scores + exploration_score_component\n\n    # Ensure priorities are non-negative and normalize to [0, 1]\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter14_response9.txt_stdout.txt",
    "code_path": "problem_iter14_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a normalized utilization bonus and adaptive epsilon-greedy exploration.\n    Favors bins that minimize slack, with a bonus for partially filled bins, and explores\n    less optimal bins probabilistically.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Component 1: Tightest Fit Score ---\n    # Prioritize bins leaving minimal slack. Score is inverse of slack.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Component 2: Normalized Utilization Bonus ---\n    # Prefer bins that are already partially filled (less remaining capacity before packing).\n    # Score is inverse of current remaining capacity, normalized by the maximum.\n    utilization_score = 1.0 / (fitting_bins_remain_cap + epsilon)\n    max_utilization_score = np.max(utilization_score)\n    normalized_utilization_bonus = (utilization_score / (max_utilization_score + epsilon)) * 0.5 # Apply a 0.5 weight\n\n    # --- Combined Exploitation Score ---\n    # Weighted sum of tightness and normalized utilization bonus.\n    combined_exploitation_scores = tightness_score + normalized_utilization_bonus\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Component 3: Adaptive Exploration (Epsilon-Greedy) ---\n    # Introduce a small probability to explore less optimal bins.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    exploration_boost = np.zeros_like(normalized_exploitation_scores)\n\n    if num_fitting_bins > 0:\n        num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins))) # Ensure at least one bin is explored if possible\n        explore_indices = np.random.choice(num_fitting_bins, size=min(num_to_explore, num_fitting_bins), replace=False)\n        \n        # Assign a moderate boost to explored bins, ensuring they have a chance but don't dominate\n        exploration_boost[explore_indices] = 0.3 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation scores with exploration boost.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_boost\n    \n    # Normalize final scores so that the highest priority is 1.0.\n    max_final_score = np.max(final_scores_unnormalized)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / max_final_score\n    else:\n        # Fallback to uniform probability if all scores are zero.\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 32.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  }
]