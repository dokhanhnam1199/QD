{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight fitting and adaptive bin utilization with epsilon-greedy exploration.\n    Prioritizes bins that leave less space and bins that are more utilized,\n    with a chance to explore less optimal but fitting bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    exploration_prob = 0.1 # Probability for epsilon-greedy\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + epsilon) / (max_remaining_overall + epsilon)\n\n    # Combine core heuristic scores (e.g., balanced preference)\n    # Give equal weight to tight fitting and utilization for now.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: with probability, pick a random fitting bin\n    num_available_bins = available_bins_remain_cap.size\n    exploration_indices = np.random.choice(\n        np.arange(num_available_bins),\n        size=int(exploration_prob * num_available_bins),\n        replace=False\n    )\n    \n    final_scores = normalized_core_scores\n\n    # Assign a high priority to randomly selected bins for exploration\n    # We can boost their score significantly to ensure they are considered\n    boost_factor = 2.0 # Make exploration picks clearly stand out\n    final_scores[exploration_indices] *= boost_factor\n    \n    # Re-normalize after boosting to keep priorities in a reasonable range\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight-fitting preference with adaptive exploration using epsilon-greedy.\n    Prioritizes bins that leave minimal waste, with a chance to explore other bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit, return zero priorities\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    suitable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate waste (remaining capacity after packing)\n    waste = suitable_bins_remain_cap - item\n    \n    # Heuristic: Prioritize bins with minimal waste (tighter fit)\n    # Use inverse of waste + epsilon for higher scores for smaller waste\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (waste + epsilon)\n    \n    # Normalize scores to be between 0 and 1 for consistency\n    if np.max(tight_fit_scores) > epsilon:\n        normalized_tight_fit_scores = tight_fit_scores / np.max(tight_fit_scores)\n    else:\n        normalized_tight_fit_scores = np.ones_like(tight_fit_scores)\n        \n    # Adaptive Exploration (Epsilon-Greedy inspired):\n    # With a small probability (epsilon), choose a random fitting bin.\n    # Otherwise, choose the bin with the best tight-fit score.\n    epsilon = 0.1  # Exploration rate\n    \n    if np.random.rand() < epsilon:\n        # Explore: pick a random bin among those that can fit\n        random_choice_index_in_suitable = np.random.randint(0, len(suitable_bins_remain_cap))\n        \n        # Assign a high priority to the randomly chosen bin\n        priorities[can_fit_mask][random_choice_index_in_suitable] = 1.0\n    else:\n        # Exploit: pick the bin with the highest tight-fit score\n        priorities[can_fit_mask] = normalized_tight_fit_scores\n\n    # Ensure priorities are non-negative\n    priorities[priorities < 0] = 0\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Epsilon-Greedy Best Fit) with Heuristic 6 (Epsilon-Greedy Best Fit, different normalization/combination), Heuristic 1 is simpler and more direct in its epsilon-greedy implementation by choosing either a random bin or the best fit. Heuristic 6's approach of normalizing and then applying weights and epsilon feels more complex and potentially less robust due to the normalization scaling.\n\nComparing Heuristic 7/8 (Epsilon-Greedy Best Fit, simple score) with Heuristic 12/13 (Epsilon-Greedy Best Fit, boosted random choice), Heuristics 12/13's approach of boosting the random choice's priority might make exploration too dominant, potentially hindering exploitation of good fits. Heuristics 7/8's direct use of normalized tight-fit scores in exploitation is cleaner.\n\nComparing Heuristic 9/10/11 (Multi-objective with adaptive utilization and boosted exploration) with Heuristic 18/19 (Multi-objective with adaptive weights based on available bins), Heuristics 18/19 have a more grounded adaptive weighting strategy based on bin availability, which is a better heuristic for adaptivity than the more generic \"boosted exploration\" in 9/10/11. However, the core objectives in 9/10/11 (tight fit + utilization) might be more generally applicable than the specific balancing in 18/19.\n\nComparing Heuristic 1 (simple epsilon-greedy Best Fit) with Heuristic 2/3/4/5 (complex combination of tight fit and \"penalty for empty bins\"), the latter heuristics attempt to incorporate more nuanced ideas like penalizing slack. However, their implementation is convoluted and the interpretation of \"penalty for empty bins\" is not clearly defined or consistently applied, leading to less straightforward logic. Heuristic 1's simplicity and clear strategy make it more robust.\n\nComparing Heuristic 20 (Worst Fit + Tightness bonus) with Heuristic 18/19 (adaptive multi-objective), Heuristic 20's approach is interesting but the combination of Worst Fit and Tightness bonus, along with the \"less used\" bonus, makes it complex and potentially contradictory in its goals. The adaptive weighting in 18/19 is a more coherent approach to balancing objectives.\n\nOverall, simpler heuristics that clearly define and combine strategies (like Heuristic 1) tend to be better than overly complex ones with ambiguously defined components (like 2-5, 18-20).\n- \nHere's a redefined approach to self-reflection for better heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Performance, adaptability, interpretability, validation.\n*   **Advice:** Design heuristics with clear objectives and simple combination mechanisms. Incorporate adaptive exploration and consider smooth, non-linear scaling for fine-tuning. Prioritize vectorized operations for efficiency.\n*   **Avoid:** Ambiguity in objectives, overly complex components, and buggy implementations. Resist relying solely on linear or binary scaling without justification.\n*   **Explanation:** Focus on making heuristics understandable and maintainable. This clarity aids in identifying bottlenecks and opportunities for improvement through controlled experimentation and validation.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}