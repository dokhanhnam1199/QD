```python
import numpy as np
import random

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    A multi-objective priority function for online Bin Packing Problem (BPP).

    This heuristic combines multiple objectives using a weighted sum,
    with adaptive exploration informed by bin utilization and item size.
    It aims for a balance between "best fit" and "worst fit" to potentially
    reduce fragmentation and improve overall packing efficiency.

    Objectives considered:
    1. Tightest Fit (Minimizing slack): Prioritizes bins that leave minimal remaining capacity.
    2. Utilization (Maximizing current fill): Prefers bins that are already more utilized.
    3. Exploration (Adaptive diversity): Encourages trying less-used bins that can still fit the item.

    The function normalizes scores for each objective before combining them,
    making the weights more interpretable and the heuristic more robust to scale.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9  # Small value to avoid division by zero

    # Identify bins that can accommodate the item
    can_fit_mask = bins_remain_cap >= item
    fitting_bins_indices = np.where(can_fit_mask)[0]
    fitting_bins_remain_cap = bins_remain_cap[fitting_bins_indices]

    if not np.any(can_fit_mask):
        return priorities  # No bin can fit the item

    num_fitting_bins = len(fitting_bins_remain_cap)

    # --- Objective 1: Tightest Fit ---
    # Lower remaining capacity after packing is better.
    # We want to maximize `1 / (remaining_capacity + epsilon)`
    slack_after_packing = fitting_bins_remain_cap - item
    tightness_scores = 1.0 / (slack_after_packing + epsilon)

    # --- Objective 2: Utilization ---
    # Higher utilization (less remaining capacity before packing) is better.
    # We want to maximize `1 / (current_remaining_capacity + epsilon)`
    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon)

    # --- Normalize Scores for Each Objective ---
    # Normalize each objective's scores to be between 0 and 1.
    # This makes the weights more comparable across objectives.

    # Normalize Tightness Scores
    max_tightness = np.max(tightness_scores)
    normalized_tightness = tightness_scores / (max_tightness + epsilon)

    # Normalize Utilization Scores
    max_utilization = np.max(utilization_scores)
    normalized_utilization = utilization_scores / (max_utilization + epsilon)

    # --- Objective 3: Exploration (Adaptive Worst Fit Tendency) ---
    # To encourage exploration and potentially better global solutions,
    # we can slightly boost bins that are NOT the tightest fit, but still fit.
    # This can be seen as a "worst fit among fitting bins" component,
    # but applied adaptively.
    # The boost is proportional to how much *worse* the fit is compared to the best fit.
    # We can also consider a small probability to pick a random fitting bin.

    exploration_boost_strength = 0.15  # Controls how much exploration affects priorities
    exploration_boost = np.zeros(num_fitting_bins)

    # Calculate a "worst-fit-like" score among fitting bins.
    # This is `remaining_capacity - item` which is `slack_after_packing`.
    # Higher slack is worse fit. We want to boost bins with higher slack.
    # So, we normalize slack and add 1 to make higher values better for boosting.
    slack_values = slack_after_packing
    max_slack = np.max(slack_values)
    if max_slack > epsilon:
        normalized_slack = slack_values / max_slack
    else:
        normalized_slack = np.zeros_like(slack_values)

    # Apply boost: bins with more slack get a higher exploration score.
    # We scale this by the exploration_boost_strength.
    exploration_scores = exploration_boost_strength * normalized_slack

    # Combine the normalized objectives
    # Weights can be tuned. For now, let's give a good balance:
    weight_tightness = 0.4
    weight_utilization = 0.3
    weight_exploration = 0.3 # This weight is applied to the exploration boost

    # Combine the scores. Exploration is added as a boost.
    # We sum the weighted objectives and add the exploration boost.
    combined_scores = (
        weight_tightness * normalized_tightness +
        weight_utilization * normalized_utilization +
        weight_exploration * exploration_scores
    )
    
    # Final normalization of the combined scores to ensure the highest priority is 1.0
    max_combined_score = np.max(combined_scores)
    if max_combined_score > epsilon:
        priorities[fitting_bins_indices] = combined_scores / max_combined_score
    else:
        # Fallback: if all scores are zero or near zero, assign equal priority
        if num_fitting_bins > 0:
            priorities[fitting_bins_indices] = 1.0 / num_fitting_bins
        else:
            priorities[fitting_bins_indices] = 0.0

    return priorities
```
