[
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with an adaptive exploration strategy.\n    Prioritizes bins with minimal remaining capacity after packing,\n    while occasionally exploring less ideal bins to broaden search.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    if fitting_bins_capacities.size == 0:\n        return priorities\n\n    # Heuristic 1: Prioritize tight fits (minimize remaining capacity after packing)\n    # Higher score for bins with less space left after item is placed.\n    tight_fit_score = 1.0 / (fitting_bins_capacities - item + epsilon)\n\n    # Heuristic 10: Epsilon-greedy exploration\n    # With a small probability (epsilon), choose a random fitting bin.\n    # Otherwise, choose the bin with the highest priority (tightest fit).\n    epsilon_explore = 0.05  # Exploration rate\n    num_fitting_bins = fitting_bins_capacities.size\n\n    if np.random.rand() < epsilon_explore:\n        # Exploration: pick a random bin among those that can fit\n        random_index_in_fitting = np.random.randint(num_fitting_bins)\n        # Assign a high priority to this randomly chosen bin to ensure it's picked\n        exploration_priority = 1.0 \n        priorities[can_fit_mask][random_index_in_fitting] = exploration_priority\n    else:\n        # Exploitation: assign priorities based on tight fit score\n        priorities[can_fit_mask] = tight_fit_score\n\n    # Normalize priorities so the maximum priority is 1.0 for consistent scaling\n    max_priority = np.max(priorities)\n    if max_priority > epsilon:\n        priorities /= max_priority\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.068607897885915,
    "SLOC": 20.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines tight-fit preference with adaptive exploration (epsilon-greedy).\n\n    Prioritizes bins that minimize waste (tight fit) and introduces a\n    controlled probability of choosing a random valid bin to avoid local optima.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_capacities = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_capacities.size == 0:\n        return priorities\n\n    # Primary heuristic: Prioritize bins with tighter fits (inverse remaining capacity after packing)\n    # Add a small epsilon to avoid division by zero.\n    waste = valid_bins_capacities - item\n    tight_fit_scores = 1.0 / (waste + 1e-9)\n\n    # Normalize tight fit scores to be between 0 and 1\n    if tight_fit_scores.max() > 0:\n        normalized_tight_fit_scores = tight_fit_scores / tight_fit_scores.max()\n    else:\n        normalized_tight_fit_scores = np.zeros_like(tight_fit_scores)\n\n    priorities[valid_bins_mask] = normalized_tight_fit_scores\n\n    # Exploration: Epsilon-greedy strategy\n    # With probability epsilon, choose a random valid bin; otherwise, choose the best.\n    epsilon = 0.1  # Exploration rate\n    num_valid_bins = valid_bins_capacities.size\n\n    if num_valid_bins > 0 and np.random.rand() < epsilon:\n        # Select a random index among the valid bins\n        random_valid_index = np.random.randint(num_valid_bins)\n        \n        # Get the original index of this randomly chosen valid bin\n        original_indices_of_valid_bins = np.where(valid_bins_mask)[0]\n        random_original_index = original_indices_of_valid_bins[random_valid_index]\n\n        # Give this randomly chosen bin a high priority to ensure it's selected\n        # This is a strong boost to make it competitive with the best fit.\n        priorities[random_original_index] = np.max(priorities) + 1.0 \n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.1284403669724865,
    "SLOC": 21.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and an epsilon-greedy exploration strategy.\n    Favors bins with minimal remaining capacity after packing,\n    with a chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 0.1\n    epsilon_value = 1e-9\n\n    fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate tightest fit priorities for suitable bins\n    valid_capacities = bins_remain_cap[fit_mask] - item + epsilon_value\n    tight_fit_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    tight_fit_priorities[fit_mask] = 1.0 / valid_capacities\n\n    # Normalize tight fit priorities\n    sum_tight_fit = np.sum(tight_fit_priorities)\n    if sum_tight_fit > 0:\n        normalized_tight_fit = tight_fit_priorities / sum_tight_fit\n    else:\n        # If no bins can fit, all get equal chance (effectively new bin)\n        normalized_tight_fit = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Epsilon-greedy: explore randomly with probability epsilon\n    # Create exploration priorities: equal probability for all fitting bins\n    exploration_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    num_fitting_bins = np.sum(fit_mask)\n    if num_fitting_bins > 0:\n        exploration_priorities[fit_mask] = 1.0 / num_fitting_bins\n    else:\n        # If no bins fit, exploration is on all bins (preparing for a new bin)\n        exploration_priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Combine exploitation (tight fit) and exploration (random)\n    combined_priorities = (1 - epsilon) * normalized_tight_fit + epsilon * exploration_priorities\n\n    # Ensure probabilities sum to 1, handling cases where no bins fit initially\n    if np.sum(combined_priorities) == 0:\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    \n    final_priorities = combined_priorities / np.sum(combined_priorities)\n\n    return final_priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with an epsilon-greedy exploration strategy.\n    Prioritizes bins with minimal remaining capacity after packing,\n    with a small chance to select a random fitting bin for exploration.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    if available_bins_cap.size == 0:\n        return priorities\n    \n    # Calculate tightest fit priority (higher for less remaining capacity)\n    waste = available_bins_cap - item\n    tight_fit_scores = 1.0 / (waste + 1e-9)\n    \n    # Apply exploration: with probability epsilon, choose a random fitting bin\n    if np.random.rand() < epsilon:\n        random_indices_in_subset = np.random.choice(len(available_bins_cap), size=1)\n        # Set priority for the randomly chosen bin to be the maximum possible tight fit score\n        # This effectively makes it the most preferred bin during exploration.\n        priorities[can_fit_mask][random_indices_in_subset] = np.max(tight_fit_scores) + 1.0 \n    else:\n        # Otherwise, use the tightest fit scores\n        priorities[can_fit_mask] = tight_fit_scores\n        \n    # Normalize priorities to be between 0 and 1\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities /= max_priority\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.098524132429212,
    "SLOC": 18.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with adaptive exploration using epsilon-greedy.\n    Prioritizes bins that leave minimal waste, with a chance to explore other bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit, return zero priorities\n    if not np.any(can_fit_mask):\n        return priorities\n    \n    suitable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate waste (remaining capacity after packing)\n    waste = suitable_bins_remain_cap - item\n    \n    # Heuristic: Prioritize bins with minimal waste (tighter fit)\n    # Use inverse of waste + epsilon for higher scores for smaller waste\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (waste + epsilon)\n    \n    # Normalize scores to be between 0 and 1 for consistency\n    if np.max(tight_fit_scores) > epsilon:\n        normalized_tight_fit_scores = tight_fit_scores / np.max(tight_fit_scores)\n    else:\n        normalized_tight_fit_scores = np.ones_like(tight_fit_scores)\n        \n    # Adaptive Exploration (Epsilon-Greedy inspired):\n    # With a small probability (epsilon), choose a random fitting bin.\n    # Otherwise, choose the bin with the best tight-fit score.\n    epsilon = 0.1  # Exploration rate\n    \n    if np.random.rand() < epsilon:\n        # Explore: pick a random bin among those that can fit\n        random_choice_index_in_suitable = np.random.randint(0, len(suitable_bins_remain_cap))\n        \n        # Assign a high priority to the randomly chosen bin\n        priorities[can_fit_mask][random_choice_index_in_suitable] = 1.0\n    else:\n        # Exploit: pick the bin with the highest tight-fit score\n        priorities[can_fit_mask] = normalized_tight_fit_scores\n\n    # Ensure priorities are non-negative\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.098524132429212,
    "SLOC": 21.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response5.txt_stdout.txt",
    "code_path": "problem_iter5_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fitting and adaptive bin utilization with epsilon-greedy exploration.\n    Prioritizes bins that leave less space and bins that are more utilized,\n    with a chance to explore less optimal but fitting bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    exploration_prob = 0.1 # Probability for epsilon-greedy\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + epsilon) / (max_remaining_overall + epsilon)\n\n    # Combine core heuristic scores (e.g., balanced preference)\n    # Give equal weight to tight fitting and utilization for now.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: with probability, pick a random fitting bin\n    num_available_bins = available_bins_remain_cap.size\n    exploration_indices = np.random.choice(\n        np.arange(num_available_bins),\n        size=int(exploration_prob * num_available_bins),\n        replace=False\n    )\n    \n    final_scores = normalized_core_scores\n\n    # Assign a high priority to randomly selected bins for exploration\n    # We can boost their score significantly to ensure they are considered\n    boost_factor = 2.0 # Make exploration picks clearly stand out\n    final_scores[exploration_indices] *= boost_factor\n    \n    # Re-normalize after boosting to keep priorities in a reasonable range\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.078579976067022,
    "SLOC": 33.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response6.txt_stdout.txt",
    "code_path": "problem_iter5_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest fit) with an epsilon-greedy exploration strategy.\n    Prioritizes bins that leave the smallest remaining capacity after packing,\n    with a small chance of picking any suitable bin to encourage exploration.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Exploration phase: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Find the index within the 'suitable_bins_indices' array that has the minimum gap\n        best_fit_in_suitable_idx = np.argmin(gaps)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 6,
    "tryHS": true,
    "obj": 4.038691663342641,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response7.txt_stdout.txt",
    "code_path": "problem_iter5_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines tightest fit with an epsilon-greedy exploration strategy.\n\n    Prioritizes bins with minimal remaining capacity after packing,\n    while occasionally exploring other bins to avoid local optima.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.1  # Exploration rate\n    epsilon_boost = 1.5 # Boost factor for exploration\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Filter to only consider bins that can fit the item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Heuristic: Prioritize tightest fit (minimum waste)\n    # Calculate remaining capacity after packing for available bins\n    remaining_after_packing = available_bins_remain_cap - item\n\n    # Higher priority for bins with less remaining capacity (tighter fit)\n    # Add a small epsilon to avoid division by zero\n    tight_fit_score = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Normalize the tight fit scores to a 0-1 range\n    if np.max(tight_fit_score) > 0:\n        normalized_tight_fit_score = tight_fit_score / np.max(tight_fit_score)\n    else:\n        normalized_tight_fit_score = np.zeros_like(tight_fit_score)\n\n    # Epsilon-greedy exploration:\n    # With probability epsilon, select a random available bin.\n    # Otherwise, select the bin with the best (highest) tight fit score.\n    num_available_bins = available_bins_remain_cap.size\n    if num_available_bins > 0 and np.random.rand() < epsilon:\n        # Exploration: Randomly select an available bin and boost its priority\n        random_indices_in_available = np.random.choice(num_available_bins)\n        # Apply a boost to the chosen bin's priority\n        normalized_tight_fit_score[random_indices_in_available] *= epsilon_boost\n\n    # Place the calculated priorities back into the original priorities array\n    priorities[can_fit_mask] = normalized_tight_fit_score\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response8.txt_stdout.txt",
    "code_path": "problem_iter5_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for empty bins,\n    favoring fuller bins and efficient space utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Calculate penalty for empty bins. This encourages using existing bins.\n    # An empty bin is one whose remaining capacity is equal to the maximum possible bin capacity.\n    # We need to find the actual maximum capacity across all bins if they are not uniform.\n    # However, for simplicity and common BPP scenarios, we assume a standard bin capacity.\n    # A robust approach would require bin capacity as an input or infer it.\n    # For this example, let's assume `bins_remain_cap` reflects current state, and a bin with\n    # capacity equal to its initial maximum (if known) or a very large remaining capacity\n    # implies it's effectively \"empty\" in terms of prior usage.\n    # A simpler proxy for \"empty\" is a very large remaining capacity compared to the item.\n    # Let's use a large constant penalty for bins that are significantly underutilized relative to their max potential.\n    # A more direct approach is to penalize bins that are still \"full\" (i.e., have not been used yet).\n    # Let's assume an \"empty\" bin has a remaining capacity equal to the max seen in bins_remain_cap.\n    \n    # This logic aims to combine the \"tight fit\" of Heuristic 4 with the \"penalty for empty bins\" of Heuristic 17.\n    # We prioritize bins that fit snugly, but if there's a tie or similar fit, we prefer bins that have already been used.\n    \n    # Penalty for bins that appear \"empty\" (e.g., have not been utilized).\n    # A bin is considered \"empty\" if its remaining capacity is close to the maximum capacity.\n    # Let's assume the maximum capacity of a bin is the largest value found in bins_remain_cap\n    # for bins that can fit the item, or a known global bin capacity if provided.\n    # For this implementation, we'll use a penalty for bins that are *not* significantly filled.\n    # A simple proxy for \"not significantly filled\" is to consider bins that are still \"very full\".\n    \n    # Using the penalty from Heuristic 17: a large constant for empty bins.\n    # We need to identify which of the `can_fit_mask` bins are also \"empty\".\n    # A common definition of \"empty\" in BPP heuristics is a bin that has not yet received any items.\n    # If we don't have explicit information about which bins are truly empty vs. just having high remaining capacity,\n    # we can approximate it by looking for bins with remaining capacity that is very close to the maximum capacity found.\n    \n    # For simplicity, let's redefine \"empty\" as a bin that has *just* enough capacity for the item,\n    # and we want to penalize these if better options exist.\n    # Alternatively, we can directly apply a penalty to bins that are initially \"full\".\n    # Let's go with a direct penalty for bins that are very close to being full (i.e., their remaining capacity is very large).\n    \n    # Instead of a fixed large penalty, let's make it relative.\n    # We want to discourage packing into a \"new\" bin if an \"existing\" bin offers a similar tightness.\n    # A heuristic that combines \"tight fit\" and a \"penalty for empty bins\" can be achieved by:\n    # 1. Calculating tightness scores for all fitting bins.\n    # 2. Identifying \"empty\" bins among the fitting ones (e.g., those with capacity close to the maximum).\n    # 3. Reducing the priority of these \"empty\" bins by a certain factor or offset.\n    \n    # Let's try a simpler approach that is common: penalize bins that have a lot of remaining space.\n    # This is implicitly handled by the inverse of remaining capacity.\n    # The \"penalty for empty bins\" is often to ensure that if a bin has lots of space, we only use it\n    # if other bins are not good fits.\n    \n    # Let's combine Heuristic 4 (tight fit) with a penalty for bins that have a lot of leftover space\n    # after packing the item. This is similar to the `oversize_penalty` idea but perhaps simpler.\n    \n    # The 'tightness_score' already rewards bins with less remaining capacity.\n    # The 'penalty for empty bins' is about preferring to fill existing bins.\n    # If we consider bins with remaining_after_packing == 0 to be ideal, and larger values to be less ideal,\n    # then the inverse already handles this.\n    \n    # Let's integrate the \"penalty for empty bins\" by reducing the score of bins that have\n    # a large `remaining_after_packing`. This is contrary to the tight-fit idea.\n    \n    # A better combination: prioritize tight fits. For bins with similar tightness,\n    # prefer those that are already partially filled.\n    \n    # Let's stick to the core idea of Heuristic 4 (tightest fit) and enhance it by\n    # penalizing bins that are \"too large\", similar to the `oversize_penalty` in the `priority_v0` example.\n    # This would mean prioritizing bins where `remaining_after_packing` is small.\n    \n    # Combining tight fit (inverse of remaining space) with a penalty for \"excessive\" remaining space.\n    \n    # Let's use the tightness score and add a penalty for bins that have a lot of slack.\n    # If a bin has `R` remaining capacity, and item `I` is packed, the new remaining is `R-I`.\n    # We want to minimize `R-I`.\n    # The `tightness_score` does this: `1 / (R-I + epsilon)`.\n    \n    # For the \"penalty for empty bins\": This is meant to encourage using partially filled bins before new ones.\n    # If we consider bins with `bins_remain_cap` close to some maximum capacity as \"empty\",\n    # we should reduce their priority.\n    \n    # Let's define a threshold for \"empty\". A bin might be considered \"empty\" if its remaining capacity\n    # is, say, more than 75% of the maximum capacity encountered.\n    \n    # Let's consider the combined logic of tight-fit and a penalty for bins that are\n    # unnecessarily large for the item.\n    \n    # Recalculate tightness: higher for smaller remaining capacity after packing\n    # This is good.\n    \n    # Consider a penalty for bins that are \"overly large\" for the item.\n    # If `bins_remain_cap[i]` is much larger than `item`, we might want to penalize it.\n    # For instance, if `bins_remain_cap[i] > 2 * item`, apply a penalty.\n    \n    oversize_penalty_factor = 2.0 # Penalize if bin capacity is more than twice the item size\n    penalty_value = 0.5 # Reduce priority by this factor\n    \n    oversize_penalties = np.ones_like(bins_remain_cap, dtype=float)\n    oversize_mask = (bins_remain_cap > oversize_penalty_factor * item) & can_fit_mask\n    oversize_penalties[oversize_mask] = penalty_value\n    \n    # Combine tightness score with oversize penalty\n    # We want to amplify tightness, and reduce for oversized bins.\n    # A multiplicative approach seems suitable.\n    \n    combined_score = tightness_score * oversize_penalties\n    \n    # Normalize priorities to avoid extremely large values and ensure a fair comparison.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    # Now, let's add the \"penalty for empty bins\".\n    # This means if a bin is \"empty\" and also fits the item, reduce its priority.\n    # Let's define \"empty\" as having remaining capacity equal to the *initial* capacity of that bin.\n    # Without initial capacities, we can use a proxy: bins with remaining capacity above a certain threshold.\n    # For instance, if remaining capacity is > 90% of the max capacity observed among fitting bins.\n    \n    # Let's use a simpler interpretation: \"empty\" bins are those that are initially full.\n    # If we have no information about initial fill, we can use a proxy: bins that are currently\n    # much larger than the item. This is already somewhat handled by `oversize_penalties`.\n    \n    # A more direct interpretation of \"penalty for empty bins\" from Heuristic 17:\n    # Apply a significant reduction to bins that are still \"full\" (i.e., have high remaining capacity).\n    # Let's consider bins with `bins_remain_cap >= item` as candidates.\n    # Among these, identify those that are \"empty\".\n    # A simple approach to identify \"empty\" bins is to look at bins whose remaining capacity\n    # is very close to a common maximum bin capacity. If we don't know the max capacity,\n    # we can use the maximum value in `bins_remain_cap` as a reference.\n    \n    # Let's implement a penalty that reduces the priority of bins that have\n    # a large amount of slack *after* the item is placed. This is related to\n    # minimizing waste.\n    \n    # Consider the \"waste\" as `bins_remain_cap - item` for fitting bins.\n    # We want to minimize waste. The `tightness_score` already does this.\n    \n    # Let's focus on the explicit \"penalty for empty bins\".\n    # A common interpretation is that if we have partially filled bins, we should\n    # prefer them over completely new (empty) bins.\n    # If `bins_remain_cap` represents the state, and we don't know initial states,\n    # we can approximate \"empty\" bins as those with very large `bins_remain_cap`.\n    \n    # Let's combine Heuristic 4 (tight fit) with Heuristic 17 (penalty for empty bins).\n    # Heuristic 4: `1.0 / (bins_remain_cap - item + epsilon)`\n    # Heuristic 17: Penalize \"empty\" bins.\n    \n    # Let's define \"empty\" as `bins_remain_cap` being very close to some `MAX_CAPACITY`.\n    # Without `MAX_CAPACITY`, we can use a relative measure: is `bins_remain_cap`\n    # very large compared to `item`? The `oversize_penalties` handle this.\n    \n    # Let's refine the \"penalty for empty bins\" by considering bins that have\n    # a large amount of *available* space for *future* items, relative to the current item.\n    # This is about balancing immediate fit with future potential.\n    \n    # Combining \"tight fit\" (Heuristic 4) and \"prefer partially filled bins\"\n    # (a common strategy for \"penalty for empty bins\").\n    \n    # Let's define a score based on how \"full\" the bin is *before* packing.\n    # `fill_ratio = (BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. We'd need BIN_CAPACITY.\n    # A proxy: `1 - (bins_remain_cap / MAX_REMAINING_CAP)`.\n    \n    # Alternative: Prioritize bins with small `bins_remain_cap` (tight fit) first.\n    # Then, among those with similar tightness, prefer those that are *not* \"empty\".\n    # \"Empty\" can mean `bins_remain_cap` is very large.\n    \n    # Let's go with the most direct combination of Heuristic 4 and the concept of\n    # \"penalty for empty bins\" interpreted as penalizing bins with *large* remaining capacity.\n    \n    # 1. Calculate tightness score (inverse of slack).\n    # 2. Apply a penalty to bins with large slack *after* packing.\n    \n    # The `tightness_score` already penalizes bins with large slack.\n    # `tightness_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)`\n    \n    # Now, add a penalty for bins that have a large *initial* remaining capacity.\n    # Let's assume an \"empty\" bin is one with `bins_remain_cap` significantly larger than `item`.\n    # This is similar to `oversize_penalty` but applied differently.\n    \n    # Let's use a penalty on the `tightness_score` itself.\n    # If `bins_remain_cap[i]` is large, we want to reduce its priority.\n    # A simple penalty: `1 / (1 + k * bins_remain_cap[i])`.\n    \n    penalty_factor_slack = 0.1 # Adjust this to control the penalty for slack\n    slack_penalty = np.ones_like(bins_remain_cap, dtype=float)\n    \n    # Apply penalty to bins that can fit, based on their *initial* remaining capacity.\n    # A larger initial remaining capacity gets a higher penalty (lower score).\n    slack_penalty[can_fit_mask] = 1.0 / (1.0 + penalty_factor_slack * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness score and slack penalty.\n    # We want high tightness, but also penalize large initial slack.\n    # Multiplication is a good way to combine these:\n    # `priority = tightness_score * slack_penalty`\n    \n    combined_score = tightness_score * slack_penalty\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    # Ensure that bins that cannot fit have a priority of 0, which is already set.\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 25.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response9.txt_stdout.txt",
    "code_path": "problem_iter5_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight packing (inverse remaining capacity after item) with an\n    epsilon-greedy exploration strategy for selecting bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate tight fit score: higher for less remaining capacity after packing\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (available_bins_cap - item + epsilon)\n    \n    # Normalize tight fit scores to be between 0 and 1\n    max_tight_fit = np.max(tight_fit_scores)\n    if max_tight_fit > 0:\n        normalized_tight_fit = tight_fit_scores / max_tight_fit\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n\n    # Epsilon-greedy exploration: \n    # With probability epsilon, pick a random fitting bin.\n    # Otherwise, pick the bin with the highest tight_fit_score.\n    epsilon = 0.1\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Assign uniform high priority to all fitting bins\n        priorities[can_fit_mask] = 1.0\n    else:\n        # Exploitation: Use normalized tight fit scores\n        priorities[can_fit_mask] = normalized_tight_fit\n        \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.058635819704831,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a multi-objective approach with adaptive behavior.\n    Prioritizes bins based on a weighted sum of criteria:\n    1. Tightest fit (minimizing remaining capacity after packing).\n    2. Bin fullness (maximizing current bin utilization, which indirectly\n       encourages opening new bins for larger items).\n    3. A penalty for bins that are too full and might cause immediate\n       overflow for slightly larger items (encouraging better distribution).\n\n    The weights are adaptive, favoring tighter fits when bins are abundant\n    and fuller bins when capacity is scarce.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Criteria calculation for suitable bins\n    # 1. Tightest Fit (lower is better, so we invert for priority)\n    gaps = suitable_bins_capacities - item\n    tightness_score = 1.0 / (gaps + 1e-6) # Add small epsilon to avoid division by zero\n\n    # 2. Bin Fullness (higher is better)\n    # Assuming bin_remain_cap represents remaining capacity of TOTAL capacity.\n    # We need total capacity for fullness. For simplicity, let's assume a common max capacity.\n    # In a real scenario, this would be passed or inferred.\n    # Let's assume a default max capacity if not provided.\n    # For this example, let's derive a plausible max capacity from the data if possible,\n    # or use a large number as a proxy if only remaining capacities are available.\n    # A more robust approach would involve passing the bin's total capacity.\n    # For demonstration, we'll consider the average remaining capacity as a proxy indicator.\n    # A better proxy for fullness, given only remaining capacity, is how much *space* is left\n    # relative to what has been used. Since we don't know total capacity directly,\n    # we can indirectly infer fullness by how much capacity is REMAINING.\n    # A smaller remaining capacity implies a fuller bin. So, we'll use the inverse of remaining capacity.\n    # However, the prompt is about 'bin fullness', implying usage.\n    # If we only have remaining capacity, we can't directly calculate fullness.\n    # Let's re-interpret \"bin fullness\" in the context of remaining capacity:\n    # A bin is \"fuller\" if it has *less* remaining capacity.\n    # So, higher fullness is associated with *lower* remaining capacity.\n    # We want to prioritize bins with less remaining capacity (if they can fit the item).\n    # So, we want to maximize `1 / bins_remain_cap`.\n    # However, we are only considering `suitable_bins_capacities`, which are >= item.\n    # Let's consider a different interpretation: priority should be given to bins that are \"almost full\"\n    # in the sense that they have a good amount of remaining space, but not too much.\n    # A simple measure for \"good\" remaining space might be related to the item size itself.\n    # Let's consider how \"well\" the item fits into the remaining space.\n    # A bin that can accommodate the item with a moderate amount of remaining capacity might be good.\n    # Let's try to prioritize bins that have capacity CLOSE to the item size, but not necessarily the tightest.\n    # This could be represented by the absolute difference between remaining capacity and item size,\n    # but we want to favor bins with *more* remaining capacity to avoid fragmentation initially.\n    # Let's try maximizing remaining capacity among suitable bins, as a proxy for \"not too full\".\n    fullness_score = suitable_bins_capacities\n\n    # 3. Penalty for being too full (lower is better, so invert for priority)\n    # If remaining capacity is very small, it's good for tight fit but bad for future items.\n    # Let's penalize bins with very little remaining capacity *after* fitting the item.\n    # This is related to the gap, but we want to avoid bins that are *almost* full.\n    # A small gap is good (tight fit), but if the gap is extremely small (e.g., < item/4),\n    # it might be detrimental.\n    # Let's define a \"too full\" threshold. If remaining_capacity - item < some_fraction_of_item,\n    # apply a penalty.\n    too_full_penalty = np.zeros_like(suitable_bins_capacities)\n    # Consider bins where remaining capacity is less than 2 times the item size as potentially \"too full\"\n    # if they still have substantial space left. This is tricky.\n    # Let's simplify: penalize bins with very small remaining capacity BEFORE fitting the item.\n    # This would mean bins that are already quite full.\n    # If `bins_remain_cap` is small, it's \"full\". We want to avoid picking these if possible,\n    # UNLESS they are the tightest fit.\n    # Let's rethink: the goal is to minimize the number of bins.\n    # So, we want to utilize existing bins effectively.\n\n    # Let's consider a score that balances:\n    # 1. Maximizing the remaining capacity AFTER packing (to leave space for future items).\n    # 2. Minimizing the number of bins used (by packing tightly).\n\n    # Revised strategy:\n    # Focus on using existing bins efficiently.\n    # The \"best fit\" is often good. What if we also consider bins that have\n    # *just enough* space for the current item, but not too much excess?\n    # This could be measured by how close `bins_remain_cap` is to `item`.\n    # So, we want to maximize `bins_remain_cap` minus `item`, but only for suitable bins.\n    # Or, minimize `bins_remain_cap` among suitable bins. This is Best Fit.\n\n    # What if we add a term that favors bins that have a \"good amount\" of remaining space\n    # *after* packing? This would be `bins_remain_cap[i] - item`.\n    # We want to maximize this value.\n    # So, we have two conflicting goals for suitable bins:\n    # - Minimize `bins_remain_cap[i] - item` (tightest fit)\n    # - Maximize `bins_remain_cap[i] - item` (leave more space)\n\n    # Let's try a heuristic that favors bins that have a remaining capacity\n    # that is \"just enough\" for the item, or slightly more.\n    # This means we want to find a bin where `bins_remain_cap` is close to `item`.\n    # The gap `bins_remain_cap - item` should be small, but not necessarily zero.\n    # We want to maximize `-(gap)^2` or minimize `(gap)^2`. This is Best Fit.\n\n    # Let's consider a different approach inspired by \"First Fit Decreasing\" idea,\n    # but for online. We want to use bins that are \"most appropriate\".\n    # A bin is \"most appropriate\" if it can fit the item and also has\n    # significant remaining capacity to potentially fit other items.\n    # This suggests prioritizing bins that are not too full.\n    # So, among suitable bins, we want to maximize `bins_remain_cap`.\n\n    # Combining Best Fit (minimizing gap) and Maximize Remaining Capacity:\n    # Let's prioritize bins that have the smallest gap AND then, among those with the same gap,\n    # pick the one with the largest remaining capacity (which is the same bin).\n    # This doesn't add much.\n\n    # What if we prioritize bins that have *just enough* capacity?\n    # Consider the ratio: `item / bins_remain_cap[i]`. We want this ratio to be close to 1.\n    # Or, `bins_remain_cap[i] / item`. We want this ratio to be close to 1.\n    # Maximize `bins_remain_cap[i] / item` among suitable bins.\n\n    # Let's try a multi-objective weighted sum.\n    # Objective 1: Maximize (remaining_capacity - item) --> leave more space.\n    # Objective 2: Minimize (remaining_capacity - item) --> tightest fit.\n\n    # If we want to maximize space left, we pick the largest `bins_remain_cap` that fits.\n    # If we want to minimize waste, we pick the smallest `bins_remain_cap` that fits.\n\n    # Let's define a score that is a combination of these.\n    # A simple heuristic could be to prioritize bins that are \"moderately\" full.\n    # This means they have enough capacity for the item, but not an excessive amount.\n    # So, we are looking for `bins_remain_cap[i]` such that `item <= bins_remain_cap[i] < some_threshold`.\n    # And among these, maybe we want the smallest `bins_remain_cap[i]`? (Best Fit).\n\n    # Let's consider the \"slack\" or `bins_remain_cap[i] - item`.\n    # We want this slack to be small (Best Fit).\n    # However, we also want to avoid putting items into bins that are ALMOST full,\n    # because the next item might not fit.\n    # So, let's penalize bins where `bins_remain_cap[i]` is very close to `item`.\n\n    # Let's try a score that penalizes bins that are too full *after* packing.\n    # `bins_remain_cap[i] - item` should not be too small.\n    # So, let's maximize `bins_remain_cap[i] - item`. This is the opposite of Best Fit.\n\n    # How about we combine Best Fit with a \"Next Fit\" like behavior,\n    # favoring bins that are currently \"open\" and have sufficient space.\n    # The \"openness\" can be proxied by how much capacity is available.\n\n    # Let's try prioritizing bins that have a remaining capacity that is\n    # *just enough* for the item.\n    # This means `bins_remain_cap[i]` is close to `item`.\n    # We can measure this by `1 / abs(bins_remain_cap[i] - item + epsilon)`\n    # but this would prioritize exact fits.\n\n    # Let's use a weighted combination of Best Fit and a measure of \"good capacity\".\n    # Score = w1 * (BestFitScore) + w2 * (GoodCapacityScore)\n\n    # Best Fit Score (minimize gap): higher value for smaller gap.\n    # We want to maximize `1 / (gap + epsilon)`\n    best_fit_priority = 1.0 / (gaps + 1e-6)\n\n    # Good Capacity Score: We want bins that have \"enough\" space, but not excessive.\n    # This is hard to define without knowing the distribution of item sizes.\n    # Let's consider favoring bins that have a good amount of remaining capacity,\n    # so they can potentially fit more items later.\n    # This would mean maximizing `bins_remain_cap[i]`.\n    good_capacity_priority = suitable_bins_capacities\n\n    # Adaptive weights:\n    # If bins are scarce (many items, few bins), we want to be more aggressive with tight fits.\n    # If bins are abundant (few items, many bins), we can afford to leave more space.\n    # Let's use the total number of bins used so far as a proxy for scarcity.\n    # However, we don't have the total number of bins.\n    # Let's use the number of available suitable bins as a proxy for how \"easy\" it is to find a fit.\n    # If `len(suitable_bins_indices)` is small, we are in a tighter situation.\n\n    num_suitable = len(suitable_bins_indices)\n    # If few suitable bins, prioritize tightest fit more.\n    # If many suitable bins, prioritize having more remaining space.\n    if num_suitable > 0:\n        # Weight for Best Fit: increases as number of suitable bins decreases\n        w_best_fit = 2.0 / (num_suitable + 1)\n        # Weight for Good Capacity: increases as number of suitable bins increases\n        w_good_capacity = num_suitable / (num_suitable + 1)\n    else:\n        w_best_fit = 0.5\n        w_good_capacity = 0.5\n\n    # Combined score for each suitable bin\n    # We want to maximize the combined score.\n    combined_scores = w_best_fit * best_fit_priority + w_good_capacity * good_capacity_priority\n\n    # Normalize scores to be between 0 and 1 for easier interpretation if needed,\n    # but for argmax it's not strictly necessary.\n    # Let's find the index corresponding to the maximum combined score.\n    if combined_scores.size > 0:\n        best_fit_in_suitable_idx = np.argmax(combined_scores)\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 86.58755484643,
    "SLOC": 26.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit Decreasing (BFD) logic with a \"least recently used\"\n    bias for a more adaptive online bin packing strategy.\n    It prioritizes bins that result in the smallest remaining capacity (Best Fit),\n    but also favors bins that haven't been used recently, preventing a few\n    bins from being overused early on.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Multi-objective approach ---\n    # Objective 1: Minimize remaining capacity (Best Fit)\n    suitable_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_capacities - item\n    \n    # Normalize gaps to a [0, 1] range. Smaller gap is better, so we invert.\n    # Add a small epsilon to avoid division by zero if all gaps are the same.\n    min_gap = np.min(gaps)\n    max_gap = np.max(gaps)\n    if max_gap == min_gap:\n        normalized_gaps = np.ones_like(gaps) * 0.5 # If all gaps are same, assign neutral score\n    else:\n        normalized_gaps = 1 - (gaps - min_gap) / (max_gap - min_gap)\n\n    # Objective 2: Favor less recently used bins.\n    # This is simulated by using the index as a proxy for \"recency\".\n    # Lower index means it was likely available/created earlier.\n    # We want to prioritize bins that have been \"available\" longer.\n    # So, we want smaller indices to have higher priority here.\n    # Normalize indices. Larger index means less \"recent\" in this simulation.\n    min_idx = np.min(suitable_bins_indices)\n    max_idx = np.max(suitable_bins_indices)\n    if max_idx == min_idx:\n        normalized_recency = np.ones_like(suitable_bins_indices) * 0.5\n    else:\n        normalized_recency = (suitable_bins_indices - min_idx) / (max_idx - min_idx)\n\n    # Combine objectives with weights.\n    # Let's give a slight preference to Best Fit initially (0.6)\n    # and a moderate preference to less recently used bins (0.4).\n    # These weights can be tuned.\n    weight_bf = 0.6\n    weight_recency = 0.4\n\n    combined_scores = (weight_bf * normalized_gaps) + (weight_recency * normalized_recency)\n\n    # Find the index within the 'suitable_bins_indices' array that has the highest combined score\n    best_fit_in_suitable_idx = np.argmax(combined_scores)\n    \n    # Get the original index of this best-fitting bin\n    best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n    \n    priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a greedy approach with a diversification strategy,\n    prioritizing bins that are \"almost full\" but can still accommodate the item,\n    while also giving a slight preference to bins that offer a more \"balanced\" fit.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Heuristic 1: Prioritize bins that leave a small, but not necessarily the smallest, gap.\n    # This encourages filling bins more thoroughly without necessarily aiming for a perfect fit.\n    # We use a non-linear scaling to favor gaps that are substantial but not too large.\n    # A simple approach is to use inverse of (gap + 1) or a similar function.\n    # Here, we'll use 1 / (gap + a) where 'a' is a small constant to avoid division by zero.\n    # We also consider a threshold to avoid very large gaps.\n    threshold = np.percentile(gaps, 75) # Consider bins where gap is not in the top 25%\n    scores_diversify = np.zeros_like(gaps)\n    for i, gap in enumerate(gaps):\n        if gap <= threshold:\n            scores_diversify[i] = 1.0 / (gap + 0.1) # Favor smaller gaps, but not just the smallest\n\n    # Heuristic 2: Add a small bias towards bins that have a moderate amount of remaining capacity.\n    # This can help in balancing bin usage and avoiding a situation where all bins are nearly full.\n    # We can score bins based on their remaining capacity relative to the item size.\n    # A simple heuristic is to favor bins with remaining capacity that is a certain multiple of the item size.\n    # Let's favor bins where remaining_cap is between item_size * 1.1 and item_size * 2.0\n    scores_balance = np.zeros_like(gaps)\n    for i, capacity in enumerate(suitable_bins_capacities):\n        if item * 1.1 <= capacity < item * 2.0:\n            scores_balance[i] = 0.5 # Small positive score for balanced bins\n\n    # Combine scores\n    combined_scores = scores_diversify + scores_balance\n\n    # Normalize scores for stability and to avoid issues with magnitude\n    if np.max(combined_scores) > 0:\n        combined_scores /= np.max(combined_scores)\n\n    # Add a small random element to break ties and encourage some exploration\n    noise = np.random.rand(len(combined_scores)) * 0.1\n    final_scores = combined_scores + noise\n\n    # Assign priorities to the original bins\n    best_fit_in_suitable_idx = np.argmax(final_scores)\n    best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n    priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.99996613600524 seconds"
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a \"Worst Fit\" strategy with a \"First Fit Decreasing\" like consideration.\n    Prioritizes bins that leave the largest remaining capacity (Worst Fit)\n    but gives a slight advantage to bins that are \"just big enough\" for the item,\n    mimicking the FFD idea of not wasting space on smaller items by putting them\n    in bins that could accommodate larger ones.\n    Also includes a mechanism to favor previously less-used bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    \n    # Calculate 'gaps' for Worst Fit consideration\n    gaps = suitable_bins_capacities - item\n\n    # Calculate a \"tightness\" score: smaller is better (closer to item size)\n    # This encourages putting items into bins that are just large enough,\n    # saving larger bins for potentially larger future items.\n    tightness_scores = suitable_bins_capacities - item # Same as gaps for suitable bins\n\n    # Normalize the tightness scores to be between 0 and 1 (higher is more 'tight')\n    # Avoid division by zero if all suitable bins have the exact same capacity\n    min_gap = np.min(tightness_scores)\n    max_gap = np.max(tightness_scores)\n    \n    normalized_tightness = np.zeros_like(tightness_scores)\n    if max_gap - min_gap > 1e-9: # Avoid division by zero if all gaps are equal\n        normalized_tightness = (max_gap - tightness_scores) / (max_gap - min_gap)\n    else:\n        # If all gaps are the same, all are equally 'tight'\n        normalized_tightness = np.ones_like(tightness_scores) * 0.5 # Neutral value\n\n    # Combine Worst Fit (favoring larger gaps) and Tightness (favoring smaller gaps)\n    # We want to prioritize larger gaps (Worst Fit) but give a boost for 'tight' fits.\n    # Let's try to balance: give a score based on the inverse of the gap (larger gap = higher score for WF)\n    # and add a bonus for tightness.\n    \n    # Score for Worst Fit: higher score for larger gaps\n    worst_fit_scores = np.zeros_like(tightness_scores)\n    if np.max(tightness_scores) > 1e-9: # Avoid division by zero\n      worst_fit_scores = tightness_scores / np.max(tightness_scores)\n    else:\n      worst_fit_scores = np.ones_like(tightness_scores) * 0.5 # Neutral if all gaps are zero\n\n    # Combined score: Primarily Worst Fit, with a bonus for tightness.\n    # Higher score is better.\n    combined_scores = worst_fit_scores + normalized_tightness * 0.3 # Weight for tightness\n\n    # Add a small bonus for bins that were \"less used\" previously.\n    # We can approximate \"less used\" by looking at the current remaining capacity.\n    # Bins with more remaining capacity were perhaps \"less used\" for previous items\n    # or received larger items.\n    # Normalize remaining capacities to get a \"low usage\" score (higher means less used)\n    min_cap = np.min(bins_remain_cap)\n    max_cap = np.max(bins_remain_cap)\n    \n    less_used_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    if max_cap - min_cap > 1e-9:\n        normalized_remaining_cap = (bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        normalized_remaining_cap = np.ones_like(bins_remain_cap) * 0.5\n\n    # Apply this bonus only to suitable bins\n    less_used_bonus[suitable_bins_indices] = normalized_remaining_cap[suitable_bins_indices] * 0.2 # Weight for less used\n\n    # Assign the combined scores to the priority array\n    priorities[suitable_bins_indices] = combined_scores\n\n    # Add the less_used_bonus\n    priorities += less_used_bonus\n\n    # Ensure we only return positive priorities for suitable bins\n    # This is a safeguard, the calculation should already handle this\n    priorities[~suitable_bins_mask] = 0\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 149.30195452732352,
    "SLOC": 34.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit Decreasing (BFD) principle with a novelty-seeking approach.\n    Prioritizes bins that are a \"good fit\" but also discourages repeatedly picking\n    bins that have been \"over-utilized\" recently, promoting a more balanced packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Calculate the \"tightness\" of the fit: smaller gap is better\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Normalize gaps to create a \"tightness score\" (smaller gap = higher score)\n    # Add a small epsilon to avoid division by zero if all gaps are zero\n    normalized_gaps = gaps / (np.max(gaps) - np.min(gaps) + 1e-9)\n    tightness_scores = 1.0 - normalized_gaps\n\n    # Introduce a \"novelty\" score. This is a proxy for how \"under-utilized\" a bin has been.\n    # A simple approach is to use the inverse of the remaining capacity relative to the bin size.\n    # For simplicity here, we can use the inverse of the current remaining capacity.\n    # A larger remaining capacity suggests the bin is \"less utilized\" recently.\n    # We only consider suitable bins.\n    inverse_remaining_capacity = 1.0 / (suitable_bins_capacities + 1e-9)\n\n    # Combine tightness and novelty. Weighting can be tuned.\n    # Here, we give more weight to tightness, but novelty provides a boost to less used bins.\n    combined_scores = 0.7 * tightness_scores + 0.3 * inverse_remaining_capacity\n\n    # Assign priorities based on combined scores\n    # The highest combined score gets the highest priority\n    priorities[suitable_bins_indices] = combined_scores\n\n    # Normalize priorities to sum to 1 for a probabilistic interpretation if needed,\n    # or simply return raw scores as priority weights.\n    # For this implementation, we return raw scores where higher means more preferred.\n    \n    # Simple normalization for better distribution if needed, but not strictly required\n    # if np.sum(priorities) > 0:\n    #     priorities = priorities / np.sum(priorities)\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.7245040659527122) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest fit) with an epsilon-greedy exploration strategy.\n    Prioritizes bins that leave the smallest remaining capacity after packing,\n    with a small chance of picking any suitable bin to encourage exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.487435181491823,
    "SLOC": 6.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]