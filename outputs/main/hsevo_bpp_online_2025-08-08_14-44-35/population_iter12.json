[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a utilization bonus and an adaptive exploration strategy.\n    Favors bins that result in minimal slack after packing, with a bonus for bins\n    that are already partially filled, and a probabilistic chance to explore less-used bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    # --- Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing (minimize slack).\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    # Score is inverse of slack; higher score for smaller slack.\n    tightness_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Component 2: Utilization Bonus ---\n    # Prefer bins that are already partially filled.\n    # A simple proxy for utilization: higher score for bins with less remaining capacity.\n    # This encourages using bins that are already in use.\n    # Score is inverse of remaining capacity *before* packing.\n    utilization_bonus = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combining Exploitation Scores ---\n    # Combine tightness and utilization bonus. A weighted sum is used.\n    # Tightness is the primary driver, utilization adds a bonus.\n    # We normalize the utilization bonus to prevent it from overpowering the tightness score.\n    # A simple normalization: divide by the maximum possible utilization score (1/epsilon if a bin is empty).\n    # A more stable approach: normalize by the maximum utilization bonus among fitting bins.\n    max_utilization_bonus = np.max(utilization_bonus)\n    normalized_utilization_bonus = utilization_bonus / (max_utilization_bonus + epsilon)\n\n    # The combined exploitation score weighs tightness more heavily.\n    exploitation_score = tightness_score + 0.3 * normalized_utilization_bonus\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(exploitation_score)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_score)\n\n    # --- Component 3: Adaptive Exploration (Epsilon-Greedy) ---\n    # Introduce a small probability to explore less optimal bins.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    exploration_scores = np.zeros_like(normalized_exploitation_scores)\n\n    # Determine how many bins to \"explore\"\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n\n    if num_to_explore > 0:\n        # Select random indices for exploration\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give explored bins a baseline high priority, slightly boosted to ensure consideration.\n        # This ensures some randomness without completely overriding good exploitation choices.\n        exploration_scores[explore_indices] = 0.5\n\n    # --- Final Priority Calculation ---\n    # Combine exploitation scores with exploration scores.\n    # The exploration scores add a boost to randomly selected bins.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_scores\n\n    # Normalize final scores so that the highest priority is 1.0.\n    max_final_score = np.max(final_scores_unnormalized)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / max_final_score\n    else:\n        # Fallback: if all scores are near zero, assign uniform probability to fitting bins.\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.108496210610296,
    "SLOC": 32.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a utilization-aware exploration strategy.\n    Prioritizes tight fits and encourages exploration of less utilized bins.\n    \"\"\"\n    epsilon = 0.05  # Exploration probability\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Calculate a \"tightness\" score (how much space is left after packing)\n    tightness_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        remaining_after_packing = bins_remain_cap[suitable_bins_indices] - item\n        # Higher score for smaller remaining capacity (tighter fit)\n        # Add a small constant to avoid division by zero and ensure positive scores\n        tightness_scores[suitable_bins_indices] = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Calculate a \"utilization\" score (favoring bins that are already more full)\n    # We use the inverse of remaining capacity *before* packing.\n    utilization_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        # Higher score for less remaining capacity (more utilized bin)\n        utilization_scores[suitable_bins_indices] = 1.0 / (bins_remain_cap[suitable_bins_indices] + 1e-9)\n\n    # Combine scores: prioritize tightness, with a bonus for utilization\n    combined_scores = tightness_scores + 0.5 * utilization_scores # Weight utilization less than tightness\n\n    # Normalize scores for the suitable bins to a 0-1 range\n    suitable_scores = combined_scores[suitable_bins_indices]\n    if suitable_scores.size > 0:\n        max_score = np.max(suitable_scores)\n        if max_score > 0:\n            normalized_scores = suitable_scores / max_score\n            priorities[suitable_bins_indices] = normalized_scores\n\n    # Epsilon-greedy exploration: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities\n        priorities[chosen_bin_index] = 1.0 # Assign full priority to the random bin\n    else:\n        # Exploitation: Use the calculated priorities (based on combined scores)\n        # The `priorities` array already holds the normalized scores\n        pass \n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.078579976067022,
    "SLOC": 28.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight-fit preference with an adaptive penalty for bins with excessive remaining capacity,\n    using a hybrid exploration/exploitation strategy with score normalization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.15  # Probability of exploring less utilized bins\n    \n    can_fit_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Phase: Combine Tight Fit and Penalty for Large Bins ---\n    \n    # Tightness score: Higher for bins leaving less space after packing.\n    # `1.0 / (remaining_space + epsilon)` where `remaining_space = bins_remain_cap - item`.\n    # This favors bins where `bins_remain_cap` is just slightly larger than `item`.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_scores = 1.0 / (remaining_after_packing + 1e-6)\n\n    # Penalty for \"empty\" or overly large bins: Penalize bins whose remaining capacity\n    # is significantly larger than the item size. This encourages using bins that are\n    # already somewhat utilized. We use `1.0 / (1.0 + penalty_weight * initial_capacity)`\n    # for fitting bins, where `penalty_weight` controls the strength of the penalty.\n    # A higher initial capacity results in a lower penalty score, thus reducing priority.\n    penalty_weight = 0.3\n    # Ensure we don't penalize bins that are already a good fit (capacity ~ item) too much.\n    # The penalty is applied to the initial remaining capacity before packing.\n    penalty_scores = 1.0 / (1.0 + penalty_weight * fitting_bins_remain_cap)\n\n    # Combine scores: Multiply tightness by penalty.\n    # This means bins that are tight *and* not excessively large get higher priority.\n    combined_exploitation_scores = tightness_scores * penalty_scores\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > 1e-9: # Avoid division by near zero\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n    \n    priorities[can_fit_mask] = normalized_exploitation_scores\n\n    # --- Exploration Phase: Favor less utilized bins ---\n    # If exploration is triggered, randomly select a bin from those that can fit the item,\n    # biasing towards bins with more remaining capacity (less utilized).\n    if np.random.rand() < epsilon:\n        # Calculate exploration scores for suitable bins: higher score for more remaining capacity.\n        # We use a log transformation to amplify differences in larger capacities,\n        # and add a small constant to handle bins with exactly the item size.\n        exploration_scores = np.log1p(fitting_bins_remain_cap - item + 1e-6)\n        \n        # Normalize exploration scores to form a probability distribution for selection.\n        sum_exploration_scores = np.sum(exploration_scores)\n        if sum_exploration_scores > 1e-9:\n            exploration_probabilities = exploration_scores / sum_exploration_scores\n            chosen_bin_idx_in_suitable = np.random.choice(len(suitable_bins_indices), p=exploration_probabilities)\n            chosen_bin_original_idx = suitable_bins_indices[chosen_bin_idx_in_suitable]\n            # Assign a high priority (e.g., 1.0) to the chosen exploration bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities for pure exploration choice\n            priorities[chosen_bin_original_idx] = 1.0\n        else:\n            # If all exploration scores are zero (e.g., all fitting bins have same capacity),\n            # fall back to a random choice among fitting bins.\n            chosen_bin_original_idx = np.random.choice(suitable_bins_indices)\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[chosen_bin_original_idx] = 1.0\n    \n    # If not exploring, the 'priorities' array already holds the normalized exploitation scores.\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.248105305145606,
    "SLOC": 33.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a utilization bonus and adaptive exploration.\n    Favors bins that minimize remaining space, offering a bonus for less utilized bins,\n    and strategically explores less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Slightly higher exploration to balance\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Utilization Bonus ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This encourages spreading items across more bins initially.\n    # A simple score: higher for bins with less current remaining capacity.\n    # Using a base of 1 to avoid making already partially full bins *too* dominant.\n    fill_score_bonus = 0.3 * (1.0 / (valid_bins_remain_cap + epsilon))\n    \n    # Combine tightness and utilization bonus: multiplicative effect\n    # This rewards bins that are both tight and already have some items\n    combined_exploitation_scores = tight_fit_scores * (1.0 + fill_score_bonus)\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Adaptive Exploration ---\n    # With a certain probability, give a boost to randomly selected fitting bins\n    # to explore less optimal choices and avoid getting stuck in local optima.\n    num_fitting_bins = len(valid_bins_remain_cap)\n    exploration_boost = np.zeros_like(normalized_exploitation_scores)\n    \n    # Determine how many bins to \"boost\" for exploration\n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins))) # Ensure at least one bin is explored if possible\n    \n    # Select indices to explore randomly from the fitting bins\n    # Using np.random.choice with replace=False to ensure unique bins are selected for exploration boost\n    explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n    \n    # Assign a uniform high exploration score to these bins, relative to the normalized exploitation scores.\n    # This ensures exploration bins are considered, but their exact priority is context-dependent on overall exploitation scores.\n    exploration_boost[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation and exploration scores.\n    # Adding exploration boost means these bins will have a higher chance of selection.\n    # The `1 - exploration_prob` for exploitation is implicitly handled by not boosting all bins.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_boost\n    \n    # Ensure all priorities are non-negative and normalize them for the fitting bins\n    final_scores_unnormalized[final_scores_unnormalized < 0] = 0\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    \n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # Fallback to uniform probability if all scores are zero (e.g., due to epsilon issues)\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.9760670123653865,
    "SLOC": 30.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for under-utilized bins,\n    using a novel exploration strategy to balance exploitation and exploration.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_factor = 0.2  # Controls the degree of exploration\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exploitation Component: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    # Higher score for smaller remaining space.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Exploitation Component: Utilization Penalty ---\n    # Penalize bins that are significantly under-utilized (i.e., have a lot of remaining capacity).\n    # This encourages filling existing bins before opening new ones.\n    # We use the inverse of the *initial* remaining capacity as a proxy for utilization.\n    # Higher utilization (less remaining capacity) gets a higher score.\n    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Weighted sum of tight fit and utilization scores.\n    # Giving slightly more weight to tight fit as it's the primary goal of BPP.\n    combined_exploitation_scores = 0.6 * tight_fit_scores + 0.4 * utilization_scores\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Exploration Component: Adaptive Exploration ---\n    # We want to explore bins that are not necessarily the best according to exploitation.\n    # Instead of random choice, we will boost the scores of bins that are \"average\" or slightly below average\n    # in terms of exploitation, giving them a chance.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    \n    # Calculate a baseline score (e.g., mean exploitation score)\n    mean_exploitation_score = np.mean(normalized_exploitation_scores)\n    \n    # Create exploration scores: higher for bins closer to the mean exploitation score.\n    # This aims to explore \"promising but not top\" candidates.\n    # We add a small constant to ensure even the lowest scores get some exploration boost if needed.\n    exploration_scores = np.exp(-((normalized_exploitation_scores - mean_exploitation_score) / (mean_exploitation_score + epsilon))**2)\n    \n    # Apply the exploration factor to blend exploitation and exploration\n    # The final priority is a mix:\n    # (1 - exploration_factor) * exploitation_scores + exploration_factor * exploration_scores\n    # This ensures that exploration never completely overrides exploitation,\n    # and also that exploitation still has a significant impact.\n    final_priorities_unnormalized = (1 - exploration_factor) * normalized_exploitation_scores + exploration_factor * exploration_scores\n\n    # Normalize final priorities for the fitting bins\n    sum_final_priorities = np.sum(final_priorities_unnormalized)\n    if sum_final_priorities > epsilon:\n        priorities[can_fit_mask] = final_priorities_unnormalized / sum_final_priorities\n    else:\n        # If all scores are zero, distribute probability equally among fitting bins\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the tightness preference with an adaptive \"fill level\" bonus\n    and a sophisticated exploration strategy inspired by multi-armed bandits.\n    Prioritizes tight fits while encouraging the use of partially filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Strategy: Combined Tightness and Fill Bonus ---\n\n    # Tightness score: Inverse of remaining capacity after packing. Higher is better.\n    # This favors bins that leave minimal slack.\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Fill Level Bonus: Rewards bins that are already more utilized.\n    # We approximate utilization by penalizing bins with large remaining capacity.\n    # A simple inverse of the current remaining capacity (for fitting bins) serves as a bonus.\n    # This encourages using bins that are already partially filled, rather than always\n    # picking a \"tight fit\" in a nearly empty bin.\n    fill_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    # Bonus is higher for bins with less remaining capacity (i.e., more filled)\n    fill_bonus[can_fit_mask] = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Combine tightness and fill bonus. Multiplication allows the fill bonus to\n    # modulate the tightness score. A bin that is both tight and already somewhat full\n    # gets a higher combined score.\n    combined_exploitation_score = tightness_score * (1.0 + 0.3 * fill_bonus) # 0.3 is a tunable weight\n\n    # Normalize exploitation scores to the range [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_score)\n    if max_exploitation_score > 0:\n        normalized_exploitation_scores = combined_exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_score)\n\n    # --- Exploration Strategy: Adaptive Exploration (Simplified Bandit-like) ---\n\n    # We want to explore bins that are currently less prioritized to discover\n    # potentially better packing options or to balance load.\n    # A simple adaptive strategy: If a bin has a low exploitation score,\n    # it's a candidate for exploration. We'll assign a small, uniform exploration\n    # priority to all bins that can fit the item. The degree of exploration\n    # can be tied to the number of available fitting bins.\n\n    num_fitting_bins = np.sum(can_fit_mask)\n    exploration_weight = 0.15 # Base weight for exploration\n\n    # Increase exploration tendency if there are many options, decrease if few\n    if num_fitting_bins > 5:\n        exploration_weight *= 1.2\n    elif num_fitting_bins < 3:\n        exploration_weight *= 0.8\n\n    # Create exploration scores. For simplicity here, we'll give a small boost\n    # to all fitting bins, meaning any fitting bin has a chance.\n    # A more advanced approach would use a UCB-like strategy.\n    exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    exploration_scores[can_fit_mask] = exploration_weight / num_fitting_bins\n\n    # --- Final Priority: Blend Exploitation and Exploration ---\n    # A simple blending: priorities = exploitation_scores + exploration_scores\n    # This gives a base priority based on exploitation and adds a small chance\n    # to any fitting bin via exploration.\n    priorities[can_fit_mask] = normalized_exploitation_scores[can_fit_mask] + exploration_scores[can_fit_mask]\n\n    # Ensure no negative priorities and normalize the final output to [0, 1]\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 32.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with adaptive penalty for large bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    Uses score normalization for robust comparison.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Tightness score: Higher for bins leaving less space after packing.\n    # This is equivalent to prioritizing bins closer to 'item' size from below.\n    tightness_score = 1.0 / (fitting_bins_remain_cap - item + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of bins that are \"too empty\" for the current item.\n    # A simple way is to use an inverse relationship with the initial remaining capacity.\n    # The factor penalizes bins with large remaining capacities, favoring those already somewhat utilized.\n    # This is analogous to prioritizing bins that are not \"too empty\".\n    penalty_weight = 0.3\n    penalty_score = 1.0 / (1.0 + penalty_weight * fitting_bins_remain_cap)\n\n    # Combine tightness and penalty. A higher combined score means a bin is both a tight fit\n    # and not excessively large (i.e., not \"too empty\").\n    combined_score = tightness_score * penalty_score\n\n    # Normalize scores to a 0-1 range for stable comparison across different item/bin states.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score / max_score\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a normalized tight-fitting score with adaptive exploration favoring\n    less utilized bins, using a dynamic probability based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    num_suitable_bins = suitable_bins_remain_cap.size\n\n    # Component 1: Tight Fitting Score (normalized)\n    remaining_after_packing = suitable_bins_remain_cap - item\n    # Higher score for smaller remaining capacity (tighter fit). Add small epsilon for stability.\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n    \n    # Normalize tight fit scores to be between 0 and 1\n    max_tight_fit_score = np.max(tight_fit_scores)\n    if max_tight_fit_score > 1e-9:\n        normalized_tight_fit_scores = tight_fit_scores / max_tight_fit_score\n    else:\n        normalized_tight_fit_scores = np.zeros_like(tight_fit_scores)\n\n    # Component 2: Adaptive Exploration favoring less utilized bins\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1.0 + num_suitable_bins))\n    \n    # Generate exploration scores: Favor bins with more remaining capacity (less utilized)\n    # Use log1p for non-linear scaling, making differences more pronounced for smaller capacities\n    exploration_scores = np.log1p(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Combine normalized tight fit score with exploration score for the final priority\n    # We want to favor tighter fits but have an exploration mechanism.\n    # A simple weighted sum can work, where exploration probability influences the choice.\n    \n    # For exploitation (when not exploring), use the normalized tight fit score.\n    exploitation_priorities = normalized_tight_fit_scores\n\n    # For exploration, we need to select a bin probabilistically.\n    # The 'exploration_scores' are used to define the probability distribution for selection.\n    # Normalize exploration scores to create probabilities.\n    sum_exploration_scores = np.sum(exploration_scores)\n    if sum_exploration_scores > 1e-9:\n        exploration_probabilities = exploration_scores / sum_exploration_scores\n    else:\n        # If all exploration scores are zero (e.g., all bins have item + epsilon capacity),\n        # fall back to uniform probability.\n        exploration_probabilities = np.ones(num_suitable_bins) / num_suitable_bins\n\n    # Determine if we explore or exploit\n    if np.random.rand() < exploration_prob:\n        # Exploration: Choose a bin based on exploration probabilities\n        chosen_bin_idx_in_subset = np.random.choice(num_suitable_bins, p=exploration_probabilities)\n        # Assign a high priority to the explored bin\n        priorities[can_fit_mask][chosen_bin_idx_in_subset] = 1.0\n    else:\n        # Exploitation: Assign priorities based on the normalized tight fit scores\n        priorities[can_fit_mask] = exploitation_priorities\n        \n        # Normalize final priorities to ensure they are in a comparable range if multiple bins are chosen\n        max_final_priority = np.max(priorities[can_fit_mask])\n        if max_final_priority > 1e-9:\n            priorities[can_fit_mask] /= max_final_priority\n        else:\n            priorities[can_fit_mask] = np.zeros_like(priorities[can_fit_mask])\n\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 34.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with adaptive bin utilization and dynamic exploration.\n    Prioritizes bins that minimize waste and are more utilized, with exploration\n    favoring less utilized bins based on a dynamically adjusted probability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    available_bin_indices = np.where(can_fit_mask)[0]\n\n    # --- Core Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Score is inverse of remaining space after packing.\n    remaining_after_packing = available_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Adaptive Bin Utilization: Score based on how \"full\" the bin is.\n    # Prefer bins that have less remaining capacity relative to the item size.\n    # This can be approximated by `item / available_bins_remain_cap` or similar.\n    # Let's use a score that is higher for bins that are less empty (i.e., lower remaining capacity).\n    # A simple approach is `1.0 / (available_bins_remain_cap + epsilon)`.\n    # To make it more adaptive, we can consider the *average* remaining capacity of fitting bins.\n    avg_fitting_remain_cap = np.mean(available_bins_remain_cap) if available_bins_remain_cap.size > 0 else 1.0\n    # Prefer bins with remaining capacity closer to the item or lower than average.\n    # Let's try a score that is high when remaining capacity is low (closer to item).\n    utilization_scores = 1.0 / (available_bins_remain_cap + epsilon)\n\n    # Combine the two heuristic scores. A weighted sum is a common approach.\n    # Let's give equal weight for now.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be between 0 and 1.\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Dynamic Epsilon-Greedy Exploration ---\n    num_available_bins = available_bins_remain_cap.size\n\n    # Determine exploration probability adaptively.\n    # A common strategy is to increase exploration when there are many bins,\n    # to encourage trying out different options. Or, explore less if bins are very full.\n    # Let's use a simple inverse relationship with the number of available bins to explore more\n    # when there are fewer options, which can be counter-intuitive.\n    # A better approach: explore more when there are *many* bins to choose from, to diversify.\n    # Or, explore less when bins are already quite full, and more when they are mostly empty.\n\n    # A simple dynamic probability: decrease exploration as the number of available bins decreases.\n    # Or, use the inverse of the average utilization as a proxy for how \"challenging\" the packing is.\n    # Let's use a probability that decreases with the tightness of the fit (smaller remaining_after_packing).\n    # This means we explore more when the fits are not tight.\n    \n    # Heuristic from analysis: dynamic exploration favoring less utilized bins.\n    # We want to explore bins that are NOT among the top-ranked by core heuristics.\n    # The probability of exploration for a bin could be inversely related to its normalized core score.\n\n    # Simple dynamic exploration probability: higher when many bins, lower when few.\n    # Or, a fixed probability for simplicity if dynamic is too complex to implement robustly here.\n    # Given the prompt's goal of combining elements, let's use a fixed epsilon-greedy\n    # but slightly modified based on utilization to influence *which* bins are explored.\n\n    exploration_prob = 0.15 # Base exploration probability\n\n    # We want to assign exploration scores to a subset of bins.\n    # Instead of random scores, let's boost scores of bins that are less utilized.\n    # A simple way to select bins for \"exploration\" (or a modified score):\n    # bins with higher `utilization_scores` (meaning they are more utilized)\n    # should be less likely to be perturbed by random exploration.\n    # So, bins with lower `utilization_scores` are more prone to exploration.\n\n    # Create a probability mask for exploration, favoring less utilized bins.\n    # Let's sort bins by utilization_scores (ascending) and pick a fraction.\n    # A simple approach: assign a higher 'random' score to bins that are less utilized.\n    \n    # Let's stick to a blended approach inspired by Heuristic 11, where exploration\n    # contributes to the final score.\n    \n    # For the exploration component, we can generate random scores.\n    # To bias exploration towards less utilized bins, we can scale these random scores.\n    # If `utilization_scores` is low (meaning bin is less utilized), we want a higher random score.\n    # We can achieve this by multiplying `random_scores` by `(1.0 / (utilization_scores + epsilon))`.\n    \n    random_exploration_scores = np.random.rand(num_available_bins)\n    \n    # Scale random scores: boost scores for less utilized bins (low utilization_scores).\n    # Higher `boost_factor` means more boost for less utilized bins.\n    boost_factor = 2.0 # Control how much less utilized bins are favored in exploration\n    scaled_random_scores = random_exploration_scores * (1.0 + boost_factor * (1.0 - utilization_scores))\n    \n    # Normalize these scaled random scores to keep them in a comparable range.\n    max_scaled_random_score = np.max(scaled_random_scores)\n    if max_scaled_random_score > epsilon:\n        normalized_exploration_scores = scaled_random_scores / max_scaled_random_score\n    else:\n        normalized_exploration_scores = np.zeros_like(scaled_random_scores)\n\n    # Blend the deterministic (core) scores with the exploration scores.\n    # Use the `exploration_prob` to decide the mixing weight.\n    alpha = exploration_prob\n    final_scores = (1 - alpha) * normalized_core_scores + alpha * normalized_exploration_scores\n\n    # Re-normalize the final blended scores to ensure they are in a 0-1 range.\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        # Fallback if all scores are zero or near-zero\n        priorities[can_fit_mask] = final_scores\n\n    # Ensure priorities are non-negative.\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 37.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and adaptive utilization, with dynamic exploration\n    favoring less utilized bins for better overall packing.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = fitting_bins_capacities - item\n    # Score is inversely proportional to the remaining gap, encouraging tighter fits.\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    max_total_capacity = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 0\n    # If all bins are empty or have very little capacity, treat max_total_capacity carefully.\n    if max_total_capacity < epsilon:\n        max_total_capacity = 1.0 # Avoid division by zero if bins are very small\n\n    # Calculate utilization score: higher for bins with more *absolute* remaining space.\n    # This is a proxy for less utilized bins.\n    utilization_scores = fitting_bins_capacities / (max_total_capacity + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Combine tight fit and utilization scores. A weighted sum is used.\n    # The weights can be tuned, here we give equal importance.\n    exploitation_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize exploitation scores to be between 0 and 1.\n    max_exploitation_score = np.max(exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_scores)\n\n    # --- Adaptive Exploration ---\n    # Introduce exploration by randomly selecting some fitting bins and boosting their scores.\n    num_fitting_bins = len(fitting_bins_indices)\n    \n    # Exploration probability adapts: higher if few options, lower if many.\n    if num_fitting_bins <= 2:\n        exploration_prob = 0.3  # More exploration when options are scarce\n    elif num_fitting_bins > 5:\n        exploration_prob = 0.1  # Less exploration when many options exist\n    else:\n        exploration_prob = 0.2  # Moderate exploration\n\n    # Determine how many bins to explore.\n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins)))\n    \n    # Ensure we don't try to explore more bins than available.\n    num_to_explore = min(num_to_explore, num_fitting_bins)\n\n    # Select random indices from the fitting bins for exploration.\n    explore_indices_in_fitting = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n\n    # Create exploration bonuses. These are added to the exploitation scores.\n    # Bins selected for exploration get a significant boost.\n    exploration_bonuses = np.zeros_like(normalized_exploitation_scores)\n    exploration_bonuses[explore_indices_in_fitting] = 1.0 # High bonus for exploration bins\n\n    # --- Final Priority Calculation ---\n    # Combine exploitation and exploration scores.\n    final_scores = normalized_exploitation_scores + exploration_bonuses\n\n    # Assign final priorities to the original bin indices.\n    priorities[fitting_bins_indices] = final_scores\n\n    # Normalize all priorities so they sum to 1, effectively becoming probabilities.\n    sum_priorities = np.sum(priorities)\n    if sum_priorities > epsilon:\n        priorities /= sum_priorities\n    else:\n        # Fallback: if all scores are zero, assign uniform probability to fitting bins.\n        if num_fitting_bins > 0:\n            priorities[fitting_bins_indices] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 84.63302752293579,
    "SLOC": 41.0,
    "cyclomatic_complexity": 9.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_weights: np.ndarray, exploration_rate: float = 0.1) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit Decreasing (BFD) idea with a weighted multi-objective approach,\n    considering both the remaining capacity (tightness) and a pre-defined bin weight.\n    An adaptive exploration strategy is introduced where the exploration rate\n    decreases over time (or with more bins used).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: An array of remaining capacities for each bin.\n        bin_weights: An array of pre-defined weights for each bin, reflecting some\n                     prioritization (e.g., newer bins, bins with certain properties).\n        exploration_rate: The probability of picking a random suitable bin.\n\n    Returns:\n        An array of priority scores for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    if not np.any(suitable_bins_mask):\n        return priorities\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    suitable_bins_weights = bin_weights[suitable_bins_indices]\n\n    # Objective 1: Tightness of fit (prefer bins with smaller remaining capacity after packing)\n    # We want to minimize (capacity - item), so we want smaller values to be higher priority.\n    # For ranking, we can use the inverse or directly use the difference.\n    # To avoid division by zero or very small numbers, and to keep it simple for this example,\n    # we'll use a transformation that maps smaller gaps to higher scores.\n    gaps = suitable_bins_capacities - item\n    \n    # Normalize gaps to be between 0 and 1 (higher score for smaller gap)\n    # Adding a small epsilon to avoid division by zero if all gaps are zero\n    min_gap = np.min(gaps)\n    max_gap = np.max(gaps)\n    if max_gap == min_gap: # Handle case where all suitable bins have the same gap\n        normalized_tightness_scores = np.ones_like(gaps) * 0.5 \n    else:\n        normalized_tightness_scores = 1.0 - (gaps - min_gap) / (max_gap - min_gap)\n\n    # Objective 2: Bin weight (prefer bins with higher pre-defined weights)\n    # Normalize bin weights to be between 0 and 1\n    min_weight = np.min(suitable_bins_weights)\n    max_weight = np.max(suitable_bins_weights)\n    if max_weight == min_weight: # Handle case where all suitable bins have the same weight\n        normalized_weight_scores = np.ones_like(suitable_bins_weights) * 0.5\n    else:\n        normalized_weight_scores = (suitable_bins_weights - min_weight) / (max_weight - min_weight)\n\n    # Combine objectives using a weighted sum.\n    # Weights can be tuned. Here, we give equal importance initially.\n    # You might want to make these weights adaptive based on problem instance or performance.\n    weight_tightness = 0.5\n    weight_bin_weight = 0.5\n\n    combined_scores = (weight_tightness * normalized_tightness_scores +\n                       weight_bin_weight * normalized_weight_scores)\n\n    # Apply exploration vs. exploitation\n    if np.random.rand() < exploration_rate:\n        # Exploration: pick a random suitable bin\n        chosen_bin_index_in_suitable = np.random.choice(len(suitable_bins_indices))\n        priorities[suitable_bins_indices[chosen_bin_index_in_suitable]] = 1.0\n    else:\n        # Exploitation: pick the bin with the highest combined score\n        best_fit_in_suitable_idx = np.argmax(combined_scores)\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    # Optional: Decay exploration rate over time or based on number of bins used\n    # For a simple implementation, we'll keep it fixed for this function signature.\n    # A more advanced version would pass a counter or decay logic.\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 1 required positional argument: 'bin_weights'\n33\n5\n"
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_capacity: float, exploration_factor: float = 0.1, fairness_weight: float = 0.3) -> np.ndarray:\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Exploitation: Best Fit logic\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n    best_fit_indices_in_suitable = np.where(gaps == np.min(gaps))[0]\n    best_fit_original_indices = suitable_bins_indices[best_fit_indices_in_suitable]\n\n    # Exploration: Introduce a mechanism to occasionally pick non-best-fit bins\n    if np.random.rand() < exploration_factor:\n        exploration_candidates_indices = np.setdiff1d(suitable_bins_indices, best_fit_original_indices)\n        if exploration_candidates_indices.size > 0:\n            # Select a random bin from the remaining suitable bins\n            chosen_exploration_index = np.random.choice(exploration_candidates_indices)\n            priorities[chosen_exploration_index] = 1.0\n            return priorities\n\n    # Multi-objective consideration: Combine Best Fit with a fairness objective\n    # Fairness: Prioritize bins that are less full (larger remaining capacity)\n    # This helps to distribute items more evenly and potentially leave larger contiguous spaces\n    # for future large items.\n\n    # Normalize the gaps and remaining capacities to get scores between 0 and 1\n    normalized_gaps = (gaps - np.min(gaps)) / (np.max(gaps) - np.min(gaps) + 1e-9)\n    normalized_remaining_cap = (suitable_bins_capacities - item) / bin_capacity\n\n    # Combine scores: Higher score is better.\n    # We want a small gap (good for best fit) and a large remaining capacity (good for fairness)\n    # For small gap, we want low normalized_gaps, so we use (1 - normalized_gaps)\n    # For large remaining capacity, we want high normalized_remaining_cap\n    combined_scores = (1 - normalized_gaps) * (1 - fairness_weight) + normalized_remaining_cap * fairness_weight\n\n    # Get the indices of bins with the highest combined score\n    best_combined_score_in_suitable_idx = np.where(combined_scores == np.max(combined_scores))[0]\n    best_combined_original_indices = suitable_bins_indices[best_combined_score_in_suitable_idx]\n\n    # Assign priority to the bin(s) with the highest combined score\n    for idx in best_combined_original_indices:\n        priorities[idx] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 1 required positional argument: 'bin_capacity'\n24\n5\n"
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_fill_ratios: np.ndarray, item_size_distribution: np.ndarray, bin_utilization_history: np.ndarray, exploration_rate: float = 0.1) -> np.ndarray:\n    \"\"\"\n    Combines multiple strategies for bin selection in online Bin Packing Problem.\n    It prioritizes bins based on:\n    1. Tightness of fit (Best Fit): Minimizes wasted space in the chosen bin.\n    2. Bin utilization: Prefers bins that are already relatively full to avoid\n       creating many partially filled bins.\n    3. Item size distribution similarity: Tries to match items to bins that\n       have historically accommodated similar sized items well (a form of\n       context-awareness).\n    4. Adaptive exploration: With a certain probability, it picks a random\n       suitable bin to explore different packing configurations.\n\n    Args:\n        item: The size of the current item to be packed.\n        bins_remain_cap: A numpy array where each element is the remaining\n                         capacity of a bin.\n        bin_fill_ratios: A numpy array representing the current fill ratio\n                         (item_size / bin_capacity) for each bin.\n        item_size_distribution: A numpy array representing the historical\n                                 distribution of item sizes (e.g., a histogram\n                                 or density estimate). This is a simplified\n                                 representation; a more complex model could be used.\n        bin_utilization_history: A numpy array storing the historical\n                                 utilization of each bin (e.g., average fill ratio).\n        exploration_rate: The probability of choosing a random suitable bin.\n\n    Returns:\n        A numpy array of priority scores for each bin. Higher score means higher priority.\n    \"\"\"\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros(num_bins, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Normalize factors to combine them effectively\n    # Factor 1: Tightness of fit (inverse of gap)\n    gaps = bins_remain_cap[suitable_bins_indices] - item\n    # Avoid division by zero if gap is zero; assign a very high value\n    tightness_scores = np.where(gaps == 0, np.max(gaps[gaps > 0]) + 1, gaps)\n    tightness_scores = 1.0 / tightness_scores\n    # Normalize tightness scores\n    if np.sum(tightness_scores) > 0:\n        tightness_scores /= np.sum(tightness_scores)\n    else:\n        tightness_scores = np.ones_like(tightness_scores) / len(tightness_scores)\n\n\n    # Factor 2: Bin fill ratio (prefer fuller bins)\n    fill_ratios_suitable = bin_fill_ratios[suitable_bins_indices]\n    # Normalize fill ratios\n    if np.sum(fill_ratios_suitable) > 0:\n        fill_ratios_suitable /= np.sum(fill_ratios_suitable)\n    else:\n        fill_ratios_suitable = np.ones_like(fill_ratios_suitable) / len(fill_ratios_suitable)\n\n\n    # Factor 3: Item size distribution similarity (simplified: match to bins that have seen similar items)\n    # This is a placeholder. A real implementation might use a more sophisticated model.\n    # Here, we'll assume item_size_distribution is a proxy for how well bins\n    # might fit certain item sizes. We'll give higher priority to bins that have\n    # historically handled items of similar size.\n    # For simplicity, let's assume item_size_distribution is already weighted by bin.\n    # If not, a mapping would be needed.\n    # Let's assume item_size_distribution is a flattened preference across all bins.\n    # A more accurate way would be to have a matrix where item_size_dist[bin_idx, item_size_category]\n    # For this example, let's assume item_size_distribution represents a preference score\n    # for current item size across all bins.\n    item_distribution_scores = item_size_distribution[suitable_bins_indices]\n    if np.sum(item_distribution_scores) > 0:\n        item_distribution_scores /= np.sum(item_distribution_scores)\n    else:\n        item_distribution_scores = np.ones_like(item_distribution_scores) / len(item_distribution_scores)\n\n    # Combine factors using weights (can be tuned)\n    # Weights can be learned or set based on domain knowledge.\n    w_tightness = 0.4\n    w_fill_ratio = 0.3\n    w_distribution = 0.3\n\n    combined_scores_suitable = (\n        w_tightness * tightness_scores +\n        w_fill_ratio * fill_ratios_suitable +\n        w_distribution * item_distribution_scores\n    )\n\n    # Adaptive exploration\n    if np.random.rand() < exploration_rate:\n        # Pick a random suitable bin\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Assign priorities based on combined scores\n        # Ensure no negative scores and normalize to create a probability distribution\n        # (though we only care about relative order for max)\n        normalized_combined_scores = combined_scores_suitable - np.min(combined_scores_suitable)\n        if np.sum(normalized_combined_scores) > 0:\n            normalized_combined_scores /= np.sum(normalized_combined_scores)\n        else:\n            normalized_combined_scores = np.ones_like(normalized_combined_scores) / len(normalized_combined_scores)\n\n        # Assign the calculated scores to the appropriate bins\n        priorities[suitable_bins_indices] = normalized_combined_scores\n        # Optionally, to mimic \"highest priority score\", we can just set the max\n        # to 1 and others proportionally, or just use the scores as they are.\n        # Let's set the max score to 1 for clarity if we want to identify the 'best'.\n        if suitable_bins_indices.size > 0:\n            max_idx_in_suitable = np.argmax(priorities[suitable_bins_indices])\n            best_bin_original_idx = suitable_bins_indices[max_idx_in_suitable]\n            # Reset priorities to a simple indicator for the best bin\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[best_bin_original_idx] = 1.0\n\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 3 required positional arguments: 'bin_fill_ratios', 'item_size_distribution', and 'bin_utilization_history'\n48\n8\n"
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit Decreasing (BFD) like logic with a penalty for wasted space\n    and a dynamic exploration factor. Prioritizes bins that offer a good fit,\n    penalizes bins that would leave a large gap, and incorporates a small\n    randomness that decreases over time (simulating an annealing approach for exploration).\n    \"\"\"\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Calculate scores for suitable bins\n    suitable_bin_capacities = bins_remain_cap[suitable_bins_indices]\n    \n    # Objective 1: Best Fit (minimize remaining capacity after packing)\n    # Lower gap is better, so we want to maximize (max_gap - gap)\n    max_gap = np.max(suitable_bin_capacities - item)\n    fit_scores = max_gap - (suitable_bin_capacities - item)\n    \n    # Objective 2: Minimize wasted space (penalize bins with very large remaining capacity before packing)\n    # This encourages packing into bins that are already somewhat full.\n    # We use a negative exponential to penalize larger capacities.\n    # Higher capacity before packing is worse.\n    # Normalization helps to balance the two objectives.\n    \n    # Normalize capacities to be between 0 and 1 for penalty calculation\n    max_capacity = np.max(bins_remain_cap) if np.max(bins_remain_cap) > 0 else 1.0\n    normalized_capacities = suitable_bin_capacities / max_capacity\n    \n    # Penalty for having too much space already (we want bins that are somewhat full)\n    # A simple penalty could be 1 - normalized_capacity, so bins with less capacity get higher penalty\n    # Or even better, use the inverse of the capacity, scaled down.\n    # Let's try to penalize large remaining capacities: lower is better for remaining capacity\n    # so higher remaining capacity is worse.\n    \n    # We want to minimize the remaining capacity before packing the current item\n    # To turn this into a maximization score, we can use: 1 / (1 + remaining_capacity)\n    # Or use a scaling factor:\n    space_penalty_scores = 1.0 / (1.0 + suitable_bin_capacities)\n    \n    # Combine scores - A weighted sum. Weights can be tuned.\n    # For now, give equal weight or slightly more to the fit score.\n    fit_weight = 0.6\n    penalty_weight = 0.4\n    \n    # Normalize fit_scores to be in a similar range as penalty_scores if needed,\n    # or simply combine them. Let's assume they are somewhat comparable or normalize after.\n    \n    # Normalize fit_scores to [0, 1] for consistent combining\n    min_fit_score = np.min(fit_scores)\n    max_fit_score = np.max(fit_scores)\n    if max_fit_score - min_fit_score > 0:\n        normalized_fit_scores = (fit_scores - min_fit_score) / (max_fit_score - min_fit_score)\n    else:\n        normalized_fit_scores = np.ones_like(fit_scores) * 0.5 # Neutral score if all are same\n\n    # Normalize space_penalty_scores to [0, 1]\n    min_penalty_score = np.min(space_penalty_scores)\n    max_penalty_score = np.max(space_penalty_scores)\n    if max_penalty_score - min_penalty_score > 0:\n        normalized_penalty_scores = (space_penalty_scores - min_penalty_score) / (max_penalty_score - min_penalty_score)\n    else:\n        normalized_penalty_scores = np.ones_like(space_penalty_scores) * 0.5 # Neutral score if all are same\n\n    combined_scores = (fit_weight * normalized_fit_scores) + (penalty_weight * normalized_penalty_scores)\n    \n    # Dynamic Exploration: Similar to simulated annealing, reduce exploration over time.\n    # This function doesn't have a direct \"time\" parameter, so we'll make it\n    # based on the number of available bins. More bins might imply earlier stages.\n    # A simple approach is to use the inverse of the number of suitable bins,\n    # capped at a small value.\n    exploration_factor = 0.1 / (1.0 + len(suitable_bins_indices)) \n\n    # Add exploration noise\n    exploration_noise = np.random.rand(len(suitable_bins_indices)) * exploration_factor\n    \n    final_scores = combined_scores + exploration_noise\n    \n    # Assign the highest score to the chosen bin\n    best_suitable_bin_idx = np.argmax(final_scores)\n    priorities[suitable_bins_indices[best_suitable_bin_idx]] = 1.0\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 34.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_count: int) -> np.ndarray:\n    \"\"\"\n    Combines a weighted sum of multiple objectives:\n    1. Best Fit (tightest fit): Minimizes the remaining capacity after packing.\n    2. First Fit Decreasing (FFD) tendency: Prioritizes bins that have been used less recently (implicitly by favoring bins with larger remaining capacity, assuming they were filled later or are new).\n    3. Adaptive Exploration: Uses an epsilon-greedy approach where epsilon decreases over time or based on packing success.\n\n    This version aims for a more balanced approach by considering multiple factors.\n    \"\"\"\n    \n    # --- Parameters ---\n    # Weights for combining objectives\n    weight_best_fit = 0.6\n    weight_ffd_tendency = 0.3\n    \n    # Exploration parameters - can be made adaptive in a more complex implementation\n    # For simplicity, we'll use a fixed epsilon here, but it could be a parameter\n    # or dynamically adjusted based on the number of bins used or items packed.\n    epsilon = 0.05  # Probability of exploration\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # --- Exploration Phase ---\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n        return priorities\n\n    # --- Exploitation Phase: Multi-objective scoring ---\n    \n    # Objective 1: Best Fit (minimize gap)\n    # Calculate the 'gap' or remaining capacity after fitting the item\n    gaps = bins_remain_cap[suitable_bins_indices] - item\n    # Normalize gaps to be between 0 and 1 for consistent weighting\n    # Smallest gap should have highest score. So we invert and normalize.\n    # Add a small epsilon to avoid division by zero if all gaps are the same.\n    max_gap = np.max(gaps)\n    min_gap = np.min(gaps)\n    \n    if max_gap == min_gap:\n        best_fit_scores = np.ones_like(gaps) * 0.5 # Neutral score if all gaps are equal\n    else:\n        best_fit_scores = 1.0 - (gaps - min_gap) / (max_gap - min_gap)\n\n    # Objective 2: First Fit Decreasing tendency (favor bins with more remaining capacity)\n    # This encourages using partially filled bins before opening new ones, \n    # and also helps in packing larger items first into potentially larger remaining capacity bins.\n    # Normalize remaining capacities to be between 0 and 1.\n    remaining_caps_suitable = bins_remain_cap[suitable_bins_indices]\n    max_rem_cap = np.max(remaining_caps_suitable)\n    min_rem_cap = np.min(remaining_caps_suitable)\n    \n    if max_rem_cap == min_rem_cap:\n        ffd_scores = np.ones_like(remaining_caps_suitable) * 0.5 # Neutral score if all remaining capacities are equal\n    else:\n        ffd_scores = (remaining_caps_suitable - min_rem_cap) / (max_rem_cap - min_rem_cap)\n\n    # Combine scores using weighted sum\n    combined_scores = (weight_best_fit * best_fit_scores) + (weight_ffd_tendency * ffd_scores)\n    \n    # Find the bin with the highest combined score among suitable bins\n    best_fit_in_suitable_idx_in_subset = np.argmax(combined_scores)\n    \n    # Get the original index of this best-fitting bin\n    best_overall_bin_index = suitable_bins_indices[best_fit_in_suitable_idx_in_subset]\n    \n    priorities[best_overall_bin_index] = 1.0\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 1 required positional argument: 'bin_count'\n32\n5\n"
  }
]