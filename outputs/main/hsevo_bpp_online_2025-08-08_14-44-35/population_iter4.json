[
  {
    "stdout_filepath": "problem_iter2_response0.txt_stdout.txt",
    "code_path": "problem_iter2_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the tight-fitting priority of inverse remaining capacity\n    with an epsilon-greedy exploration strategy for better bin packing.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploring a random bin\n\n    # Calculate priorities based on tightest fit (inverse remaining capacity)\n    # Only consider bins that can fit the item\n    fit_mask = bins_remain_cap >= item\n    tight_fit_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Add a small epsilon to avoid division by zero if remaining capacity == item\n    valid_capacities = bins_remain_cap[fit_mask] - item + 1e-9\n    tight_fit_priorities[fit_mask] = 1.0 / valid_capacities\n\n    # Normalize priorities so they sum to 1 (if no exploration)\n    sum_priorities = np.sum(tight_fit_priorities)\n    if sum_priorities > 0:\n        normalized_priorities = tight_fit_priorities / sum_priorities\n    else:\n        # If no bins can fit the item, assign equal probability to all (effectively a new bin)\n        normalized_priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Epsilon-greedy: explore randomly with probability epsilon\n    exploration_priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    \n    # Combine exploitation (tight fit) and exploration (random)\n    # With probability (1 - epsilon), choose the tight fit priority\n    # With probability epsilon, choose the exploration priority\n    combined_priorities = (1 - epsilon) * normalized_priorities + epsilon * exploration_priorities\n\n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies or edge cases)\n    final_priorities = combined_priorities / np.sum(combined_priorities)\n\n    return final_priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response1.txt_stdout.txt",
    "code_path": "problem_iter2_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the inverse remaining capacity strategy with a sigmoid function for smoother prioritization.\n    It favors bins with smaller remaining capacity after placing the item, using a sigmoid to normalize scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            remaining_capacity = bins_remain_cap[i] - item\n            # Use inverse of remaining capacity, scaled by sigmoid to smooth prioritization\n            # Adding a small epsilon to denominator to avoid division by zero\n            # Scaling factor (e.g., 5.0) can be tuned to control sensitivity\n            priorities[i] = 1 / (remaining_capacity + 1e-9)\n            \n    # Normalize priorities using sigmoid to create a smoother distribution and avoid extreme values\n    # The sigmoid function squashes values between 0 and 1\n    # We can scale the input to sigmoid to control the steepness of the transition\n    # Using a simple division by the maximum priority to prevent overflow and normalize\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        normalized_priorities = priorities / max_priority\n        # Apply sigmoid\n        return 1 / (1 + np.exp(-5.0 * (normalized_priorities - 0.5))) # Tunable steepness factor 5.0\n    else:\n        return np.zeros_like(bins_remain_cap, dtype=float)",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.99996592500247 seconds"
  },
  {
    "stdout_filepath": "problem_iter2_response2.txt_stdout.txt",
    "code_path": "problem_iter2_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the \"tightest fit\" priority of inverse remaining capacity with an\n    epsilon-greedy exploration strategy. Favors bins that leave minimal space\n    after packing, with a small chance of selecting a random suitable bin.\n    \"\"\"\n    suitable_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if np.any(suitable_bins_mask):\n        suitable_bins_remain_cap = bins_remain_cap[suitable_bins_mask]\n        \n        # Prioritize bins with the tightest fit (inverse remaining capacity)\n        tight_fit_priorities = 1.0 / (suitable_bins_remain_cap - item + 1e-9)\n        \n        # Normalize tight fit priorities for a smoother distribution\n        normalized_tight_fit_priorities = (tight_fit_priorities - np.min(tight_fit_priorities)) / \\\n                                          (np.max(tight_fit_priorities) - np.min(tight_fit_priorities) + 1e-9)\n        \n        # Introduce epsilon-greedy exploration: a small chance to pick any suitable bin\n        epsilon = 0.1\n        random_priorities = np.random.rand(np.sum(suitable_bins_mask))\n        \n        # Combine tight fit and random exploration\n        combined_priorities = (1 - epsilon) * normalized_tight_fit_priorities + epsilon * random_priorities\n        \n        priorities[suitable_bins_mask] = combined_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response3.txt_stdout.txt",
    "code_path": "problem_iter2_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with an adaptive exploration strategy.\n    It prioritizes bins with the tightest fit, but with a probability,\n    explores other fitting bins to avoid getting stuck in local optima.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    possible_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(possible_bins_indices) == 0:\n        return priorities  # No bin can fit the item\n\n    # Exploration phase: with probability epsilon, choose a random fitting bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(possible_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Best Fit strategy\n        fitting_bins_capacities = bins_remain_cap[possible_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = fitting_bins_capacities - item\n        # Find the index within the 'possible_bins_indices' array that has the minimum gap\n        best_fit_in_possible_idx = np.argmin(gaps)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = possible_bins_indices[best_fit_in_possible_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.148384523334677,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response4.txt_stdout.txt",
    "code_path": "problem_iter2_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines inverse remaining capacity with a penalty for empty bins,\n    favoring tighter fits while encouraging filling existing bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-6\n    \n    for i, cap in enumerate(bins_remain_cap):\n        if cap >= item:\n            remaining_after_fit = cap - item\n            \n            # Favor tighter fits (inverse remaining capacity)\n            priority_fit = 1.0 / (remaining_after_fit + epsilon)\n            \n            # Penalty for completely empty bins if not an exact fit\n            # This encourages using existing bins before opening new ones unless it's an exact fit\n            if cap > item and cap == bins_remain_cap[i]: # if bin was initially empty\n                priority_fit *= 0.8 # slight penalty for using an empty bin\n                \n            priorities[i] = priority_fit\n            \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.9999738750048 seconds"
  },
  {
    "stdout_filepath": "problem_iter2_response5.txt_stdout.txt",
    "code_path": "problem_iter2_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by favoring tighter fits (inverse remaining capacity) while adding a penalty for completely empty bins.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate priorities for suitable bins: higher priority for less remaining capacity (tighter fit)\n    # Add a small epsilon to avoid division by zero if a bin is exactly full\n    epsilon = 1e-9\n    suitable_bins_capacity = bins_remain_cap[suitable_bins_mask]\n    priorities[suitable_bins_mask] = 1.0 / (suitable_bins_capacity - item + epsilon)\n\n    # Apply a penalty to bins that are completely empty to encourage packing into existing bins first.\n    # Bins with remaining capacity equal to the bin capacity are considered empty.\n    # This penalty is designed to be significantly larger than any priority value from the inverse capacity calculation.\n    empty_bins_mask = bins_remain_cap == np.max(bins_remain_cap) # Assuming max capacity is the bin size\n    \n    # Ensure we only penalize empty bins that are also suitable\n    empty_and_suitable_mask = empty_bins_mask & suitable_bins_mask\n    \n    # A large constant to penalize empty bins. This ensures they are considered only after non-empty bins are exhausted.\n    empty_bin_penalty = 1e6  \n    priorities[empty_and_suitable_mask] = empty_bin_penalty\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 86.58755484643,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response6.txt_stdout.txt",
    "code_path": "problem_iter2_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes bins by favoring tighter fits and incorporating a soft epsilon-greedy exploration.\n    Combines inverse remaining capacity with a small probability of choosing a random bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_mask]\n\n    if suitable_bins_capacities.size > 0:\n        gaps = suitable_bins_capacities - item\n        \n        # Inverse remaining capacity for tightest fits\n        # Add a small epsilon to avoid division by zero and make priorities finite\n        inverse_gaps = 1.0 / (gaps + 1e-6)\n        \n        # Normalize priorities to be between 0 and 1 (similar to softmax idea but simpler)\n        # This rewards bins with smaller remaining capacity more\n        max_inverse_gap = np.max(inverse_gaps)\n        if max_inverse_gap > 0:\n            normalized_priorities = inverse_gaps / max_inverse_gap\n        else:\n            normalized_priorities = np.ones_like(inverse_gaps) # All bins have 0 gap after fit\n\n        priorities[suitable_bins_mask] = normalized_priorities\n        \n    # Introduce a small probability of choosing any suitable bin to encourage exploration\n    # This is a simplified epsilon-greedy approach.\n    epsilon = 0.05\n    num_suitable_bins = np.sum(suitable_bins_mask)\n    \n    if num_suitable_bins > 0:\n        # Assign a small, uniform probability to all suitable bins\n        exploration_prob = epsilon / num_suitable_bins\n        priorities[suitable_bins_mask] += exploration_prob\n\n        # Re-normalize to ensure the sum of probabilities for suitable bins is manageable\n        # This step ensures that exploration doesn't completely dominate exploitation\n        sum_priorities = np.sum(priorities[suitable_bins_mask])\n        if sum_priorities > 0:\n             priorities[suitable_bins_mask] /= sum_priorities\n\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 22.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response7.txt_stdout.txt",
    "code_path": "problem_iter2_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes bins by inverse remaining capacity with adaptive exploration.\n\n    Combines the tight-fit preference of inverse remaining capacity with\n    a controlled exploration mechanism similar to epsilon-greedy.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins_mask = bins_remain_cap >= item\n    valid_bins_capacities = bins_remain_cap[valid_bins_mask]\n\n    if valid_bins_capacities.size > 0:\n        # Primary heuristic: Prioritize bins with tighter fits (inverse remaining capacity)\n        inverse_distances = 1.0 / (bins_remain_cap[valid_bins_mask] - item + 1e-9)\n        priorities[valid_bins_mask] = inverse_distances\n\n        # Adaptive exploration: Introduce a small probability of choosing a non-best fit\n        # This helps prevent getting stuck in local optima.\n        # The exploration probability can be tuned. Here, we use a small fixed probability.\n        exploration_prob = 0.05\n        num_valid_bins = valid_bins_capacities.size\n        \n        if num_valid_bins > 1:\n            # Randomly select a subset of valid bins to give a small boost,\n            # simulating exploration. This is a simplified epsilon-greedy approach.\n            exploration_indices_in_valid = np.random.choice(num_valid_bins, \n                                                          size=max(1, int(num_valid_bins * exploration_prob)), \n                                                          replace=False)\n            \n            # Get the actual indices in the original bins_remain_cap array\n            exploration_indices_in_original = np.where(valid_bins_mask)[0][exploration_indices_in_valid]\n            \n            # Add a small, uniform boost to these bins to make them potentially selectable\n            # even if they are not the absolute tightest fit.\n            priorities[exploration_indices_in_original] += 1e-6 # A small boost\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response8.txt_stdout.txt",
    "code_path": "problem_iter2_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Inverse Distance with an epsilon-greedy strategy for exploration.\n\n    Prioritizes bins with a tighter fit (less waste) while occasionally\n    exploring other bins to avoid local optima.\n    \"\"\"\n    available_bins_mask = bins_remain_cap >= item\n    available_bins_cap = bins_remain_cap[available_bins_mask]\n\n    if available_bins_cap.size == 0:\n        return np.zeros_like(bins_remain_cap)\n\n    waste = available_bins_cap - item\n    priorities = 1.0 / (waste + 1e-9)\n\n    full_priorities = np.zeros_like(bins_remain_cap)\n    full_priorities[available_bins_mask] = priorities\n\n    epsilon = 0.1  # Exploration rate\n    num_available_bins = available_bins_mask.sum()\n\n    if num_available_bins > 0 and np.random.rand() < epsilon:\n        # Randomly select an available bin for exploration\n        random_indices = np.where(available_bins_mask)[0]\n        random_bin_index = np.random.choice(random_indices)\n        full_priorities[random_bin_index] = np.max(full_priorities) + 1.0 # Boost priority for exploration\n\n    return full_priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 5.434782608695652,
    "SLOC": 16.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter2_response9.txt_stdout.txt",
    "code_path": "problem_iter2_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the tight-fitting preference of inverse capacity with adaptive exploration.\n    Prioritizes bins that offer a tighter fit, with a controlled amount of randomness\n    to encourage exploration of potentially better, albeit slightly less optimal, fits.\n    \"\"\"\n    \n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    \n    suitable_bins_capacities = bins_remain_cap[suitable_bins_mask]\n    \n    if suitable_bins_capacities.size == 0:\n        return priorities\n    \n    # Calculate the \"gap\" for suitable bins (remaining capacity after placing item)\n    gaps = suitable_bins_capacities - item\n    \n    # Prioritize bins with smaller gaps (tighter fit). Using inverse of (1 + gap)\n    # to ensure a priority score that is higher for smaller gaps.\n    # Adding 1 to the denominator to avoid division by zero and a small epsilon\n    # for numerical stability if gaps can be zero.\n    fit_scores = 1.0 / (1.0 + gaps + 1e-9)\n    \n    # Introduce adaptive exploration: add a small amount of noise based on the std dev\n    # of the fit scores of the suitable bins. This nudges the selection slightly\n    # away from purely greedy choices, promoting exploration.\n    if fit_scores.size > 0:\n        std_dev_fit_scores = np.std(fit_scores)\n        random_noise = np.random.normal(0, std_dev_fit_scores * 0.05, fit_scores.shape)\n        priorities[suitable_bins_mask] = fit_scores + random_noise\n    \n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.307937774232155,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response0.txt_stdout.txt",
    "code_path": "problem_iter3_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using an adaptive\n    strategy that balances tight fitting with utilizing bins that are nearly full.\n    This version aims for better performance by considering both the immediate\n    benefit of tight packing and the long-term strategy of keeping larger bins\n    available.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Calculate remaining capacity after packing the item for bins that can fit\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n    \n    # --- Heuristic Design Principles ---\n    # 1. Adaptive Exploration: Dynamically adjust priorities.\n    # 2. Multi-objective Balancing: Combine \"tight fit\" with \"prefer nearly full\".\n    # 3. Dynamic Scaling: Use scaling factors based on context.\n    \n    # Objective 1: Encourage tight packing (similar to v1)\n    # Higher priority for bins that leave less remaining space.\n    # Add a small epsilon to avoid division by zero.\n    epsilon = 1e-9\n    tight_fit_score = 1.0 / (remaining_after_packing + epsilon)\n    \n    # Objective 2: Prefer bins that are already somewhat full but still have space.\n    # This helps in consolidating items and potentially leaving larger bins open for larger items.\n    # The score is higher for bins with less remaining capacity *before* packing.\n    # We use a sigmoid-like function to give a boost to bins that are moderately filled,\n    # not too empty and not completely full.\n    # A simple approach is to use inverse of current remaining capacity, but cap it.\n    # Let's normalize the current remaining capacity for bins that can fit the item.\n    normalized_current_remain_cap = bins_remain_cap[can_fit_mask] / np.max(bins_remain_cap[can_fit_mask] + epsilon) # Normalize by max capacity to keep it within [0, 1]\n    nearly_full_score = 1.0 - normalized_current_remain_cap # Higher score for smaller remaining capacity\n\n    # Combine scores. We can use a weighted sum.\n    # The weights can be thought of as dynamic parameters or set based on observed performance.\n    # For this example, let's give a slight preference to tight fitting, but also reward\n    # using bins that are already somewhat utilized.\n    \n    # Let's assign a weight to each objective.\n    # weight_tight_fit = 0.7\n    # weight_nearly_full = 0.3\n\n    # A more adaptive approach: The preference for tight fit vs. using a nearly full bin\n    # could depend on the item size itself. Smaller items might benefit more from tight fitting,\n    # while larger items might benefit from using bins that are already somewhat full to keep\n    # larger empty bins available.\n\n    # Let's scale the item size relative to the maximum bin capacity available.\n    max_possible_capacity = np.max(bins_remain_cap[can_fit_mask]) if np.any(can_fit_mask) else 1.0\n    relative_item_size = item / (max_possible_capacity + epsilon)\n    \n    # Dynamic weighting:\n    # If item is small (relative_item_size < 0.5), favor tight fit more.\n    # If item is large (relative_item_size >= 0.5), favor using nearly full bins more.\n    weight_tight_fit = 0.4 + 0.6 * (1.0 - relative_item_size) # Range: [0.4, 1.0]\n    weight_nearly_full = 0.6 + 0.4 * relative_item_size      # Range: [0.6, 1.0]\n    # Ensure weights sum to 1 (approximately, due to scaling factors)\n    total_weight = weight_tight_fit + weight_nearly_full\n    weight_tight_fit /= total_weight\n    weight_nearly_full /= total_weight\n    \n\n    combined_score = (weight_tight_fit * tight_fit_score) + (weight_nearly_full * nearly_full_score)\n\n    # Assign the combined scores to the bins that can fit the item\n    priorities[can_fit_mask] = combined_score\n    \n    # Normalize priorities to have a maximum of 1.0. This makes comparisons relative.\n    if np.max(priorities) > 0:\n        priorities /= np.max(priorities)\n        \n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 20.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response1.txt_stdout.txt",
    "code_path": "problem_iter3_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using an adaptive\n    strategy that balances First Fit Decreasing-like behavior with the goal of\n    minimizing the number of bins. Priority is higher for bins that can\n    accommodate the item and are 'closer' to being full, while also considering\n    bins that are significantly underutilized to encourage spreading items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    \n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, we calculate a score that combines two aspects:\n    # 1. How tightly the bin will be packed (favors smaller remaining capacity)\n    # 2. How full the bin is *before* packing (favors bins that are already somewhat full)\n    # We also add a small constant to the denominator to avoid division by zero and\n    # to ensure that bins that are perfectly filled still get a non-zero priority if needed.\n    \n    # Score based on how full the bin is before packing (higher if more full)\n    # This encourages using already partially filled bins first.\n    # We normalize by bin capacity to make it relative.\n    filled_ratio = (bins_remain_cap - bins_remain_cap[can_fit_mask] ) / (bins_remain_cap[can_fit_mask] + epsilon)\n    # The above line is wrong, it should be based on original capacity, but we don't have it.\n    # Let's rephrase: favor bins that have a relatively small remaining capacity compared to their original capacity.\n    # Since we don't have original capacity, let's use the inverse of remaining capacity as a proxy for fullness.\n    \n    # Consider bins that can fit the item\n    fitting_bins_capacities = bins_remain_cap[can_fit_mask]\n\n    # Score 1: Tighter packing (inverse of remaining capacity after packing)\n    # Higher value for bins that will have less space left.\n    tight_packing_score = 1.0 / (fitting_bins_capacities - item + epsilon)\n\n    # Score 2: Encouraging use of partially filled bins\n    # Higher value for bins that are already relatively full.\n    # We use the inverse of the current remaining capacity.\n    # This is a bit of a trade-off: tight_packing_score favors small remaining space after packing,\n    # while this favors small remaining space *before* packing.\n    # The combination aims to fill bins that have some capacity but are not empty.\n    current_fullness_score = 1.0 / (fitting_bins_capacities + epsilon)\n    \n    # Combine scores: give more weight to the current fullness\n    # The idea is to prioritize bins that are somewhat full AND will become even tighter.\n    # We use a weighted sum. The weights can be tuned. Here, we give a bit more\n    # emphasis to current fullness.\n    combined_score = 0.6 * current_fullness_score + 0.4 * tight_packing_score\n    \n    # Assign priorities to the bins that can fit the item\n    priorities[can_fit_mask] = combined_score\n\n    # Normalize priorities so the maximum priority is 1.0\n    # This makes the relative priorities consistent.\n    max_priority = np.max(priorities)\n    if max_priority > 0:\n        priorities /= max_priority\n        \n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response2.txt_stdout.txt",
    "code_path": "problem_iter3_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a hybrid approach\n    combining First Fit Decreasing (FFD) intuition with a focus on minimizing wasted space.\n\n    This heuristic prioritizes bins that can fit the item and have a remaining capacity\n    that is \"close\" to the item size, aiming to reduce fragmentation. It also incorporates\n    a penalty for bins that are too large relative to the item, discouraging the use\n     of oversized bins for small items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # For bins that can fit the item, calculate a priority score.\n    # We want to prioritize bins where the remaining capacity is *just* enough or slightly more.\n    # A bin with remaining capacity `r` after packing the item `i` will have `r = bins_remain_cap - item`.\n    # We want to maximize `1 / (r + epsilon)` for tighter packing.\n    # Additionally, we penalize bins that are excessively large compared to the item.\n    \n    epsilon = 1e-9\n    \n    # Calculate 'tightness' score: higher for smaller remaining capacity after packing\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n    \n    # Calculate 'oversize' penalty: penalize bins that are much larger than the item.\n    # This encourages using bins that are closer in size to the item if possible.\n    # We use a logarithmic scale to dampen the effect of very large bins.\n    oversize_penalty = np.zeros_like(bins_remain_cap, dtype=float)\n    oversize_factor = bins_remain_cap[can_fit_mask] / (item + epsilon)\n    # Penalize if oversize_factor is significantly greater than 1.\n    # A threshold of 2 means a bin is twice the item size. We can adjust this.\n    oversize_threshold = 2.0\n    penalty_strength = 0.5 # Tune this parameter\n    \n    oversize_penalty[can_fit_mask] = np.maximum(0, 1 - penalty_strength * np.log(oversize_factor / oversize_threshold + 1))\n    \n    # Combine scores. Prioritize tightness, but also penalize significant oversizing.\n    # A weighted sum can be used, or a multiplicative approach.\n    # Here we'll use a multiplicative approach where oversizing reduces the priority.\n    \n    combined_priority = tightness_score * oversize_penalty\n    \n    # Normalize priorities to have a max of 1.0\n    max_priority = np.max(combined_priority)\n    if max_priority > 0:\n        priorities[can_fit_mask] = combined_priority / max_priority\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response3.txt_stdout.txt",
    "code_path": "problem_iter3_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a hybrid strategy.\n    It balances the tendency to fill bins tightly (similar to v1) with a preference\n    for bins that have a larger capacity to accommodate potentially larger future items.\n    It also introduces an adaptive component that favors bins that have been utilized\n    more (i.e., have less remaining capacity overall, but can still fit the current item),\n    promoting a more balanced distribution.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Filter to only consider bins that can fit the item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tighter Packing (similar to v1)\n    # Higher priority for bins that leave less remaining capacity after packing.\n    remaining_after_packing = available_bins_remain_cap - item\n    tight_packing_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # Component 2: Future Accommodation (preference for larger bins)\n    # Higher priority for bins with more capacity, even after fitting the item.\n    # This can be useful if we anticipate larger items later.\n    future_accommodation_score = available_bins_remain_cap\n\n    # Component 3: Adaptive Bin Utilization (promotes balanced loading)\n    # Bins with less total remaining capacity (meaning they have been used more)\n    # might be preferred to distribute items more evenly, preventing some bins\n    # from staying almost empty for too long. We invert the remaining capacity.\n    # To avoid very small values causing issues, we can use a base capacity\n    # (e.g., max possible bin capacity or average capacity) for normalization.\n    # For simplicity here, we'll use the maximum remaining capacity among available bins.\n    max_remaining_among_available = np.max(available_bins_remain_cap)\n    adaptive_utilization_score = (max_remaining_among_available - available_bins_remain_cap + epsilon) / (max_remaining_among_available + epsilon)\n\n\n    # Combine the scores. We can use weighted sums.\n    # Weights can be tuned. Here we give a slight preference to tighter packing\n    # and balanced utilization.\n    weight_tight = 0.4\n    weight_future = 0.2\n    weight_adaptive = 0.4\n\n    combined_score = (weight_tight * tight_packing_score +\n                      weight_future * future_accommodation_score +\n                      weight_adaptive * adaptive_utilization_score)\n\n    # Normalize the combined scores for the available bins\n    if np.max(combined_score) > 0:\n        normalized_combined_score = combined_score / np.max(combined_score)\n    else:\n        normalized_combined_score = np.zeros_like(combined_score)\n\n    # Place the normalized scores back into the original priorities array\n    priorities[can_fit_mask] = normalized_combined_score\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 86.58755484643,
    "SLOC": 24.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter3_response4.txt_stdout.txt",
    "code_path": "problem_iter3_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a multi-objective approach.\n    This heuristic balances fitting the item tightly (like v1) with considering bins that\n    are \"nearly full\" but might still accommodate the item, promoting a more balanced packing.\n    It also dynamically scales priorities based on the item size relative to bin capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # --- Component 1: Tight Fitting (similar to v1) ---\n    # Prioritize bins that leave less remaining capacity after packing\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Component 2: Nearly Full Preference ---\n    # Prioritize bins that are already quite full (but can still fit the item)\n    # This encourages using up existing partially filled bins more effectively.\n    # We use a negative value of remaining capacity, scaled inversely.\n    # Bins with less remaining capacity get a higher score here.\n    nearly_full_scores = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n\n\n    # --- Component 3: Dynamic Scaling ---\n    # Scale the influence of the \"tight fitting\" component based on item size relative to typical bin capacity.\n    # If the item is large relative to bin capacity, tight fitting is more critical.\n    # If the item is small, slightly fuller bins might be more beneficial overall.\n    # We can use the ratio of the item size to the average remaining capacity of *all* bins.\n    avg_remain_cap_all = np.mean(bins_remain_cap)\n    if avg_remain_cap_all > epsilon:\n        tight_fit_weight = min(1.0, item / avg_remain_cap_all) # Weight increases with item size relative to average\n    else:\n        tight_fit_weight = 1.0\n\n    # Combine scores: weighted sum of tight fitting and nearly full preference\n    combined_scores = (tight_fit_weight * tight_fit_scores) + ((1 - tight_fit_weight) * nearly_full_scores)\n\n    # Assign the combined scores to the appropriate bins\n    priorities[can_fit_mask] = combined_scores\n\n    # Normalize priorities: Ensure the highest priority is 1.0 for clear selection\n    max_priority = np.max(priorities)\n    if max_priority > epsilon:\n        priorities /= max_priority\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, epsilon: float = 0.48573899498012607, epsilon_value: float = 7.072882589808971e-07) -> np.ndarray:\n    \"\"\"\n    Combines the tight-fitting priority of inverse remaining capacity\n    with an epsilon-greedy exploration strategy for better bin packing.\n\n    Args:\n        item (float): The item to be packed.\n        bins_remain_cap (np.ndarray): An array of remaining capacities for each bin.\n        epsilon (float): Probability of exploring a random bin. Defaults to 0.1.\n        epsilon_value (float): A small value added to avoid division by zero. Defaults to 1e-9.\n\n    Returns:\n        np.ndarray: An array of probabilities for packing the item into each bin.\n    \"\"\"\n    # Calculate priorities based on tightest fit (inverse remaining capacity)\n    # Only consider bins that can fit the item\n    fit_mask = bins_remain_cap >= item\n    tight_fit_priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Add a small epsilon to avoid division by zero if remaining capacity == item\n    valid_capacities = bins_remain_cap[fit_mask] - item + epsilon_value\n    tight_fit_priorities[fit_mask] = 1.0 / valid_capacities\n\n    # Normalize priorities so they sum to 1 (if no exploration)\n    sum_priorities = np.sum(tight_fit_priorities)\n    if sum_priorities > 0:\n        normalized_priorities = tight_fit_priorities / sum_priorities\n    else:\n        # If no bins can fit the item, assign equal probability to all (effectively a new bin)\n        normalized_priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n\n    # Epsilon-greedy: explore randomly with probability epsilon\n    exploration_priorities = np.ones_like(bins_remain_cap) / len(bins_remain_cap)\n    \n    # Combine exploitation (tight fit) and exploration (random)\n    # With probability (1 - epsilon), choose the tight fit priority\n    # With probability epsilon, choose the exploration priority\n    combined_priorities = (1 - epsilon) * normalized_priorities + epsilon * exploration_priorities\n\n    # Ensure probabilities sum to 1 (due to potential floating point inaccuracies or edge cases)\n    final_priorities = combined_priorities / np.sum(combined_priorities)\n\n    return final_priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 14.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]