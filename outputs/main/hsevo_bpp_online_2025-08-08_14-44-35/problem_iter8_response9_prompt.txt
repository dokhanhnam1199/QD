{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a multi-objective approach with adaptive behavior.\n    Prioritizes bins based on a weighted sum of criteria:\n    1. Tightest fit (minimizing remaining capacity after packing).\n    2. Bin fullness (maximizing current bin utilization, which indirectly\n       encourages opening new bins for larger items).\n    3. A penalty for bins that are too full and might cause immediate\n       overflow for slightly larger items (encouraging better distribution).\n\n    The weights are adaptive, favoring tighter fits when bins are abundant\n    and fuller bins when capacity is scarce.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Criteria calculation for suitable bins\n    # 1. Tightest Fit (lower is better, so we invert for priority)\n    gaps = suitable_bins_capacities - item\n    tightness_score = 1.0 / (gaps + 1e-6) # Add small epsilon to avoid division by zero\n\n    # 2. Bin Fullness (higher is better)\n    # Assuming bin_remain_cap represents remaining capacity of TOTAL capacity.\n    # We need total capacity for fullness. For simplicity, let's assume a common max capacity.\n    # In a real scenario, this would be passed or inferred.\n    # Let's assume a default max capacity if not provided.\n    # For this example, let's derive a plausible max capacity from the data if possible,\n    # or use a large number as a proxy if only remaining capacities are available.\n    # A more robust approach would involve passing the bin's total capacity.\n    # For demonstration, we'll consider the average remaining capacity as a proxy indicator.\n    # A better proxy for fullness, given only remaining capacity, is how much *space* is left\n    # relative to what has been used. Since we don't know total capacity directly,\n    # we can indirectly infer fullness by how much capacity is REMAINING.\n    # A smaller remaining capacity implies a fuller bin. So, we'll use the inverse of remaining capacity.\n    # However, the prompt is about 'bin fullness', implying usage.\n    # If we only have remaining capacity, we can't directly calculate fullness.\n    # Let's re-interpret \"bin fullness\" in the context of remaining capacity:\n    # A bin is \"fuller\" if it has *less* remaining capacity.\n    # So, higher fullness is associated with *lower* remaining capacity.\n    # We want to prioritize bins with less remaining capacity (if they can fit the item).\n    # So, we want to maximize `1 / bins_remain_cap`.\n    # However, we are only considering `suitable_bins_capacities`, which are >= item.\n    # Let's consider a different interpretation: priority should be given to bins that are \"almost full\"\n    # in the sense that they have a good amount of remaining space, but not too much.\n    # A simple measure for \"good\" remaining space might be related to the item size itself.\n    # Let's consider how \"well\" the item fits into the remaining space.\n    # A bin that can accommodate the item with a moderate amount of remaining capacity might be good.\n    # Let's try to prioritize bins that have capacity CLOSE to the item size, but not necessarily the tightest.\n    # This could be represented by the absolute difference between remaining capacity and item size,\n    # but we want to favor bins with *more* remaining capacity to avoid fragmentation initially.\n    # Let's try maximizing remaining capacity among suitable bins, as a proxy for \"not too full\".\n    fullness_score = suitable_bins_capacities\n\n    # 3. Penalty for being too full (lower is better, so invert for priority)\n    # If remaining capacity is very small, it's good for tight fit but bad for future items.\n    # Let's penalize bins with very little remaining capacity *after* fitting the item.\n    # This is related to the gap, but we want to avoid bins that are *almost* full.\n    # A small gap is good (tight fit), but if the gap is extremely small (e.g., < item/4),\n    # it might be detrimental.\n    # Let's define a \"too full\" threshold. If remaining_capacity - item < some_fraction_of_item,\n    # apply a penalty.\n    too_full_penalty = np.zeros_like(suitable_bins_capacities)\n    # Consider bins where remaining capacity is less than 2 times the item size as potentially \"too full\"\n    # if they still have substantial space left. This is tricky.\n    # Let's simplify: penalize bins with very small remaining capacity BEFORE fitting the item.\n    # This would mean bins that are already quite full.\n    # If `bins_remain_cap` is small, it's \"full\". We want to avoid picking these if possible,\n    # UNLESS they are the tightest fit.\n    # Let's rethink: the goal is to minimize the number of bins.\n    # So, we want to utilize existing bins effectively.\n\n    # Let's consider a score that balances:\n    # 1. Maximizing the remaining capacity AFTER packing (to leave space for future items).\n    # 2. Minimizing the number of bins used (by packing tightly).\n\n    # Revised strategy:\n    # Focus on using existing bins efficiently.\n    # The \"best fit\" is often good. What if we also consider bins that have\n    # *just enough* space for the current item, but not too much excess?\n    # This could be measured by how close `bins_remain_cap` is to `item`.\n    # So, we want to maximize `bins_remain_cap` minus `item`, but only for suitable bins.\n    # Or, minimize `bins_remain_cap` among suitable bins. This is Best Fit.\n\n    # What if we add a term that favors bins that have a \"good amount\" of remaining space\n    # *after* packing? This would be `bins_remain_cap[i] - item`.\n    # We want to maximize this value.\n    # So, we have two conflicting goals for suitable bins:\n    # - Minimize `bins_remain_cap[i] - item` (tightest fit)\n    # - Maximize `bins_remain_cap[i] - item` (leave more space)\n\n    # Let's try a heuristic that favors bins that have a remaining capacity\n    # that is \"just enough\" for the item, or slightly more.\n    # This means we want to find a bin where `bins_remain_cap` is close to `item`.\n    # The gap `bins_remain_cap - item` should be small, but not necessarily zero.\n    # We want to maximize `-(gap)^2` or minimize `(gap)^2`. This is Best Fit.\n\n    # Let's consider a different approach inspired by \"First Fit Decreasing\" idea,\n    # but for online. We want to use bins that are \"most appropriate\".\n    # A bin is \"most appropriate\" if it can fit the item and also has\n    # significant remaining capacity to potentially fit other items.\n    # This suggests prioritizing bins that are not too full.\n    # So, among suitable bins, we want to maximize `bins_remain_cap`.\n\n    # Combining Best Fit (minimizing gap) and Maximize Remaining Capacity:\n    # Let's prioritize bins that have the smallest gap AND then, among those with the same gap,\n    # pick the one with the largest remaining capacity (which is the same bin).\n    # This doesn't add much.\n\n    # What if we prioritize bins that have *just enough* capacity?\n    # Consider the ratio: `item / bins_remain_cap[i]`. We want this ratio to be close to 1.\n    # Or, `bins_remain_cap[i] / item`. We want this ratio to be close to 1.\n    # Maximize `bins_remain_cap[i] / item` among suitable bins.\n\n    # Let's try a multi-objective weighted sum.\n    # Objective 1: Maximize (remaining_capacity - item) --> leave more space.\n    # Objective 2: Minimize (remaining_capacity - item) --> tightest fit.\n\n    # If we want to maximize space left, we pick the largest `bins_remain_cap` that fits.\n    # If we want to minimize waste, we pick the smallest `bins_remain_cap` that fits.\n\n    # Let's define a score that is a combination of these.\n    # A simple heuristic could be to prioritize bins that are \"moderately\" full.\n    # This means they have enough capacity for the item, but not an excessive amount.\n    # So, we are looking for `bins_remain_cap[i]` such that `item <= bins_remain_cap[i] < some_threshold`.\n    # And among these, maybe we want the smallest `bins_remain_cap[i]`? (Best Fit).\n\n    # Let's consider the \"slack\" or `bins_remain_cap[i] - item`.\n    # We want this slack to be small (Best Fit).\n    # However, we also want to avoid putting items into bins that are ALMOST full,\n    # because the next item might not fit.\n    # So, let's penalize bins where `bins_remain_cap[i]` is very close to `item`.\n\n    # Let's try a score that penalizes bins that are too full *after* packing.\n    # `bins_remain_cap[i] - item` should not be too small.\n    # So, let's maximize `bins_remain_cap[i] - item`. This is the opposite of Best Fit.\n\n    # How about we combine Best Fit with a \"Next Fit\" like behavior,\n    # favoring bins that are currently \"open\" and have sufficient space.\n    # The \"openness\" can be proxied by how much capacity is available.\n\n    # Let's try prioritizing bins that have a remaining capacity that is\n    # *just enough* for the item.\n    # This means `bins_remain_cap[i]` is close to `item`.\n    # We can measure this by `1 / abs(bins_remain_cap[i] - item + epsilon)`\n    # but this would prioritize exact fits.\n\n    # Let's use a weighted combination of Best Fit and a measure of \"good capacity\".\n    # Score = w1 * (BestFitScore) + w2 * (GoodCapacityScore)\n\n    # Best Fit Score (minimize gap): higher value for smaller gap.\n    # We want to maximize `1 / (gap + epsilon)`\n    best_fit_priority = 1.0 / (gaps + 1e-6)\n\n    # Good Capacity Score: We want bins that have \"enough\" space, but not excessive.\n    # This is hard to define without knowing the distribution of item sizes.\n    # Let's consider favoring bins that have a good amount of remaining capacity,\n    # so they can potentially fit more items later.\n    # This would mean maximizing `bins_remain_cap[i]`.\n    good_capacity_priority = suitable_bins_capacities\n\n    # Adaptive weights:\n    # If bins are scarce (many items, few bins), we want to be more aggressive with tight fits.\n    # If bins are abundant (few items, many bins), we can afford to leave more space.\n    # Let's use the total number of bins used so far as a proxy for scarcity.\n    # However, we don't have the total number of bins.\n    # Let's use the number of available suitable bins as a proxy for how \"easy\" it is to find a fit.\n    # If `len(suitable_bins_indices)` is small, we are in a tighter situation.\n\n    num_suitable = len(suitable_bins_indices)\n    # If few suitable bins, prioritize tightest fit more.\n    # If many suitable bins, prioritize having more remaining space.\n    if num_suitable > 0:\n        # Weight for Best Fit: increases as number of suitable bins decreases\n        w_best_fit = 2.0 / (num_suitable + 1)\n        # Weight for Good Capacity: increases as number of suitable bins increases\n        w_good_capacity = num_suitable / (num_suitable + 1)\n    else:\n        w_best_fit = 0.5\n        w_good_capacity = 0.5\n\n    # Combined score for each suitable bin\n    # We want to maximize the combined score.\n    combined_scores = w_best_fit * best_fit_priority + w_good_capacity * good_capacity_priority\n\n    # Normalize scores to be between 0 and 1 for easier interpretation if needed,\n    # but for argmax it's not strictly necessary.\n    # Let's find the index corresponding to the maximum combined score.\n    if combined_scores.size > 0:\n        best_fit_in_suitable_idx = np.argmax(combined_scores)\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a \"Worst Fit\" strategy with a \"First Fit Decreasing\" like consideration.\n    Prioritizes bins that leave the largest remaining capacity (Worst Fit)\n    but gives a slight advantage to bins that are \"just big enough\" for the item,\n    mimicking the FFD idea of not wasting space on smaller items by putting them\n    in bins that could accommodate larger ones.\n    Also includes a mechanism to favor previously less-used bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    \n    # Calculate 'gaps' for Worst Fit consideration\n    gaps = suitable_bins_capacities - item\n\n    # Calculate a \"tightness\" score: smaller is better (closer to item size)\n    # This encourages putting items into bins that are just large enough,\n    # saving larger bins for potentially larger future items.\n    tightness_scores = suitable_bins_capacities - item # Same as gaps for suitable bins\n\n    # Normalize the tightness scores to be between 0 and 1 (higher is more 'tight')\n    # Avoid division by zero if all suitable bins have the exact same capacity\n    min_gap = np.min(tightness_scores)\n    max_gap = np.max(tightness_scores)\n    \n    normalized_tightness = np.zeros_like(tightness_scores)\n    if max_gap - min_gap > 1e-9: # Avoid division by zero if all gaps are equal\n        normalized_tightness = (max_gap - tightness_scores) / (max_gap - min_gap)\n    else:\n        # If all gaps are the same, all are equally 'tight'\n        normalized_tightness = np.ones_like(tightness_scores) * 0.5 # Neutral value\n\n    # Combine Worst Fit (favoring larger gaps) and Tightness (favoring smaller gaps)\n    # We want to prioritize larger gaps (Worst Fit) but give a boost for 'tight' fits.\n    # Let's try to balance: give a score based on the inverse of the gap (larger gap = higher score for WF)\n    # and add a bonus for tightness.\n    \n    # Score for Worst Fit: higher score for larger gaps\n    worst_fit_scores = np.zeros_like(tightness_scores)\n    if np.max(tightness_scores) > 1e-9: # Avoid division by zero\n      worst_fit_scores = tightness_scores / np.max(tightness_scores)\n    else:\n      worst_fit_scores = np.ones_like(tightness_scores) * 0.5 # Neutral if all gaps are zero\n\n    # Combined score: Primarily Worst Fit, with a bonus for tightness.\n    # Higher score is better.\n    combined_scores = worst_fit_scores + normalized_tightness * 0.3 # Weight for tightness\n\n    # Add a small bonus for bins that were \"less used\" previously.\n    # We can approximate \"less used\" by looking at the current remaining capacity.\n    # Bins with more remaining capacity were perhaps \"less used\" for previous items\n    # or received larger items.\n    # Normalize remaining capacities to get a \"low usage\" score (higher means less used)\n    min_cap = np.min(bins_remain_cap)\n    max_cap = np.max(bins_remain_cap)\n    \n    less_used_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    if max_cap - min_cap > 1e-9:\n        normalized_remaining_cap = (bins_remain_cap - min_cap) / (max_cap - min_cap)\n    else:\n        normalized_remaining_cap = np.ones_like(bins_remain_cap) * 0.5\n\n    # Apply this bonus only to suitable bins\n    less_used_bonus[suitable_bins_indices] = normalized_remaining_cap[suitable_bins_indices] * 0.2 # Weight for less used\n\n    # Assign the combined scores to the priority array\n    priorities[suitable_bins_indices] = combined_scores\n\n    # Add the less_used_bonus\n    priorities += less_used_bonus\n\n    # Ensure we only return positive priorities for suitable bins\n    # This is a safeguard, the calculation should already handle this\n    priorities[~suitable_bins_mask] = 0\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Epsilon-Greedy Best Fit) with Heuristic 6 (Epsilon-Greedy Best Fit, different normalization/combination), Heuristic 1 is simpler and more direct in its epsilon-greedy implementation by choosing either a random bin or the best fit. Heuristic 6's approach of normalizing and then applying weights and epsilon feels more complex and potentially less robust due to the normalization scaling.\n\nComparing Heuristic 7/8 (Epsilon-Greedy Best Fit, simple score) with Heuristic 12/13 (Epsilon-Greedy Best Fit, boosted random choice), Heuristics 12/13's approach of boosting the random choice's priority might make exploration too dominant, potentially hindering exploitation of good fits. Heuristics 7/8's direct use of normalized tight-fit scores in exploitation is cleaner.\n\nComparing Heuristic 9/10/11 (Multi-objective with adaptive utilization and boosted exploration) with Heuristic 18/19 (Multi-objective with adaptive weights based on available bins), Heuristics 18/19 have a more grounded adaptive weighting strategy based on bin availability, which is a better heuristic for adaptivity than the more generic \"boosted exploration\" in 9/10/11. However, the core objectives in 9/10/11 (tight fit + utilization) might be more generally applicable than the specific balancing in 18/19.\n\nComparing Heuristic 1 (simple epsilon-greedy Best Fit) with Heuristic 2/3/4/5 (complex combination of tight fit and \"penalty for empty bins\"), the latter heuristics attempt to incorporate more nuanced ideas like penalizing slack. However, their implementation is convoluted and the interpretation of \"penalty for empty bins\" is not clearly defined or consistently applied, leading to less straightforward logic. Heuristic 1's simplicity and clear strategy make it more robust.\n\nComparing Heuristic 20 (Worst Fit + Tightness bonus) with Heuristic 18/19 (adaptive multi-objective), Heuristic 20's approach is interesting but the combination of Worst Fit and Tightness bonus, along with the \"less used\" bonus, makes it complex and potentially contradictory in its goals. The adaptive weighting in 18/19 is a more coherent approach to balancing objectives.\n\nOverall, simpler heuristics that clearly define and combine strategies (like Heuristic 1) tend to be better than overly complex ones with ambiguously defined components (like 2-5, 18-20).\n- \nHere's a redefined approach to self-reflection for better heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Performance, adaptability, interpretability, validation.\n*   **Advice:** Design heuristics with clear objectives and simple combination mechanisms. Incorporate adaptive exploration and consider smooth, non-linear scaling for fine-tuning. Prioritize vectorized operations for efficiency.\n*   **Avoid:** Ambiguity in objectives, overly complex components, and buggy implementations. Resist relying solely on linear or binary scaling without justification.\n*   **Explanation:** Focus on making heuristics understandable and maintainable. This clarity aids in identifying bottlenecks and opportunities for improvement through controlled experimentation and validation.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}