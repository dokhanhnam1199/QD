{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines the tightness preference with an adaptive \"fill level\" bonus\n    and a sophisticated exploration strategy inspired by multi-armed bandits.\n    Prioritizes tight fits while encouraging the use of partially filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Strategy: Combined Tightness and Fill Bonus ---\n\n    # Tightness score: Inverse of remaining capacity after packing. Higher is better.\n    # This favors bins that leave minimal slack.\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Fill Level Bonus: Rewards bins that are already more utilized.\n    # We approximate utilization by penalizing bins with large remaining capacity.\n    # A simple inverse of the current remaining capacity (for fitting bins) serves as a bonus.\n    # This encourages using bins that are already partially filled, rather than always\n    # picking a \"tight fit\" in a nearly empty bin.\n    fill_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    # Bonus is higher for bins with less remaining capacity (i.e., more filled)\n    fill_bonus[can_fit_mask] = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Combine tightness and fill bonus. Multiplication allows the fill bonus to\n    # modulate the tightness score. A bin that is both tight and already somewhat full\n    # gets a higher combined score.\n    combined_exploitation_score = tightness_score * (1.0 + 0.3 * fill_bonus) # 0.3 is a tunable weight\n\n    # Normalize exploitation scores to the range [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_score)\n    if max_exploitation_score > 0:\n        normalized_exploitation_scores = combined_exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_score)\n\n    # --- Exploration Strategy: Adaptive Exploration (Simplified Bandit-like) ---\n\n    # We want to explore bins that are currently less prioritized to discover\n    # potentially better packing options or to balance load.\n    # A simple adaptive strategy: If a bin has a low exploitation score,\n    # it's a candidate for exploration. We'll assign a small, uniform exploration\n    # priority to all bins that can fit the item. The degree of exploration\n    # can be tied to the number of available fitting bins.\n\n    num_fitting_bins = np.sum(can_fit_mask)\n    exploration_weight = 0.15 # Base weight for exploration\n\n    # Increase exploration tendency if there are many options, decrease if few\n    if num_fitting_bins > 5:\n        exploration_weight *= 1.2\n    elif num_fitting_bins < 3:\n        exploration_weight *= 0.8\n\n    # Create exploration scores. For simplicity here, we'll give a small boost\n    # to all fitting bins, meaning any fitting bin has a chance.\n    # A more advanced approach would use a UCB-like strategy.\n    exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    exploration_scores[can_fit_mask] = exploration_weight / num_fitting_bins\n\n    # --- Final Priority: Blend Exploitation and Exploration ---\n    # A simple blending: priorities = exploitation_scores + exploration_scores\n    # This gives a base priority based on exploitation and adds a small chance\n    # to any fitting bin via exploration.\n    priorities[can_fit_mask] = normalized_exploitation_scores[can_fit_mask] + exploration_scores[can_fit_mask]\n\n    # Ensure no negative priorities and normalize the final output to [0, 1]\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a utilization-aware exploration strategy.\n    Prioritizes tight fits and encourages exploration of less utilized bins.\n    \"\"\"\n    epsilon = 0.05  # Exploration probability\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Calculate a \"tightness\" score (how much space is left after packing)\n    tightness_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        remaining_after_packing = bins_remain_cap[suitable_bins_indices] - item\n        # Higher score for smaller remaining capacity (tighter fit)\n        # Add a small constant to avoid division by zero and ensure positive scores\n        tightness_scores[suitable_bins_indices] = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Calculate a \"utilization\" score (favoring bins that are already more full)\n    # We use the inverse of remaining capacity *before* packing.\n    utilization_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        # Higher score for less remaining capacity (more utilized bin)\n        utilization_scores[suitable_bins_indices] = 1.0 / (bins_remain_cap[suitable_bins_indices] + 1e-9)\n\n    # Combine scores: prioritize tightness, with a bonus for utilization\n    combined_scores = tightness_scores + 0.5 * utilization_scores # Weight utilization less than tightness\n\n    # Normalize scores for the suitable bins to a 0-1 range\n    suitable_scores = combined_scores[suitable_bins_indices]\n    if suitable_scores.size > 0:\n        max_score = np.max(suitable_scores)\n        if max_score > 0:\n            normalized_scores = suitable_scores / max_score\n            priorities[suitable_bins_indices] = normalized_scores\n\n    # Epsilon-greedy exploration: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities\n        priorities[chosen_bin_index] = 1.0 # Assign full priority to the random bin\n    else:\n        # Exploitation: Use the calculated priorities (based on combined scores)\n        # The `priorities` array already holds the normalized scores\n        pass \n\n    return priorities\n\n### Analyze & experience\n- Comparing (Heuristics 1st) vs (Heuristics 2nd), they are identical.\n\nComparing (Heuristics 3rd) vs (Heuristics 4th), Heuristics 4th introduces a \"Fill Level Bonus\" and a more complex exploration strategy. The fill level bonus (penalizing large remaining capacities) in Heuristics 4th is a good addition for utilization. The exploration strategy in Heuristics 4th, while more complex, aims for a more nuanced exploration by adapting to the number of fitting bins, which is a step up.\n\nComparing (Heuristics 4th) vs (Heuristics 5th), they are identical.\n\nComparing (Heuristics 5th) vs (Heuristics 6th), Heuristics 5th uses a simpler exploration (uniform boost to all fitting bins with probability). Heuristics 6th attempts a more sophisticated exploration by boosting bins closer to the mean exploitation score, aiming to explore \"promising but not top\" candidates. This seems more targeted than uniform boosting. Heuristics 6th also uses a weighted sum of tightness and utilization, which is a reasonable way to combine objectives.\n\nComparing (Heuristics 6th) vs (Heuristics 7th), Heuristics 7th is identical to Heuristics 5th. This indicates a lack of progression or differentiation in the provided list for these specific entries.\n\nComparing (Heuristics 7th) vs (Heuristics 8th), they are identical.\n\nComparing (Heuristics 8th) vs (Heuristics 9th), Heuristics 8th has a complex exploration that scales randomly. Heuristics 9th uses a simple epsilon-greedy with a random pick for exploration. Heuristics 9th's combined score (tightness + 0.5 * utilization) is simpler than Heuristics 8th's multiplicative approach. The normalization in 9th is also simpler.\n\nComparing (Heuristics 9th) vs (Heuristics 10th), Heuristics 10th introduces a \"Dynamic Epsilon-Greedy Exploration\" and a more adaptive utilization score, attempting to boost scores of less utilized bins. This is more sophisticated than the simple epsilon-greedy in Heuristics 9th. Heuristics 10th's approach to combining scores and exploring seems more nuanced.\n\nComparing (Heuristics 10th) vs (Heuristics 11th), they are identical.\n\nComparing (Heuristics 11th) vs (Heuristics 12th), Heuristics 11th uses a fixed exploration probability with a scaled random score favoring less utilized bins. Heuristics 12th uses a fixed `exploration_prob` to select a *number* of bins to boost uniformly. Heuristics 12th's combination of tightness and utilization seems more balanced with normalized utilization bonus. The exploration in 12th is simpler (uniform boost) but ensures at least one bin is explored if possible.\n\nComparing (Heuristics 12th) vs (Heuristics 13th), they are identical.\n\nComparing (Heuristics 13th) vs (Heuristics 14th), Heuristics 14th has significantly different imports (random, math, scipy, torch) and unused parameters, indicating it's an incomplete or placeholder heuristic, making direct comparison difficult and marking it as worse due to lack of implementation and unnecessary complexity.\n\nComparing (Heuristics 14th) vs (Heuristics 15th), they are identical and incomplete.\n\nComparing (Heuristics 15th) vs (Heuristics 16th), they are identical and incomplete.\n\nComparing (Heuristics 16th) vs (Heuristics 17th), Heuristics 17th is a complete heuristic with a clear strategy, unlike the incomplete Heuristics 16th. Heuristics 17th has a multiplicative combination of tightness and utilization bonus, and its exploration boosts specific bins.\n\nComparing (Heuristics 17th) vs (Heuristics 18th), they are identical.\n\nComparing (Heuristics 18th) vs (Heuristics 19th), they are identical.\n\nComparing (Heuristics 19th) vs (Heuristics 20th), they are identical.\n\nOverall: Heuristics 1st-2nd and 5th-8th and 11th-13th and 17th-20th are largely identical and represent a solid base combining tightness, utilization bonus, and basic exploration. Heuristics 3rd and 4th/5th introduce more sophisticated utilization bonuses. Heuristics 6th tries a more advanced exploration. Heuristics 10th/11th attempt dynamic exploration. Heuristics 12th/13th offer a structured exploration boost. The most distinct and potentially improved approaches seem to be those in Heuristics 4th/5th (more nuanced utilization), 6th (smarter exploration targeting), and 10th/11th (dynamic exploration). However, due to extensive duplication, the progression is unclear. The latter group of identical heuristics (17th-20th) represents a reasonable, albeit unoriginal, approach. Heuristics 14th-16th are clearly worse due to incompleteness.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective, Normalization, Adaptive Exploration, Robustness.\n*   **Advice:** Focus on designing heuristics that explicitly manage trade-offs between competing objectives using techniques like normalized weighted sums or Pareto-based selection. Implement sophisticated exploration strategies that dynamically adjust based on search progress, such as Thompson sampling or Upper Confidence Bound (UCB).\n*   **Avoid:** Blindly combining objectives with fixed weights that might not generalize. Using brittle or ad-hoc handling of edge cases without a clear fallback strategy. Over-reliance on simple epsilon-greedy for exploration, which can be inefficient.\n*   **Explanation:** This approach emphasizes principled methods for handling multi-objective optimization and exploration, ensuring heuristics are adaptable, less prone to local optima, and better at managing complexity without becoming overly brittle.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}