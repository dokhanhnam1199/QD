```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:

    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9

    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]
    fitting_indices = np.where(can_fit_mask)[0]

    num_fitting_bins = len(fitting_bins_remain_cap)
    
    if num_fitting_bins == 0:
        return priorities

    # Objective 1: Minimize remaining capacity (Tightest Fit)
    remaining_after_packing = fitting_bins_remain_cap - item
    # Higher score for smaller remaining capacity
    tightness_score = -remaining_after_packing 

    # Objective 2: Maximize utilization (prefer bins that are more full)
    # Higher score for bins with less remaining capacity BEFORE packing
    utilization_score = -fitting_bins_remain_cap

    # Combine objectives using a sum. Normalization is crucial here.
    # Let's normalize each objective to be between 0 and 1 over the fitting bins.
    
    # Normalize tightness score: higher is better (less remaining)
    min_tightness = np.min(tightness_score)
    max_tightness = np.max(tightness_score)
    if max_tightness - min_tightness > epsilon:
        normalized_tightness = (tightness_score - min_tightness) / (max_tightness - min_tightness)
    else:
        normalized_tightness = np.ones_like(tightness_score) * 0.5 # All are same, give neutral score

    # Normalize utilization score: higher is better (more full)
    min_utilization = np.min(utilization_score)
    max_utilization = np.max(utilization_score)
    if max_utilization - min_utilization > epsilon:
        normalized_utilization = (utilization_score - min_utilization) / (max_utilization - min_utilization)
    else:
        normalized_utilization = np.ones_like(utilization_score) * 0.5 # All are same, give neutral score

    # Weighted sum of normalized objectives. Weights can be tuned.
    # Let's try to balance tightness and utilization equally for now.
    exploitation_score = 0.5 * normalized_tightness + 0.5 * normalized_utilization

    # Adaptive Exploration: UCB-like approach.
    # We want to explore bins that have good exploitation scores but haven't been chosen often,
    # or bins that have slightly lower exploitation scores but are "under-explored".
    # For simplicity in this context, we'll boost bins that are good candidates but also
    # have relatively higher remaining capacity (meaning they are less "exploited" in terms of filling them up).
    
    # A simple exploration strategy: boost bins with higher remaining capacity among fitting bins.
    # This encourages using bins that haven't been filled much.
    # We can scale this boost based on the item size relative to bin capacity.
    
    # Calculate a bonus based on how much capacity is left *before* packing.
    # Higher remaining capacity suggests more "unused" potential in that bin.
    # We'll normalize this to make it comparable.
    
    # "Exploration potential" - higher means more room to explore in this bin
    exploration_potential = fitting_bins_remain_cap 
    min_exp_pot = np.min(exploration_potential)
    max_exp_pot = np.max(exploration_potential)
    
    exploration_boost_values = np.zeros_like(exploitation_score)

    if max_exp_pot - min_exp_pot > epsilon:
        normalized_exploration_potential = (exploration_potential - min_exp_pot) / (max_exp_pot - min_exp_pot)
        
        # Apply a moderate boost to bins with higher exploration potential.
        # The boost intensity can be a parameter, let's try a simple scaling.
        boost_intensity = 0.2 # A parameter to control how much exploration matters
        exploration_boost_values = boost_intensity * normalized_exploration_potential
    else:
        # If all have same exploration potential, no boost from this factor.
        pass

    # Final combined score
    final_scores_unnormalized = exploitation_score + exploration_boost_values

    # Normalize final scores to be between 0 and 1 for selection
    max_final_score = np.max(final_scores_unnormalized)
    if max_final_score > epsilon:
        priorities[can_fit_mask] = final_scores_unnormalized / max_final_score
    else:
        # Fallback if all scores are zero (e.g., all bins identical)
        priorities[can_fit_mask] = 1.0 / num_fitting_bins if num_fitting_bins > 0 else 0.0
        
    return priorities
```
