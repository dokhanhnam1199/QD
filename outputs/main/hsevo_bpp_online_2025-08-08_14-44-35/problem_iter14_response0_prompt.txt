{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for under-utilized bins,\n    using a novel exploration strategy to balance exploitation and exploration.\n    \"\"\"\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tightest fit with a utilization bonus and adaptive exploration.\n    Favors bins that minimize remaining space, offering a bonus for less utilized bins,\n    and strategically explores less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Slightly higher exploration to balance\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Utilization Bonus ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This encourages spreading items across more bins initially.\n    # A simple score: higher for bins with less current remaining capacity.\n    # Using a base of 1 to avoid making already partially full bins *too* dominant.\n    fill_score_bonus = 0.3 * (1.0 / (valid_bins_remain_cap + epsilon))\n    \n    # Combine tightness and utilization bonus: multiplicative effect\n    # This rewards bins that are both tight and already have some items\n    combined_exploitation_scores = tight_fit_scores * (1.0 + fill_score_bonus)\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Adaptive Exploration ---\n    # With a certain probability, give a boost to randomly selected fitting bins\n    # to explore less optimal choices and avoid getting stuck in local optima.\n    num_fitting_bins = len(valid_bins_remain_cap)\n    exploration_boost = np.zeros_like(normalized_exploitation_scores)\n    \n    # Determine how many bins to \"boost\" for exploration\n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins))) # Ensure at least one bin is explored if possible\n    \n    # Select indices to explore randomly from the fitting bins\n    # Using np.random.choice with replace=False to ensure unique bins are selected for exploration boost\n    explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n    \n    # Assign a uniform high exploration score to these bins, relative to the normalized exploitation scores.\n    # This ensures exploration bins are considered, but their exact priority is context-dependent on overall exploitation scores.\n    exploration_boost[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation and exploration scores.\n    # Adding exploration boost means these bins will have a higher chance of selection.\n    # The `1 - exploration_prob` for exploitation is implicitly handled by not boosting all bins.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_boost\n    \n    # Ensure all priorities are non-negative and normalize them for the fitting bins\n    final_scores_unnormalized[final_scores_unnormalized < 0] = 0\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    \n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # Fallback to uniform probability if all scores are zero (e.g., due to epsilon issues)\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n### Analyze & experience\n- Comparing (Heuristics 1st) vs (Heuristics 2nd), they are identical.\n\nComparing (Heuristics 3rd) vs (Heuristics 4th), Heuristics 4th introduces a \"Fill Level Bonus\" and a more complex exploration strategy. The fill level bonus (penalizing large remaining capacities) in Heuristics 4th is a good addition for utilization. The exploration strategy in Heuristics 4th, while more complex, aims for a more nuanced exploration by adapting to the number of fitting bins, which is a step up.\n\nComparing (Heuristics 4th) vs (Heuristics 5th), they are identical.\n\nComparing (Heuristics 5th) vs (Heuristics 6th), Heuristics 5th uses a simpler exploration (uniform boost to all fitting bins with probability). Heuristics 6th attempts a more sophisticated exploration by boosting bins closer to the mean exploitation score, aiming to explore \"promising but not top\" candidates. This seems more targeted than uniform boosting. Heuristics 6th also uses a weighted sum of tightness and utilization, which is a reasonable way to combine objectives.\n\nComparing (Heuristics 6th) vs (Heuristics 7th), Heuristics 7th is identical to Heuristics 5th. This indicates a lack of progression or differentiation in the provided list for these specific entries.\n\nComparing (Heuristics 7th) vs (Heuristics 8th), they are identical.\n\nComparing (Heuristics 8th) vs (Heuristics 9th), Heuristics 8th has a complex exploration that scales randomly. Heuristics 9th uses a simple epsilon-greedy with a random pick for exploration. Heuristics 9th's combined score (tightness + 0.5 * utilization) is simpler than Heuristics 8th's multiplicative approach. The normalization in 9th is also simpler.\n\nComparing (Heuristics 9th) vs (Heuristics 10th), Heuristics 10th introduces a \"Dynamic Epsilon-Greedy Exploration\" and a more adaptive utilization score, attempting to boost scores of less utilized bins. This is more sophisticated than the simple epsilon-greedy in Heuristics 9th. Heuristics 10th's approach to combining scores and exploring seems more nuanced.\n\nComparing (Heuristics 10th) vs (Heuristics 11th), they are identical.\n\nComparing (Heuristics 11th) vs (Heuristics 12th), Heuristics 11th uses a fixed exploration probability with a scaled random score favoring less utilized bins. Heuristics 12th uses a fixed `exploration_prob` to select a *number* of bins to boost uniformly. Heuristics 12th's combination of tightness and utilization seems more balanced with normalized utilization bonus. The exploration in 12th is simpler (uniform boost) but ensures at least one bin is explored if possible.\n\nComparing (Heuristics 12th) vs (Heuristics 13th), they are identical.\n\nComparing (Heuristics 13th) vs (Heuristics 14th), Heuristics 14th has significantly different imports (random, math, scipy, torch) and unused parameters, indicating it's an incomplete or placeholder heuristic, making direct comparison difficult and marking it as worse due to lack of implementation and unnecessary complexity.\n\nComparing (Heuristics 14th) vs (Heuristics 15th), they are identical and incomplete.\n\nComparing (Heuristics 15th) vs (Heuristics 16th), they are identical and incomplete.\n\nComparing (Heuristics 16th) vs (Heuristics 17th), Heuristics 17th is a complete heuristic with a clear strategy, unlike the incomplete Heuristics 16th. Heuristics 17th has a multiplicative combination of tightness and utilization bonus, and its exploration boosts specific bins.\n\nComparing (Heuristics 17th) vs (Heuristics 18th), they are identical.\n\nComparing (Heuristics 18th) vs (Heuristics 19th), they are identical.\n\nComparing (Heuristics 19th) vs (Heuristics 20th), they are identical.\n\nOverall: Heuristics 1st-2nd and 5th-8th and 11th-13th and 17th-20th are largely identical and represent a solid base combining tightness, utilization bonus, and basic exploration. Heuristics 3rd and 4th/5th introduce more sophisticated utilization bonuses. Heuristics 6th tries a more advanced exploration. Heuristics 10th/11th attempt dynamic exploration. Heuristics 12th/13th offer a structured exploration boost. The most distinct and potentially improved approaches seem to be those in Heuristics 4th/5th (more nuanced utilization), 6th (smarter exploration targeting), and 10th/11th (dynamic exploration). However, due to extensive duplication, the progression is unclear. The latter group of identical heuristics (17th-20th) represents a reasonable, albeit unoriginal, approach. Heuristics 14th-16th are clearly worse due to incompleteness.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective, Normalization, Adaptive Exploration, Robustness.\n*   **Advice:** Focus on designing heuristics that explicitly manage trade-offs between competing objectives using techniques like normalized weighted sums or Pareto-based selection. Implement sophisticated exploration strategies that dynamically adjust based on search progress, such as Thompson sampling or Upper Confidence Bound (UCB).\n*   **Avoid:** Blindly combining objectives with fixed weights that might not generalize. Using brittle or ad-hoc handling of edge cases without a clear fallback strategy. Over-reliance on simple epsilon-greedy for exploration, which can be inefficient.\n*   **Explanation:** This approach emphasizes principled methods for handling multi-objective optimization and exploration, ensuring heuristics are adaptable, less prone to local optima, and better at managing complexity without becoming overly brittle.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}