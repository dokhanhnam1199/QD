[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a utilization bonus and an adaptive exploration strategy.\n    Favors bins that result in minimal slack after packing, with a bonus for bins\n    that are already partially filled, and a probabilistic chance to explore less-used bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    # --- Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing (minimize slack).\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    # Score is inverse of slack; higher score for smaller slack.\n    tightness_score = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Component 2: Utilization Bonus ---\n    # Prefer bins that are already partially filled.\n    # A simple proxy for utilization: higher score for bins with less remaining capacity.\n    # This encourages using bins that are already in use.\n    # Score is inverse of remaining capacity *before* packing.\n    utilization_bonus = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combining Exploitation Scores ---\n    # Combine tightness and utilization bonus. A weighted sum is used.\n    # Tightness is the primary driver, utilization adds a bonus.\n    # We normalize the utilization bonus to prevent it from overpowering the tightness score.\n    # A simple normalization: divide by the maximum possible utilization score (1/epsilon if a bin is empty).\n    # A more stable approach: normalize by the maximum utilization bonus among fitting bins.\n    max_utilization_bonus = np.max(utilization_bonus)\n    normalized_utilization_bonus = utilization_bonus / (max_utilization_bonus + epsilon)\n\n    # The combined exploitation score weighs tightness more heavily.\n    exploitation_score = tightness_score + 0.3 * normalized_utilization_bonus\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(exploitation_score)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_score)\n\n    # --- Component 3: Adaptive Exploration (Epsilon-Greedy) ---\n    # Introduce a small probability to explore less optimal bins.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    exploration_scores = np.zeros_like(normalized_exploitation_scores)\n\n    # Determine how many bins to \"explore\"\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n\n    if num_to_explore > 0:\n        # Select random indices for exploration\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give explored bins a baseline high priority, slightly boosted to ensure consideration.\n        # This ensures some randomness without completely overriding good exploitation choices.\n        exploration_scores[explore_indices] = 0.5\n\n    # --- Final Priority Calculation ---\n    # Combine exploitation scores with exploration scores.\n    # The exploration scores add a boost to randomly selected bins.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_scores\n\n    # Normalize final scores so that the highest priority is 1.0.\n    max_final_score = np.max(final_scores_unnormalized)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / max_final_score\n    else:\n        # Fallback: if all scores are near zero, assign uniform probability to fitting bins.\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.108496210610296,
    "SLOC": 32.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a utilization-aware exploration strategy.\n    Prioritizes tight fits and encourages exploration of less utilized bins.\n    \"\"\"\n    epsilon = 0.05  # Exploration probability\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    # Calculate a \"tightness\" score (how much space is left after packing)\n    tightness_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        remaining_after_packing = bins_remain_cap[suitable_bins_indices] - item\n        # Higher score for smaller remaining capacity (tighter fit)\n        # Add a small constant to avoid division by zero and ensure positive scores\n        tightness_scores[suitable_bins_indices] = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Calculate a \"utilization\" score (favoring bins that are already more full)\n    # We use the inverse of remaining capacity *before* packing.\n    utilization_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    if suitable_bins_indices.size > 0:\n        # Higher score for less remaining capacity (more utilized bin)\n        utilization_scores[suitable_bins_indices] = 1.0 / (bins_remain_cap[suitable_bins_indices] + 1e-9)\n\n    # Combine scores: prioritize tightness, with a bonus for utilization\n    combined_scores = tightness_scores + 0.5 * utilization_scores # Weight utilization less than tightness\n\n    # Normalize scores for the suitable bins to a 0-1 range\n    suitable_scores = combined_scores[suitable_bins_indices]\n    if suitable_scores.size > 0:\n        max_score = np.max(suitable_scores)\n        if max_score > 0:\n            normalized_scores = suitable_scores / max_score\n            priorities[suitable_bins_indices] = normalized_scores\n\n    # Epsilon-greedy exploration: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities\n        priorities[chosen_bin_index] = 1.0 # Assign full priority to the random bin\n    else:\n        # Exploitation: Use the calculated priorities (based on combined scores)\n        # The `priorities` array already holds the normalized scores\n        pass \n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.078579976067022,
    "SLOC": 28.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a tight-fit preference with an adaptive penalty for bins with excessive remaining capacity,\n    using a hybrid exploration/exploitation strategy with score normalization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 0.15  # Probability of exploring less utilized bins\n    \n    can_fit_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Phase: Combine Tight Fit and Penalty for Large Bins ---\n    \n    # Tightness score: Higher for bins leaving less space after packing.\n    # `1.0 / (remaining_space + epsilon)` where `remaining_space = bins_remain_cap - item`.\n    # This favors bins where `bins_remain_cap` is just slightly larger than `item`.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_scores = 1.0 / (remaining_after_packing + 1e-6)\n\n    # Penalty for \"empty\" or overly large bins: Penalize bins whose remaining capacity\n    # is significantly larger than the item size. This encourages using bins that are\n    # already somewhat utilized. We use `1.0 / (1.0 + penalty_weight * initial_capacity)`\n    # for fitting bins, where `penalty_weight` controls the strength of the penalty.\n    # A higher initial capacity results in a lower penalty score, thus reducing priority.\n    penalty_weight = 0.3\n    # Ensure we don't penalize bins that are already a good fit (capacity ~ item) too much.\n    # The penalty is applied to the initial remaining capacity before packing.\n    penalty_scores = 1.0 / (1.0 + penalty_weight * fitting_bins_remain_cap)\n\n    # Combine scores: Multiply tightness by penalty.\n    # This means bins that are tight *and* not excessively large get higher priority.\n    combined_exploitation_scores = tightness_scores * penalty_scores\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > 1e-9: # Avoid division by near zero\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n    \n    priorities[can_fit_mask] = normalized_exploitation_scores\n\n    # --- Exploration Phase: Favor less utilized bins ---\n    # If exploration is triggered, randomly select a bin from those that can fit the item,\n    # biasing towards bins with more remaining capacity (less utilized).\n    if np.random.rand() < epsilon:\n        # Calculate exploration scores for suitable bins: higher score for more remaining capacity.\n        # We use a log transformation to amplify differences in larger capacities,\n        # and add a small constant to handle bins with exactly the item size.\n        exploration_scores = np.log1p(fitting_bins_remain_cap - item + 1e-6)\n        \n        # Normalize exploration scores to form a probability distribution for selection.\n        sum_exploration_scores = np.sum(exploration_scores)\n        if sum_exploration_scores > 1e-9:\n            exploration_probabilities = exploration_scores / sum_exploration_scores\n            chosen_bin_idx_in_suitable = np.random.choice(len(suitable_bins_indices), p=exploration_probabilities)\n            chosen_bin_original_idx = suitable_bins_indices[chosen_bin_idx_in_suitable]\n            # Assign a high priority (e.g., 1.0) to the chosen exploration bin.\n            priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities for pure exploration choice\n            priorities[chosen_bin_original_idx] = 1.0\n        else:\n            # If all exploration scores are zero (e.g., all fitting bins have same capacity),\n            # fall back to a random choice among fitting bins.\n            chosen_bin_original_idx = np.random.choice(suitable_bins_indices)\n            priorities = np.zeros_like(bins_remain_cap, dtype=float)\n            priorities[chosen_bin_original_idx] = 1.0\n    \n    # If not exploring, the 'priorities' array already holds the normalized exploitation scores.\n    \n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.248105305145606,
    "SLOC": 33.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a utilization bonus and adaptive exploration.\n    Favors bins that minimize remaining space, offering a bonus for less utilized bins,\n    and strategically explores less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Slightly higher exploration to balance\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = valid_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Utilization Bonus ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This encourages spreading items across more bins initially.\n    # A simple score: higher for bins with less current remaining capacity.\n    # Using a base of 1 to avoid making already partially full bins *too* dominant.\n    fill_score_bonus = 0.3 * (1.0 / (valid_bins_remain_cap + epsilon))\n    \n    # Combine tightness and utilization bonus: multiplicative effect\n    # This rewards bins that are both tight and already have some items\n    combined_exploitation_scores = tight_fit_scores * (1.0 + fill_score_bonus)\n\n    # Normalize exploitation scores to be between 0 and 1\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Adaptive Exploration ---\n    # With a certain probability, give a boost to randomly selected fitting bins\n    # to explore less optimal choices and avoid getting stuck in local optima.\n    num_fitting_bins = len(valid_bins_remain_cap)\n    exploration_boost = np.zeros_like(normalized_exploitation_scores)\n    \n    # Determine how many bins to \"boost\" for exploration\n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins))) # Ensure at least one bin is explored if possible\n    \n    # Select indices to explore randomly from the fitting bins\n    # Using np.random.choice with replace=False to ensure unique bins are selected for exploration boost\n    explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n    \n    # Assign a uniform high exploration score to these bins, relative to the normalized exploitation scores.\n    # This ensures exploration bins are considered, but their exact priority is context-dependent on overall exploitation scores.\n    exploration_boost[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation and exploration scores.\n    # Adding exploration boost means these bins will have a higher chance of selection.\n    # The `1 - exploration_prob` for exploitation is implicitly handled by not boosting all bins.\n    final_scores_unnormalized = normalized_exploitation_scores + exploration_boost\n    \n    # Ensure all priorities are non-negative and normalize them for the fitting bins\n    final_scores_unnormalized[final_scores_unnormalized < 0] = 0\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    \n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # Fallback to uniform probability if all scores are zero (e.g., due to epsilon issues)\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.9760670123653865,
    "SLOC": 30.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for under-utilized bins,\n    using a novel exploration strategy to balance exploitation and exploration.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_factor = 0.2  # Controls the degree of exploration\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exploitation Component: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    # Higher score for smaller remaining space.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Exploitation Component: Utilization Penalty ---\n    # Penalize bins that are significantly under-utilized (i.e., have a lot of remaining capacity).\n    # This encourages filling existing bins before opening new ones.\n    # We use the inverse of the *initial* remaining capacity as a proxy for utilization.\n    # Higher utilization (less remaining capacity) gets a higher score.\n    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Weighted sum of tight fit and utilization scores.\n    # Giving slightly more weight to tight fit as it's the primary goal of BPP.\n    combined_exploitation_scores = 0.6 * tight_fit_scores + 0.4 * utilization_scores\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Exploration Component: Adaptive Exploration ---\n    # We want to explore bins that are not necessarily the best according to exploitation.\n    # Instead of random choice, we will boost the scores of bins that are \"average\" or slightly below average\n    # in terms of exploitation, giving them a chance.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    \n    # Calculate a baseline score (e.g., mean exploitation score)\n    mean_exploitation_score = np.mean(normalized_exploitation_scores)\n    \n    # Create exploration scores: higher for bins closer to the mean exploitation score.\n    # This aims to explore \"promising but not top\" candidates.\n    # We add a small constant to ensure even the lowest scores get some exploration boost if needed.\n    exploration_scores = np.exp(-((normalized_exploitation_scores - mean_exploitation_score) / (mean_exploitation_score + epsilon))**2)\n    \n    # Apply the exploration factor to blend exploitation and exploration\n    # The final priority is a mix:\n    # (1 - exploration_factor) * exploitation_scores + exploration_factor * exploration_scores\n    # This ensures that exploration never completely overrides exploitation,\n    # and also that exploitation still has a significant impact.\n    final_priorities_unnormalized = (1 - exploration_factor) * normalized_exploitation_scores + exploration_factor * exploration_scores\n\n    # Normalize final priorities for the fitting bins\n    sum_final_priorities = np.sum(final_priorities_unnormalized)\n    if sum_final_priorities > epsilon:\n        priorities[can_fit_mask] = final_priorities_unnormalized / sum_final_priorities\n    else:\n        # If all scores are zero, distribute probability equally among fitting bins\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 27.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines the tightness preference with an adaptive \"fill level\" bonus\n    and a sophisticated exploration strategy inspired by multi-armed bandits.\n    Prioritizes tight fits while encouraging the use of partially filled bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bins can fit, return all zeros\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # --- Exploitation Strategy: Combined Tightness and Fill Bonus ---\n\n    # Tightness score: Inverse of remaining capacity after packing. Higher is better.\n    # This favors bins that leave minimal slack.\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Fill Level Bonus: Rewards bins that are already more utilized.\n    # We approximate utilization by penalizing bins with large remaining capacity.\n    # A simple inverse of the current remaining capacity (for fitting bins) serves as a bonus.\n    # This encourages using bins that are already partially filled, rather than always\n    # picking a \"tight fit\" in a nearly empty bin.\n    fill_bonus = np.zeros_like(bins_remain_cap, dtype=float)\n    # Bonus is higher for bins with less remaining capacity (i.e., more filled)\n    fill_bonus[can_fit_mask] = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # Combine tightness and fill bonus. Multiplication allows the fill bonus to\n    # modulate the tightness score. A bin that is both tight and already somewhat full\n    # gets a higher combined score.\n    combined_exploitation_score = tightness_score * (1.0 + 0.3 * fill_bonus) # 0.3 is a tunable weight\n\n    # Normalize exploitation scores to the range [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_score)\n    if max_exploitation_score > 0:\n        normalized_exploitation_scores = combined_exploitation_score / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_score)\n\n    # --- Exploration Strategy: Adaptive Exploration (Simplified Bandit-like) ---\n\n    # We want to explore bins that are currently less prioritized to discover\n    # potentially better packing options or to balance load.\n    # A simple adaptive strategy: If a bin has a low exploitation score,\n    # it's a candidate for exploration. We'll assign a small, uniform exploration\n    # priority to all bins that can fit the item. The degree of exploration\n    # can be tied to the number of available fitting bins.\n\n    num_fitting_bins = np.sum(can_fit_mask)\n    exploration_weight = 0.15 # Base weight for exploration\n\n    # Increase exploration tendency if there are many options, decrease if few\n    if num_fitting_bins > 5:\n        exploration_weight *= 1.2\n    elif num_fitting_bins < 3:\n        exploration_weight *= 0.8\n\n    # Create exploration scores. For simplicity here, we'll give a small boost\n    # to all fitting bins, meaning any fitting bin has a chance.\n    # A more advanced approach would use a UCB-like strategy.\n    exploration_scores = np.zeros_like(bins_remain_cap, dtype=float)\n    exploration_scores[can_fit_mask] = exploration_weight / num_fitting_bins\n\n    # --- Final Priority: Blend Exploitation and Exploration ---\n    # A simple blending: priorities = exploitation_scores + exploration_scores\n    # This gives a base priority based on exploitation and adds a small chance\n    # to any fitting bin via exploration.\n    priorities[can_fit_mask] = normalized_exploitation_scores[can_fit_mask] + exploration_scores[can_fit_mask]\n\n    # Ensure no negative priorities and normalize the final output to [0, 1]\n    priorities = np.maximum(0, priorities)\n    max_final_priority = np.max(priorities)\n    if max_final_priority > 0:\n        priorities /= max_final_priority\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 32.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with adaptive penalty for large bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    Uses score normalization for robust comparison.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # Tightness score: Higher for bins leaving less space after packing.\n    # This is equivalent to prioritizing bins closer to 'item' size from below.\n    tightness_score = 1.0 / (fitting_bins_remain_cap - item + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of bins that are \"too empty\" for the current item.\n    # A simple way is to use an inverse relationship with the initial remaining capacity.\n    # The factor penalizes bins with large remaining capacities, favoring those already somewhat utilized.\n    # This is analogous to prioritizing bins that are not \"too empty\".\n    penalty_weight = 0.3\n    penalty_score = 1.0 / (1.0 + penalty_weight * fitting_bins_remain_cap)\n\n    # Combine tightness and penalty. A higher combined score means a bin is both a tight fit\n    # and not excessively large (i.e., not \"too empty\").\n    combined_score = tightness_score * penalty_score\n\n    # Normalize scores to a 0-1 range for stable comparison across different item/bin states.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score / max_score\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a normalized tight-fitting score with adaptive exploration favoring\n    less utilized bins, using a dynamic probability based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    suitable_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    num_suitable_bins = suitable_bins_remain_cap.size\n\n    # Component 1: Tight Fitting Score (normalized)\n    remaining_after_packing = suitable_bins_remain_cap - item\n    # Higher score for smaller remaining capacity (tighter fit). Add small epsilon for stability.\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n    \n    # Normalize tight fit scores to be between 0 and 1\n    max_tight_fit_score = np.max(tight_fit_scores)\n    if max_tight_fit_score > 1e-9:\n        normalized_tight_fit_scores = tight_fit_scores / max_tight_fit_score\n    else:\n        normalized_tight_fit_scores = np.zeros_like(tight_fit_scores)\n\n    # Component 2: Adaptive Exploration favoring less utilized bins\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1.0 + num_suitable_bins))\n    \n    # Generate exploration scores: Favor bins with more remaining capacity (less utilized)\n    # Use log1p for non-linear scaling, making differences more pronounced for smaller capacities\n    exploration_scores = np.log1p(suitable_bins_remain_cap - item + 1e-9)\n    \n    # Combine normalized tight fit score with exploration score for the final priority\n    # We want to favor tighter fits but have an exploration mechanism.\n    # A simple weighted sum can work, where exploration probability influences the choice.\n    \n    # For exploitation (when not exploring), use the normalized tight fit score.\n    exploitation_priorities = normalized_tight_fit_scores\n\n    # For exploration, we need to select a bin probabilistically.\n    # The 'exploration_scores' are used to define the probability distribution for selection.\n    # Normalize exploration scores to create probabilities.\n    sum_exploration_scores = np.sum(exploration_scores)\n    if sum_exploration_scores > 1e-9:\n        exploration_probabilities = exploration_scores / sum_exploration_scores\n    else:\n        # If all exploration scores are zero (e.g., all bins have item + epsilon capacity),\n        # fall back to uniform probability.\n        exploration_probabilities = np.ones(num_suitable_bins) / num_suitable_bins\n\n    # Determine if we explore or exploit\n    if np.random.rand() < exploration_prob:\n        # Exploration: Choose a bin based on exploration probabilities\n        chosen_bin_idx_in_subset = np.random.choice(num_suitable_bins, p=exploration_probabilities)\n        # Assign a high priority to the explored bin\n        priorities[can_fit_mask][chosen_bin_idx_in_subset] = 1.0\n    else:\n        # Exploitation: Assign priorities based on the normalized tight fit scores\n        priorities[can_fit_mask] = exploitation_priorities\n        \n        # Normalize final priorities to ensure they are in a comparable range if multiple bins are chosen\n        max_final_priority = np.max(priorities[can_fit_mask])\n        if max_final_priority > 1e-9:\n            priorities[can_fit_mask] /= max_final_priority\n        else:\n            priorities[can_fit_mask] = np.zeros_like(priorities[can_fit_mask])\n\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 34.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with adaptive bin utilization and dynamic exploration.\n    Prioritizes bins that minimize waste and are more utilized, with exploration\n    favoring less utilized bins based on a dynamically adjusted probability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    available_bin_indices = np.where(can_fit_mask)[0]\n\n    # --- Core Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Score is inverse of remaining space after packing.\n    remaining_after_packing = available_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Adaptive Bin Utilization: Score based on how \"full\" the bin is.\n    # Prefer bins that have less remaining capacity relative to the item size.\n    # This can be approximated by `item / available_bins_remain_cap` or similar.\n    # Let's use a score that is higher for bins that are less empty (i.e., lower remaining capacity).\n    # A simple approach is `1.0 / (available_bins_remain_cap + epsilon)`.\n    # To make it more adaptive, we can consider the *average* remaining capacity of fitting bins.\n    avg_fitting_remain_cap = np.mean(available_bins_remain_cap) if available_bins_remain_cap.size > 0 else 1.0\n    # Prefer bins with remaining capacity closer to the item or lower than average.\n    # Let's try a score that is high when remaining capacity is low (closer to item).\n    utilization_scores = 1.0 / (available_bins_remain_cap + epsilon)\n\n    # Combine the two heuristic scores. A weighted sum is a common approach.\n    # Let's give equal weight for now.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be between 0 and 1.\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Dynamic Epsilon-Greedy Exploration ---\n    num_available_bins = available_bins_remain_cap.size\n\n    # Determine exploration probability adaptively.\n    # A common strategy is to increase exploration when there are many bins,\n    # to encourage trying out different options. Or, explore less if bins are very full.\n    # Let's use a simple inverse relationship with the number of available bins to explore more\n    # when there are fewer options, which can be counter-intuitive.\n    # A better approach: explore more when there are *many* bins to choose from, to diversify.\n    # Or, explore less when bins are already quite full, and more when they are mostly empty.\n\n    # A simple dynamic probability: decrease exploration as the number of available bins decreases.\n    # Or, use the inverse of the average utilization as a proxy for how \"challenging\" the packing is.\n    # Let's use a probability that decreases with the tightness of the fit (smaller remaining_after_packing).\n    # This means we explore more when the fits are not tight.\n    \n    # Heuristic from analysis: dynamic exploration favoring less utilized bins.\n    # We want to explore bins that are NOT among the top-ranked by core heuristics.\n    # The probability of exploration for a bin could be inversely related to its normalized core score.\n\n    # Simple dynamic exploration probability: higher when many bins, lower when few.\n    # Or, a fixed probability for simplicity if dynamic is too complex to implement robustly here.\n    # Given the prompt's goal of combining elements, let's use a fixed epsilon-greedy\n    # but slightly modified based on utilization to influence *which* bins are explored.\n\n    exploration_prob = 0.15 # Base exploration probability\n\n    # We want to assign exploration scores to a subset of bins.\n    # Instead of random scores, let's boost scores of bins that are less utilized.\n    # A simple way to select bins for \"exploration\" (or a modified score):\n    # bins with higher `utilization_scores` (meaning they are more utilized)\n    # should be less likely to be perturbed by random exploration.\n    # So, bins with lower `utilization_scores` are more prone to exploration.\n\n    # Create a probability mask for exploration, favoring less utilized bins.\n    # Let's sort bins by utilization_scores (ascending) and pick a fraction.\n    # A simple approach: assign a higher 'random' score to bins that are less utilized.\n    \n    # Let's stick to a blended approach inspired by Heuristic 11, where exploration\n    # contributes to the final score.\n    \n    # For the exploration component, we can generate random scores.\n    # To bias exploration towards less utilized bins, we can scale these random scores.\n    # If `utilization_scores` is low (meaning bin is less utilized), we want a higher random score.\n    # We can achieve this by multiplying `random_scores` by `(1.0 / (utilization_scores + epsilon))`.\n    \n    random_exploration_scores = np.random.rand(num_available_bins)\n    \n    # Scale random scores: boost scores for less utilized bins (low utilization_scores).\n    # Higher `boost_factor` means more boost for less utilized bins.\n    boost_factor = 2.0 # Control how much less utilized bins are favored in exploration\n    scaled_random_scores = random_exploration_scores * (1.0 + boost_factor * (1.0 - utilization_scores))\n    \n    # Normalize these scaled random scores to keep them in a comparable range.\n    max_scaled_random_score = np.max(scaled_random_scores)\n    if max_scaled_random_score > epsilon:\n        normalized_exploration_scores = scaled_random_scores / max_scaled_random_score\n    else:\n        normalized_exploration_scores = np.zeros_like(scaled_random_scores)\n\n    # Blend the deterministic (core) scores with the exploration scores.\n    # Use the `exploration_prob` to decide the mixing weight.\n    alpha = exploration_prob\n    final_scores = (1 - alpha) * normalized_core_scores + alpha * normalized_exploration_scores\n\n    # Re-normalize the final blended scores to ensure they are in a 0-1 range.\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        # Fallback if all scores are zero or near-zero\n        priorities[can_fit_mask] = final_scores\n\n    # Ensure priorities are non-negative.\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 37.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit and adaptive utilization, with dynamic exploration\n    favoring less utilized bins for better overall packing.\n    \"\"\"\n    epsilon = 1e-9\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    fitting_bins_indices = np.where(can_fit_mask)[0]\n    fitting_bins_capacities = bins_remain_cap[fitting_bins_indices]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    remaining_after_packing = fitting_bins_capacities - item\n    # Score is inversely proportional to the remaining gap, encouraging tighter fits.\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    max_total_capacity = np.max(bins_remain_cap) if bins_remain_cap.size > 0 else 0\n    # If all bins are empty or have very little capacity, treat max_total_capacity carefully.\n    if max_total_capacity < epsilon:\n        max_total_capacity = 1.0 # Avoid division by zero if bins are very small\n\n    # Calculate utilization score: higher for bins with more *absolute* remaining space.\n    # This is a proxy for less utilized bins.\n    utilization_scores = fitting_bins_capacities / (max_total_capacity + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Combine tight fit and utilization scores. A weighted sum is used.\n    # The weights can be tuned, here we give equal importance.\n    exploitation_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize exploitation scores to be between 0 and 1.\n    max_exploitation_score = np.max(exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(exploitation_scores)\n\n    # --- Adaptive Exploration ---\n    # Introduce exploration by randomly selecting some fitting bins and boosting their scores.\n    num_fitting_bins = len(fitting_bins_indices)\n    \n    # Exploration probability adapts: higher if few options, lower if many.\n    if num_fitting_bins <= 2:\n        exploration_prob = 0.3  # More exploration when options are scarce\n    elif num_fitting_bins > 5:\n        exploration_prob = 0.1  # Less exploration when many options exist\n    else:\n        exploration_prob = 0.2  # Moderate exploration\n\n    # Determine how many bins to explore.\n    num_to_explore = max(1, int(np.floor(exploration_prob * num_fitting_bins)))\n    \n    # Ensure we don't try to explore more bins than available.\n    num_to_explore = min(num_to_explore, num_fitting_bins)\n\n    # Select random indices from the fitting bins for exploration.\n    explore_indices_in_fitting = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n\n    # Create exploration bonuses. These are added to the exploitation scores.\n    # Bins selected for exploration get a significant boost.\n    exploration_bonuses = np.zeros_like(normalized_exploitation_scores)\n    exploration_bonuses[explore_indices_in_fitting] = 1.0 # High bonus for exploration bins\n\n    # --- Final Priority Calculation ---\n    # Combine exploitation and exploration scores.\n    final_scores = normalized_exploitation_scores + exploration_bonuses\n\n    # Assign final priorities to the original bin indices.\n    priorities[fitting_bins_indices] = final_scores\n\n    # Normalize all priorities so they sum to 1, effectively becoming probabilities.\n    sum_priorities = np.sum(priorities)\n    if sum_priorities > epsilon:\n        priorities /= sum_priorities\n    else:\n        # Fallback: if all scores are zero, assign uniform probability to fitting bins.\n        if num_fitting_bins > 0:\n            priorities[fitting_bins_indices] = 1.0 / num_fitting_bins\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 84.63302752293579,
    "SLOC": 41.0,
    "cyclomatic_complexity": 9.0,
    "exec_success": true
  }
]