{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Calculates priorities for packing an item into bins using a Softmax-Based Fit strategy.\n\n    This strategy assigns higher priority to bins that are a good fit for the item,\n    meaning bins with remaining capacity close to the item's size. A smaller\n    difference between bin capacity and item size results in a higher score.\n    Softmax is used to convert these scores into probabilities (priorities).\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A NumPy array representing the remaining capacity of each bin.\n\n    Returns:\n        A NumPy array of the same size as bins_remain_cap, where each element\n        is the priority score for placing the item into the corresponding bin.\n    \"\"\"\n    # Calculate the difference between remaining capacity and item size.\n    # We are interested in bins where remaining capacity is >= item size.\n    # For bins where remaining capacity is less than item size, the difference will be negative.\n    # Adding a small epsilon to avoid issues with log(0) if capacity is exactly item size.\n    differences = bins_remain_cap - item + 1e-9\n\n    # We want to prioritize bins where the difference is small (good fit).\n    # A larger difference means a worse fit. We can use the negative difference\n    # to effectively treat smaller positive differences as \"more positive\".\n    # We'll filter out bins where capacity < item size by making their score very low.\n    # A large negative number will result in a very small exponent in softmax.\n    scores = np.where(differences >= 0, -differences, -1e9)\n\n    # Apply the softmax function to convert scores into probabilities (priorities)\n    # Softmax: exp(score_i) / sum(exp(score_j))\n    # This will naturally give higher probabilities to bins with smaller (less negative) scores.\n    exp_scores = np.exp(scores)\n    priorities = exp_scores / np.sum(exp_scores)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    priorities = np.zeros_like(bins_remain_cap)\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + 1e-9)\n    return priorities\n\n### Analyze & experience\n- Comparing heuristic 1 (Softmax-Based Fit) with heuristic 2 (Inverse Remaining Capacity, exact fit priority), heuristic 1 offers a more nuanced prioritization by converting differences into probabilities, allowing for a smoother selection and potential exploration. Heuristic 2 rigidly assigns 1.0 or 1.0 / (difference + epsilon), which can lead to abrupt shifts in priority.\n\nComparing heuristic 3 (Almost Full Fit with penalty) with heuristic 4 (identical to 2), heuristic 3 introduces a penalty for empty bins which might not always be desirable. Heuristic 2 focuses solely on minimizing remaining capacity after fitting.\n\nComparing heuristic 5 (Inverse Remaining Capacity, vectorized) and heuristic 6 (Inverse Remaining Capacity, looped) with heuristic 7 (identical to 6), heuristics 5, 6, and 7 all implement the inverse remaining capacity strategy. Heuristic 5 is more performant due to vectorization. Heuristic 8 is identical to 6 and 7.\n\nComparing heuristic 9 (Epsilon-Greedy with Best Fit) with heuristic 10 (Inverse Distance, vectorized) and heuristic 11 (Sigmoid Fit Ratio), heuristic 9 introduces exploration via epsilon-greedy, which is a powerful meta-heuristic. Heuristic 10 is a standard inverse distance approach. Heuristic 11 uses a sigmoid on fit ratios, which can be sensitive to scaling.\n\nComparing heuristic 12 and 13 (First Fit with boosted priority) with heuristic 14 and 15 (Sigmoid on gaps), heuristics 12 and 13 artificially boost the priority of the *first* fit, which is not always optimal and might miss better fits later in the list. Heuristics 14 and 15 use a sigmoid on the gap, offering a smoother prioritization than a simple binary or boosted value.\n\nComparing heuristic 16 (Sigmoid on fit ratios, shifted) with heuristic 17 (Proportional Remaining Capacity) and heuristic 18 (Difference as priority, negative infinity for no fit), heuristic 16 uses a sigmoid on fit ratios, similar to 11 but with a different shift. Heuristic 17 prioritizes bins based on the proportion of remaining capacity, which might not best capture the \"tightest\" fit. Heuristic 18 directly uses the difference, with a strong penalty for non-fits, which is a simple but effective approach.\n\nComparing heuristic 19 and 20 (Difference with random noise) with heuristic 18, heuristics 19 and 20 add Gaussian noise to the difference, aiming for exploration. However, adding noise to the *difference* can make suboptimal choices more likely, and the magnitude of the noise is data-dependent. Heuristic 18's clear prioritization of minimum difference is more direct.\n\nOverall: Softmax-based (1) and those using inverse remaining capacity (5, 6, 7, 8, 10) generally provide good prioritization by favoring tighter fits. Epsilon-greedy (9) adds a valuable exploration component. Sigmoid-based approaches (11, 14, 15, 16) offer tunable sensitivities but can be complex. Artificial boosting (12, 13) is less robust. Pure difference (18) is simple and effective. Noise addition (19, 20) can be unpredictable.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Adaptive Exploration, Multi-objective Balancing, Dynamic Scaling, Algorithmic Synergy.\n*   **Advice:** Focus on heuristics that dynamically adjust their exploration/exploitation balance based on observed performance and problem characteristics. Explore how to combine or nest heuristics (meta-heuristics) to leverage their complementary strengths.\n*   **Avoid:** Rigid, one-size-fits-all parameter settings. Over-reliance on single-dimension optimization without considering broader search space dynamics.\n*   **Explanation:** Effective self-reflection involves understanding *when* and *why* certain heuristic components perform well, enabling the design of more adaptive and robust strategies that can navigate complex search landscapes efficiently.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}