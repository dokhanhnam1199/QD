```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines Best Fit with a novel "nearness" score and adaptive exploration.
    Prioritizes bins that are a good fit, but also considers bins that are "almost"
    a good fit to prevent situations where only very large bins are left.
    The exploration strategy adapts based on the number of suitable bins.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    suitable_bins_mask = bins_remain_cap >= item

    suitable_bins_indices = np.where(suitable_bins_mask)[0]

    if suitable_bins_indices.size == 0:
        return priorities

    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]
    gaps = suitable_bins_capacities - item

    # Adaptive exploration: increase exploration probability if there are many suitable bins
    # and decrease if there are very few to avoid wasting opportunities.
    num_suitable_bins = suitable_bins_indices.size
    if num_suitable_bins <= 2:
        exploration_prob = 0.2  # Higher exploration for very few options
    elif num_suitable_bins > 5:
        exploration_prob = 0.05 # Lower exploration for many options
    else:
        exploration_prob = 0.1 # Default exploration

    if np.random.rand() < exploration_prob:
        chosen_bin_index = np.random.choice(suitable_bins_indices)
        priorities[chosen_bin_index] = 1.0
    else:
        # Exploitation: Combine Best Fit with a "nearness" score.
        # The "nearness" score is higher for bins that are close to the optimal gap,
        # but not so close that they become inefficient.
        # We use a scaled inverse of the gap + a small constant to avoid division by zero,
        # and a sigmoid-like scaling to smooth the priority.
        
        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits
        gaps_for_scoring = gaps + 1e-6 
        
        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps
        # that might be too tight. We use a form of smooth inverse.
        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.
        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.
        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.
        
        # Let's use a score that is higher for smaller gaps, but tapers off.
        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.
        # We'll use `exp(-k * gap)` for a smoother, non-linear response.
        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap
        nearness_scores = np.exp(-k_factor * gaps)

        # Normalize scores so the maximum is 1
        if np.max(nearness_scores) > 0:
            normalized_scores = nearness_scores / np.max(nearness_scores)
        else:
            normalized_scores = np.zeros_like(nearness_scores)

        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.
        # Or, we can simply use the nearness_scores directly if they are well-calibrated.
        # For this version, let's directly use the normalized nearness scores as priorities.
        
        # Get the index of the bin with the highest nearness score
        best_nearness_idx_in_suitable = np.argmax(normalized_scores)
        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]
        
        priorities[best_nearness_original_idx] = 1.0

    return priorities
```
