[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for unused bins,\n    favoring bins that leave less space and are already in use.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for \"empty\" bins: encourage using partially filled bins.\n    # We define \"empty\" as bins with a large remaining capacity (e.g., > 75% of max observed).\n    # This encourages filling existing bins before opening new ones.\n    \n    # First, find a baseline for \"large remaining capacity\".\n    # We can use the maximum remaining capacity among *fitting* bins as a reference.\n    # If no bins fit, this part is skipped.\n    if fitting_bins_remain_cap.size > 0:\n        max_fitting_capacity = np.max(fitting_bins_remain_cap)\n        \n        # Identify bins that are \"empty\" or significantly underutilized\n        # A bin is considered \"empty\" if its remaining capacity is substantially large.\n        # Let's use a threshold, e.g., 75% of the max fitting capacity.\n        # This heuristic aims to penalize bins that are \"too large\" for the current item,\n        # and more importantly, to prefer bins that are already in use.\n        \n        # Penalty factor: reduce priority for bins with high remaining capacity.\n        # We want to down-weight bins that have a lot of \"slack\".\n        # The inverse of (1 + slack_penalty_factor * remaining_capacity) can work.\n        # A simpler approach derived from \"penalty for empty bins\" is to reduce the score\n        # of bins that are still \"full\" (i.e., have lots of remaining capacity).\n        \n        # Let's define a \"utilization score\" which is inverse of remaining capacity.\n        # A bin that is almost full has a high utilization score.\n        # High utilization is preferred.\n        \n        utilization_score = np.zeros_like(bins_remain_cap, dtype=float)\n        # We consider the inverse of the *initial* remaining capacity for utilization.\n        # Higher value means more utilized (less remaining capacity initially).\n        # We apply this only to fitting bins.\n        utilization_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n\n        # Combine tightness and utilization.\n        # We want both: small remaining space *after* packing (tightness)\n        # AND high initial utilization (prefer fuller bins).\n        # Multiplying them seems reasonable: prioritize bins that are already full AND become tight.\n        combined_score = tightness_score * utilization_score\n        \n        # Normalize scores to be in a similar range, e.g., [0, 1]\n        max_score = np.max(combined_score)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fitting with adaptive exploration and dynamic scoring.\n    Prioritizes bins that minimize waste and are more utilized, with a\n    probabilistic chance to explore less optimal but fitting bins,\n    dynamically adjusting exploration based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    # Use a small constant to prevent division by zero and give higher score to tighter fits\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + 1e-9) / (max_remaining_overall + 1e-9)\n\n    # Combine core heuristic scores: balanced preference\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be in a comparable range\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > 1e-9:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: Dynamically adjust exploration probability\n    num_available_bins = available_bins_remain_cap.size\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1 + num_available_bins))\n    \n    final_scores = np.copy(normalized_core_scores)\n\n    if np.random.rand() < exploration_prob:\n        # Select a random bin among those that can fit the item\n        random_index_in_subset = np.random.randint(0, num_available_bins)\n        # Boost the score of the randomly chosen bin to make exploration significant\n        # Use a boost factor relative to the best core score to make it competitive\n        boost_factor = 1.5 # Boost exploration picks\n        final_scores[random_index_in_subset] += np.max(normalized_core_scores) * boost_factor\n\n    # Re-normalize final scores to ensure they are between 0 and 1\n    max_final_score = np.max(final_scores)\n    if max_final_score > 1e-9:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a robust Best Fit strategy with an adaptive penalty for slack.\n    Prioritizes bins that offer the tightest fit, with a penalty for\n    bins that leave excessive remaining capacity, encouraging better space utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Calculate the gap for best fit (lower is better)\n    gaps = suitable_bins_capacities - item\n\n    # Calculate a penalty for bins with large remaining capacity (slack).\n    # We want to penalize larger gaps to encourage tighter packing.\n    # A simple penalty could be the gap itself, so we want to MINIMIZE gap.\n    # To convert to a priority score (higher is better), we can use 1 / (gap + epsilon).\n    # However, if we want to PENALIZE large gaps, we can subtract the gap from a base value\n    # or use a function that decreases with gap size.\n    # Let's consider a score where we want to MAXIMIZE `-(gap)^2` to favor small gaps.\n    # A simpler approach for penalizing large gaps is to ensure the priority decreases\n    # as the gap increases.\n    # Let's use a scoring system that directly favors smaller gaps.\n    # Score = 1 / (gap + epsilon) aims to maximize preference for smallest gap.\n\n    # Let's introduce a penalty for bins that have *too much* remaining capacity.\n    # This \"too much\" can be relative. A bin with capacity 100 and item 10 has gap 90.\n    # A bin with capacity 20 and item 10 has gap 10.\n    # The gap of 10 is better for tight fit.\n    # If we want to penalize the gap of 90, we can use a function that decreases rapidly.\n    # A linear penalty might be `penalty_weight * gap`.\n    # Let's try to score bins by `(some_value - gap)`.\n    # A common approach is to use the negative of the gap, or `1/(gap + epsilon)`.\n\n    # Let's combine Best Fit (minimize gap) with an incentive to not leave too much space.\n    # This can be achieved by rewarding bins that have a remaining capacity closer to the item size.\n    # Maximize `-(gap)^2` is one way to do this.\n    # Or, let's consider the inverse of the remaining capacity after packing.\n    # We want to maximize the *fullness* of the bin after packing.\n    # This would mean maximizing `1 / (gap + epsilon)`.\n    # This is essentially Best Fit.\n\n    # What if we want to penalize bins that have a *very* large gap?\n    # For example, if `gap > item * threshold`.\n    # Let's consider a score that heavily favors small gaps but also slightly penalizes very large gaps.\n    # A function like `1 / (gap + 1)` would give higher scores to smaller gaps.\n    # To penalize large gaps, we can ensure the score doesn't increase linearly with `1/gap`.\n    # Let's consider the efficiency: `item / bins_remain_cap[i]`. Maximize this.\n    # This is equivalent to minimizing `bins_remain_cap[i] / item`.\n    # And thus, minimizing `bins_remain_cap[i] - item`.\n\n    # A good heuristic might be to prioritize bins that have a small gap,\n    # but not necessarily the absolute smallest if that leaves very little room.\n    # Let's try to combine Best Fit (minimize gap) with a factor that\n    # penalizes large gaps.\n    # We want to maximize a score that is high for small gaps.\n    # Score: `1.0 / (gaps + 1e-6)` is a strong Best Fit.\n    # To penalize large gaps, we can apply a discount to this score for large gaps.\n    # Or, consider `-(gaps)`. Maximizing this means minimizing gaps.\n\n    # Let's use the gap as the primary driver and add a slight adjustment for very large gaps.\n    # Consider a score that is inversely proportional to the gap, but caps out or decreases for extremely large gaps.\n    # A simple penalty for large gaps can be achieved by subtracting a term proportional to the gap itself.\n    # Score = C - gap, where C is a large constant. This still favors minimum gap.\n\n    # Let's define the score for each suitable bin as:\n    # `priority_score = 1.0 / (gap + 1e-6)`  (favors tightest fit)\n    # This is standard Best Fit.\n\n    # To add a penalty for excessive slack (large gaps), we can subtract a term related to the gap.\n    # Let's consider the \"waste\" created. `waste = gap`.\n    # We want to minimize waste.\n    # Let's define a score that is higher for lower waste.\n    # Score = -waste = -(bins_remain_capacities - item). Maximizing this means minimizing waste.\n\n    # Let's try a combined score: prioritize small gaps, but penalize very large gaps.\n    # Consider the reciprocal of the *normalized* gap.\n    # Normalized gap: `gaps / item`.\n    # Score = `1.0 / (gaps / item + 1e-6)` ? No, this amplifies small item sizes.\n\n    # Let's go back to the idea of penalizing large remaining capacity.\n    # The residual capacity after packing is `gap`.\n    # We want to maximize `1 / (gap + epsilon)`.\n    # However, if `gap` is very large, this score might still be relatively high,\n    # which is not ideal if we want to penalize large remaining spaces.\n\n    # Let's introduce a penalty term that grows with the gap.\n    # Score = `(1.0 / (gaps + 1e-6)) - penalty_weight * gaps`\n    # This might over-penalize.\n\n    # A more direct way to penalize large remaining capacity:\n    # Consider the \"fullness\" after packing, which is `1 - (item / bins_remain_cap)`.\n    # This is related to `1 - item / (gap + item) = gap / (gap + item)`.\n    # We want to maximize this ratio.\n    # So, `score = gaps / (gaps + 1e-6)`. This aims to maximize slack.\n\n    # Let's combine Best Fit (minimize gap) and a factor that discourages large gaps.\n    # We want to maximize a value that is high for small gaps.\n    # Consider `1 / (gap + 1)` as a base score.\n    # To penalize large gaps, we can subtract a term proportional to the gap.\n    # Example: `priority = 100 - gap`. This favors small gaps.\n    # Let's make it more dynamic.\n\n    # Let's consider a score that balances tight fitting and leaving moderate space.\n    # Best Fit component: `1.0 / (gaps + 1e-6)`\n    # A component that penalizes large gaps: `1.0 / (gaps**2 + 1e-6)`\n    # This amplifies the penalty for larger gaps.\n    # Or, simply subtract `gaps`.\n\n    # Let's use the inverse of the remaining capacity after packing as a measure of how full the bin is.\n    # We want to maximize this: `1.0 / (gaps + 1e-6)`. This IS Best Fit.\n\n    # A strategy that penalizes \"too much\" remaining capacity could be:\n    # prioritize bins where `gap` is small, but not excessively small if it leads to very large gaps.\n    # This is tricky.\n\n    # Let's simplify: prioritize tightest fit, but add a small penalty to bins with very large slack.\n    # Consider the score `1 / (gap + 1)`.\n    # If gap > item, apply a discount.\n    # This means a bin that is almost full (small gap) is preferred.\n    # A bin that is much larger than needed (large gap) is less preferred.\n\n    # Let's define the priority score for each suitable bin as:\n    # `score = 1.0 / (gaps + 1e-6)`\n    # This directly implements Best Fit.\n\n    # To add a penalty for excessive slack (large gaps), we can modify this.\n    # If a bin has `gap > SOME_THRESHOLD`, reduce its score.\n    # What's a good threshold? Maybe related to `item` or average `bins_remain_cap`.\n    # Let's use `item` as a reference. If `gap > item`, the bin is more than twice the size needed.\n\n    # Let's assign a score that favors small gaps, and decreases for larger gaps.\n    # `score = 1.0 / (gaps + 1e-6)`  -- This is best fit.\n    # To penalize large gaps, let's subtract a term proportional to the gap.\n    # `score = (1.0 / (gaps + 1e-6)) - (0.1 * gaps)`\n    # The `0.1` is a tunable parameter.\n\n    # Let's try a different formulation. We want to maximize remaining space after packing,\n    # but not excessively.\n    # Consider the score that prioritizes bins where `bins_remain_cap` is *just* large enough.\n    # This means `bins_remain_cap` is close to `item`.\n    # We want to maximize `bins_remain_cap` among suitable bins, which is \"Worst Fit\".\n\n    # Let's try to combine Best Fit with a penalty for \"too empty\" bins.\n    # For a suitable bin, the remaining capacity is `gap`.\n    # If `gap` is very large, it's \"too empty\".\n    # We want to prioritize small `gap`.\n    # Consider `score = -gap`. Maximizing this minimizes `gap`.\n    # Let's add a slight bonus for bins that are not excessively empty.\n    # If `gap < item * tolerance`, give a bonus.\n\n    # Let's use a scoring function that is derived from Best Fit, but with a modification for large gaps.\n    # Base score (Best Fit): `1.0 / (gaps + 1e-6)`\n    # Penalty for large gaps: Let's make the score decrease faster for larger gaps.\n    # Consider `1.0 / (gaps + 1e-6)`. If gap is large, this value is small.\n    # What if we try to maximize `(bins_remain_cap[i] - item)` but subject to `bins_remain_cap[i] - item` being small?\n\n    # Let's define the score for each suitable bin as:\n    # `score = 1.0 / (gaps + 1e-6)`  # High score for small gaps (tight fit)\n    # To penalize large gaps (excessive slack), we can add a term that subtracts from the score as gap increases.\n    # Let's use a penalty proportional to the gap itself.\n    # `penalty = 0.1 * gaps`\n    # `final_score = (1.0 / (gaps + 1e-6)) - (0.1 * gaps)`\n\n    # This might penalize too aggressively.\n    # Let's try a simpler approach: favor bins that have remaining capacity that is \"just enough\" or slightly more.\n    # This means we want to MAXIMIZE `bins_remain_cap` among suitable bins, subject to `bins_remain_cap` not being excessively large.\n    # Let's prioritize bins that have a reasonable amount of capacity remaining after packing.\n    # This means maximizing `gap`. This is the opposite of Best Fit.\n\n    # Let's stick to Best Fit but modify the scoring to penalize large gaps.\n    # We want a score that is high for small gaps and decreases as gaps grow.\n    # Consider `score = exp(-gaps / some_scale)`. This is good but potentially non-linear.\n\n    # Let's use a score that is `1.0 / (gap + 1)` and apply a discount if `gap` is large.\n    # For example, if `gap > item`, apply a penalty.\n    # `score = 1.0 / (gaps + 1e-6)`\n    # `penalty_factor = np.where(gaps > item, 0.5, 1.0)`\n    # `final_scores = score * penalty_factor`\n\n    # Let's try a more direct approach using the negative gap, but with a penalty for large gaps.\n    # We want to maximize `(-gaps)` which means minimizing gaps.\n    # To penalize large gaps, we can subtract a term that grows with gaps.\n    # `score = -gaps - (0.1 * gaps**2)`\n    # This will heavily penalize large gaps.\n\n    # Let's define the score based on minimizing the gap and penalizing its size.\n    # We want to maximize `-(gap + a*gap^2)` where 'a' is a penalty factor.\n    # For simplicity, let's use `score = -gaps`. This is just Best Fit.\n\n    # Let's reconsider the penalty for large gaps.\n    # If `gap` is large, the bin is very empty after packing.\n    # This is generally okay, but if there are many such bins, it might not be optimal.\n    # Let's try a score that is `1 / (gap + 1)` and a penalty for `gap > item`.\n    # This promotes Best Fit but discourages very loose fits.\n\n    # Let's implement a score that favors tight fits, with a penalty for excessive remaining capacity.\n    # The penalty will be proportional to the remaining capacity after packing.\n    # Score for each suitable bin `i`:\n    # `remaining_after_packing = bins_remain_cap[i] - item`\n    # `base_score = 1.0 / (remaining_after_packing + 1e-6)` (favors small remaining capacity)\n    # `penalty = 0.1 * remaining_after_packing` (penalizes large remaining capacity)\n    # `final_score = base_score - penalty`\n    # This attempts to combine tight fit with a disincentive for very loose fits.\n\n    final_scores = np.zeros_like(suitable_bins_capacities, dtype=float)\n    for i, bin_idx in enumerate(suitable_bins_indices):\n        remaining_after_packing = suitable_bins_capacities[i] - item\n        \n        # Base score: favors smaller remaining capacity (tightest fit)\n        # Add a small epsilon to avoid division by zero.\n        base_score = 1.0 / (remaining_after_packing + 1e-6)\n        \n        # Penalty: discourages bins with excessively large remaining capacity.\n        # This penalty increases linearly with the remaining capacity after packing.\n        # The coefficient (0.1) is a tunable parameter.\n        penalty = 0.1 * remaining_after_packing\n        \n        final_scores[i] = base_score - penalty\n\n    if final_scores.size > 0:\n        best_fit_in_suitable_idx = np.argmax(final_scores)\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.9999669920071 seconds"
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized tightest fit with an epsilon-greedy exploration strategy.\n    Prioritizes bins with minimal remaining capacity after packing,\n    with a small chance to select any fitting bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return zero priorities\n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate tightest fit scores: higher score for less remaining capacity\n    # Adding epsilon to avoid division by zero if remaining capacity equals item size\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (available_bins_cap - item + epsilon)\n    \n    # Normalize tight fit scores to a 0-1 range for consistent comparison\n    max_tight_fit = np.max(tight_fit_scores)\n    if max_tight_fit > 0:\n        normalized_tight_fit = tight_fit_scores / max_tight_fit\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n\n    # Epsilon-greedy exploration:\n    # With probability epsilon, pick a random fitting bin (uniform priority).\n    # Otherwise, pick the bin with the highest normalized tight_fit_score.\n    exploration_prob = 0.1\n    \n    if np.random.rand() < exploration_prob:\n        # Exploration: Assign uniform high priority to all fitting bins\n        priorities[can_fit_mask] = 1.0\n    else:\n        # Exploitation: Use normalized tight fit scores\n        priorities[can_fit_mask] = normalized_tight_fit\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with adaptive bin utilization and a balanced exploration strategy.\n    Prioritizes bins that minimize waste and are more utilized, with controlled random exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Probability for epsilon-greedy exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Higher score for bins leaving less space.\n    remaining_after_packing = available_bins_remain_cap - item\n    # Add epsilon to avoid division by zero. Higher score for smaller remaining space.\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Adaptive Bin Utilization: Prefer bins that are already more full.\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    # Avoid division by zero if all bins are empty.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + epsilon) / (max_remaining_overall + epsilon)\n\n    # Combine core heuristic scores with equal weighting.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be between 0 and 1.\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_available_bins = available_bins_remain_cap.size\n    \n    # Generate random scores for exploration. The idea is to allow some randomness\n    # to potentially discover better packing strategies over time, rather than\n    # always picking the deterministically \"best\" based on current heuristics.\n    # We want to give a chance to bins that might not be top-ranked by core heuristics.\n    # We generate random numbers and then sort them to get a random permutation.\n    random_exploration_scores = np.random.rand(num_available_bins)\n    \n    # Blend between the deterministic (core) score and the random exploration score.\n    # With probability `exploration_prob`, the priority will lean towards the random score.\n    # A simple linear interpolation is used: priority = (1-alpha)*core + alpha*random\n    # where alpha is `exploration_prob` for some bins, and 0 for others.\n    \n    # Create a mask for exploration bins.\n    is_exploration_bin = np.random.rand(num_available_bins) < exploration_prob\n    \n    # Calculate final scores: use core scores by default, and exploration scores for chosen bins.\n    final_scores = np.where(is_exploration_bin, random_exploration_scores, normalized_core_scores)\n    \n    # Re-normalize the final scores to ensure they are within a comparable range (0 to 1).\n    # This prevents extreme values from dominating due to the random component.\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores # Should not happen if there are fitting bins, but for safety.\n\n    # Ensure priorities are non-negative (though they should be due to operations).\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.058635819704831,
    "SLOC": 29.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for excessive remaining capacity,\n    favoring bins that utilize space efficiently and avoid large leftovers.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity *after* packing.\n    # Higher score for bins that leave less remaining space.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    \n    # Avoid division by zero or very small numbers\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for bins with excessive slack (large remaining capacity *before* packing).\n    # This discourages using bins that are much larger than needed for the current item.\n    # We use a sigmoid-like penalty that is close to 1 for small remaining capacities\n    # and approaches 0 for very large remaining capacities.\n    # This is inspired by the idea of penalizing \"empty\" or \"underutilized\" bins.\n    \n    # Define a sensitivity parameter for slack penalty.\n    # A higher value makes the penalty more aggressive for smaller amounts of slack.\n    slack_penalty_sensitivity = 0.1\n    \n    slack_penalty = np.ones_like(bins_remain_cap, dtype=float)\n    # Apply penalty only to bins that can fit the item\n    slack_penalty[can_fit_mask] = 1.0 / (1.0 + slack_penalty_sensitivity * bins_remain_cap[can_fit_mask])\n\n    # Combine tightness score and slack penalty multiplicatively.\n    # This ensures that bins must be both tightly fitting and not have excessive slack.\n    combined_score = tightness_score * slack_penalty\n    \n    # Normalize scores to a 0-1 range to ensure comparability.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.307937774232155,
    "SLOC": 33.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for slack, adapting weights based on bin availability.\n    Prioritizes bins that fit the item tightly, with a penalty for bins that leave excessive slack.\n    Weights adapt to bin scarcity, favoring tight fits when few options exist.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Criterion 1: Tightest Fit (minimize remaining capacity after packing)\n    # Higher score for smaller gaps (bins that fit the item snugly)\n    gaps = suitable_bins_capacities - item\n    # Add a small epsilon to avoid division by zero and prioritize very tight fits\n    tightness_score = 1.0 / (gaps + 1e-6) \n\n    # Criterion 2: Penalty for Slack (discourage bins with excessive remaining capacity)\n    # We want to penalize large gaps. A high penalty for large slack means a low score.\n    # So, we invert the slack for a higher score for smaller slack.\n    # The penalty is higher when slack is larger.\n    # Let's use `slack = remaining_capacity - item`. We want to penalize large slack.\n    # A simple penalty could be `1 / (slack + epsilon)`. This rewards small slack.\n    # Or, let's consider the inverse of `slack` as a score (higher for smaller slack).\n    # `slack_score = 1.0 / (slack + 1e-6)`\n    # This is similar to tightness_score, but let's make it distinct.\n    # Let's penalize bins where `remaining_capacity - item` is large.\n    # We want to maximize `-(remaining_capacity - item)`.\n    # This is equivalent to minimizing `(remaining_capacity - item)`.\n    # For priority (higher is better), we want to maximize `-(gap)`.\n    # Or, minimize `gap`.\n    # Let's define a \"slack penalty score\" which is high for small slack.\n    slack_penalty_score = 1.0 / (gaps + 1e-6) # Same as tightness_score for now, need to differentiate\n\n    # Let's redefine:\n    # Tightness: Prefer bins where `bins_remain_cap[i]` is close to `item`.\n    # This means `gap` is small. Maximize `1 / (gap + epsilon)`.\n\n    # Slack Penalty: Prefer bins that don't leave too much *extra* space.\n    # If `bins_remain_cap[i]` is much larger than `item`, it's bad.\n    # So, we want to minimize `bins_remain_cap[i]`.\n    # For priority, we want to maximize `-bins_remain_cap[i]`.\n    # This would select the smallest capacity bins first.\n    # Let's try to penalize large remaining capacities.\n    # `slack_penalty = -suitable_bins_capacities` (lower is better, so need to invert for priority).\n    # Maximize `-suitable_bins_capacities`.\n    # Or, let's consider `1 / (suitable_bins_capacities + epsilon)` - this favors smaller capacities.\n    slack_penalty_score = 1.0 / (suitable_bins_capacities + 1e-6)\n\n\n    # Adaptive weights based on the number of suitable bins.\n    # If few suitable bins, be more aggressive with tight fits.\n    num_suitable = len(suitable_bins_indices)\n    if num_suitable > 0:\n        # Weight for Tightness: Higher when few suitable bins exist.\n        w_tightness = 2.0 / (num_suitable + 1.0)\n        # Weight for Slack Penalty: Higher when many suitable bins exist (allows more choice).\n        w_slack_penalty = num_suitable / (num_suitable + 1.0)\n    else:\n        w_tightness = 0.5\n        w_slack_penalty = 0.5\n\n    # Combined score: Weighted sum of the two criteria\n    # We want to maximize both `tightness_score` and `slack_penalty_score`.\n    combined_scores = w_tightness * tightness_score + w_slack_penalty * slack_penalty_score\n\n    # Assign scores to the original priority array for the suitable bins\n    priorities[suitable_bins_indices] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  }
]