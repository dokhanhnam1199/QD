{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight-fit preference with a penalty for empty bins,\n    favoring fuller bins and efficient space utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Calculate penalty for empty bins. This encourages using existing bins.\n    # An empty bin is one whose remaining capacity is equal to the maximum possible bin capacity.\n    # We need to find the actual maximum capacity across all bins if they are not uniform.\n    # However, for simplicity and common BPP scenarios, we assume a standard bin capacity.\n    # A robust approach would require bin capacity as an input or infer it.\n    # For this example, let's assume `bins_remain_cap` reflects current state, and a bin with\n    # capacity equal to its initial maximum (if known) or a very large remaining capacity\n    # implies it's effectively \"empty\" in terms of prior usage.\n    # A simpler proxy for \"empty\" is a very large remaining capacity compared to the item.\n    # Let's use a large constant penalty for bins that are significantly underutilized relative to their max potential.\n    # A more direct approach is to penalize bins that are still \"full\" (i.e., have not been used yet).\n    # Let's assume an \"empty\" bin has a remaining capacity equal to the max seen in bins_remain_cap.\n    \n    # This logic aims to combine the \"tight fit\" of Heuristic 4 with the \"penalty for empty bins\" of Heuristic 17.\n    # We prioritize bins that fit snugly, but if there's a tie or similar fit, we prefer bins that have already been used.\n    \n    # Penalty for bins that appear \"empty\" (e.g., have not been utilized).\n    # A bin is considered \"empty\" if its remaining capacity is close to the maximum capacity.\n    # Let's assume the maximum capacity of a bin is the largest value found in bins_remain_cap\n    # for bins that can fit the item, or a known global bin capacity if provided.\n    # For this implementation, we'll use a penalty for bins that are *not* significantly filled.\n    # A simple proxy for \"not significantly filled\" is to consider bins that are still \"very full\".\n    \n    # Using the penalty from Heuristic 17: a large constant for empty bins.\n    # We need to identify which of the `can_fit_mask` bins are also \"empty\".\n    # A common definition of \"empty\" in BPP heuristics is a bin that has not yet received any items.\n    # If we don't have explicit information about which bins are truly empty vs. just having high remaining capacity,\n    # we can approximate it by looking for bins with remaining capacity that is very close to the maximum capacity found.\n    \n    # For simplicity, let's redefine \"empty\" as a bin that has *just* enough capacity for the item,\n    # and we want to penalize these if better options exist.\n    # Alternatively, we can directly apply a penalty to bins that are initially \"full\".\n    # Let's go with a direct penalty for bins that are very close to being full (i.e., their remaining capacity is very large).\n    \n    # Instead of a fixed large penalty, let's make it relative.\n    # We want to discourage packing into a \"new\" bin if an \"existing\" bin offers a similar tightness.\n    # A heuristic that combines \"tight fit\" and a \"penalty for empty bins\" can be achieved by:\n    # 1. Calculating tightness scores for all fitting bins.\n    # 2. Identifying \"empty\" bins among the fitting ones (e.g., those with capacity close to the maximum).\n    # 3. Reducing the priority of these \"empty\" bins by a certain factor or offset.\n    \n    # Let's try a simpler approach that is common: penalize bins that have a lot of remaining space.\n    # This is implicitly handled by the inverse of remaining capacity.\n    # The \"penalty for empty bins\" is often to ensure that if a bin has lots of space, we only use it\n    # if other bins are not good fits.\n    \n    # Let's combine Heuristic 4 (tight fit) with a penalty for bins that have a lot of leftover space\n    # after packing the item. This is similar to the `oversize_penalty` idea but perhaps simpler.\n    \n    # The 'tightness_score' already rewards bins with less remaining capacity.\n    # The 'penalty for empty bins' is about preferring to fill existing bins.\n    # If we consider bins with remaining_after_packing == 0 to be ideal, and larger values to be less ideal,\n    # then the inverse already handles this.\n    \n    # Let's integrate the \"penalty for empty bins\" by reducing the score of bins that have\n    # a large `remaining_after_packing`. This is contrary to the tight-fit idea.\n    \n    # A better combination: prioritize tight fits. For bins with similar tightness,\n    # prefer those that are already partially filled.\n    \n    # Let's stick to the core idea of Heuristic 4 (tightest fit) and enhance it by\n    # penalizing bins that are \"too large\", similar to the `oversize_penalty` in the `priority_v0` example.\n    # This would mean prioritizing bins where `remaining_after_packing` is small.\n    \n    # Combining tight fit (inverse of remaining space) with a penalty for \"excessive\" remaining space.\n    \n    # Let's use the tightness score and add a penalty for bins that have a lot of slack.\n    # If a bin has `R` remaining capacity, and item `I` is packed, the new remaining is `R-I`.\n    # We want to minimize `R-I`.\n    # The `tightness_score` does this: `1 / (R-I + epsilon)`.\n    \n    # For the \"penalty for empty bins\": This is meant to encourage using partially filled bins before new ones.\n    # If we consider bins with `bins_remain_cap` close to some maximum capacity as \"empty\",\n    # we should reduce their priority.\n    \n    # Let's define a threshold for \"empty\". A bin might be considered \"empty\" if its remaining capacity\n    # is, say, more than 75% of the maximum capacity encountered.\n    \n    # Let's consider the combined logic of tight-fit and a penalty for bins that are\n    # unnecessarily large for the item.\n    \n    # Recalculate tightness: higher for smaller remaining capacity after packing\n    # This is good.\n    \n    # Consider a penalty for bins that are \"overly large\" for the item.\n    # If `bins_remain_cap[i]` is much larger than `item`, we might want to penalize it.\n    # For instance, if `bins_remain_cap[i] > 2 * item`, apply a penalty.\n    \n    oversize_penalty_factor = 2.0 # Penalize if bin capacity is more than twice the item size\n    penalty_value = 0.5 # Reduce priority by this factor\n    \n    oversize_penalties = np.ones_like(bins_remain_cap, dtype=float)\n    oversize_mask = (bins_remain_cap > oversize_penalty_factor * item) & can_fit_mask\n    oversize_penalties[oversize_mask] = penalty_value\n    \n    # Combine tightness score with oversize penalty\n    # We want to amplify tightness, and reduce for oversized bins.\n    # A multiplicative approach seems suitable.\n    \n    combined_score = tightness_score * oversize_penalties\n    \n    # Normalize priorities to avoid extremely large values and ensure a fair comparison.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    # Now, let's add the \"penalty for empty bins\".\n    # This means if a bin is \"empty\" and also fits the item, reduce its priority.\n    # Let's define \"empty\" as having remaining capacity equal to the *initial* capacity of that bin.\n    # Without initial capacities, we can use a proxy: bins with remaining capacity above a certain threshold.\n    # For instance, if remaining capacity is > 90% of the max capacity observed among fitting bins.\n    \n    # Let's use a simpler interpretation: \"empty\" bins are those that are initially full.\n    # If we have no information about initial fill, we can use a proxy: bins that are currently\n    # much larger than the item. This is already somewhat handled by `oversize_penalties`.\n    \n    # A more direct interpretation of \"penalty for empty bins\" from Heuristic 17:\n    # Apply a significant reduction to bins that are still \"full\" (i.e., have high remaining capacity).\n    # Let's consider bins with `bins_remain_cap >= item` as candidates.\n    # Among these, identify those that are \"empty\".\n    # A simple approach to identify \"empty\" bins is to look at bins whose remaining capacity\n    # is very close to a common maximum bin capacity. If we don't know the max capacity,\n    # we can use the maximum value in `bins_remain_cap` as a reference.\n    \n    # Let's implement a penalty that reduces the priority of bins that have\n    # a large amount of slack *after* the item is placed. This is related to\n    # minimizing waste.\n    \n    # Consider the \"waste\" as `bins_remain_cap - item` for fitting bins.\n    # We want to minimize waste. The `tightness_score` already does this.\n    \n    # Let's focus on the explicit \"penalty for empty bins\".\n    # A common interpretation is that if we have partially filled bins, we should\n    # prefer them over completely new (empty) bins.\n    # If `bins_remain_cap` represents the state, and we don't know initial states,\n    # we can approximate \"empty\" bins as those with very large `bins_remain_cap`.\n    \n    # Let's combine Heuristic 4 (tight fit) with Heuristic 17 (penalty for empty bins).\n    # Heuristic 4: `1.0 / (bins_remain_cap - item + epsilon)`\n    # Heuristic 17: Penalize \"empty\" bins.\n    \n    # Let's define \"empty\" as `bins_remain_cap` being very close to some `MAX_CAPACITY`.\n    # Without `MAX_CAPACITY`, we can use a relative measure: is `bins_remain_cap`\n    # very large compared to `item`? The `oversize_penalties` handle this.\n    \n    # Let's refine the \"penalty for empty bins\" by considering bins that have\n    # a large amount of *available* space for *future* items, relative to the current item.\n    # This is about balancing immediate fit with future potential.\n    \n    # Combining \"tight fit\" (Heuristic 4) and \"prefer partially filled bins\"\n    # (a common strategy for \"penalty for empty bins\").\n    \n    # Let's define a score based on how \"full\" the bin is *before* packing.\n    # `fill_ratio = (BIN_CAPACITY - bins_remain_cap) / BIN_CAPACITY`. We'd need BIN_CAPACITY.\n    # A proxy: `1 - (bins_remain_cap / MAX_REMAINING_CAP)`.\n    \n    # Alternative: Prioritize bins with small `bins_remain_cap` (tight fit) first.\n    # Then, among those with similar tightness, prefer those that are *not* \"empty\".\n    # \"Empty\" can mean `bins_remain_cap` is very large.\n    \n    # Let's go with the most direct combination of Heuristic 4 and the concept of\n    # \"penalty for empty bins\" interpreted as penalizing bins with *large* remaining capacity.\n    \n    # 1. Calculate tightness score (inverse of slack).\n    # 2. Apply a penalty to bins with large slack *after* packing.\n    \n    # The `tightness_score` already penalizes bins with large slack.\n    # `tightness_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)`\n    \n    # Now, add a penalty for bins that have a large *initial* remaining capacity.\n    # Let's assume an \"empty\" bin is one with `bins_remain_cap` significantly larger than `item`.\n    # This is similar to `oversize_penalty` but applied differently.\n    \n    # Let's use a penalty on the `tightness_score` itself.\n    # If `bins_remain_cap[i]` is large, we want to reduce its priority.\n    # A simple penalty: `1 / (1 + k * bins_remain_cap[i])`.\n    \n    penalty_factor_slack = 0.1 # Adjust this to control the penalty for slack\n    slack_penalty = np.ones_like(bins_remain_cap, dtype=float)\n    \n    # Apply penalty to bins that can fit, based on their *initial* remaining capacity.\n    # A larger initial remaining capacity gets a higher penalty (lower score).\n    slack_penalty[can_fit_mask] = 1.0 / (1.0 + penalty_factor_slack * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness score and slack penalty.\n    # We want high tightness, but also penalize large initial slack.\n    # Multiplication is a good way to combine these:\n    # `priority = tightness_score * slack_penalty`\n    \n    combined_score = tightness_score * slack_penalty\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    # Ensure that bins that cannot fit have a priority of 0, which is already set.\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight packing (inverse remaining capacity after item) with an\n    epsilon-greedy exploration strategy for selecting bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate tight fit score: higher for less remaining capacity after packing\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (available_bins_cap - item + epsilon)\n    \n    # Normalize tight fit scores to be between 0 and 1\n    max_tight_fit = np.max(tight_fit_scores)\n    if max_tight_fit > 0:\n        normalized_tight_fit = tight_fit_scores / max_tight_fit\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n\n    # Epsilon-greedy exploration: \n    # With probability epsilon, pick a random fitting bin.\n    # Otherwise, pick the bin with the highest tight_fit_score.\n    epsilon = 0.1\n    \n    if np.random.rand() < epsilon:\n        # Exploration: Assign uniform high priority to all fitting bins\n        priorities[can_fit_mask] = 1.0\n    else:\n        # Exploitation: Use normalized tight fit scores\n        priorities[can_fit_mask] = normalized_tight_fit\n        \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 (Epsilon-Greedy Best Fit) with Heuristic 6 (Epsilon-Greedy Best Fit, different normalization/combination), Heuristic 1 is simpler and more direct in its epsilon-greedy implementation by choosing either a random bin or the best fit. Heuristic 6's approach of normalizing and then applying weights and epsilon feels more complex and potentially less robust due to the normalization scaling.\n\nComparing Heuristic 7/8 (Epsilon-Greedy Best Fit, simple score) with Heuristic 12/13 (Epsilon-Greedy Best Fit, boosted random choice), Heuristics 12/13's approach of boosting the random choice's priority might make exploration too dominant, potentially hindering exploitation of good fits. Heuristics 7/8's direct use of normalized tight-fit scores in exploitation is cleaner.\n\nComparing Heuristic 9/10/11 (Multi-objective with adaptive utilization and boosted exploration) with Heuristic 18/19 (Multi-objective with adaptive weights based on available bins), Heuristics 18/19 have a more grounded adaptive weighting strategy based on bin availability, which is a better heuristic for adaptivity than the more generic \"boosted exploration\" in 9/10/11. However, the core objectives in 9/10/11 (tight fit + utilization) might be more generally applicable than the specific balancing in 18/19.\n\nComparing Heuristic 1 (simple epsilon-greedy Best Fit) with Heuristic 2/3/4/5 (complex combination of tight fit and \"penalty for empty bins\"), the latter heuristics attempt to incorporate more nuanced ideas like penalizing slack. However, their implementation is convoluted and the interpretation of \"penalty for empty bins\" is not clearly defined or consistently applied, leading to less straightforward logic. Heuristic 1's simplicity and clear strategy make it more robust.\n\nComparing Heuristic 20 (Worst Fit + Tightness bonus) with Heuristic 18/19 (adaptive multi-objective), Heuristic 20's approach is interesting but the combination of Worst Fit and Tightness bonus, along with the \"less used\" bonus, makes it complex and potentially contradictory in its goals. The adaptive weighting in 18/19 is a more coherent approach to balancing objectives.\n\nOverall, simpler heuristics that clearly define and combine strategies (like Heuristic 1) tend to be better than overly complex ones with ambiguously defined components (like 2-5, 18-20).\n- \nHere's a redefined approach to self-reflection for better heuristic design, focusing on actionable insights:\n\n*   **Keywords:** Performance, adaptability, interpretability, validation.\n*   **Advice:** Design heuristics with clear objectives and simple combination mechanisms. Incorporate adaptive exploration and consider smooth, non-linear scaling for fine-tuning. Prioritize vectorized operations for efficiency.\n*   **Avoid:** Ambiguity in objectives, overly complex components, and buggy implementations. Resist relying solely on linear or binary scaling without justification.\n*   **Explanation:** Focus on making heuristics understandable and maintainable. This clarity aids in identifying bottlenecks and opportunities for improvement through controlled experimentation and validation.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}