{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit (tightest fit) with an epsilon-greedy exploration strategy.\n    Prioritizes bins that leave the smallest remaining capacity after packing,\n    with a small chance of picking any suitable bin to encourage exploration.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Exploration phase: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Find the index within the 'suitable_bins_indices' array that has the minimum gap\n        best_fit_in_suitable_idx = np.argmin(gaps)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for excessive remaining capacity,\n    favoring bins that utilize space efficiently and avoid large leftovers.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity *after* packing.\n    # Higher score for bins that leave less remaining space.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    \n    # Avoid division by zero or very small numbers\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for bins with excessive slack (large remaining capacity *before* packing).\n    # This discourages using bins that are much larger than needed for the current item.\n    # We use a sigmoid-like penalty that is close to 1 for small remaining capacities\n    # and approaches 0 for very large remaining capacities.\n    # This is inspired by the idea of penalizing \"empty\" or \"underutilized\" bins.\n    \n    # Define a sensitivity parameter for slack penalty.\n    # A higher value makes the penalty more aggressive for smaller amounts of slack.\n    slack_penalty_sensitivity = 0.1\n    \n    slack_penalty = np.ones_like(bins_remain_cap, dtype=float)\n    # Apply penalty only to bins that can fit the item\n    slack_penalty[can_fit_mask] = 1.0 / (1.0 + slack_penalty_sensitivity * bins_remain_cap[can_fit_mask])\n\n    # Combine tightness score and slack penalty multiplicatively.\n    # This ensures that bins must be both tightly fitting and not have excessive slack.\n    combined_score = tightness_score * slack_penalty\n    \n    # Normalize scores to a 0-1 range to ensure comparability.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n    \n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for unused bins,\n    favoring bins that leave less space and are already in use.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for \"empty\" bins: encourage using partially filled bins.\n    # We define \"empty\" as bins with a large remaining capacity (e.g., > 75% of max observed).\n    # This encourages filling existing bins before opening new ones.\n    \n    # First, find a baseline for \"large remaining capacity\".\n    # We can use the maximum remaining capacity among *fitting* bins as a reference.\n    # If no bins fit, this part is skipped.\n    if fitting_bins_remain_cap.size > 0:\n        max_fitting_capacity = np.max(fitting_bins_remain_cap)\n        \n        # Identify bins that are \"empty\" or significantly underutilized\n        # A bin is considered \"empty\" if its remaining capacity is substantially large.\n        # Let's use a threshold, e.g., 75% of the max fitting capacity.\n        # This heuristic aims to penalize bins that are \"too large\" for the current item,\n        # and more importantly, to prefer bins that are already in use.\n        \n        # Penalty factor: reduce priority for bins with high remaining capacity.\n        # We want to down-weight bins that have a lot of \"slack\".\n        # The inverse of (1 + slack_penalty_factor * remaining_capacity) can work.\n        # A simpler approach derived from \"penalty for empty bins\" is to reduce the score\n        # of bins that are still \"full\" (i.e., have lots of remaining capacity).\n        \n        # Let's define a \"utilization score\" which is inverse of remaining capacity.\n        # A bin that is almost full has a high utilization score.\n        # High utilization is preferred.\n        \n        utilization_score = np.zeros_like(bins_remain_cap, dtype=float)\n        # We consider the inverse of the *initial* remaining capacity for utilization.\n        # Higher value means more utilized (less remaining capacity initially).\n        # We apply this only to fitting bins.\n        utilization_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n\n        # Combine tightness and utilization.\n        # We want both: small remaining space *after* packing (tightness)\n        # AND high initial utilization (prefer fuller bins).\n        # Multiplying them seems reasonable: prioritize bins that are already full AND become tight.\n        combined_score = tightness_score * utilization_score\n        \n        # Normalize scores to be in a similar range, e.g., [0, 1]\n        max_score = np.max(combined_score)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fitting with adaptive exploration and dynamic scoring.\n    Prioritizes bins that minimize waste and are more utilized, with a\n    probabilistic chance to explore less optimal but fitting bins,\n    dynamically adjusting exploration based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    # Use a small constant to prevent division by zero and give higher score to tighter fits\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + 1e-9) / (max_remaining_overall + 1e-9)\n\n    # Combine core heuristic scores: balanced preference\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be in a comparable range\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > 1e-9:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: Dynamically adjust exploration probability\n    num_available_bins = available_bins_remain_cap.size\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1 + num_available_bins))\n    \n    final_scores = np.copy(normalized_core_scores)\n\n    if np.random.rand() < exploration_prob:\n        # Select a random bin among those that can fit the item\n        random_index_in_subset = np.random.randint(0, num_available_bins)\n        # Boost the score of the randomly chosen bin to make exploration significant\n        # Use a boost factor relative to the best core score to make it competitive\n        boost_factor = 1.5 # Boost exploration picks\n        final_scores[random_index_in_subset] += np.max(normalized_core_scores) * boost_factor\n\n    # Re-normalize final scores to ensure they are between 0 and 1\n    max_final_score = np.max(final_scores)\n    if max_final_score > 1e-9:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with adaptive bin utilization and a balanced exploration strategy.\n    Prioritizes bins that minimize waste and are more utilized, with controlled random exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Probability for epsilon-greedy exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Higher score for bins leaving less space.\n    remaining_after_packing = available_bins_remain_cap - item\n    # Add epsilon to avoid division by zero. Higher score for smaller remaining space.\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Adaptive Bin Utilization: Prefer bins that are already more full.\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    # Avoid division by zero if all bins are empty.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + epsilon) / (max_remaining_overall + epsilon)\n\n    # Combine core heuristic scores with equal weighting.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be between 0 and 1.\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_available_bins = available_bins_remain_cap.size\n    \n    # Generate random scores for exploration. The idea is to allow some randomness\n    # to potentially discover better packing strategies over time, rather than\n    # always picking the deterministically \"best\" based on current heuristics.\n    # We want to give a chance to bins that might not be top-ranked by core heuristics.\n    # We generate random numbers and then sort them to get a random permutation.\n    random_exploration_scores = np.random.rand(num_available_bins)\n    \n    # Blend between the deterministic (core) score and the random exploration score.\n    # With probability `exploration_prob`, the priority will lean towards the random score.\n    # A simple linear interpolation is used: priority = (1-alpha)*core + alpha*random\n    # where alpha is `exploration_prob` for some bins, and 0 for others.\n    \n    # Create a mask for exploration bins.\n    is_exploration_bin = np.random.rand(num_available_bins) < exploration_prob\n    \n    # Calculate final scores: use core scores by default, and exploration scores for chosen bins.\n    final_scores = np.where(is_exploration_bin, random_exploration_scores, normalized_core_scores)\n    \n    # Re-normalize the final scores to ensure they are within a comparable range (0 to 1).\n    # This prevents extreme values from dominating due to the random component.\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores # Should not happen if there are fitting bins, but for safety.\n\n    # Ensure priorities are non-negative (though they should be due to operations).\n    priorities[priorities < 0] = 0\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized tightest fit with an epsilon-greedy exploration strategy.\n    Prioritizes bins with minimal remaining capacity after packing,\n    with a small chance to select any fitting bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return zero priorities\n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate tightest fit scores: higher score for less remaining capacity\n    # Adding epsilon to avoid division by zero if remaining capacity equals item size\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (available_bins_cap - item + epsilon)\n    \n    # Normalize tight fit scores to a 0-1 range for consistent comparison\n    max_tight_fit = np.max(tight_fit_scores)\n    if max_tight_fit > 0:\n        normalized_tight_fit = tight_fit_scores / max_tight_fit\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n\n    # Epsilon-greedy exploration:\n    # With probability epsilon, pick a random fitting bin (uniform priority).\n    # Otherwise, pick the bin with the highest normalized tight_fit_score.\n    exploration_prob = 0.1\n    \n    if np.random.rand() < exploration_prob:\n        # Exploration: Assign uniform high priority to all fitting bins\n        priorities[can_fit_mask] = 1.0\n    else:\n        # Exploitation: Use normalized tight fit scores\n        priorities[can_fit_mask] = normalized_tight_fit\n        \n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a novel \"nearness\" score and adaptive exploration.\n    Prioritizes bins that are a good fit, but also considers bins that are \"almost\"\n    a good fit to prevent situations where only very large bins are left.\n    The exploration strategy adapts based on the number of suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Adaptive exploration: increase exploration probability if there are many suitable bins\n    # and decrease if there are very few to avoid wasting opportunities.\n    num_suitable_bins = suitable_bins_indices.size\n    if num_suitable_bins <= 2:\n        exploration_prob = 0.2  # Higher exploration for very few options\n    elif num_suitable_bins > 5:\n        exploration_prob = 0.05 # Lower exploration for many options\n    else:\n        exploration_prob = 0.1 # Default exploration\n\n    if np.random.rand() < exploration_prob:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Combine Best Fit with a \"nearness\" score.\n        # The \"nearness\" score is higher for bins that are close to the optimal gap,\n        # but not so close that they become inefficient.\n        # We use a scaled inverse of the gap + a small constant to avoid division by zero,\n        # and a sigmoid-like scaling to smooth the priority.\n        \n        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits\n        gaps_for_scoring = gaps + 1e-6 \n        \n        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps\n        # that might be too tight. We use a form of smooth inverse.\n        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.\n        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.\n        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.\n        \n        # Let's use a score that is higher for smaller gaps, but tapers off.\n        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.\n        # We'll use `exp(-k * gap)` for a smoother, non-linear response.\n        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap\n        nearness_scores = np.exp(-k_factor * gaps)\n\n        # Normalize scores so the maximum is 1\n        if np.max(nearness_scores) > 0:\n            normalized_scores = nearness_scores / np.max(nearness_scores)\n        else:\n            normalized_scores = np.zeros_like(nearness_scores)\n\n        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.\n        # Or, we can simply use the nearness_scores directly if they are well-calibrated.\n        # For this version, let's directly use the normalized nearness scores as priorities.\n        \n        # Get the index of the bin with the highest nearness score\n        best_nearness_idx_in_suitable = np.argmax(normalized_scores)\n        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]\n        \n        priorities[best_nearness_original_idx] = 1.0\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a novel \"nearness\" score and adaptive exploration.\n    Prioritizes bins that are a good fit, but also considers bins that are \"almost\"\n    a good fit to prevent situations where only very large bins are left.\n    The exploration strategy adapts based on the number of suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Adaptive exploration: increase exploration probability if there are many suitable bins\n    # and decrease if there are very few to avoid wasting opportunities.\n    num_suitable_bins = suitable_bins_indices.size\n    if num_suitable_bins <= 2:\n        exploration_prob = 0.2  # Higher exploration for very few options\n    elif num_suitable_bins > 5:\n        exploration_prob = 0.05 # Lower exploration for many options\n    else:\n        exploration_prob = 0.1 # Default exploration\n\n    if np.random.rand() < exploration_prob:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Combine Best Fit with a \"nearness\" score.\n        # The \"nearness\" score is higher for bins that are close to the optimal gap,\n        # but not so close that they become inefficient.\n        # We use a scaled inverse of the gap + a small constant to avoid division by zero,\n        # and a sigmoid-like scaling to smooth the priority.\n        \n        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits\n        gaps_for_scoring = gaps + 1e-6 \n        \n        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps\n        # that might be too tight. We use a form of smooth inverse.\n        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.\n        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.\n        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.\n        \n        # Let's use a score that is higher for smaller gaps, but tapers off.\n        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.\n        # We'll use `exp(-k * gap)` for a smoother, non-linear response.\n        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap\n        nearness_scores = np.exp(-k_factor * gaps)\n\n        # Normalize scores so the maximum is 1\n        if np.max(nearness_scores) > 0:\n            normalized_scores = nearness_scores / np.max(nearness_scores)\n        else:\n            normalized_scores = np.zeros_like(nearness_scores)\n\n        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.\n        # Or, we can simply use the nearness_scores directly if they are well-calibrated.\n        # For this version, let's directly use the normalized nearness scores as priorities.\n        \n        # Get the index of the bin with the highest nearness score\n        best_nearness_idx_in_suitable = np.argmax(normalized_scores)\n        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]\n        \n        priorities[best_nearness_original_idx] = 1.0\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a modified Best Fit (prioritizing bins with a smaller, non-linear gap)\n    with a dynamic exploration strategy that favors less-utilized bins.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Dynamic Exploration: Favor bins that are less utilized (higher remaining capacity)\n    if np.random.rand() < epsilon:\n        # Calculate a score for exploration: higher score for more remaining capacity\n        exploration_scores = bins_remain_cap[suitable_bins_indices]\n        # Apply a non-linear scaling (e.g., log) to emphasize differences in larger capacities\n        # Add a small constant to avoid log(0) if a bin has exactly the item size\n        exploration_scores = np.log1p(exploration_scores - item + 1) \n        exploration_scores /= np.sum(exploration_scores) # Normalize to form a probability distribution\n        chosen_bin_index = np.random.choice(suitable_bins_indices, p=exploration_scores)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Modified Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Prioritize bins with smaller gaps, but also consider the absolute remaining capacity.\n        # This encourages tighter fits but also favors bins that were already quite empty\n        # if the gap is similar.\n        # Using a ratio or a weighted sum can be effective.\n        # Here, we use a simple heuristic: a score that is high for small gaps,\n        # but also rewards bins with larger initial remaining capacity if gaps are comparable.\n        \n        # Score = 1 / (gap + 1) + (remaining_capacity / max_remaining_capacity)\n        # Adding 1 to gap to avoid division by zero if gap is 0.\n        # Normalizing remaining_capacity to prevent it from dominating the score.\n        max_total_capacity = np.max(bins_remain_cap) # Assuming a general max capacity or using the max available\n        if max_total_capacity == 0: # Handle case where all bins have 0 capacity (shouldn't happen if suitable bins exist)\n            max_total_capacity = 1\n            \n        modified_scores = (1.0 / (gaps + 1e-6)) + (suitable_bins_capacities / (max_total_capacity + 1e-6))\n        \n        # Find the index within the 'suitable_bins_indices' array that has the maximum modified score\n        best_fit_in_suitable_idx = np.argmax(modified_scores)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a modified Best Fit (prioritizing bins with a smaller, non-linear gap)\n    with a dynamic exploration strategy that favors less-utilized bins.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Dynamic Exploration: Favor bins that are less utilized (higher remaining capacity)\n    if np.random.rand() < epsilon:\n        # Calculate a score for exploration: higher score for more remaining capacity\n        exploration_scores = bins_remain_cap[suitable_bins_indices]\n        # Apply a non-linear scaling (e.g., log) to emphasize differences in larger capacities\n        # Add a small constant to avoid log(0) if a bin has exactly the item size\n        exploration_scores = np.log1p(exploration_scores - item + 1) \n        exploration_scores /= np.sum(exploration_scores) # Normalize to form a probability distribution\n        chosen_bin_index = np.random.choice(suitable_bins_indices, p=exploration_scores)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Modified Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Prioritize bins with smaller gaps, but also consider the absolute remaining capacity.\n        # This encourages tighter fits but also favors bins that were already quite empty\n        # if the gap is similar.\n        # Using a ratio or a weighted sum can be effective.\n        # Here, we use a simple heuristic: a score that is high for small gaps,\n        # but also rewards bins with larger initial remaining capacity if gaps are comparable.\n        \n        # Score = 1 / (gap + 1) + (remaining_capacity / max_remaining_capacity)\n        # Adding 1 to gap to avoid division by zero if gap is 0.\n        # Normalizing remaining_capacity to prevent it from dominating the score.\n        max_total_capacity = np.max(bins_remain_cap) # Assuming a general max capacity or using the max available\n        if max_total_capacity == 0: # Handle case where all bins have 0 capacity (shouldn't happen if suitable bins exist)\n            max_total_capacity = 1\n            \n        modified_scores = (1.0 / (gaps + 1e-6)) + (suitable_bins_capacities / (max_total_capacity + 1e-6))\n        \n        # Find the index within the 'suitable_bins_indices' array that has the maximum modified score\n        best_fit_in_suitable_idx = np.argmax(modified_scores)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}