```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines tight-fit preference with adaptive bin utilization and dynamic exploration.
    Prioritizes bins that minimize waste and are more utilized, with exploration
    favoring less utilized bins based on a dynamically adjusted probability.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9

    # Mask for bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    available_bins_remain_cap = bins_remain_cap[can_fit_mask]
    available_bin_indices = np.where(can_fit_mask)[0]

    # --- Core Heuristic Components ---

    # 1. Tight Fitting (Minimize Waste): Score is inverse of remaining space after packing.
    remaining_after_packing = available_bins_remain_cap - item
    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)

    # 2. Adaptive Bin Utilization: Score based on how "full" the bin is.
    # Prefer bins that have less remaining capacity relative to the item size.
    # This can be approximated by `item / available_bins_remain_cap` or similar.
    # Let's use a score that is higher for bins that are less empty (i.e., lower remaining capacity).
    # A simple approach is `1.0 / (available_bins_remain_cap + epsilon)`.
    # To make it more adaptive, we can consider the *average* remaining capacity of fitting bins.
    avg_fitting_remain_cap = np.mean(available_bins_remain_cap) if available_bins_remain_cap.size > 0 else 1.0
    # Prefer bins with remaining capacity closer to the item or lower than average.
    # Let's try a score that is high when remaining capacity is low (closer to item).
    utilization_scores = 1.0 / (available_bins_remain_cap + epsilon)

    # Combine the two heuristic scores. A weighted sum is a common approach.
    # Let's give equal weight for now.
    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores

    # Normalize core scores to be between 0 and 1.
    max_core_score = np.max(combined_core_scores)
    if max_core_score > epsilon:
        normalized_core_scores = combined_core_scores / max_core_score
    else:
        normalized_core_scores = np.zeros_like(combined_core_scores)

    # --- Dynamic Epsilon-Greedy Exploration ---
    num_available_bins = available_bins_remain_cap.size

    # Determine exploration probability adaptively.
    # A common strategy is to increase exploration when there are many bins,
    # to encourage trying out different options. Or, explore less if bins are very full.
    # Let's use a simple inverse relationship with the number of available bins to explore more
    # when there are fewer options, which can be counter-intuitive.
    # A better approach: explore more when there are *many* bins to choose from, to diversify.
    # Or, explore less when bins are already quite full, and more when they are mostly empty.

    # A simple dynamic probability: decrease exploration as the number of available bins decreases.
    # Or, use the inverse of the average utilization as a proxy for how "challenging" the packing is.
    # Let's use a probability that decreases with the tightness of the fit (smaller remaining_after_packing).
    # This means we explore more when the fits are not tight.
    
    # Heuristic from analysis: dynamic exploration favoring less utilized bins.
    # We want to explore bins that are NOT among the top-ranked by core heuristics.
    # The probability of exploration for a bin could be inversely related to its normalized core score.

    # Simple dynamic exploration probability: higher when many bins, lower when few.
    # Or, a fixed probability for simplicity if dynamic is too complex to implement robustly here.
    # Given the prompt's goal of combining elements, let's use a fixed epsilon-greedy
    # but slightly modified based on utilization to influence *which* bins are explored.

    exploration_prob = 0.15 # Base exploration probability

    # We want to assign exploration scores to a subset of bins.
    # Instead of random scores, let's boost scores of bins that are less utilized.
    # A simple way to select bins for "exploration" (or a modified score):
    # bins with higher `utilization_scores` (meaning they are more utilized)
    # should be less likely to be perturbed by random exploration.
    # So, bins with lower `utilization_scores` are more prone to exploration.

    # Create a probability mask for exploration, favoring less utilized bins.
    # Let's sort bins by utilization_scores (ascending) and pick a fraction.
    # A simple approach: assign a higher 'random' score to bins that are less utilized.
    
    # Let's stick to a blended approach inspired by Heuristic 11, where exploration
    # contributes to the final score.
    
    # For the exploration component, we can generate random scores.
    # To bias exploration towards less utilized bins, we can scale these random scores.
    # If `utilization_scores` is low (meaning bin is less utilized), we want a higher random score.
    # We can achieve this by multiplying `random_scores` by `(1.0 / (utilization_scores + epsilon))`.
    
    random_exploration_scores = np.random.rand(num_available_bins)
    
    # Scale random scores: boost scores for less utilized bins (low utilization_scores).
    # Higher `boost_factor` means more boost for less utilized bins.
    boost_factor = 2.0 # Control how much less utilized bins are favored in exploration
    scaled_random_scores = random_exploration_scores * (1.0 + boost_factor * (1.0 - utilization_scores))
    
    # Normalize these scaled random scores to keep them in a comparable range.
    max_scaled_random_score = np.max(scaled_random_scores)
    if max_scaled_random_score > epsilon:
        normalized_exploration_scores = scaled_random_scores / max_scaled_random_score
    else:
        normalized_exploration_scores = np.zeros_like(scaled_random_scores)

    # Blend the deterministic (core) scores with the exploration scores.
    # Use the `exploration_prob` to decide the mixing weight.
    alpha = exploration_prob
    final_scores = (1 - alpha) * normalized_core_scores + alpha * normalized_exploration_scores

    # Re-normalize the final blended scores to ensure they are in a 0-1 range.
    max_final_score = np.max(final_scores)
    if max_final_score > epsilon:
        priorities[can_fit_mask] = final_scores / max_final_score
    else:
        # Fallback if all scores are zero or near-zero
        priorities[can_fit_mask] = final_scores

    # Ensure priorities are non-negative.
    priorities[priorities < 0] = 0

    return priorities
```
