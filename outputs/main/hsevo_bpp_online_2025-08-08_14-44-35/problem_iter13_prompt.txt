{"system": "You are an expert in code review. Your task extract all threshold, weight or hardcode variable of the function make it become default parameters.", "user": "[code]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with a penalty for under-utilized bins,\n    using a novel exploration strategy to balance exploitation and exploration.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_factor = 0.2  # Controls the degree of exploration\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exploitation Component: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    # Higher score for smaller remaining space.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Exploitation Component: Utilization Penalty ---\n    # Penalize bins that are significantly under-utilized (i.e., have a lot of remaining capacity).\n    # This encourages filling existing bins before opening new ones.\n    # We use the inverse of the *initial* remaining capacity as a proxy for utilization.\n    # Higher utilization (less remaining capacity) gets a higher score.\n    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Weighted sum of tight fit and utilization scores.\n    # Giving slightly more weight to tight fit as it's the primary goal of BPP.\n    combined_exploitation_scores = 0.6 * tight_fit_scores + 0.4 * utilization_scores\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Exploration Component: Adaptive Exploration ---\n    # We want to explore bins that are not necessarily the best according to exploitation.\n    # Instead of random choice, we will boost the scores of bins that are \"average\" or slightly below average\n    # in terms of exploitation, giving them a chance.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    \n    # Calculate a baseline score (e.g., mean exploitation score)\n    mean_exploitation_score = np.mean(normalized_exploitation_scores)\n    \n    # Create exploration scores: higher for bins closer to the mean exploitation score.\n    # This aims to explore \"promising but not top\" candidates.\n    # We add a small constant to ensure even the lowest scores get some exploration boost if needed.\n    exploration_scores = np.exp(-((normalized_exploitation_scores - mean_exploitation_score) / (mean_exploitation_score + epsilon))**2)\n    \n    # Apply the exploration factor to blend exploitation and exploration\n    # The final priority is a mix:\n    # (1 - exploration_factor) * exploitation_scores + exploration_factor * exploration_scores\n    # This ensures that exploration never completely overrides exploitation,\n    # and also that exploitation still has a significant impact.\n    final_priorities_unnormalized = (1 - exploration_factor) * normalized_exploitation_scores + exploration_factor * exploration_scores\n\n    # Normalize final priorities for the fitting bins\n    sum_final_priorities = np.sum(final_priorities_unnormalized)\n    if sum_final_priorities > epsilon:\n        priorities[can_fit_mask] = final_priorities_unnormalized / sum_final_priorities\n    else:\n        # If all scores are zero, distribute probability equally among fitting bins\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities\n\nNow extract all threshold, weight or hardcode variable of the function make it become default parameters and give me a 'parameter_ranges' dictionary representation. Key of dict is name of variable. Value of key is a tuple in Python MUST include 2 float elements, first element is begin value, second element is end value corresponding with parameter.\n\n- Output code only and enclose your code with Python code block: ```python ... ```.\n- Output 'parameter_ranges' dictionary only and enclose your code with other Python code block: ```python ... ```."}