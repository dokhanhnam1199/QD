{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight fitting with adaptive exploration and dynamic scoring.\n    Prioritizes bins that minimize waste and are more utilized, with a\n    probabilistic chance to explore less optimal but fitting bins,\n    dynamically adjusting exploration based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    # Use a small constant to prevent division by zero and give higher score to tighter fits\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + 1e-9) / (max_remaining_overall + 1e-9)\n\n    # Combine core heuristic scores: balanced preference\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be in a comparable range\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > 1e-9:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: Dynamically adjust exploration probability\n    num_available_bins = available_bins_remain_cap.size\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1 + num_available_bins))\n    \n    final_scores = np.copy(normalized_core_scores)\n\n    if np.random.rand() < exploration_prob:\n        # Select a random bin among those that can fit the item\n        random_index_in_subset = np.random.randint(0, num_available_bins)\n        # Boost the score of the randomly chosen bin to make exploration significant\n        # Use a boost factor relative to the best core score to make it competitive\n        boost_factor = 1.5 # Boost exploration picks\n        final_scores[random_index_in_subset] += np.max(normalized_core_scores) * boost_factor\n\n    # Re-normalize final scores to ensure they are between 0 and 1\n    max_final_score = np.max(final_scores)\n    if max_final_score > 1e-9:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a modified Best Fit (prioritizing bins with a smaller, non-linear gap)\n    with a dynamic exploration strategy that favors less-utilized bins.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Dynamic Exploration: Favor bins that are less utilized (higher remaining capacity)\n    if np.random.rand() < epsilon:\n        # Calculate a score for exploration: higher score for more remaining capacity\n        exploration_scores = bins_remain_cap[suitable_bins_indices]\n        # Apply a non-linear scaling (e.g., log) to emphasize differences in larger capacities\n        # Add a small constant to avoid log(0) if a bin has exactly the item size\n        exploration_scores = np.log1p(exploration_scores - item + 1) \n        exploration_scores /= np.sum(exploration_scores) # Normalize to form a probability distribution\n        chosen_bin_index = np.random.choice(suitable_bins_indices, p=exploration_scores)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Modified Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Prioritize bins with smaller gaps, but also consider the absolute remaining capacity.\n        # This encourages tighter fits but also favors bins that were already quite empty\n        # if the gap is similar.\n        # Using a ratio or a weighted sum can be effective.\n        # Here, we use a simple heuristic: a score that is high for small gaps,\n        # but also rewards bins with larger initial remaining capacity if gaps are comparable.\n        \n        # Score = 1 / (gap + 1) + (remaining_capacity / max_remaining_capacity)\n        # Adding 1 to gap to avoid division by zero if gap is 0.\n        # Normalizing remaining_capacity to prevent it from dominating the score.\n        max_total_capacity = np.max(bins_remain_cap) # Assuming a general max capacity or using the max available\n        if max_total_capacity == 0: # Handle case where all bins have 0 capacity (shouldn't happen if suitable bins exist)\n            max_total_capacity = 1\n            \n        modified_scores = (1.0 / (gaps + 1e-6)) + (suitable_bins_capacities / (max_total_capacity + 1e-6))\n        \n        # Find the index within the 'suitable_bins_indices' array that has the maximum modified score\n        best_fit_in_suitable_idx = np.argmax(modified_scores)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 12:** Heuristic 1 combines Best Fit with epsilon-greedy. Heuristic 12 also combines Best Fit with epsilon-greedy but normalizes the tight-fit scores. Normalization (12) is generally better as it makes scores comparable and less dependent on absolute values, but Heuristic 1's exploration is a bit more nuanced (random choice of suitable bin vs. uniform priority). The ranking difference here is minimal, but 12's normalization might offer slightly more robust behavior.\n*   **Heuristic 1 vs. Heuristic 13/14/20:** Heuristics 13/14/20 introduce a \"nearness\" score based on exponential decay of the gap and adaptive exploration. This is more sophisticated than Heuristic 1's simple Best Fit. The dynamic exploration probability in 13/14/20 is also an improvement. The ranking of these depends on how well the \"nearness\" score and adaptive exploration perform in practice, but they represent a clear step up in complexity and potential effectiveness over Heuristic 1.\n*   **Heuristic 1 vs. Heuristic 19/20:** Heuristics 19 and 20 combine a modified Best Fit (using log for exploration and a composite score for exploitation) with dynamic exploration favoring less-utilized bins. This is more complex than Heuristic 1 and likely more effective due to the multi-faceted scoring and targeted exploration. The slight edge of 19/20 over 13/14 suggests a better combination of exploration and exploitation logic.\n*   **Heuristic 2/5/7/8/10 vs. Heuristic 11:** Heuristics 2, 5, 7, 8, and 10 are quite similar, combining tightness with a utilization bonus (often `1/(capacity + epsilon)`). Heuristic 11 refines this by using a more robust utilization score (`(max_remaining - current_remaining) / max_remaining`) and a blended epsilon-greedy approach (using random scores for exploration bins). This refined approach in Heuristic 11 is likely better, leading to its higher ranking compared to the earlier utilization heuristics.\n*   **Heuristic 4 vs. Heuristic 10:** Both combine tightness with a penalty for empty/large bins using inverse relationships with remaining capacity. Heuristic 10 uses a more direct inverse `1/(1 + penalty_weight * capacity)` whereas Heuristic 4 uses `1/(1 + penalty_weight * (capacity - item))`. The latter might be slightly more nuanced by considering the `item` size, but the difference is subtle. The ranking suggests 10 is slightly preferred.\n*   **Heuristic 9 vs. Heuristic 11:** Both combine tightness and utilization with adaptive exploration. Heuristic 9 has a dynamically adjusted `exploration_prob` based on bin count. Heuristic 11 uses a fixed `exploration_prob` but blends scores. Heuristic 11's blended score approach for exploration might be more stable and predictable than dynamically adjusting `exploration_prob`. However, the core scoring in 11 (more robust utilization, blending) is generally superior.\n*   **Heuristic 15/16/17/18 vs. Heuristic 11:** These heuristics are identical to each other and share the same core logic as Heuristic 11 but with a different exploration strategy (boosting random bins with a fixed priority). Heuristic 11's blended exploration might offer a smoother integration. The slight drop in ranking for 15-18 compared to 11 suggests the blending method in 11 is preferred over additive boosting of random bins.\n*   **Overall Trend:** Higher ranked heuristics tend to combine multiple scoring criteria (tightness, utilization), use more sophisticated normalization or blending for scores, and employ more refined exploration strategies (dynamic probability, score blending, or favoring less utilized bins for exploration). The simplest heuristics (like pure Best Fit or basic epsilon-greedy) are ranked lower.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Multi-objective, normalization, adaptive exploration, performance, clarity, robustness.\n*   **Advice:** Focus on combining multiple, clearly defined objectives using robust methods like weighted sums or Pareto fronts. Implement adaptive exploration mechanisms (e.g., annealing, bandit-based) and ensure all components are well-tested for correctness.\n*   **Avoid:** Overly complex, non-linear, or ad-hoc scoring functions without clear justification. Blindly prioritizing one objective without considering trade-offs. Bugs and unclear logic.\n*   **Explanation:** This approach emphasizes a structured, evidence-based design process. By clearly defining objectives and using proven techniques for combining them and managing exploration, we build heuristics that are both effective and maintainable, avoiding the pitfalls of complexity and ambiguity.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}