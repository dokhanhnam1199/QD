{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines a modified Best Fit (prioritizing bins with a smaller, non-linear gap)\n    with a dynamic exploration strategy that favors less-utilized bins.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Dynamic Exploration: Favor bins that are less utilized (higher remaining capacity)\n    if np.random.rand() < epsilon:\n        # Calculate a score for exploration: higher score for more remaining capacity\n        exploration_scores = bins_remain_cap[suitable_bins_indices]\n        # Apply a non-linear scaling (e.g., log) to emphasize differences in larger capacities\n        # Add a small constant to avoid log(0) if a bin has exactly the item size\n        exploration_scores = np.log1p(exploration_scores - item + 1) \n        exploration_scores /= np.sum(exploration_scores) # Normalize to form a probability distribution\n        chosen_bin_index = np.random.choice(suitable_bins_indices, p=exploration_scores)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Modified Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Prioritize bins with smaller gaps, but also consider the absolute remaining capacity.\n        # This encourages tighter fits but also favors bins that were already quite empty\n        # if the gap is similar.\n        # Using a ratio or a weighted sum can be effective.\n        # Here, we use a simple heuristic: a score that is high for small gaps,\n        # but also rewards bins with larger initial remaining capacity if gaps are comparable.\n        \n        # Score = 1 / (gap + 1) + (remaining_capacity / max_remaining_capacity)\n        # Adding 1 to gap to avoid division by zero if gap is 0.\n        # Normalizing remaining_capacity to prevent it from dominating the score.\n        max_total_capacity = np.max(bins_remain_cap) # Assuming a general max capacity or using the max available\n        if max_total_capacity == 0: # Handle case where all bins have 0 capacity (shouldn't happen if suitable bins exist)\n            max_total_capacity = 1\n            \n        modified_scores = (1.0 / (gaps + 1e-6)) + (suitable_bins_capacities / (max_total_capacity + 1e-6))\n        \n        # Find the index within the 'suitable_bins_indices' array that has the maximum modified score\n        best_fit_in_suitable_idx = np.argmax(modified_scores)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 12:** Heuristic 1 combines Best Fit with epsilon-greedy. Heuristic 12 also combines Best Fit with epsilon-greedy but normalizes the tight-fit scores. Normalization (12) is generally better as it makes scores comparable and less dependent on absolute values, but Heuristic 1's exploration is a bit more nuanced (random choice of suitable bin vs. uniform priority). The ranking difference here is minimal, but 12's normalization might offer slightly more robust behavior.\n*   **Heuristic 1 vs. Heuristic 13/14/20:** Heuristics 13/14/20 introduce a \"nearness\" score based on exponential decay of the gap and adaptive exploration. This is more sophisticated than Heuristic 1's simple Best Fit. The dynamic exploration probability in 13/14/20 is also an improvement. The ranking of these depends on how well the \"nearness\" score and adaptive exploration perform in practice, but they represent a clear step up in complexity and potential effectiveness over Heuristic 1.\n*   **Heuristic 1 vs. Heuristic 19/20:** Heuristics 19 and 20 combine a modified Best Fit (using log for exploration and a composite score for exploitation) with dynamic exploration favoring less-utilized bins. This is more complex than Heuristic 1 and likely more effective due to the multi-faceted scoring and targeted exploration. The slight edge of 19/20 over 13/14 suggests a better combination of exploration and exploitation logic.\n*   **Heuristic 2/5/7/8/10 vs. Heuristic 11:** Heuristics 2, 5, 7, 8, and 10 are quite similar, combining tightness with a utilization bonus (often `1/(capacity + epsilon)`). Heuristic 11 refines this by using a more robust utilization score (`(max_remaining - current_remaining) / max_remaining`) and a blended epsilon-greedy approach (using random scores for exploration bins). This refined approach in Heuristic 11 is likely better, leading to its higher ranking compared to the earlier utilization heuristics.\n*   **Heuristic 4 vs. Heuristic 10:** Both combine tightness with a penalty for empty/large bins using inverse relationships with remaining capacity. Heuristic 10 uses a more direct inverse `1/(1 + penalty_weight * capacity)` whereas Heuristic 4 uses `1/(1 + penalty_weight * (capacity - item))`. The latter might be slightly more nuanced by considering the `item` size, but the difference is subtle. The ranking suggests 10 is slightly preferred.\n*   **Heuristic 9 vs. Heuristic 11:** Both combine tightness and utilization with adaptive exploration. Heuristic 9 has a dynamically adjusted `exploration_prob` based on bin count. Heuristic 11 uses a fixed `exploration_prob` but blends scores. Heuristic 11's blended score approach for exploration might be more stable and predictable than dynamically adjusting `exploration_prob`. However, the core scoring in 11 (more robust utilization, blending) is generally superior.\n*   **Heuristic 15/16/17/18 vs. Heuristic 11:** These heuristics are identical to each other and share the same core logic as Heuristic 11 but with a different exploration strategy (boosting random bins with a fixed priority). Heuristic 11's blended exploration might offer a smoother integration. The slight drop in ranking for 15-18 compared to 11 suggests the blending method in 11 is preferred over additive boosting of random bins.\n*   **Overall Trend:** Higher ranked heuristics tend to combine multiple scoring criteria (tightness, utilization), use more sophisticated normalization or blending for scores, and employ more refined exploration strategies (dynamic probability, score blending, or favoring less utilized bins for exploration). The simplest heuristics (like pure Best Fit or basic epsilon-greedy) are ranked lower.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Multi-objective, normalization, adaptive exploration, performance, clarity, robustness.\n*   **Advice:** Focus on combining multiple, clearly defined objectives using robust methods like weighted sums or Pareto fronts. Implement adaptive exploration mechanisms (e.g., annealing, bandit-based) and ensure all components are well-tested for correctness.\n*   **Avoid:** Overly complex, non-linear, or ad-hoc scoring functions without clear justification. Blindly prioritizing one objective without considering trade-offs. Bugs and unclear logic.\n*   **Explanation:** This approach emphasizes a structured, evidence-based design process. By clearly defining objectives and using proven techniques for combining them and managing exploration, we build heuristics that are both effective and maintainable, avoiding the pitfalls of complexity and ambiguity.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}