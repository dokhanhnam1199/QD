```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines a "Worst Fit" strategy with a "First Fit Decreasing" like consideration.
    Prioritizes bins that leave the largest remaining capacity (Worst Fit)
    but gives a slight advantage to bins that are "just big enough" for the item,
    mimicking the FFD idea of not wasting space on smaller items by putting them
    in bins that could accommodate larger ones.
    Also includes a mechanism to favor previously less-used bins.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    suitable_bins_mask = bins_remain_cap >= item
    suitable_bins_indices = np.where(suitable_bins_mask)[0]

    if suitable_bins_indices.size == 0:
        return priorities

    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]
    
    # Calculate 'gaps' for Worst Fit consideration
    gaps = suitable_bins_capacities - item

    # Calculate a "tightness" score: smaller is better (closer to item size)
    # This encourages putting items into bins that are just large enough,
    # saving larger bins for potentially larger future items.
    tightness_scores = suitable_bins_capacities - item # Same as gaps for suitable bins

    # Normalize the tightness scores to be between 0 and 1 (higher is more 'tight')
    # Avoid division by zero if all suitable bins have the exact same capacity
    min_gap = np.min(tightness_scores)
    max_gap = np.max(tightness_scores)
    
    normalized_tightness = np.zeros_like(tightness_scores)
    if max_gap - min_gap > 1e-9: # Avoid division by zero if all gaps are equal
        normalized_tightness = (max_gap - tightness_scores) / (max_gap - min_gap)
    else:
        # If all gaps are the same, all are equally 'tight'
        normalized_tightness = np.ones_like(tightness_scores) * 0.5 # Neutral value

    # Combine Worst Fit (favoring larger gaps) and Tightness (favoring smaller gaps)
    # We want to prioritize larger gaps (Worst Fit) but give a boost for 'tight' fits.
    # Let's try to balance: give a score based on the inverse of the gap (larger gap = higher score for WF)
    # and add a bonus for tightness.
    
    # Score for Worst Fit: higher score for larger gaps
    worst_fit_scores = np.zeros_like(tightness_scores)
    if np.max(tightness_scores) > 1e-9: # Avoid division by zero
      worst_fit_scores = tightness_scores / np.max(tightness_scores)
    else:
      worst_fit_scores = np.ones_like(tightness_scores) * 0.5 # Neutral if all gaps are zero

    # Combined score: Primarily Worst Fit, with a bonus for tightness.
    # Higher score is better.
    combined_scores = worst_fit_scores + normalized_tightness * 0.3 # Weight for tightness

    # Add a small bonus for bins that were "less used" previously.
    # We can approximate "less used" by looking at the current remaining capacity.
    # Bins with more remaining capacity were perhaps "less used" for previous items
    # or received larger items.
    # Normalize remaining capacities to get a "low usage" score (higher means less used)
    min_cap = np.min(bins_remain_cap)
    max_cap = np.max(bins_remain_cap)
    
    less_used_bonus = np.zeros_like(bins_remain_cap, dtype=float)
    if max_cap - min_cap > 1e-9:
        normalized_remaining_cap = (bins_remain_cap - min_cap) / (max_cap - min_cap)
    else:
        normalized_remaining_cap = np.ones_like(bins_remain_cap) * 0.5

    # Apply this bonus only to suitable bins
    less_used_bonus[suitable_bins_indices] = normalized_remaining_cap[suitable_bins_indices] * 0.2 # Weight for less used

    # Assign the combined scores to the priority array
    priorities[suitable_bins_indices] = combined_scores

    # Add the less_used_bonus
    priorities += less_used_bonus

    # Ensure we only return positive priorities for suitable bins
    # This is a safeguard, the calculation should already handle this
    priorities[~suitable_bins_mask] = 0

    return priorities
```
