{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a novel \"nearness\" score and adaptive exploration.\n    Prioritizes bins that are a good fit, but also considers bins that are \"almost\"\n    a good fit to prevent situations where only very large bins are left.\n    The exploration strategy adapts based on the number of suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Adaptive exploration: increase exploration probability if there are many suitable bins\n    # and decrease if there are very few to avoid wasting opportunities.\n    num_suitable_bins = suitable_bins_indices.size\n    if num_suitable_bins <= 2:\n        exploration_prob = 0.2  # Higher exploration for very few options\n    elif num_suitable_bins > 5:\n        exploration_prob = 0.05 # Lower exploration for many options\n    else:\n        exploration_prob = 0.1 # Default exploration\n\n    if np.random.rand() < exploration_prob:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Combine Best Fit with a \"nearness\" score.\n        # The \"nearness\" score is higher for bins that are close to the optimal gap,\n        # but not so close that they become inefficient.\n        # We use a scaled inverse of the gap + a small constant to avoid division by zero,\n        # and a sigmoid-like scaling to smooth the priority.\n        \n        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits\n        gaps_for_scoring = gaps + 1e-6 \n        \n        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps\n        # that might be too tight. We use a form of smooth inverse.\n        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.\n        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.\n        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.\n        \n        # Let's use a score that is higher for smaller gaps, but tapers off.\n        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.\n        # We'll use `exp(-k * gap)` for a smoother, non-linear response.\n        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap\n        nearness_scores = np.exp(-k_factor * gaps)\n\n        # Normalize scores so the maximum is 1\n        if np.max(nearness_scores) > 0:\n            normalized_scores = nearness_scores / np.max(nearness_scores)\n        else:\n            normalized_scores = np.zeros_like(nearness_scores)\n\n        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.\n        # Or, we can simply use the nearness_scores directly if they are well-calibrated.\n        # For this version, let's directly use the normalized nearness scores as priorities.\n        \n        # Get the index of the bin with the highest nearness score\n        best_nearness_idx_in_suitable = np.argmax(normalized_scores)\n        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]\n        \n        priorities[best_nearness_original_idx] = 1.0\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 12:** Heuristic 1 combines Best Fit with epsilon-greedy. Heuristic 12 also combines Best Fit with epsilon-greedy but normalizes the tight-fit scores. Normalization (12) is generally better as it makes scores comparable and less dependent on absolute values, but Heuristic 1's exploration is a bit more nuanced (random choice of suitable bin vs. uniform priority). The ranking difference here is minimal, but 12's normalization might offer slightly more robust behavior.\n*   **Heuristic 1 vs. Heuristic 13/14/20:** Heuristics 13/14/20 introduce a \"nearness\" score based on exponential decay of the gap and adaptive exploration. This is more sophisticated than Heuristic 1's simple Best Fit. The dynamic exploration probability in 13/14/20 is also an improvement. The ranking of these depends on how well the \"nearness\" score and adaptive exploration perform in practice, but they represent a clear step up in complexity and potential effectiveness over Heuristic 1.\n*   **Heuristic 1 vs. Heuristic 19/20:** Heuristics 19 and 20 combine a modified Best Fit (using log for exploration and a composite score for exploitation) with dynamic exploration favoring less-utilized bins. This is more complex than Heuristic 1 and likely more effective due to the multi-faceted scoring and targeted exploration. The slight edge of 19/20 over 13/14 suggests a better combination of exploration and exploitation logic.\n*   **Heuristic 2/5/7/8/10 vs. Heuristic 11:** Heuristics 2, 5, 7, 8, and 10 are quite similar, combining tightness with a utilization bonus (often `1/(capacity + epsilon)`). Heuristic 11 refines this by using a more robust utilization score (`(max_remaining - current_remaining) / max_remaining`) and a blended epsilon-greedy approach (using random scores for exploration bins). This refined approach in Heuristic 11 is likely better, leading to its higher ranking compared to the earlier utilization heuristics.\n*   **Heuristic 4 vs. Heuristic 10:** Both combine tightness with a penalty for empty/large bins using inverse relationships with remaining capacity. Heuristic 10 uses a more direct inverse `1/(1 + penalty_weight * capacity)` whereas Heuristic 4 uses `1/(1 + penalty_weight * (capacity - item))`. The latter might be slightly more nuanced by considering the `item` size, but the difference is subtle. The ranking suggests 10 is slightly preferred.\n*   **Heuristic 9 vs. Heuristic 11:** Both combine tightness and utilization with adaptive exploration. Heuristic 9 has a dynamically adjusted `exploration_prob` based on bin count. Heuristic 11 uses a fixed `exploration_prob` but blends scores. Heuristic 11's blended score approach for exploration might be more stable and predictable than dynamically adjusting `exploration_prob`. However, the core scoring in 11 (more robust utilization, blending) is generally superior.\n*   **Heuristic 15/16/17/18 vs. Heuristic 11:** These heuristics are identical to each other and share the same core logic as Heuristic 11 but with a different exploration strategy (boosting random bins with a fixed priority). Heuristic 11's blended exploration might offer a smoother integration. The slight drop in ranking for 15-18 compared to 11 suggests the blending method in 11 is preferred over additive boosting of random bins.\n*   **Overall Trend:** Higher ranked heuristics tend to combine multiple scoring criteria (tightness, utilization), use more sophisticated normalization or blending for scores, and employ more refined exploration strategies (dynamic probability, score blending, or favoring less utilized bins for exploration). The simplest heuristics (like pure Best Fit or basic epsilon-greedy) are ranked lower.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Multi-objective, normalization, adaptive exploration, performance, clarity, robustness.\n*   **Advice:** Focus on combining multiple, clearly defined objectives using robust methods like weighted sums or Pareto fronts. Implement adaptive exploration mechanisms (e.g., annealing, bandit-based) and ensure all components are well-tested for correctness.\n*   **Avoid:** Overly complex, non-linear, or ad-hoc scoring functions without clear justification. Blindly prioritizing one objective without considering trade-offs. Bugs and unclear logic.\n*   **Explanation:** This approach emphasizes a structured, evidence-based design process. By clearly defining objectives and using proven techniques for combining them and managing exploration, we build heuristics that are both effective and maintainable, avoiding the pitfalls of complexity and ambiguity.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}