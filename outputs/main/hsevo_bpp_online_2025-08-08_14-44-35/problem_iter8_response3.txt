```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines tight-fit preference with an adaptive penalty for empty bins,
    favoring bins that are neither too full nor too empty relative to item size.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    epsilon = 1e-9

    # Mask for bins that can fit the item
    can_fit_mask = bins_remain_cap >= item

    if not np.any(can_fit_mask):
        return priorities

    # Calculate tightness score: higher for bins leaving less space after packing
    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]
    remaining_after_packing = fitting_bins_remain_cap - item
    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)
    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)

    # Adaptive penalty for "empty" or overly large bins:
    # Penalize bins whose remaining capacity is significantly larger than the item size.
    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.
    # The penalty is inversely related to the "slack" (bins_remain_cap - item).
    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.
    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.
    
    # Define a factor that scales the penalty based on how much "excess" capacity exists.
    # A bin with remaining capacity equal to item size should have no penalty from this part.
    # A bin with much larger remaining capacity gets a higher penalty.
    
    # Let's use a penalty term that is high for bins with large `bins_remain_cap`
    # and low for bins with `bins_remain_cap` closer to `item`.
    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`
    # This means if bins_remain_cap[i] == item, penalty is 1.0.
    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.
    
    # Corrected logic: We want to *boost* bins with reasonable existing capacity
    # and *penalize* bins that are very large (effectively "empty").
    # The "penalty for empty bins" suggests reducing priority for bins with lots of unused space.
    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.
    
    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.
    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.
    
    # A simpler approach: boost bins that are not "too empty".
    # Consider bins with `bins_remain_cap` relative to `item`.
    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.
    # We want to assign a lower priority to these "empty" bins.
    
    # Let's combine tightness with a preference for bins that are not "overly large" for the item.
    # This is similar to Heuristic 4 but with an added factor for slack.
    
    # We want to combine:
    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.
    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.
    
    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.
    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.
    # Or, we can simply penalize bins with large remaining capacity.
    
    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.
    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.
    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`
    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.
    
    # Let's consider "penalty for empty bins" as favoring bins that are not entirely unused.
    # If `bins_remain_cap` is very large, it might indicate an unused bin.
    # We can penalize such bins by reducing their priority.
    
    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.
    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`
    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.
    # We want to penalize large `bins_remain_cap`.
    
    # Consider a penalty that is high when `bins_remain_cap` is large.
    # For fitting bins, let's create a "utility" score that is higher for bins that are "just right".
    # "Just right" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.
    
    # Let's try a combination:
    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`
    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.
    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.
    
    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of
    # penalizing bins that are "overly large" for the item, which implicitly relates to "empty" bins.
    
    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.
    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.
    
    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.
    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.
    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`
    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`
    # This would penalize bins with large remaining space.
    
    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.
    
    # Calculate the slack penalty: a higher value means less penalty (score is higher).
    # We want to penalize large `bins_remain_cap`.
    # Let's define a factor that favors bins that are NOT excessively large.
    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`
    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.
    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.
    
    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.
    # The "penalty for empty bins" suggests that if a bin is mostly empty, we should try to avoid it.
    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.
    
    # We can combine tightness with a penalty term that is high for large remaining capacities.
    # Let's use a factor that is `1.0` for bins that are "just right" and decreases for larger capacities.
    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.
    
    # For simplicity and clarity, let's combine the tightness score with a penalty that
    # is inversely proportional to the *initial* remaining capacity.
    # This favors bins that are already somewhat filled.
    
    # Let's define the penalty for "empty" bins:
    # A bin is considered "empty" if its remaining capacity is significantly larger than the item size.
    # We want to reduce the priority of such bins.
    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`
    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.
    
    # Let's use a penalty that rewards bins that are not excessively large.
    # A Gaussian-like function centered around a "good" capacity might be too complex.
    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.
    
    # Let's try to combine the inverse of remaining space (tightness) with a penalty
    # that is high for large initial capacities.
    
    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.
    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.
    
    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`
    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.
    # This penalizes bins with large slack.
    
    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.
    
    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)
    # and low for bins with high initial remaining capacity (more "empty").
    # This is the inverse of what we want. We want to *reduce* score for large bins.
    
    # Let's create a "fill_preference" score.
    # Higher score for bins that are not "too empty".
    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.
    # However, we only care about fitting bins.
    
    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.
    # The penalty should reduce priority for bins that have a lot of space left.
    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`
    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.
    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.
    
    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)
    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])
    
    # Combine tightness and penalty
    combined_score = tightness_score * penalty_score
    
    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)
    max_score = np.max(combined_score)
    if max_score > 0:
        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score
        
    return priorities
```
