[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for unused bins,\n    favoring bins that leave less space and are already in use.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for \"empty\" bins: encourage using partially filled bins.\n    # We define \"empty\" as bins with a large remaining capacity (e.g., > 75% of max observed).\n    # This encourages filling existing bins before opening new ones.\n    \n    # First, find a baseline for \"large remaining capacity\".\n    # We can use the maximum remaining capacity among *fitting* bins as a reference.\n    # If no bins fit, this part is skipped.\n    if fitting_bins_remain_cap.size > 0:\n        max_fitting_capacity = np.max(fitting_bins_remain_cap)\n        \n        # Identify bins that are \"empty\" or significantly underutilized\n        # A bin is considered \"empty\" if its remaining capacity is substantially large.\n        # Let's use a threshold, e.g., 75% of the max fitting capacity.\n        # This heuristic aims to penalize bins that are \"too large\" for the current item,\n        # and more importantly, to prefer bins that are already in use.\n        \n        # Penalty factor: reduce priority for bins with high remaining capacity.\n        # We want to down-weight bins that have a lot of \"slack\".\n        # The inverse of (1 + slack_penalty_factor * remaining_capacity) can work.\n        # A simpler approach derived from \"penalty for empty bins\" is to reduce the score\n        # of bins that are still \"full\" (i.e., have lots of remaining capacity).\n        \n        # Let's define a \"utilization score\" which is inverse of remaining capacity.\n        # A bin that is almost full has a high utilization score.\n        # High utilization is preferred.\n        \n        utilization_score = np.zeros_like(bins_remain_cap, dtype=float)\n        # We consider the inverse of the *initial* remaining capacity for utilization.\n        # Higher value means more utilized (less remaining capacity initially).\n        # We apply this only to fitting bins.\n        utilization_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n\n        # Combine tightness and utilization.\n        # We want both: small remaining space *after* packing (tightness)\n        # AND high initial utilization (prefer fuller bins).\n        # Multiplying them seems reasonable: prioritize bins that are already full AND become tight.\n        combined_score = tightness_score * utilization_score\n        \n        # Normalize scores to be in a similar range, e.g., [0, 1]\n        max_score = np.max(combined_score)\n        if max_score > 0:\n            priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight fitting with adaptive exploration and dynamic scoring.\n    Prioritizes bins that minimize waste and are more utilized, with a\n    probabilistic chance to explore less optimal but fitting bins,\n    dynamically adjusting exploration based on bin availability.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon_base = 0.1  # Base probability for exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    if available_bins_remain_cap.size == 0:\n        return priorities\n\n    # Component 1: Tight Fitting (minimize waste)\n    remaining_after_packing = available_bins_remain_cap - item\n    # Use a small constant to prevent division by zero and give higher score to tighter fits\n    tight_fit_scores = 1.0 / (remaining_after_packing + 1e-9)\n\n    # Component 2: Adaptive Bin Utilization (prefer fuller bins)\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + 1e-9) / (max_remaining_overall + 1e-9)\n\n    # Combine core heuristic scores: balanced preference\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be in a comparable range\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > 1e-9:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # Epsilon-greedy exploration: Dynamically adjust exploration probability\n    num_available_bins = available_bins_remain_cap.size\n    # Exploration probability decreases as more bins become available, encouraging exploitation\n    exploration_prob = epsilon_base * (1.0 / (1 + num_available_bins))\n    \n    final_scores = np.copy(normalized_core_scores)\n\n    if np.random.rand() < exploration_prob:\n        # Select a random bin among those that can fit the item\n        random_index_in_subset = np.random.randint(0, num_available_bins)\n        # Boost the score of the randomly chosen bin to make exploration significant\n        # Use a boost factor relative to the best core score to make it competitive\n        boost_factor = 1.5 # Boost exploration picks\n        final_scores[random_index_in_subset] += np.max(normalized_core_scores) * boost_factor\n\n    # Re-normalize final scores to ensure they are between 0 and 1\n    max_final_score = np.max(final_scores)\n    if max_final_score > 1e-9:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 30.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a robust Best Fit strategy with an adaptive penalty for slack.\n    Prioritizes bins that offer the tightest fit, with a penalty for\n    bins that leave excessive remaining capacity, encouraging better space utilization.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Calculate the gap for best fit (lower is better)\n    gaps = suitable_bins_capacities - item\n\n    # Calculate a penalty for bins with large remaining capacity (slack).\n    # We want to penalize larger gaps to encourage tighter packing.\n    # A simple penalty could be the gap itself, so we want to MINIMIZE gap.\n    # To convert to a priority score (higher is better), we can use 1 / (gap + epsilon).\n    # However, if we want to PENALIZE large gaps, we can subtract the gap from a base value\n    # or use a function that decreases with gap size.\n    # Let's consider a score where we want to MAXIMIZE `-(gap)^2` to favor small gaps.\n    # A simpler approach for penalizing large gaps is to ensure the priority decreases\n    # as the gap increases.\n    # Let's use a scoring system that directly favors smaller gaps.\n    # Score = 1 / (gap + epsilon) aims to maximize preference for smallest gap.\n\n    # Let's introduce a penalty for bins that have *too much* remaining capacity.\n    # This \"too much\" can be relative. A bin with capacity 100 and item 10 has gap 90.\n    # A bin with capacity 20 and item 10 has gap 10.\n    # The gap of 10 is better for tight fit.\n    # If we want to penalize the gap of 90, we can use a function that decreases rapidly.\n    # A linear penalty might be `penalty_weight * gap`.\n    # Let's try to score bins by `(some_value - gap)`.\n    # A common approach is to use the negative of the gap, or `1/(gap + epsilon)`.\n\n    # Let's combine Best Fit (minimize gap) with an incentive to not leave too much space.\n    # This can be achieved by rewarding bins that have a remaining capacity closer to the item size.\n    # Maximize `-(gap)^2` is one way to do this.\n    # Or, let's consider the inverse of the remaining capacity after packing.\n    # We want to maximize the *fullness* of the bin after packing.\n    # This would mean maximizing `1 / (gap + epsilon)`.\n    # This is essentially Best Fit.\n\n    # What if we want to penalize bins that have a *very* large gap?\n    # For example, if `gap > item * threshold`.\n    # Let's consider a score that heavily favors small gaps but also slightly penalizes very large gaps.\n    # A function like `1 / (gap + 1)` would give higher scores to smaller gaps.\n    # To penalize large gaps, we can ensure the score doesn't increase linearly with `1/gap`.\n    # Let's consider the efficiency: `item / bins_remain_cap[i]`. Maximize this.\n    # This is equivalent to minimizing `bins_remain_cap[i] / item`.\n    # And thus, minimizing `bins_remain_cap[i] - item`.\n\n    # A good heuristic might be to prioritize bins that have a small gap,\n    # but not necessarily the absolute smallest if that leaves very little room.\n    # Let's try to combine Best Fit (minimize gap) with a factor that\n    # penalizes large gaps.\n    # We want to maximize a score that is high for small gaps.\n    # Score: `1.0 / (gaps + 1e-6)` is a strong Best Fit.\n    # To penalize large gaps, we can apply a discount to this score for large gaps.\n    # Or, consider `-(gaps)`. Maximizing this means minimizing gaps.\n\n    # Let's use the gap as the primary driver and add a slight adjustment for very large gaps.\n    # Consider a score that is inversely proportional to the gap, but caps out or decreases for extremely large gaps.\n    # A simple penalty for large gaps can be achieved by subtracting a term proportional to the gap itself.\n    # Score = C - gap, where C is a large constant. This still favors minimum gap.\n\n    # Let's define the score for each suitable bin as:\n    # `priority_score = 1.0 / (gap + 1e-6)`  (favors tightest fit)\n    # This is standard Best Fit.\n\n    # To add a penalty for excessive slack (large gaps), we can subtract a term related to the gap.\n    # Let's consider the \"waste\" created. `waste = gap`.\n    # We want to minimize waste.\n    # Let's define a score that is higher for lower waste.\n    # Score = -waste = -(bins_remain_capacities - item). Maximizing this means minimizing waste.\n\n    # Let's try a combined score: prioritize small gaps, but penalize very large gaps.\n    # Consider the reciprocal of the *normalized* gap.\n    # Normalized gap: `gaps / item`.\n    # Score = `1.0 / (gaps / item + 1e-6)` ? No, this amplifies small item sizes.\n\n    # Let's go back to the idea of penalizing large remaining capacity.\n    # The residual capacity after packing is `gap`.\n    # We want to maximize `1 / (gap + epsilon)`.\n    # However, if `gap` is very large, this score might still be relatively high,\n    # which is not ideal if we want to penalize large remaining spaces.\n\n    # Let's introduce a penalty term that grows with the gap.\n    # Score = `(1.0 / (gaps + 1e-6)) - penalty_weight * gaps`\n    # This might over-penalize.\n\n    # A more direct way to penalize large remaining capacity:\n    # Consider the \"fullness\" after packing, which is `1 - (item / bins_remain_cap)`.\n    # This is related to `1 - item / (gap + item) = gap / (gap + item)`.\n    # We want to maximize this ratio.\n    # So, `score = gaps / (gaps + 1e-6)`. This aims to maximize slack.\n\n    # Let's combine Best Fit (minimize gap) and a factor that discourages large gaps.\n    # We want to maximize a value that is high for small gaps.\n    # Consider `1 / (gap + 1)` as a base score.\n    # To penalize large gaps, we can subtract a term proportional to the gap.\n    # Example: `priority = 100 - gap`. This favors small gaps.\n    # Let's make it more dynamic.\n\n    # Let's consider a score that balances tight fitting and leaving moderate space.\n    # Best Fit component: `1.0 / (gaps + 1e-6)`\n    # A component that penalizes large gaps: `1.0 / (gaps**2 + 1e-6)`\n    # This amplifies the penalty for larger gaps.\n    # Or, simply subtract `gaps`.\n\n    # Let's use the inverse of the remaining capacity after packing as a measure of how full the bin is.\n    # We want to maximize this: `1.0 / (gaps + 1e-6)`. This IS Best Fit.\n\n    # A strategy that penalizes \"too much\" remaining capacity could be:\n    # prioritize bins where `gap` is small, but not excessively small if it leads to very large gaps.\n    # This is tricky.\n\n    # Let's simplify: prioritize tightest fit, but add a small penalty to bins with very large slack.\n    # Consider the score `1 / (gap + 1)`.\n    # If gap > item, apply a discount.\n    # This means a bin that is almost full (small gap) is preferred.\n    # A bin that is much larger than needed (large gap) is less preferred.\n\n    # Let's define the priority score for each suitable bin as:\n    # `score = 1.0 / (gaps + 1e-6)`\n    # This directly implements Best Fit.\n\n    # To add a penalty for excessive slack (large gaps), we can modify this.\n    # If a bin has `gap > SOME_THRESHOLD`, reduce its score.\n    # What's a good threshold? Maybe related to `item` or average `bins_remain_cap`.\n    # Let's use `item` as a reference. If `gap > item`, the bin is more than twice the size needed.\n\n    # Let's assign a score that favors small gaps, and decreases for larger gaps.\n    # `score = 1.0 / (gaps + 1e-6)`  -- This is best fit.\n    # To penalize large gaps, let's subtract a term proportional to the gap.\n    # `score = (1.0 / (gaps + 1e-6)) - (0.1 * gaps)`\n    # The `0.1` is a tunable parameter.\n\n    # Let's try a different formulation. We want to maximize remaining space after packing,\n    # but not excessively.\n    # Consider the score that prioritizes bins where `bins_remain_cap` is *just* large enough.\n    # This means `bins_remain_cap` is close to `item`.\n    # We want to maximize `bins_remain_cap` among suitable bins, which is \"Worst Fit\".\n\n    # Let's try to combine Best Fit with a penalty for \"too empty\" bins.\n    # For a suitable bin, the remaining capacity is `gap`.\n    # If `gap` is very large, it's \"too empty\".\n    # We want to prioritize small `gap`.\n    # Consider `score = -gap`. Maximizing this minimizes `gap`.\n    # Let's add a slight bonus for bins that are not excessively empty.\n    # If `gap < item * tolerance`, give a bonus.\n\n    # Let's use a scoring function that is derived from Best Fit, but with a modification for large gaps.\n    # Base score (Best Fit): `1.0 / (gaps + 1e-6)`\n    # Penalty for large gaps: Let's make the score decrease faster for larger gaps.\n    # Consider `1.0 / (gaps + 1e-6)`. If gap is large, this value is small.\n    # What if we try to maximize `(bins_remain_cap[i] - item)` but subject to `bins_remain_cap[i] - item` being small?\n\n    # Let's define the score for each suitable bin as:\n    # `score = 1.0 / (gaps + 1e-6)`  # High score for small gaps (tight fit)\n    # To penalize large gaps (excessive slack), we can add a term that subtracts from the score as gap increases.\n    # Let's use a penalty proportional to the gap itself.\n    # `penalty = 0.1 * gaps`\n    # `final_score = (1.0 / (gaps + 1e-6)) - (0.1 * gaps)`\n\n    # This might penalize too aggressively.\n    # Let's try a simpler approach: favor bins that have remaining capacity that is \"just enough\" or slightly more.\n    # This means we want to MAXIMIZE `bins_remain_cap` among suitable bins, subject to `bins_remain_cap` not being excessively large.\n    # Let's prioritize bins that have a reasonable amount of capacity remaining after packing.\n    # This means maximizing `gap`. This is the opposite of Best Fit.\n\n    # Let's stick to Best Fit but modify the scoring to penalize large gaps.\n    # We want a score that is high for small gaps and decreases as gaps grow.\n    # Consider `score = exp(-gaps / some_scale)`. This is good but potentially non-linear.\n\n    # Let's use a score that is `1.0 / (gap + 1)` and apply a discount if `gap` is large.\n    # For example, if `gap > item`, apply a penalty.\n    # `score = 1.0 / (gaps + 1e-6)`\n    # `penalty_factor = np.where(gaps > item, 0.5, 1.0)`\n    # `final_scores = score * penalty_factor`\n\n    # Let's try a more direct approach using the negative gap, but with a penalty for large gaps.\n    # We want to maximize `(-gaps)` which means minimizing gaps.\n    # To penalize large gaps, we can subtract a term that grows with gaps.\n    # `score = -gaps - (0.1 * gaps**2)`\n    # This will heavily penalize large gaps.\n\n    # Let's define the score based on minimizing the gap and penalizing its size.\n    # We want to maximize `-(gap + a*gap^2)` where 'a' is a penalty factor.\n    # For simplicity, let's use `score = -gaps`. This is just Best Fit.\n\n    # Let's reconsider the penalty for large gaps.\n    # If `gap` is large, the bin is very empty after packing.\n    # This is generally okay, but if there are many such bins, it might not be optimal.\n    # Let's try a score that is `1 / (gap + 1)` and a penalty for `gap > item`.\n    # This promotes Best Fit but discourages very loose fits.\n\n    # Let's implement a score that favors tight fits, with a penalty for excessive remaining capacity.\n    # The penalty will be proportional to the remaining capacity after packing.\n    # Score for each suitable bin `i`:\n    # `remaining_after_packing = bins_remain_cap[i] - item`\n    # `base_score = 1.0 / (remaining_after_packing + 1e-6)` (favors small remaining capacity)\n    # `penalty = 0.1 * remaining_after_packing` (penalizes large remaining capacity)\n    # `final_score = base_score - penalty`\n    # This attempts to combine tight fit with a disincentive for very loose fits.\n\n    final_scores = np.zeros_like(suitable_bins_capacities, dtype=float)\n    for i, bin_idx in enumerate(suitable_bins_indices):\n        remaining_after_packing = suitable_bins_capacities[i] - item\n        \n        # Base score: favors smaller remaining capacity (tightest fit)\n        # Add a small epsilon to avoid division by zero.\n        base_score = 1.0 / (remaining_after_packing + 1e-6)\n        \n        # Penalty: discourages bins with excessively large remaining capacity.\n        # This penalty increases linearly with the remaining capacity after packing.\n        # The coefficient (0.1) is a tunable parameter.\n        penalty = 0.1 * remaining_after_packing\n        \n        final_scores[i] = base_score - penalty\n\n    if final_scores.size > 0:\n        best_fit_in_suitable_idx = np.argmax(final_scores)\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.9999669920071 seconds"
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for empty bins,\n    favoring bins that are neither too full nor too empty relative to item size.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate tightness score: higher for bins leaving less space after packing\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or overly large bins:\n    # Penalize bins whose remaining capacity is significantly larger than the item size.\n    # This encourages using bins that are already somewhat utilized, rather than entirely new/large bins.\n    # The penalty is inversely related to the \"slack\" (bins_remain_cap - item).\n    # A common way to represent this is a penalty that decreases as bins_remain_cap gets smaller.\n    # We'll use a scaled inverse of the remaining capacity *before* packing, but only for fitting bins.\n    \n    # Define a factor that scales the penalty based on how much \"excess\" capacity exists.\n    # A bin with remaining capacity equal to item size should have no penalty from this part.\n    # A bin with much larger remaining capacity gets a higher penalty.\n    \n    # Let's use a penalty term that is high for bins with large `bins_remain_cap`\n    # and low for bins with `bins_remain_cap` closer to `item`.\n    # A possible function: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This means if bins_remain_cap[i] == item, penalty is 1.0.\n    # If bins_remain_cap[i] >> item, penalty is close to 0. We want the opposite.\n    \n    # Corrected logic: We want to *boost* bins with reasonable existing capacity\n    # and *penalize* bins that are very large (effectively \"empty\").\n    # The \"penalty for empty bins\" suggests reducing priority for bins with lots of unused space.\n    # Let's try to penalize bins where `bins_remain_cap` is much larger than `item`.\n    \n    # Let's use a Gaussian-like penalty centered around `item` or `2*item`.\n    # Or simply, penalize bins where `bins_remain_cap` is much larger than the average remaining capacity of fitting bins.\n    \n    # A simpler approach: boost bins that are not \"too empty\".\n    # Consider bins with `bins_remain_cap` relative to `item`.\n    # If `bins_remain_cap[i]` is very large, it means the bin is largely empty.\n    # We want to assign a lower priority to these \"empty\" bins.\n    \n    # Let's combine tightness with a preference for bins that are not \"overly large\" for the item.\n    # This is similar to Heuristic 4 but with an added factor for slack.\n    \n    # We want to combine:\n    # 1. Tight fit: Minimize `bins_remain_cap - item`. This is `1.0 / (remaining_after_packing + epsilon)`.\n    # 2. Penalty for empty/large bins: If `bins_remain_cap` is very large compared to `item`, reduce priority.\n    \n    # Let's define a score that favors bins where `bins_remain_cap` is closer to `item` without being too small.\n    # A function like `exp(-(bins_remain_cap - item)^2 / sigma^2)` can center preference.\n    # Or, we can simply penalize bins with large remaining capacity.\n    \n    # Let's use a penalty based on the *initial* remaining capacity relative to the item size.\n    # A common heuristic is to penalize bins that have a large amount of slack *after* packing.\n    # `slack_after_packing = bins_remain_cap[can_fit_mask] - item`\n    # We want to penalize large `slack_after_packing`. The `tightness_score` already does this.\n    \n    # Let's consider \"penalty for empty bins\" as favoring bins that are not entirely unused.\n    # If `bins_remain_cap` is very large, it might indicate an unused bin.\n    # We can penalize such bins by reducing their priority.\n    \n    # Let's implement a penalty that decreases the score for bins with excessively large remaining capacity.\n    # A simple penalty factor: `1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This penalty is 1 when `bins_remain_cap[i] == item`, and approaches 0 as `bins_remain_cap[i]` grows.\n    # We want to penalize large `bins_remain_cap`.\n    \n    # Consider a penalty that is high when `bins_remain_cap` is large.\n    # For fitting bins, let's create a \"utility\" score that is higher for bins that are \"just right\".\n    # \"Just right\" means not too much remaining capacity, but not so little that it's a tight fit that might be suboptimal for future items.\n    \n    # Let's try a combination:\n    # 1. Tightness: `1.0 / (bins_remain_cap[i] - item + epsilon)`\n    # 2. Penalty for large initial capacity: For bins `i` where `bins_remain_cap[i]` is much larger than `item`, reduce score.\n    #    A factor like `exp(-k * (bins_remain_cap[i] / item))` could work.\n    \n    # Let's adopt a simpler approach inspired by Heuristic 4 (tight fit) and the concept of\n    # penalizing bins that are \"overly large\" for the item, which implicitly relates to \"empty\" bins.\n    \n    # We want to maximize tightness, and penalize large initial remaining capacities for fitting bins.\n    # Let's define a penalty term for bins that have a lot of excess capacity relative to the item.\n    \n    # Consider a penalty function `P(R, I)` where `R` is `bins_remain_cap` and `I` is `item`.\n    # We want `P` to be small when `R` is close to `I`, and larger when `R` is much greater than `I`.\n    # A simple function: `1 + penalty_factor * max(0, bins_remain_cap[i] - item - some_threshold)`\n    # Or, a simpler inverse relationship: `1.0 / (1.0 + penalty_factor * (bins_remain_cap[i] - item))`\n    # This would penalize bins with large remaining space.\n    \n    penalty_factor_slack = 0.2 # Controls how much large initial capacity is penalized.\n    \n    # Calculate the slack penalty: a higher value means less penalty (score is higher).\n    # We want to penalize large `bins_remain_cap`.\n    # Let's define a factor that favors bins that are NOT excessively large.\n    # For fitting bins, `slack_factor = (bins_remain_cap[i] - item) / bins_remain_cap[i]`\n    # A small slack_factor (close to 0) means bins_remain_cap[i] is large, we want to penalize this.\n    # A large slack_factor (close to 1) means bins_remain_cap[i] is close to item, this is good.\n    \n    # Let's use a penalty that reduces the score if `bins_remain_cap` is much larger than `item`.\n    # The \"penalty for empty bins\" suggests that if a bin is mostly empty, we should try to avoid it.\n    # `bins_remain_cap[i] >> item` implies an empty or mostly empty bin.\n    \n    # We can combine tightness with a penalty term that is high for large remaining capacities.\n    # Let's use a factor that is `1.0` for bins that are \"just right\" and decreases for larger capacities.\n    # A factor like `exp(-(bins_remain_cap[i] / item - 1) * penalty_weight)` might work.\n    \n    # For simplicity and clarity, let's combine the tightness score with a penalty that\n    # is inversely proportional to the *initial* remaining capacity.\n    # This favors bins that are already somewhat filled.\n    \n    # Let's define the penalty for \"empty\" bins:\n    # A bin is considered \"empty\" if its remaining capacity is significantly larger than the item size.\n    # We want to reduce the priority of such bins.\n    # A simple penalty function for fitting bins: `penalty = exp(-k * (bins_remain_cap[i] / item))`\n    # This penalty decreases as `bins_remain_cap[i]` increases, which is what we want.\n    \n    # Let's use a penalty that rewards bins that are not excessively large.\n    # A Gaussian-like function centered around a \"good\" capacity might be too complex.\n    # A simpler approach: penalize bins whose remaining capacity is more than X times the item size.\n    \n    # Let's try to combine the inverse of remaining space (tightness) with a penalty\n    # that is high for large initial capacities.\n    \n    # The `tightness_score` is `1.0 / (bins_remain_cap[i] - item + epsilon)`.\n    # We want to multiply this by a factor that decreases as `bins_remain_cap[i]` increases.\n    \n    # Consider a penalty factor: `penalty = 1.0 / (1.0 + penalty_weight * (bins_remain_cap[i] - item))`\n    # This factor is 1 when `bins_remain_cap[i] == item`, and decreases as `bins_remain_cap[i]` grows.\n    # This penalizes bins with large slack.\n    \n    penalty_weight = 0.3 # Adjust to control the penalty strength for large remaining capacities.\n    \n    # Calculate a penalty score that is high for bins with low initial remaining capacity (closer to item)\n    # and low for bins with high initial remaining capacity (more \"empty\").\n    # This is the inverse of what we want. We want to *reduce* score for large bins.\n    \n    # Let's create a \"fill_preference\" score.\n    # Higher score for bins that are not \"too empty\".\n    # A simple inverse relation to remaining capacity: `1.0 / (bins_remain_cap[i] + epsilon)`.\n    # However, we only care about fitting bins.\n    \n    # Let's combine the `tightness_score` with a penalty based on the *initial* remaining capacity.\n    # The penalty should reduce priority for bins that have a lot of space left.\n    # For fitting bins: `penalty_score = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])`\n    # This score is high for small `bins_remain_cap` and low for large `bins_remain_cap`.\n    # So, we should multiply `tightness_score` by this `penalty_score` to achieve the goal.\n    \n    penalty_score = np.zeros_like(bins_remain_cap, dtype=float)\n    penalty_score[can_fit_mask] = 1.0 / (1.0 + penalty_weight * bins_remain_cap[can_fit_mask])\n    \n    # Combine tightness and penalty\n    combined_score = tightness_score * penalty_score\n    \n    # Normalize to ensure priorities are in a comparable range (e.g., 0 to 1)\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 19.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized tightest fit with an epsilon-greedy exploration strategy.\n    Prioritizes bins with minimal remaining capacity after packing,\n    with a small chance to select any fitting bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # If no bins can fit the item, return zero priorities\n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_cap = bins_remain_cap[can_fit_mask]\n    \n    # Calculate tightest fit scores: higher score for less remaining capacity\n    # Adding epsilon to avoid division by zero if remaining capacity equals item size\n    epsilon = 1e-9\n    tight_fit_scores = 1.0 / (available_bins_cap - item + epsilon)\n    \n    # Normalize tight fit scores to a 0-1 range for consistent comparison\n    max_tight_fit = np.max(tight_fit_scores)\n    if max_tight_fit > 0:\n        normalized_tight_fit = tight_fit_scores / max_tight_fit\n    else:\n        normalized_tight_fit = np.zeros_like(tight_fit_scores)\n\n    # Epsilon-greedy exploration:\n    # With probability epsilon, pick a random fitting bin (uniform priority).\n    # Otherwise, pick the bin with the highest normalized tight_fit_score.\n    exploration_prob = 0.1\n    \n    if np.random.rand() < exploration_prob:\n        # Exploration: Assign uniform high priority to all fitting bins\n        priorities[can_fit_mask] = 1.0\n    else:\n        # Exploitation: Use normalized tight fit scores\n        priorities[can_fit_mask] = normalized_tight_fit\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.0885520542481055,
    "SLOC": 19.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fitting preference with adaptive bin utilization and a balanced exploration strategy.\n    Prioritizes bins that minimize waste and are more utilized, with controlled random exploration.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n    exploration_prob = 0.15  # Probability for epsilon-greedy exploration\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities\n\n    available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    # --- Heuristic Components ---\n\n    # 1. Tight Fitting (Minimize Waste): Higher score for bins leaving less space.\n    remaining_after_packing = available_bins_remain_cap - item\n    # Add epsilon to avoid division by zero. Higher score for smaller remaining space.\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # 2. Adaptive Bin Utilization: Prefer bins that are already more full.\n    # Normalize remaining capacity to reflect utilization. Higher score for less remaining capacity.\n    # Avoid division by zero if all bins are empty.\n    max_remaining_overall = np.max(bins_remain_cap) if np.any(bins_remain_cap > 0) else 1.0\n    utilization_scores = (max_remaining_overall - available_bins_remain_cap + epsilon) / (max_remaining_overall + epsilon)\n\n    # Combine core heuristic scores with equal weighting.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize core scores to be between 0 and 1.\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_available_bins = available_bins_remain_cap.size\n    \n    # Generate random scores for exploration. The idea is to allow some randomness\n    # to potentially discover better packing strategies over time, rather than\n    # always picking the deterministically \"best\" based on current heuristics.\n    # We want to give a chance to bins that might not be top-ranked by core heuristics.\n    # We generate random numbers and then sort them to get a random permutation.\n    random_exploration_scores = np.random.rand(num_available_bins)\n    \n    # Blend between the deterministic (core) score and the random exploration score.\n    # With probability `exploration_prob`, the priority will lean towards the random score.\n    # A simple linear interpolation is used: priority = (1-alpha)*core + alpha*random\n    # where alpha is `exploration_prob` for some bins, and 0 for others.\n    \n    # Create a mask for exploration bins.\n    is_exploration_bin = np.random.rand(num_available_bins) < exploration_prob\n    \n    # Calculate final scores: use core scores by default, and exploration scores for chosen bins.\n    final_scores = np.where(is_exploration_bin, random_exploration_scores, normalized_core_scores)\n    \n    # Re-normalize the final scores to ensure they are within a comparable range (0 to 1).\n    # This prevents extreme values from dominating due to the random component.\n    max_final_score = np.max(final_scores)\n    if max_final_score > epsilon:\n        priorities[can_fit_mask] = final_scores / max_final_score\n    else:\n        priorities[can_fit_mask] = final_scores # Should not happen if there are fitting bins, but for safety.\n\n    # Ensure priorities are non-negative (though they should be due to operations).\n    priorities[priorities < 0] = 0\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.058635819704831,
    "SLOC": 29.0,
    "cyclomatic_complexity": 5.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 18.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tight-fit preference with a penalty for excessive remaining capacity,\n    favoring bins that utilize space efficiently and avoid large leftovers.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity *after* packing.\n    # Higher score for bins that leave less remaining space.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    \n    # Avoid division by zero or very small numbers\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Penalty for bins with excessive slack (large remaining capacity *before* packing).\n    # This discourages using bins that are much larger than needed for the current item.\n    # We use a sigmoid-like penalty that is close to 1 for small remaining capacities\n    # and approaches 0 for very large remaining capacities.\n    # This is inspired by the idea of penalizing \"empty\" or \"underutilized\" bins.\n    \n    # Define a sensitivity parameter for slack penalty.\n    # A higher value makes the penalty more aggressive for smaller amounts of slack.\n    slack_penalty_sensitivity = 0.1\n    \n    slack_penalty = np.ones_like(bins_remain_cap, dtype=float)\n    # Apply penalty only to bins that can fit the item\n    slack_penalty[can_fit_mask] = 1.0 / (1.0 + slack_penalty_sensitivity * bins_remain_cap[can_fit_mask])\n\n    # Combine tightness score and slack penalty multiplicatively.\n    # This ensures that bins must be both tightly fitting and not have excessive slack.\n    combined_score = tightness_score * slack_penalty\n    \n    # Normalize scores to a 0-1 range to ensure comparability.\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n    \n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 16.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines tightest fit with adaptive exploration and utilization weighting.\n    Favors bins with less remaining capacity after packing, and bins that are less utilized,\n    with a probabilistic chance to explore less optimal bins.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_prob = 0.1\n\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities # No bin can fit the item\n\n    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Heuristic Component 1: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing\n    remaining_after_packing = valid_bins_remain_cap - item\n    # Use inverse of remaining capacity, add epsilon to avoid division by zero\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Heuristic Component 2: Adaptive Utilization Weighting ---\n    # Prefer bins that are currently less utilized (more empty space).\n    # This aims to balance utilization across bins.\n    # Calculate utilization score: Higher score for less remaining capacity.\n    # Avoid division by zero by considering a baseline if all bins are empty.\n    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0\n    \n    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.\n    if max_capacity_in_use < epsilon:\n        max_capacity_in_use = epsilon\n        \n    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)\n\n\n    # --- Combining Heuristics ---\n    # Combine tight fit and utilization scores. A simple average is used here.\n    # These are core \"exploitation\" scores.\n    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores\n\n    # Normalize these core scores to be between 0 and 1 for consistent weighting\n    max_core_score = np.max(combined_core_scores)\n    if max_core_score > epsilon:\n        normalized_core_scores = combined_core_scores / max_core_score\n    else:\n        normalized_core_scores = np.zeros_like(combined_core_scores)\n\n    # --- Epsilon-Greedy Exploration ---\n    num_fitting_bins = len(valid_bins_remain_cap)\n    \n    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.\n    # Assign a uniformly high priority to randomly selected bins for exploration.\n    exploration_priorities = np.zeros_like(normalized_core_scores)\n    \n    # Calculate number of bins to explore\n    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))\n    \n    if num_to_explore > 0:\n        # Select indices to explore randomly from the fitting bins\n        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)\n        # Give exploration bins a slightly boosted priority to ensure they are considered\n        # but not so high that they completely override good exploitation choices.\n        exploration_priorities[explore_indices] = 1.0 \n\n    # --- Final Priority Calculation ---\n    # Combine exploitation (normalized core scores) and exploration scores\n    # Exploration scores are added, which will boost the priority of randomly selected bins.\n    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.\n    final_scores_unnormalized = normalized_core_scores + exploration_priorities\n\n    # Normalize all final scores for the fitting bins\n    sum_final_scores = np.sum(final_scores_unnormalized)\n    if sum_final_scores > epsilon:\n        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores\n    else:\n        # If all scores are zero (e.g., epsilon issues), assign equal probability\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n        \n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.307937774232155,
    "SLOC": 33.0,
    "cyclomatic_complexity": 7.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a penalty for slack, adapting weights based on bin availability.\n    Prioritizes bins that fit the item tightly, with a penalty for bins that leave excessive slack.\n    Weights adapt to bin scarcity, favoring tight fits when few options exist.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Criterion 1: Tightest Fit (minimize remaining capacity after packing)\n    # Higher score for smaller gaps (bins that fit the item snugly)\n    gaps = suitable_bins_capacities - item\n    # Add a small epsilon to avoid division by zero and prioritize very tight fits\n    tightness_score = 1.0 / (gaps + 1e-6) \n\n    # Criterion 2: Penalty for Slack (discourage bins with excessive remaining capacity)\n    # We want to penalize large gaps. A high penalty for large slack means a low score.\n    # So, we invert the slack for a higher score for smaller slack.\n    # The penalty is higher when slack is larger.\n    # Let's use `slack = remaining_capacity - item`. We want to penalize large slack.\n    # A simple penalty could be `1 / (slack + epsilon)`. This rewards small slack.\n    # Or, let's consider the inverse of `slack` as a score (higher for smaller slack).\n    # `slack_score = 1.0 / (slack + 1e-6)`\n    # This is similar to tightness_score, but let's make it distinct.\n    # Let's penalize bins where `remaining_capacity - item` is large.\n    # We want to maximize `-(remaining_capacity - item)`.\n    # This is equivalent to minimizing `(remaining_capacity - item)`.\n    # For priority (higher is better), we want to maximize `-(gap)`.\n    # Or, minimize `gap`.\n    # Let's define a \"slack penalty score\" which is high for small slack.\n    slack_penalty_score = 1.0 / (gaps + 1e-6) # Same as tightness_score for now, need to differentiate\n\n    # Let's redefine:\n    # Tightness: Prefer bins where `bins_remain_cap[i]` is close to `item`.\n    # This means `gap` is small. Maximize `1 / (gap + epsilon)`.\n\n    # Slack Penalty: Prefer bins that don't leave too much *extra* space.\n    # If `bins_remain_cap[i]` is much larger than `item`, it's bad.\n    # So, we want to minimize `bins_remain_cap[i]`.\n    # For priority, we want to maximize `-bins_remain_cap[i]`.\n    # This would select the smallest capacity bins first.\n    # Let's try to penalize large remaining capacities.\n    # `slack_penalty = -suitable_bins_capacities` (lower is better, so need to invert for priority).\n    # Maximize `-suitable_bins_capacities`.\n    # Or, let's consider `1 / (suitable_bins_capacities + epsilon)` - this favors smaller capacities.\n    slack_penalty_score = 1.0 / (suitable_bins_capacities + 1e-6)\n\n\n    # Adaptive weights based on the number of suitable bins.\n    # If few suitable bins, be more aggressive with tight fits.\n    num_suitable = len(suitable_bins_indices)\n    if num_suitable > 0:\n        # Weight for Tightness: Higher when few suitable bins exist.\n        w_tightness = 2.0 / (num_suitable + 1.0)\n        # Weight for Slack Penalty: Higher when many suitable bins exist (allows more choice).\n        w_slack_penalty = num_suitable / (num_suitable + 1.0)\n    else:\n        w_tightness = 0.5\n        w_slack_penalty = 0.5\n\n    # Combined score: Weighted sum of the two criteria\n    # We want to maximize both `tightness_score` and `slack_penalty_score`.\n    combined_scores = w_tightness * tightness_score + w_slack_penalty * slack_penalty_score\n\n    # Assign scores to the original priority array for the suitable bins\n    priorities[suitable_bins_indices] = combined_scores\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a modified Best Fit (prioritizing bins with a smaller, non-linear gap)\n    with a dynamic exploration strategy that favors less-utilized bins.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Dynamic Exploration: Favor bins that are less utilized (higher remaining capacity)\n    if np.random.rand() < epsilon:\n        # Calculate a score for exploration: higher score for more remaining capacity\n        exploration_scores = bins_remain_cap[suitable_bins_indices]\n        # Apply a non-linear scaling (e.g., log) to emphasize differences in larger capacities\n        # Add a small constant to avoid log(0) if a bin has exactly the item size\n        exploration_scores = np.log1p(exploration_scores - item + 1) \n        exploration_scores /= np.sum(exploration_scores) # Normalize to form a probability distribution\n        chosen_bin_index = np.random.choice(suitable_bins_indices, p=exploration_scores)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Modified Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Prioritize bins with smaller gaps, but also consider the absolute remaining capacity.\n        # This encourages tighter fits but also favors bins that were already quite empty\n        # if the gap is similar.\n        # Using a ratio or a weighted sum can be effective.\n        # Here, we use a simple heuristic: a score that is high for small gaps,\n        # but also rewards bins with larger initial remaining capacity if gaps are comparable.\n        \n        # Score = 1 / (gap + 1) + (remaining_capacity / max_remaining_capacity)\n        # Adding 1 to gap to avoid division by zero if gap is 0.\n        # Normalizing remaining_capacity to prevent it from dominating the score.\n        max_total_capacity = np.max(bins_remain_cap) # Assuming a general max capacity or using the max available\n        if max_total_capacity == 0: # Handle case where all bins have 0 capacity (shouldn't happen if suitable bins exist)\n            max_total_capacity = 1\n            \n        modified_scores = (1.0 / (gaps + 1e-6)) + (suitable_bins_capacities / (max_total_capacity + 1e-6))\n        \n        # Find the index within the 'suitable_bins_indices' array that has the maximum modified score\n        best_fit_in_suitable_idx = np.argmax(modified_scores)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 75.35899481451935,
    "SLOC": 24.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit strategy with an adaptive exploration mechanism.\n    Prioritizes bins that minimize the wasted space (gap) after packing,\n    and uses a dynamic epsilon for exploration based on the item's size relative to\n    the average remaining capacity. It also introduces a \"most empty bin\" preference\n    to encourage utilizing larger bins when fitting smaller items, promoting better\n    distribution and potentially reducing fragmentation.\n    \"\"\"\n    n_bins = len(bins_remain_cap)\n    priorities = np.zeros(n_bins, dtype=float)\n\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_indices)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n\n    # Adaptive exploration\n    avg_remain_cap = np.mean(bins_remain_cap[bins_remain_cap > 0]) if np.any(bins_remain_cap > 0) else 0\n    if avg_remain_cap > 0:\n        epsilon = max(0.01, min(0.2, item / avg_remain_cap * 0.1)) # Dynamic epsilon\n    else:\n        epsilon = 0.1\n\n    if np.random.rand() < epsilon:\n        # Exploration: pick a random suitable bin\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Refined Best Fit with a tie-breaker for \"most empty bin\"\n        gaps = suitable_bins_capacities - item\n        min_gap = np.min(gaps)\n        best_fit_indices = suitable_bins_indices[gaps == min_gap]\n\n        if len(best_fit_indices) == 1:\n            priorities[best_fit_indices[0]] = 1.0\n        else:\n            # Tie-breaker: Prefer the bin with the largest remaining capacity among those with the minimum gap\n            best_fit_bin_capacities = bins_remain_cap[best_fit_indices]\n            most_empty_bin_idx_in_best_fit = np.argmax(best_fit_bin_capacities)\n            chosen_bin_index = best_fit_indices[most_empty_bin_idx_in_best_fit]\n            priorities[chosen_bin_index] = 1.0\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 16, in priority_v2\n    if suitable_bins_indices.size == 0:\nUnboundLocalError: local variable 'suitable_bins_indices' referenced before assignment\n28\n6\n"
  },
  {
    "stdout_filepath": "problem_iter9_response2.txt_stdout.txt",
    "code_path": "problem_iter9_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit approach with a dynamic exploration strategy.\n    Prioritizes bins that leave the smallest remaining capacity, but with\n    an adaptive exploration that favors bins with larger remaining capacity\n    when the item size is small relative to the bin capacities, encouraging\n    more flexible packing.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    \n    # Calculate potential remaining capacities\n    potential_remain_caps = suitable_bins_capacities - item\n\n    # Calculate 'fit_score': higher means a tighter fit (smaller remaining capacity)\n    # We use a small constant to avoid division by zero if an item perfectly fills a bin.\n    fit_score = 1.0 / (potential_remain_caps + 1e-6)\n\n    # Calculate 'exploration_score': favors larger remaining capacities,\n    # especially when the item is relatively small compared to the bin.\n    # This is scaled by the item's proportion to the bin's current capacity.\n    # A higher exploration_score means a more exploratory choice.\n    exploration_score = (suitable_bins_capacities / (item + 1e-6))\n\n    # Combine scores. We want to prioritize tight fits (high fit_score)\n    # but also allow exploration (exploration_score).\n    # A simple combination: prioritize tight fit, but if there's significant\n    # \"slack\" in fitting (item is much smaller than bin), give it a boost.\n    # We can use a non-linear scaling for exploration_score to make it more impactful\n    # when the item is very small relative to the bin.\n    \n    # Using a sigmoid-like scaling for exploration to modulate its influence\n    # and a power for fit_score to emphasize tighter fits.\n    \n    # Scale exploration score to be between 0 and 1, emphasizing larger ratios\n    # using a hyperbola-like function and then clamping.\n    # The function x/(1+x) or similar could be used. Let's try a simpler approach first.\n    \n    # A more direct approach: combine fit and exploration with a weighting that can adapt.\n    # For simplicity here, we'll use a fixed combination, but in a real adaptive system,\n    # this combination could be learned or tuned.\n    \n    # Let's try a weighted sum where exploration is boosted if the item is small relative to the bin.\n    # Define a threshold for \"small item relative to bin\".\n    item_size_ratio = item / suitable_bins_capacities\n    \n    # A simple heuristic: exploration priority is higher when the item_size_ratio is low.\n    # We can use 1 - item_size_ratio as a base for exploration contribution.\n    # Let's try to give a slight boost to exploration based on how much \"space\" is left.\n    \n    # Revised combination:\n    # Priority = (1-alpha) * Tightly_Fit_Score + alpha * Exploration_Score\n    # Tightly_Fit_Score: Higher for smaller remaining capacity (inverse of remaining capacity)\n    # Exploration_Score: Higher for bins that have more remaining capacity *after* packing,\n    #                    weighted by how \"easy\" it was to fit the item.\n    \n    # Let's simplify the exploration component: it's about picking a bin that\n    # has enough space but isn't necessarily the *tightest* fit.\n    # A simple exploration bonus could be added to bins that have significantly more capacity\n    # than needed.\n    \n    # We want a priority that favors bins that leave small capacity (Best Fit)\n    # but also considers bins that have a lot of room left as a secondary consideration\n    # for exploration.\n    \n    # Let's assign a \"fitness score\" based on how well the item fits.\n    # A perfect fit (remaining_capacity = 0) gets a high score.\n    # The remaining capacity after packing:\n    remaining_after_packing = suitable_bins_capacities - item\n    \n    # Score for \"tightness\": higher is better (smaller remaining capacity)\n    # We can use inverse of remaining capacity, scaled. Add a small constant to avoid division by zero.\n    tightness_score = 1.0 / (remaining_after_packing + 1e-6)\n    \n    # Score for \"exploration\": encourage picking bins with more free space if it's not a very tight fit.\n    # We can base this on the original capacity of the bin. Bins with larger original capacity\n    # might be more flexible for future items.\n    # Let's use the original bin capacity as a proxy for flexibility.\n    exploration_score_component = suitable_bins_capacities\n    \n    # Combine scores: Prioritize tightness, but add a bonus for exploration\n    # if the fit isn't extremely tight.\n    # If remaining_after_packing is very small, exploration_score_component is less relevant.\n    # We can modulate exploration_score_component by how \"loose\" the fit is.\n    \n    # Let's use a weighted sum where the weight on exploration depends on the tightness.\n    # A simple approach: Base priority is tightness. If the item is relatively small\n    # for the bin, give it an additional boost.\n    \n    # Consider the \"slack\" in the bin: original_capacity - item\n    slack = suitable_bins_capacities - item\n    \n    # A combined priority that emphasizes tight fits but also rewards bins with\n    # significant remaining capacity when the item is small.\n    # Let's use a formulation that combines inverse remaining capacity with a scaled slack.\n    # The scaling for slack should encourage larger slack values.\n    \n    # Base priority: inverse of remaining capacity (tight fit)\n    base_priority = 1.0 / (slack + 1e-6)\n    \n    # Exploration boost: proportional to slack, but maybe scaled non-linearly\n    # to avoid over-emphasis on very large slacks.\n    # Using a log scale for slack can help temper very large values.\n    # We add 1 to slack before log to handle slack=0.\n    exploration_boost = np.log1p(slack)\n    \n    # Combine them: a weighted sum, where exploration_boost is added.\n    # The relative importance can be adjusted.\n    # For a balanced approach, we can normalize these components or use a weighting factor.\n    \n    # Let's try a direct approach: prioritize bins with minimal slack.\n    # For bins with similar slack, pick the one that was more \"empty\" initially.\n    \n    # Final approach idea: prioritize bins that leave the smallest capacity after packing.\n    # If multiple bins have the same minimal remaining capacity, pick the one\n    # that had the largest original capacity among them (to allow for future flexibility).\n    \n    # Calculate the remaining capacity after packing.\n    remaining_after_packing = suitable_bins_capacities - item\n    \n    # We want to prioritize bins with minimum remaining_after_packing.\n    # If there are ties, we want to break ties by choosing the bin with the largest\n    # original capacity from the tied set.\n    \n    # To achieve this, we can create a composite score.\n    # A common way to break ties is to add a small value related to the tie-breaking\n    # criterion to the primary criterion.\n    # However, directly adding original capacity to remaining capacity might be problematic\n    # as we want to MINIMIZE remaining capacity.\n    \n    # Instead, we can construct a tuple for sorting: (-remaining_after_packing, original_capacity)\n    # Or, create a score where we invert the primary objective and add a scaled tie-breaker.\n    \n    # Let's create a score where smaller values are better for the primary objective (remaining capacity).\n    # And larger values are better for the secondary objective (original capacity).\n    # Score = remaining_after_packing - C * original_capacity (where C is a small positive constant)\n    # This way, minimizing the score means minimizing remaining capacity, and if they are equal,\n    # we prefer larger original capacity.\n    \n    # However, we need to return priorities where HIGHER is better.\n    # So we invert this score.\n    # Priority = -(remaining_after_packing - C * original_capacity)\n    # Priority = -remaining_after_packing + C * original_capacity\n    \n    # Let's choose C such that original_capacity has a noticeable but not overwhelming impact.\n    # A common strategy is to scale the tie-breaker by a factor related to the range\n    # of the primary objective.\n    \n    # Let's assume a maximum possible original capacity.\n    max_original_capacity = np.max(bins_remain_cap) # or a predefined bin capacity limit\n    \n    # A scale factor could be 1 / (max_original_capacity + 1).\n    # This ensures that the contribution from original_capacity is smaller than the contribution\n    # from remaining_after_packing for typical values.\n    \n    # Let's use a simpler composite score that directly prioritizes minimum remaining capacity,\n    # and then maximum original capacity for ties.\n    \n    # We can achieve this by creating a score where we multiply the primary criterion\n    # by a large number and add the secondary criterion.\n    # Score = remaining_after_packing * LargeMultiplier - original_capacity\n    # We want to MINIMIZE this score.\n    \n    # To get priorities where HIGHER is better:\n    # Priority = - (remaining_after_packing * LargeMultiplier - original_capacity)\n    # Priority = -remaining_after_packing * LargeMultiplier + original_capacity\n    \n    # Let's set LargeMultiplier to be significantly larger than the maximum possible original_capacity.\n    # This ensures that the remaining_after_packing dominates the score.\n    \n    # Consider the range of remaining_after_packing. It's from 0 up to (max_suitable_capacity - item).\n    # Max value of original_capacity is max(bins_remain_cap).\n    \n    # A safe LargeMultiplier:\n    large_multiplier = 1e9 # A sufficiently large number\n    \n    # Calculate the priority for each suitable bin\n    # Priority = -remaining_after_packing + original_capacity_component\n    # We want to prioritize bins with smallest `remaining_after_packing`.\n    # So, `-remaining_after_packing` will be higher for smaller values.\n    # We want to prioritize bins with largest `original_capacity` for ties.\n    # So, `original_capacity` contributes positively.\n    \n    # Construct the composite priority:\n    # This priority is designed such that sorting these values in descending order\n    # will first pick bins with minimum remaining capacity, and then among those,\n    # pick bins with maximum original capacity.\n    \n    composite_priorities = (\n        -remaining_after_packing * large_multiplier +\n        suitable_bins_capacities\n    )\n    \n    # Assign these composite priorities to the original bin indices\n    priorities[suitable_bins_indices] = composite_priorities\n\n    # Normalize priorities to be between 0 and 1 for a cleaner distribution if needed,\n    # or just use the raw scores for selection. For selection, raw scores are fine.\n    # If we want a probabilistic selection based on these priorities (like Softmax), normalization is needed.\n    # For picking the max priority, raw scores are sufficient.\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "SLOC": 26.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines Best Fit with a novel \"nearness\" score and adaptive exploration.\n    Prioritizes bins that are a good fit, but also considers bins that are \"almost\"\n    a good fit to prevent situations where only very large bins are left.\n    The exploration strategy adapts based on the number of suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Adaptive exploration: increase exploration probability if there are many suitable bins\n    # and decrease if there are very few to avoid wasting opportunities.\n    num_suitable_bins = suitable_bins_indices.size\n    if num_suitable_bins <= 2:\n        exploration_prob = 0.2  # Higher exploration for very few options\n    elif num_suitable_bins > 5:\n        exploration_prob = 0.05 # Lower exploration for many options\n    else:\n        exploration_prob = 0.1 # Default exploration\n\n    if np.random.rand() < exploration_prob:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Combine Best Fit with a \"nearness\" score.\n        # The \"nearness\" score is higher for bins that are close to the optimal gap,\n        # but not so close that they become inefficient.\n        # We use a scaled inverse of the gap + a small constant to avoid division by zero,\n        # and a sigmoid-like scaling to smooth the priority.\n        \n        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits\n        gaps_for_scoring = gaps + 1e-6 \n        \n        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps\n        # that might be too tight. We use a form of smooth inverse.\n        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.\n        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.\n        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.\n        \n        # Let's use a score that is higher for smaller gaps, but tapers off.\n        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.\n        # We'll use `exp(-k * gap)` for a smoother, non-linear response.\n        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap\n        nearness_scores = np.exp(-k_factor * gaps)\n\n        # Normalize scores so the maximum is 1\n        if np.max(nearness_scores) > 0:\n            normalized_scores = nearness_scores / np.max(nearness_scores)\n        else:\n            normalized_scores = np.zeros_like(nearness_scores)\n\n        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.\n        # Or, we can simply use the nearness_scores directly if they are well-calibrated.\n        # For this version, let's directly use the normalized nearness scores as priorities.\n        \n        # Get the index of the bin with the highest nearness score\n        best_nearness_idx_in_suitable = np.argmax(normalized_scores)\n        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]\n        \n        priorities[best_nearness_original_idx] = 1.0\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.11846828879138,
    "SLOC": 30.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response4.txt_stdout.txt",
    "code_path": "problem_iter9_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines a refined Best Fit strategy with a dynamic exploration mechanism\n    that favors bins that are \"almost full\" to encourage packing larger items\n    into fewer bins, while also considering \"almost empty\" bins for small items.\n    It uses a sigmoid-like scaling to create smoother priority transitions.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Calculate a \"closeness\" score for each suitable bin.\n    # This score is higher for bins that leave a small gap (good for Best Fit)\n    # and also for bins that are already quite full (to group items).\n    # We use a scaled inverse gap, with a bias towards larger remaining capacities\n    # for exploration.\n    \n    # Normalize capacities to have a value between 0 and 1 for the sigmoid.\n    # Add a small epsilon to avoid division by zero if a bin has exactly the item's size.\n    normalized_remaining_cap = suitable_bins_capacities / (np.max(bins_remain_cap) + 1e-9)\n    \n    # Use a sigmoid-like function to map gaps to priorities.\n    # A smaller gap should result in a higher priority.\n    # We invert the gap to make smaller gaps correspond to larger sigmoid inputs.\n    # A scaling factor `alpha` controls the steepness of the sigmoid.\n    alpha = 10.0\n    closeness_score = 1 / (1 + np.exp(-alpha * (1 - (gaps / (item + 1e-9)))))\n\n    # Introduce a factor that prefers bins with more remaining capacity (less full bins)\n    # to encourage spreading smaller items if available, using a scaled inverse\n    # of the remaining capacity.\n    # Add small value to avoid division by zero for bins with 0 remaining capacity if they exist.\n    space_utilization_score = (bins_remain_cap[suitable_bins_indices] + 1e-9) / (np.max(bins_remain_cap) + 1e-9)\n    \n    # Combine scores: lean towards closeness (Best Fit) but also consider space utilization\n    # which implicitly favors larger remaining capacities for exploration of these slots.\n    # A weighted sum: 70% closeness, 30% space utilization.\n    combined_scores = 0.7 * closeness_score + 0.3 * space_utilization_score\n\n    # Apply a small exploration factor by slightly perturbing the best bin's score.\n    # This is a deterministic way to add a little noise without a fixed epsilon.\n    # We find the index of the highest score and add a small bonus to it,\n    # but also slightly boost other high-scoring bins to avoid always picking the absolute top.\n    \n    sorted_indices_by_score = np.argsort(combined_scores)[::-1]\n    \n    # Assign priorities based on the combined scores, with a slight boost to the top few.\n    # The highest score gets a priority of 1.0, the next highest gets 0.9, etc.\n    # This provides a relative ordering and reduces reliance on absolute gap values.\n    for i, idx_in_suitable in enumerate(sorted_indices_by_score):\n        original_idx = suitable_bins_indices[idx_in_suitable]\n        priority_value = max(0, 1.0 - (i * 0.1)) # Give decreasing priority\n        priorities[original_idx] = priority_value\n\n    # Ensure the highest priority is exactly 1.0 for the best bin\n    if suitable_bins_indices.size > 0:\n        best_bin_in_suitable_idx = np.argmax(combined_scores)\n        best_bin_original_idx = suitable_bins_indices[best_bin_in_suitable_idx]\n        priorities[best_bin_original_idx] = 1.0\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.99997677000647 seconds"
  }
]