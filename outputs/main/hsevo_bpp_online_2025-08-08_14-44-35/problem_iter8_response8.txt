```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines tightest fit with adaptive exploration and utilization weighting.
    Favors bins with less remaining capacity after packing, and bins that are less utilized,
    with a probabilistic chance to explore less optimal bins.
    """
    epsilon = 1e-9
    exploration_prob = 0.1

    can_fit_mask = bins_remain_cap >= item
    priorities = np.zeros_like(bins_remain_cap, dtype=float)

    if not np.any(can_fit_mask):
        return priorities # No bin can fit the item

    valid_bins_remain_cap = bins_remain_cap[can_fit_mask]

    # --- Heuristic Component 1: Tightest Fit ---
    # Prioritize bins that leave minimal remaining capacity after packing
    remaining_after_packing = valid_bins_remain_cap - item
    # Use inverse of remaining capacity, add epsilon to avoid division by zero
    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)

    # --- Heuristic Component 2: Adaptive Utilization Weighting ---
    # Prefer bins that are currently less utilized (more empty space).
    # This aims to balance utilization across bins.
    # Calculate utilization score: Higher score for less remaining capacity.
    # Avoid division by zero by considering a baseline if all bins are empty.
    max_capacity_in_use = np.max(bins_remain_cap) - np.min(bins_remain_cap[can_fit_mask]) if np.any(bins_remain_cap) else 0
    
    # If max_capacity_in_use is very small or zero, use a small positive value for stable division.
    if max_capacity_in_use < epsilon:
        max_capacity_in_use = epsilon
        
    utilization_scores = (max_capacity_in_use - (bins_remain_cap[can_fit_mask] - item) + epsilon) / (max_capacity_in_use + epsilon)


    # --- Combining Heuristics ---
    # Combine tight fit and utilization scores. A simple average is used here.
    # These are core "exploitation" scores.
    combined_core_scores = 0.5 * tight_fit_scores + 0.5 * utilization_scores

    # Normalize these core scores to be between 0 and 1 for consistent weighting
    max_core_score = np.max(combined_core_scores)
    if max_core_score > epsilon:
        normalized_core_scores = combined_core_scores / max_core_score
    else:
        normalized_core_scores = np.zeros_like(combined_core_scores)

    # --- Epsilon-Greedy Exploration ---
    num_fitting_bins = len(valid_bins_remain_cap)
    
    # Exploration strategy: with probability `exploration_prob`, choose a random fitting bin.
    # Assign a uniformly high priority to randomly selected bins for exploration.
    exploration_priorities = np.zeros_like(normalized_core_scores)
    
    # Calculate number of bins to explore
    num_to_explore = int(np.floor(exploration_prob * num_fitting_bins))
    
    if num_to_explore > 0:
        # Select indices to explore randomly from the fitting bins
        explore_indices = np.random.choice(num_fitting_bins, size=num_to_explore, replace=False)
        # Give exploration bins a slightly boosted priority to ensure they are considered
        # but not so high that they completely override good exploitation choices.
        exploration_priorities[explore_indices] = 1.0 

    # --- Final Priority Calculation ---
    # Combine exploitation (normalized core scores) and exploration scores
    # Exploration scores are added, which will boost the priority of randomly selected bins.
    # The `1 - exploration_prob` factor is implicitly handled by how exploration_priorities is constructed and added.
    final_scores_unnormalized = normalized_core_scores + exploration_priorities

    # Normalize all final scores for the fitting bins
    sum_final_scores = np.sum(final_scores_unnormalized)
    if sum_final_scores > epsilon:
        priorities[can_fit_mask] = final_scores_unnormalized / sum_final_scores
    else:
        # If all scores are zero (e.g., epsilon issues), assign equal probability
        priorities[can_fit_mask] = 1.0 / num_fitting_bins
        
    return priorities
```
