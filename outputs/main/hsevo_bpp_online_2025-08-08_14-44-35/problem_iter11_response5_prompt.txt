{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tight-fit preference with an adaptive penalty for unused bins.\n    Favors bins that leave minimal remaining space, with a bonus for bins\n    that have already been utilized to some extent.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate tightness score: inverse of remaining capacity after packing\n    # Higher score for bins that leave less remaining space (closer to zero slack)\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tightness_score = np.zeros_like(bins_remain_cap, dtype=float)\n    tightness_score[can_fit_mask] = 1.0 / (remaining_after_packing + epsilon)\n\n    # Adaptive penalty for \"empty\" or less utilized bins:\n    # Penalize bins with high remaining capacity more significantly.\n    # This encourages using bins that are already partially filled.\n    # We use a logistic-like function (sigmoid inverse) for a smooth penalty.\n    # Bins with very high remaining capacity get a score close to 0.5.\n    # Bins with remaining capacity close to the item size get a score closer to 1.0.\n    \n    # To make it adaptive, let's consider the distribution of remaining capacities.\n    # A simple approach is to normalize the remaining capacities relative to the maximum.\n    # However, a direct penalty on large remaining capacity is more straightforward.\n    \n    # Let's use a penalty factor that is inversely proportional to the remaining capacity\n    # after packing, but smoothed.\n    \n    # The idea is to combine the \"tightest fit\" with a preference for bins that are\n    # not \"too empty\". A bin that leaves a lot of space after packing is less desirable.\n    # The slack `remaining_after_packing` is what we want to minimize.\n    \n    # Let's introduce a bonus for bins that have already been used, which we can infer\n    # from `bins_remain_cap` being significantly less than some assumed max capacity.\n    # Without knowing the initial capacity, we can penalize bins whose current `bins_remain_cap`\n    # is large relative to the item.\n    \n    # Let's define a \"fill level\" score for each bin that *can* fit the item.\n    # Higher score means more \"filled\" (less remaining capacity).\n    # A simple fill score could be `1.0 - (bins_remain_cap / MAX_CAPACITY)`.\n    # Without MAX_CAPACITY, we can normalize by the max *available* capacity.\n    \n    # Let's combine the `tightness_score` with a penalty that discourages bins\n    # that still have a lot of capacity *after* the item is packed.\n    \n    # Heuristic 4's core: minimize slack.\n    # The \"penalty for empty bins\" can be interpreted as: if we have a choice\n    # between a nearly empty bin and a partially filled bin with similar slack,\n    # prefer the partially filled one.\n    \n    # Let's use a composite score:\n    # Score = Tightness * Utilization_Bonus\n    \n    # Tightness: 1 / (slack + epsilon)\n    # Utilization_Bonus: How \"full\" is the bin *before* packing?\n    # A simple proxy for utilization without knowing initial capacity:\n    # If `bins_remain_cap` is large, it's less utilized.\n    # Let's create a bonus that increases as `bins_remain_cap` decreases.\n    \n    # Calculate a \"fill fraction\" for fitting bins.\n    # A larger fraction means the bin is more \"used\".\n    # We need a reference for \"fully empty\" vs \"full\".\n    # Let's use the maximum remaining capacity among fitting bins as a reference point for \"emptiness\".\n    # This is not ideal as it's adaptive to the current state.\n    \n    # A more robust \"penalty for empty bins\" is to give a bonus to bins that are\n    # already partially occupied.\n    \n    # Let's use a combined score: Tightness + Utilization_Bonus\n    # Tightness: `1 / (slack + epsilon)`\n    # Utilization_Bonus: Let's say, a small constant *if* the bin is not \"empty\".\n    # How to define \"empty\"? If `bins_remain_cap` is close to the maximum possible bin capacity.\n    # Without knowing MAX_CAPACITY, let's use a relative measure.\n    \n    # Let's implement a score that rewards tightness and also provides a bonus\n    # for bins that have already been used.\n    \n    # The `tightness_score` is good. Let's call it `score_tightness`.\n    score_tightness = np.zeros_like(bins_remain_cap, dtype=float)\n    score_tightness[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)\n\n    # Now, add a bonus for bins that are not \"empty\".\n    # We can define \"not empty\" as having `bins_remain_cap` below a certain threshold.\n    # A simpler approach is to give a bonus based on how *little* remaining capacity there is.\n    # This bonus should be smaller than the primary tightness score.\n    \n    # Let's define a \"fill_score\" which is higher for bins with less remaining capacity.\n    # For fitting bins: `fill_score = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)`\n    # This rewards bins that are already quite full.\n    \n    # Combine `score_tightness` and `fill_score`.\n    # A multiplicative approach: `priority = score_tightness * (1 + fill_score * bonus_weight)`\n    # This rewards bins that are both tight AND already quite full.\n    \n    bonus_weight = 0.2 # Weight for the fill score bonus\n    \n    fill_score = np.zeros_like(bins_remain_cap, dtype=float)\n    fill_score[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] + epsilon)\n    \n    combined_score = score_tightness * (1.0 + fill_score * bonus_weight)\n    \n    # Normalize scores so the highest priority is 1.0\n    max_score = np.max(combined_score)\n    if max_score > 0:\n        priorities[can_fit_mask] = combined_score[can_fit_mask] / max_score\n        \n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit with a novel \"nearness\" score and adaptive exploration.\n    Prioritizes bins that are a good fit, but also considers bins that are \"almost\"\n    a good fit to prevent situations where only very large bins are left.\n    The exploration strategy adapts based on the number of suitable bins.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    suitable_bins_mask = bins_remain_cap >= item\n\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities\n\n    suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n    gaps = suitable_bins_capacities - item\n\n    # Adaptive exploration: increase exploration probability if there are many suitable bins\n    # and decrease if there are very few to avoid wasting opportunities.\n    num_suitable_bins = suitable_bins_indices.size\n    if num_suitable_bins <= 2:\n        exploration_prob = 0.2  # Higher exploration for very few options\n    elif num_suitable_bins > 5:\n        exploration_prob = 0.05 # Lower exploration for many options\n    else:\n        exploration_prob = 0.1 # Default exploration\n\n    if np.random.rand() < exploration_prob:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation: Combine Best Fit with a \"nearness\" score.\n        # The \"nearness\" score is higher for bins that are close to the optimal gap,\n        # but not so close that they become inefficient.\n        # We use a scaled inverse of the gap + a small constant to avoid division by zero,\n        # and a sigmoid-like scaling to smooth the priority.\n        \n        # Add a small epsilon to gaps to avoid division by zero if item perfectly fits\n        gaps_for_scoring = gaps + 1e-6 \n        \n        # Calculate a score that favors smaller gaps (better fit) but penalizes extremely small gaps\n        # that might be too tight. We use a form of smooth inverse.\n        # Higher score for smaller gaps, but with a diminishing return as gap approaches zero.\n        # The scaling `1 / (gap + 1)` makes smaller gaps have higher scores.\n        # We can further shape this with a power, e.g., `(1 / (gap + 1))**1.5` for more emphasis on tighter fits.\n        \n        # Let's use a score that is higher for smaller gaps, but tapers off.\n        # A simple approach is `1.0 / (gap + constant)` or `exp(-k * gap)`.\n        # We'll use `exp(-k * gap)` for a smoother, non-linear response.\n        k_factor = 0.5 # Controls how quickly the score drops off with increasing gap\n        nearness_scores = np.exp(-k_factor * gaps)\n\n        # Normalize scores so the maximum is 1\n        if np.max(nearness_scores) > 0:\n            normalized_scores = nearness_scores / np.max(nearness_scores)\n        else:\n            normalized_scores = np.zeros_like(nearness_scores)\n\n        # Combine with Best Fit: A simple way is to give the best fit bin a slightly higher priority.\n        # Or, we can simply use the nearness_scores directly if they are well-calibrated.\n        # For this version, let's directly use the normalized nearness scores as priorities.\n        \n        # Get the index of the bin with the highest nearness score\n        best_nearness_idx_in_suitable = np.argmax(normalized_scores)\n        best_nearness_original_idx = suitable_bins_indices[best_nearness_idx_in_suitable]\n        \n        priorities[best_nearness_original_idx] = 1.0\n\n    return priorities\n\n### Analyze & experience\n- *   **Heuristic 1 vs. Heuristic 12:** Heuristic 1 combines Best Fit with epsilon-greedy. Heuristic 12 also combines Best Fit with epsilon-greedy but normalizes the tight-fit scores. Normalization (12) is generally better as it makes scores comparable and less dependent on absolute values, but Heuristic 1's exploration is a bit more nuanced (random choice of suitable bin vs. uniform priority). The ranking difference here is minimal, but 12's normalization might offer slightly more robust behavior.\n*   **Heuristic 1 vs. Heuristic 13/14/20:** Heuristics 13/14/20 introduce a \"nearness\" score based on exponential decay of the gap and adaptive exploration. This is more sophisticated than Heuristic 1's simple Best Fit. The dynamic exploration probability in 13/14/20 is also an improvement. The ranking of these depends on how well the \"nearness\" score and adaptive exploration perform in practice, but they represent a clear step up in complexity and potential effectiveness over Heuristic 1.\n*   **Heuristic 1 vs. Heuristic 19/20:** Heuristics 19 and 20 combine a modified Best Fit (using log for exploration and a composite score for exploitation) with dynamic exploration favoring less-utilized bins. This is more complex than Heuristic 1 and likely more effective due to the multi-faceted scoring and targeted exploration. The slight edge of 19/20 over 13/14 suggests a better combination of exploration and exploitation logic.\n*   **Heuristic 2/5/7/8/10 vs. Heuristic 11:** Heuristics 2, 5, 7, 8, and 10 are quite similar, combining tightness with a utilization bonus (often `1/(capacity + epsilon)`). Heuristic 11 refines this by using a more robust utilization score (`(max_remaining - current_remaining) / max_remaining`) and a blended epsilon-greedy approach (using random scores for exploration bins). This refined approach in Heuristic 11 is likely better, leading to its higher ranking compared to the earlier utilization heuristics.\n*   **Heuristic 4 vs. Heuristic 10:** Both combine tightness with a penalty for empty/large bins using inverse relationships with remaining capacity. Heuristic 10 uses a more direct inverse `1/(1 + penalty_weight * capacity)` whereas Heuristic 4 uses `1/(1 + penalty_weight * (capacity - item))`. The latter might be slightly more nuanced by considering the `item` size, but the difference is subtle. The ranking suggests 10 is slightly preferred.\n*   **Heuristic 9 vs. Heuristic 11:** Both combine tightness and utilization with adaptive exploration. Heuristic 9 has a dynamically adjusted `exploration_prob` based on bin count. Heuristic 11 uses a fixed `exploration_prob` but blends scores. Heuristic 11's blended score approach for exploration might be more stable and predictable than dynamically adjusting `exploration_prob`. However, the core scoring in 11 (more robust utilization, blending) is generally superior.\n*   **Heuristic 15/16/17/18 vs. Heuristic 11:** These heuristics are identical to each other and share the same core logic as Heuristic 11 but with a different exploration strategy (boosting random bins with a fixed priority). Heuristic 11's blended exploration might offer a smoother integration. The slight drop in ranking for 15-18 compared to 11 suggests the blending method in 11 is preferred over additive boosting of random bins.\n*   **Overall Trend:** Higher ranked heuristics tend to combine multiple scoring criteria (tightness, utilization), use more sophisticated normalization or blending for scores, and employ more refined exploration strategies (dynamic probability, score blending, or favoring less utilized bins for exploration). The simplest heuristics (like pure Best Fit or basic epsilon-greedy) are ranked lower.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on avoiding ineffective practices:\n\n*   **Keywords:** Multi-objective, normalization, adaptive exploration, performance, clarity, robustness.\n*   **Advice:** Focus on combining multiple, clearly defined objectives using robust methods like weighted sums or Pareto fronts. Implement adaptive exploration mechanisms (e.g., annealing, bandit-based) and ensure all components are well-tested for correctness.\n*   **Avoid:** Overly complex, non-linear, or ad-hoc scoring functions without clear justification. Blindly prioritizing one objective without considering trade-offs. Bugs and unclear logic.\n*   **Explanation:** This approach emphasizes a structured, evidence-based design process. By clearly defining objectives and using proven techniques for combining them and managing exploration, we build heuristics that are both effective and maintainable, avoiding the pitfalls of complexity and ambiguity.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}