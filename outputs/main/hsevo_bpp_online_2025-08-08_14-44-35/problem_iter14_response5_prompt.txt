{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines Best Fit (tightest fit) with an epsilon-greedy exploration strategy.\n    Prioritizes bins that leave the smallest remaining capacity after packing,\n    with a small chance of picking any suitable bin to encourage exploration.\n    \"\"\"\n    epsilon = 0.05  # Probability of exploration\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n    suitable_bins_indices = np.where(suitable_bins_mask)[0]\n\n    if suitable_bins_indices.size == 0:\n        return priorities  # No bin can fit the item\n\n    # Exploration phase: with probability epsilon, pick a random suitable bin\n    if np.random.rand() < epsilon:\n        chosen_bin_index = np.random.choice(suitable_bins_indices)\n        priorities[chosen_bin_index] = 1.0\n    else:\n        # Exploitation phase: Best Fit strategy\n        suitable_bins_capacities = bins_remain_cap[suitable_bins_indices]\n        # Calculate the 'gap' or remaining capacity after fitting the item\n        gaps = suitable_bins_capacities - item\n        \n        # Find the index within the 'suitable_bins_indices' array that has the minimum gap\n        best_fit_in_suitable_idx = np.argmin(gaps)\n        # Get the original index of this best-fitting bin\n        best_fit_original_idx = suitable_bins_indices[best_fit_in_suitable_idx]\n        priorities[best_fit_original_idx] = 1.0\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines tightest fit with a penalty for under-utilized bins,\n    using a novel exploration strategy to balance exploitation and exploration.\n    \"\"\"\n    epsilon = 1e-9\n    exploration_factor = 0.2  # Controls the degree of exploration\n\n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    if not np.any(can_fit_mask):\n        return priorities  # No bin can fit the item\n\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n\n    # --- Exploitation Component: Tightest Fit ---\n    # Prioritize bins that leave minimal remaining capacity after packing.\n    # Higher score for smaller remaining space.\n    remaining_after_packing = fitting_bins_remain_cap - item\n    tight_fit_scores = 1.0 / (remaining_after_packing + epsilon)\n\n    # --- Exploitation Component: Utilization Penalty ---\n    # Penalize bins that are significantly under-utilized (i.e., have a lot of remaining capacity).\n    # This encourages filling existing bins before opening new ones.\n    # We use the inverse of the *initial* remaining capacity as a proxy for utilization.\n    # Higher utilization (less remaining capacity) gets a higher score.\n    utilization_scores = 1.0 / (fitting_bins_remain_cap + epsilon)\n\n    # --- Combine Exploitation Scores ---\n    # Weighted sum of tight fit and utilization scores.\n    # Giving slightly more weight to tight fit as it's the primary goal of BPP.\n    combined_exploitation_scores = 0.6 * tight_fit_scores + 0.4 * utilization_scores\n\n    # Normalize exploitation scores to [0, 1]\n    max_exploitation_score = np.max(combined_exploitation_scores)\n    if max_exploitation_score > epsilon:\n        normalized_exploitation_scores = combined_exploitation_scores / max_exploitation_score\n    else:\n        normalized_exploitation_scores = np.zeros_like(combined_exploitation_scores)\n\n    # --- Exploration Component: Adaptive Exploration ---\n    # We want to explore bins that are not necessarily the best according to exploitation.\n    # Instead of random choice, we will boost the scores of bins that are \"average\" or slightly below average\n    # in terms of exploitation, giving them a chance.\n    num_fitting_bins = len(fitting_bins_remain_cap)\n    \n    # Calculate a baseline score (e.g., mean exploitation score)\n    mean_exploitation_score = np.mean(normalized_exploitation_scores)\n    \n    # Create exploration scores: higher for bins closer to the mean exploitation score.\n    # This aims to explore \"promising but not top\" candidates.\n    # We add a small constant to ensure even the lowest scores get some exploration boost if needed.\n    exploration_scores = np.exp(-((normalized_exploitation_scores - mean_exploitation_score) / (mean_exploitation_score + epsilon))**2)\n    \n    # Apply the exploration factor to blend exploitation and exploration\n    # The final priority is a mix:\n    # (1 - exploration_factor) * exploitation_scores + exploration_factor * exploration_scores\n    # This ensures that exploration never completely overrides exploitation,\n    # and also that exploitation still has a significant impact.\n    final_priorities_unnormalized = (1 - exploration_factor) * normalized_exploitation_scores + exploration_factor * exploration_scores\n\n    # Normalize final priorities for the fitting bins\n    sum_final_priorities = np.sum(final_priorities_unnormalized)\n    if sum_final_priorities > epsilon:\n        priorities[can_fit_mask] = final_priorities_unnormalized / sum_final_priorities\n    else:\n        # If all scores are zero, distribute probability equally among fitting bins\n        priorities[can_fit_mask] = 1.0 / num_fitting_bins\n\n    return priorities\n\n### Analyze & experience\n- Comparing (Heuristics 1st) vs (Heuristics 2nd), they are identical.\n\nComparing (Heuristics 3rd) vs (Heuristics 4th), Heuristics 4th introduces a \"Fill Level Bonus\" and a more complex exploration strategy. The fill level bonus (penalizing large remaining capacities) in Heuristics 4th is a good addition for utilization. The exploration strategy in Heuristics 4th, while more complex, aims for a more nuanced exploration by adapting to the number of fitting bins, which is a step up.\n\nComparing (Heuristics 4th) vs (Heuristics 5th), they are identical.\n\nComparing (Heuristics 5th) vs (Heuristics 6th), Heuristics 5th uses a simpler exploration (uniform boost to all fitting bins with probability). Heuristics 6th attempts a more sophisticated exploration by boosting bins closer to the mean exploitation score, aiming to explore \"promising but not top\" candidates. This seems more targeted than uniform boosting. Heuristics 6th also uses a weighted sum of tightness and utilization, which is a reasonable way to combine objectives.\n\nComparing (Heuristics 6th) vs (Heuristics 7th), Heuristics 7th is identical to Heuristics 5th. This indicates a lack of progression or differentiation in the provided list for these specific entries.\n\nComparing (Heuristics 7th) vs (Heuristics 8th), they are identical.\n\nComparing (Heuristics 8th) vs (Heuristics 9th), Heuristics 8th has a complex exploration that scales randomly. Heuristics 9th uses a simple epsilon-greedy with a random pick for exploration. Heuristics 9th's combined score (tightness + 0.5 * utilization) is simpler than Heuristics 8th's multiplicative approach. The normalization in 9th is also simpler.\n\nComparing (Heuristics 9th) vs (Heuristics 10th), Heuristics 10th introduces a \"Dynamic Epsilon-Greedy Exploration\" and a more adaptive utilization score, attempting to boost scores of less utilized bins. This is more sophisticated than the simple epsilon-greedy in Heuristics 9th. Heuristics 10th's approach to combining scores and exploring seems more nuanced.\n\nComparing (Heuristics 10th) vs (Heuristics 11th), they are identical.\n\nComparing (Heuristics 11th) vs (Heuristics 12th), Heuristics 11th uses a fixed exploration probability with a scaled random score favoring less utilized bins. Heuristics 12th uses a fixed `exploration_prob` to select a *number* of bins to boost uniformly. Heuristics 12th's combination of tightness and utilization seems more balanced with normalized utilization bonus. The exploration in 12th is simpler (uniform boost) but ensures at least one bin is explored if possible.\n\nComparing (Heuristics 12th) vs (Heuristics 13th), they are identical.\n\nComparing (Heuristics 13th) vs (Heuristics 14th), Heuristics 14th has significantly different imports (random, math, scipy, torch) and unused parameters, indicating it's an incomplete or placeholder heuristic, making direct comparison difficult and marking it as worse due to lack of implementation and unnecessary complexity.\n\nComparing (Heuristics 14th) vs (Heuristics 15th), they are identical and incomplete.\n\nComparing (Heuristics 15th) vs (Heuristics 16th), they are identical and incomplete.\n\nComparing (Heuristics 16th) vs (Heuristics 17th), Heuristics 17th is a complete heuristic with a clear strategy, unlike the incomplete Heuristics 16th. Heuristics 17th has a multiplicative combination of tightness and utilization bonus, and its exploration boosts specific bins.\n\nComparing (Heuristics 17th) vs (Heuristics 18th), they are identical.\n\nComparing (Heuristics 18th) vs (Heuristics 19th), they are identical.\n\nComparing (Heuristics 19th) vs (Heuristics 20th), they are identical.\n\nOverall: Heuristics 1st-2nd and 5th-8th and 11th-13th and 17th-20th are largely identical and represent a solid base combining tightness, utilization bonus, and basic exploration. Heuristics 3rd and 4th/5th introduce more sophisticated utilization bonuses. Heuristics 6th tries a more advanced exploration. Heuristics 10th/11th attempt dynamic exploration. Heuristics 12th/13th offer a structured exploration boost. The most distinct and potentially improved approaches seem to be those in Heuristics 4th/5th (more nuanced utilization), 6th (smarter exploration targeting), and 10th/11th (dynamic exploration). However, due to extensive duplication, the progression is unclear. The latter group of identical heuristics (17th-20th) represents a reasonable, albeit unoriginal, approach. Heuristics 14th-16th are clearly worse due to incompleteness.\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Multi-objective, Normalization, Adaptive Exploration, Robustness.\n*   **Advice:** Focus on designing heuristics that explicitly manage trade-offs between competing objectives using techniques like normalized weighted sums or Pareto-based selection. Implement sophisticated exploration strategies that dynamically adjust based on search progress, such as Thompson sampling or Upper Confidence Bound (UCB).\n*   **Avoid:** Blindly combining objectives with fixed weights that might not generalize. Using brittle or ad-hoc handling of edge cases without a clear fallback strategy. Over-reliance on simple epsilon-greedy for exploration, which can be inefficient.\n*   **Explanation:** This approach emphasizes principled methods for handling multi-objective optimization and exploration, ensuring heuristics are adaptable, less prone to local optima, and better at managing complexity without becoming overly brittle.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}