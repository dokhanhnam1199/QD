{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                small_fragment_threshold_ratio: float = 0.2,\n                small_fragment_penalty: float = 0.3,\n                large_capacity_threshold_ratio: float = 1.5,\n                large_capacity_bonus: float = 1.2,\n                base_exploration_noise: float = 0.05,\n                num_feasible_bins_threshold: int = 4,\n                exploitation_bonus: float = 0.05,\n                exploration_noise: float = 0.1,\n                fragment_penalty_threshold_ratio: float = 0.3,\n                fragment_penalty_factor: float = 0.7,\n                bin_utilization_exponent: float = 2.0,\n                item_size_threshold_ratio: float = 0.6,\n                large_item_bonus: float = 0.15,\n                almost_full_threshold_ratio: float = 0.9,\n                almost_full_bonus: float = 0.2) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_fragment_threshold_ratio: Threshold ratio for small fragment penalty.\n        small_fragment_penalty: Penalty factor for small fragments.\n        large_capacity_threshold_ratio: Threshold ratio for large capacity bonus.\n        large_capacity_bonus: Bonus factor for large capacity bins.\n        base_exploration_noise: Base exploration noise.\n        num_feasible_bins_threshold: Threshold for number of feasible bins to switch between exploration and exploitation.\n        exploitation_bonus: Bonus for exploitation when more than num_feasible_bins_threshold bins are feasible.\n        exploration_noise: Exploration noise when fewer than or equal to num_feasible_bins_threshold bins are feasible.\n        fragment_penalty_threshold_ratio: Threshold ratio for fragment penalty.\n        fragment_penalty_factor: Penalty factor for fragments.\n        bin_utilization_exponent: Exponent to control the impact of bin utilization on priority.\n        item_size_threshold_ratio: Threshold ratio for considering an item \"large.\"\n        large_item_bonus: Bonus applied when a large item is placed in a well-utilized bin.\n        almost_full_threshold_ratio: Threshold ratio for considering a bin almost full.\n        almost_full_bonus: Bonus for placing item in an almost full bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bins get the lowest priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core: Bin Utilization\n        priorities[feasible_mask] = capacity_ratio**bin_utilization_exponent\n\n        # Edge Case: Small Fragments\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold_ratio)\n        priorities[feasible_mask][small_fragment_mask] *= small_fragment_penalty\n\n        # Edge Case: Large Capacity\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold_ratio)\n        priorities[feasible_mask][large_capacity_mask] *= large_capacity_bonus\n\n        # Adaptive Exploration/Exploitation\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > num_feasible_bins_threshold:  # Exploit\n            priorities[feasible_mask] *= (1 + exploitation_bonus * capacity_ratio)\n        else:  # Explore\n            priorities += np.random.normal(0, exploration_noise, size=len(priorities))\n\n        # Dynamic Fragment Penalty\n        fragment_penalty_threshold = item * fragment_penalty_threshold_ratio\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * fragment_penalty_factor)\n\n        # Bonus for Large Items in Well-Utilized Bins\n        if item > bins_remain_cap.max() * item_size_threshold_ratio:\n            well_utilized_mask = capacity_ratio > 0.7\n            priorities[feasible_mask][well_utilized_mask] *= (1 + large_item_bonus)\n\n        # Bonus for Almost Full Bins\n        almost_full_mask = remaining_capacity_after_fit < (bins_remain_cap.max() * (1-almost_full_threshold_ratio))\n        priorities[feasible_mask][almost_full_mask] *= (1 + almost_full_bonus)\n\n        # Base exploration noise\n        priorities += np.random.normal(0, base_exploration_noise, size=len(priorities))\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    bin_capacity = bins_remain_cap.max()  # Assuming all bins have the same capacity\n\n    # Infeasible bins get negative infinity priority\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    # Feasible bins\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        \n        # Core heuristic: Normalized waste (lower is better, so use inverse)\n        normalized_waste = remaining_capacity_after_fit / bin_capacity\n        priorities[feasible_mask] = 1 / (1e-6 + normalized_waste) # adding small constant to prevent division by zero\n\n        # Adjustments:\n        # 1. Encourage filling bins well (but not perfectly)\n        utilization = item / (bin_capacity - remaining_capacity_after_fit) # item size divided by original bin capacity\n        priorities[feasible_mask] += np.clip(utilization, 0, 0.95) # clip to avoid prioritizing completely full bins\n\n        # 2. Discourage small fragments (graded penalty)\n        small_fragment_threshold = 0.1 * bin_capacity\n        small_fragment_penalty = np.where(remaining_capacity_after_fit < small_fragment_threshold,\n                                            -0.5 * (1 - (remaining_capacity_after_fit / small_fragment_threshold)),\n                                            0)\n        priorities[feasible_mask] += small_fragment_penalty\n\n        # 3. Encourage placing large items in bins with enough remaining capacity to avoid excessive fragmentation of other items.\n        large_item_threshold = 0.75 * bin_capacity\n        if item > large_item_threshold:\n            sufficient_capacity_bonus = np.where(remaining_capacity_after_fit > 0.2 * bin_capacity,\n                                                0.2, # Bonus for sufficient capacity\n                                                0)\n            priorities[feasible_mask] += sufficient_capacity_bonus\n        \n\n        # 4. Exploration: Adaptive noise based on number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible <= 3:\n             priorities[feasible_mask] += np.random.normal(0, 0.1, size=num_feasible)\n        else:\n            priorities[feasible_mask] += np.random.normal(0, 0.01, size=num_feasible)\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic uses a combination of capacity ratio, fragment avoidance, exploration noise, and bonuses/penalties based on item size and bin fullness. The worst heuristic prioritizes utilization and waste with adaptive exploration and penalties for small items and large items in empty bins.\n\nComparing (2nd best) vs (second worst), we see that the second best employs similar logic to the best, but with different coefficients, and a more detailed breakdown of parameters. The second worst combines waste normalization, bin utilization, and adaptive exploration.\n\nComparing (1st) vs (2nd), we see that (1st) uses more hyperparameter tuning with very specific values which may indicate it was more tuned to the dataset. (2nd) has cleaner documentation.\nComparing (3rd) vs (4th), we see that (3rd) uses hardcoded parameters, while (4th) introduces dynamic calculations for parameters like `randomness_scale` based on `max_capacity_used` and `exploration_boost` based on `avg_fill_level` - implying adaptivity is better.\nComparing (second worst) vs (worst), we see that (20th) has a penalty for placing large items in relatively empty bins, an attempt to avoid a specific undesirable outcome whereas (14th) lacks this specific edge case handling, suggesting importance of edge case handling.\nOverall: the better heuristics use more nuanced and adaptive methods for exploration/exploitation and edge case handling (fragmentation, bin fullness, large items) and well-tuned hyperparameters, while the worse heuristics rely on simpler calculations and fixed parameters.\n- \nOkay, I understand. We need to redefine \"current self-reflection\" to be more effective for designing better heuristics, while explicitly avoiding the pitfalls of \"ineffective self-reflection.\" Here's a redefined approach:\n\n*   **Keywords:** Adaptive, Problem-State, Balance, Simplicity.\n*   **Advice:** Focus on heuristics that adapt to the current problem state, finding the right balance between exploration and exploitation. Start with a simple, interpretable core and add complexity judiciously.\n*   **Avoid:** Overly complex formulas, hardcoded values, and ignoring edge cases. Don't neglect clear interpretability in favor of minor improvements.\n*   **Explanation:** Effective heuristics need to be agile and responsive to the challenges of the problem. Keep it understandable and tunable. Focus on meaningful factors, scaling parameters, and balancing randomness and greed.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}