{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity to diversify selection.\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Introduce some randomness to break ties and explore the search space more effectively.\n        priorities += np.random.normal(0, 0.01, size=len(priorities))\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.05, size=len(priorities)) # Higher randomness\n\n        # NEW: Dynamic fragment penalty based on item size\n        fragment_penalty_threshold = item * 0.2  # Adjust threshold relative to item size\n\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * 0.3) # Apply a graded penalty\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities\n\n### Analyze & experience\n- *   Comparing (1st) vs (2nd), we see both use capacity ratio as a primary metric and penalize small fragments. The 1st one introduces more randomness and adaptively adjusts exploration/exploitation more aggressively based on the number of feasible bins. It also has a dynamic fragment penalty based on item size. The 2nd one focuses more on bin diversity and applies a smaller penalty to large bins.\n\n*   Comparing (2nd) vs (3rd), the 2nd one includes `bin_utilization` to encourage usage of already used bins. The 3rd uses an exponential function to calculate fit score and favors bins with medium remaining capacity. Also, the penalty for infeasible bins is much larger.\n\n*   Comparing (3rd) vs (4th), the 3rd uses `fit_score` based on absolute difference of `bins_remain_cap` and `item`, while 4th uses a normalized `waste` in its `priorities` calculation. The 4th scales noise with respect to `remaining_capacities` while 3rd one scales noise based on the number of valid bins.\n\n*   Comparing (4th) vs (5th), the 5th introduces a `tanh` function to the large capacity mask, which gives diminishing returns. Also, the 5th adaptively scales the randomness based on `num_feasible`. It also has a target utilization parameter.\n\n*   Comparing (5th) vs (6th), the 5th and 6th are the same.\n\n*   Comparing (second worst) vs (worst), both heuristics are identical and incorporate waste normalization and a bin utilization bonus. They also use adaptive noise based on the number of potential bins and implement different exploitation emphasis based on the number of potential bins.\n\n*   Overall: The better heuristics tend to combine multiple factors like capacity ratio, fragment avoidance, bin utilization, and adaptive noise in a more nuanced way. They also incorporate adaptive exploration/exploitation strategies based on the number of feasible bins and other parameters like fill level. They also penalize infeasible bins.\n- \nHere's a redefined \"Current Self-Reflection\" aimed at designing better heuristics, focusing on actionable advice and avoiding common pitfalls:\n\n*   **Keywords:** Adaptive, Multi-Factor, Exploration/Exploitation Balance, Problem State Awareness.\n*   **Advice:** Combine relevant factors adaptively, adjusting parameters based on problem state. Control randomness for exploration. Prioritize balancing exploration and exploitation.\n*   **Avoid:** Overly complex formulas, ignoring problem-specific constraints, and premature optimization.\n*   **Explanation:** Good heuristics require understanding the problem's nuances, combining multiple informative factors, and strategically adjusting behavior dynamically.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}