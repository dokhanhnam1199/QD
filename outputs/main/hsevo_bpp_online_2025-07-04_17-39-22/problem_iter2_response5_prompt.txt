{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities with a base value (e.g., 0).\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item.\n    valid_bins = bins_remain_cap >= item\n\n    # If no bin can accommodate the item, return a low priority for all bins.\n    if not np.any(valid_bins):\n        return priorities - 1e9  # Very low priority for all\n\n    # Calculate fill ratios for valid bins: (capacity - item_size) / original capacity\n    # However, we don't have original capacity readily available. So, we use this approximation.\n    fill_ratios = (bins_remain_cap[valid_bins] - item) / bins_remain_cap[valid_bins]\n\n    # Apply Newton's inverse square law-inspired priority: Priority increases as fill ratio increases, but decreases inversely proportional to the remaining space.\n    priorities[valid_bins] = fill_ratios / (bins_remain_cap[valid_bins] + 1e-6) # Small constant to avoid division by zero\n\n    # Boosting bins with smaller remaining capacity after placing item.\n    # Encourage filling up bins more completely.\n\n    remaining_after_placement = bins_remain_cap[valid_bins] - item\n    # Ensure that the bin is not overfilled\n    remaining_after_placement = np.clip(remaining_after_placement, a_min=0, a_max=None)\n    priority_boost = np.exp(-remaining_after_placement)\n\n    priorities[valid_bins] *= priority_boost\n\n    #Add some noise to the priority score to handle degeneracy (Equal priority scores). This will break ties in random.\n    noise = np.random.normal(0, 1e-6, len(priorities))\n    priorities += noise\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Einstein's Field Equation Inspired Term: Curvature of Bin Space\n    #  The 'curvature' is high when bins are nearly full (close to item size).\n    #  This incentivizes using bins that are 'bent' towards capacity.\n    curvature = np.exp(-np.abs(bins_remain_cap - item))\n\n    # Cosmological Constant Inspired Term: A universal pressure to fill all bins, but modulated.\n    #   This ensures that even bins with less-than-ideal fit still get a chance, preventing premature new bin creation.\n    cosmological_constant = np.where(bins_remain_cap >= item, 0.1, -np.inf) # Give slight encouragement, large discouragement\n\n    # Relative Remaining Capacity: Encourage bins with suitable space.\n    relative_capacity = (bins_remain_cap - item) / bins_remain_cap\n    relative_capacity = np.where(bins_remain_cap >= item, np.clip(relative_capacity, 0, 1), -np.inf) #valid only if bin has space for item. Negative inf. otherwise\n\n    # Stability Term: Prevent filling nearly full bins at the expense of slightly larger bins\n    stability = np.where(bins_remain_cap >= item, bins_remain_cap - item, -np.inf)\n\n    # Combine all terms with some scaling factors.\n    priorities = 0.5 * curvature + 0.2 * cosmological_constant + 0.4 * relative_capacity + 0.1 * stability\n\n    return priorities\n\n### Analyze & experience\n- *   Comparing (1st) vs (2nd), we see that the 1st version uses `waste_normalized` (waste as a *portion* of remaining capacity) and `relative_fullness` (a measure of how full the bin *was*), which seems to provide a more nuanced priority than the `fit_score` and `is_used_bonus` of the 2nd, which is based on absolute difference.\n*   Comparing (3rd) vs (4th), the 3rd uses direct inverse of the remaining capacity after the fit, prioritizing snug fits with a specific boost for very tight fits. It uses explicit loops. Whereas, 4th is similar to the 1st and vectorised, normalizing waste.\n*   Comparing (5th) vs (6th), the 5th uses a weighted sum of `fit_priority`, `remaining_cap_penalty`, and `utilization_priority`. The 6th prioritizes based on normalized waste and relative fullness. The key difference is the weighted combination of multiple explicit heuristics versus a more direct calculation.\n*   Comparing (7th) vs (8th), there's no actual difference in the code, just the ranking.\n*   Comparing (9th) vs (10th), there's no actual difference in the code, just the ranking.\n*   Comparing (11th) vs (12th), there's no actual difference in the code, just the ranking.\n*   Comparing (13th) vs (14th), we observe a significant shift in sophistication. The 13th focuses on a basic ratio of item size to bin size, adding a term for waste. In contrast, the 14th employs a more complex approach, including a fragmentation penalty and a random factor.\n*   Comparing (15th) vs (16th), the 15th re-implements the 14th. The 16th uses \"potential energy\" to choose bins and avoids trivial fills.\n*   Comparing (17th) vs (18th), there's no actual difference in the code, just the ranking.\n*   Comparing (19th) vs (20th), there's no actual difference in the code, just the ranking.\n*   Comparing (second worst) vs (worst), we see that (19th) applies scaling factors on different equations that mimic physics to compute priorities, while the (20th) re-implements the (19th).\n* Overall: The better heuristics balance several factors (fit, fullness, waste), and *normalize* values to comparable scales. The use of np.inf to strongly discourage invalid moves (item doesn't fit) is consistently good. The very best heuristics, use `numpy` effectively for vectorized calculations, rather than explicit loops. The worst heuristics either don't handle edge cases, have very complex formulas that don't result in improvements, or apply no penalty for overfill. Simpler, normalized approaches seem to outperform complex, multi-factor equations.\n- \nOkay, let's redefine \"Current Self-Reflection\" to be more effective for designing better heuristics, keeping in mind the goal of avoiding ineffective practices (which are currently undefined, so we'll focus on generally good practices).\n\nHere's a breakdown to guide the redefined self-reflection process:\n\n*   **Keywords:** *Adaptive, Iterative, Modular, Validation, Experimentation.*\n\n*   **Advice:** Focus on iterative refinement through experimentation. Design heuristics as modular components, enabling easy modification and combination. Continuously validate performance against benchmarks and adapt based on results.\n\n*   **Avoid:** Premature optimization, rigid designs, relying solely on intuition without empirical validation, ignoring problem-specific characteristics.\n\n*   **Explanation:** Move beyond static, predefined heuristics. Embrace an adaptive approach where the heuristic's parameters or structure can be adjusted based on feedback from the search process or the problem instance itself. Modular design allows for easier testing and combination of different heuristic components. Rigorous validation is crucial to ensure the heuristic consistently improves performance.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}