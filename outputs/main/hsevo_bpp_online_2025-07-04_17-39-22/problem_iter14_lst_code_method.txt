{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity to diversify selection.\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Introduce some randomness to break ties and explore the search space more effectively.\n        priorities += np.random.normal(0, 0.01, size=len(priorities))\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.05, size=len(priorities)) # Higher randomness\n\n        # NEW: Dynamic fragment penalty based on item size\n        fragment_penalty_threshold = item * 0.2  # Adjust threshold relative to item size\n\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * 0.3) # Apply a graded penalty\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Primary priority: Fill ratio\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment penalty (adaptive)\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.3  # Reduced penalty\n\n        # Reward larger bins *less*, as we prioritize filling bins tightly, but avoid excessive fragmentation\n        # We will consider bins with capcity > item * 1.5 to be a large bin\n        large_capacity_mask = remaining_capacity_after_fit > (item * 1.5)\n        priorities[feasible_mask][large_capacity_mask] *= 0.9 # slight penalty on large bins to prevent them from hogging items.\n\n        # Introduce randomness - less agressive than v1.\n        priorities += np.random.normal(0, 0.005, size=len(priorities))\n\n        # Bin Diversity using bin utilization rate.\n        bin_utilization = (1 - bins_remain_cap / np.max(bins_remain_cap)) # normalized to largest possible bin size\n        priorities += 0.001 * bin_utilization # bias towards bins that are more utilized.\n        # Adaptive Exploration / Exploitation\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio)  # Stronger fill rate emphasis\n\n        else:\n            priorities += np.random.normal(0, 0.03, size=len(priorities))  # Increased randomness\n\n        # Apply a tiny penalty to bins near full capacity to balance usage and prevent getting stuck.\n        near_full_mask = remaining_capacity_after_fit < np.max(bins_remain_cap)*0.05\n        priorities[feasible_mask][near_full_mask] *= 0.98\n\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines fit, utilization, and adaptive noise based on feasibility.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities - 1e9\n\n    # Fit score\n    fit_score = np.exp(-np.abs(bins_remain_cap - item) / (item + 1e-9))\n    priorities[valid_bins] = fit_score[valid_bins]\n\n    # Bin utilization bonus\n    utilization = item / bins_remain_cap[valid_bins]\n    priorities[valid_bins] += utilization\n\n    # Adaptive noise based on number of valid bins\n    num_valid_bins = np.sum(valid_bins)\n    noise_scale = 0.005 if num_valid_bins > 3 else 0.05\n    noise = np.random.normal(0, noise_scale, len(priorities))\n    priorities += noise\n\n    # Encourage bins with mid-level remaining capacities\n    remaining_capacity_after_fit = bins_remain_cap[valid_bins] - item\n    medium_capacity_mask = (remaining_capacity_after_fit >= (item * 0.2)) & (remaining_capacity_after_fit <= (item * 1.5))\n    priorities[valid_bins][medium_capacity_mask] *= 1.1  # Slightly favor medium capacity\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste, and adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible = item > bins_remain_cap\n    priorities[infeasible] = -np.inf\n\n    feasible = ~infeasible\n    remaining_capacities = bins_remain_cap[feasible]\n\n    if len(remaining_capacities) > 0:\n        utilization = item / remaining_capacities\n        waste = remaining_capacities - item\n        waste = np.clip(waste, a_min=0, a_max=None) #avoid negative values in waste\n        waste_normalized = waste / remaining_capacities\n        priorities[feasible] = utilization - waste_normalized\n\n        # Adaptive noise based on number of bins\n        num_feasible = np.sum(feasible)\n        if num_feasible > 0:\n            noise_level = 1e-6 * np.mean(remaining_capacities) # scale noise\n            noise = np.random.normal(0, noise_level, len(priorities))\n            priorities += noise\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity, but with diminishing returns\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= (1 + 0.1 * np.tanh(remaining_capacity_after_fit[large_capacity_mask] / item))\n\n        # Introduce some randomness to break ties and explore the search space more effectively.  Scale the randomness based on the number of feasible bins.\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 0:\n            randomness_scale = 0.01 if num_feasible > 5 else 0.05\n            priorities[feasible_mask] += np.random.normal(0, randomness_scale, size=num_feasible)\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins and item size\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.03, size=len(priorities)) # Moderate randomness\n\n        # Prioritize bins with utilization close to a target value. This aims to balance bin utilization across the board.\n        target_utilization = 0.75  # Tunable parameter\n        utilization = (bins_remain_cap[feasible_mask] - remaining_capacity_after_fit) / bins_remain_cap[feasible_mask]\n\n        utilization_diff = np.abs(utilization - target_utilization)\n        priorities[feasible_mask] *= (1 - 0.1 * utilization_diff) # Penalize bins far from target utilization\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity, but with diminishing returns\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= (1 + 0.1 * np.tanh(remaining_capacity_after_fit[large_capacity_mask] / item))\n\n        # Introduce some randomness to break ties and explore the search space more effectively.  Scale the randomness based on the number of feasible bins.\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 0:\n            randomness_scale = 0.01 if num_feasible > 5 else 0.05\n            priorities[feasible_mask] += np.random.normal(0, randomness_scale, size=num_feasible)\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins and item size\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.03, size=len(priorities)) # Moderate randomness\n\n        # Prioritize bins with utilization close to a target value. This aims to balance bin utilization across the board.\n        target_utilization = 0.75  # Tunable parameter\n        utilization = (bins_remain_cap[feasible_mask] - remaining_capacity_after_fit) / bins_remain_cap[feasible_mask]\n\n        utilization_diff = np.abs(utilization - target_utilization)\n        priorities[feasible_mask] *= (1 - 0.1 * utilization_diff) # Penalize bins far from target utilization\n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste, and adaptive noise.\n    Balances exploration/exploitation based on feasible bin count.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    cannot_fit = item > bins_remain_cap\n    priorities[cannot_fit] = -np.inf\n\n    can_fit = ~cannot_fit\n    feasible_bins_remain_cap = bins_remain_cap[can_fit]\n\n    if np.sum(can_fit) == 0:\n        return priorities\n\n    utilization = item / feasible_bins_remain_cap\n    waste = feasible_bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None) # ensure waste is non-negative\n    waste_normalized = waste / feasible_bins_remain_cap\n\n    priorities[can_fit] = utilization - waste_normalized\n\n    num_feasible = np.sum(can_fit)\n    if num_feasible > 5:\n        priorities[can_fit] *= (1 + 0.02 * utilization)  # Exploit\n\n    exploration_rate = 0.01 + 0.03 * (1 - (np.mean(bins_remain_cap[can_fit]) / np.max(bins_remain_cap))) if np.sum(can_fit) > 0 else 0.01\n    priorities += np.random.normal(0, exploration_rate, size=len(priorities))\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines normalized waste, bin utilization, and adaptive noise.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_capacities = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_capacities - item\n        capacity_ratio = item / feasible_capacities\n\n        # Encourage bins that fit the item *relatively* well.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small\n        small_fragment_threshold = 0.1\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity\n        large_capacity_threshold = 2.0\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Adaptive adjustment of exploration vs. exploitation\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio)\n        else:\n            priorities += np.random.normal(0, 0.05, size=len(priorities))\n\n        # Dynamic adjustment based on average bin fill level\n        average_fill = np.mean((1 - bins_remain_cap[feasible_mask] / np.max(bins_remain_cap)) if feasible_capacities.size else 0)\n        if average_fill > 0.7:\n            priorities[feasible_mask][small_fragment_mask] *= 0.3\n            priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio)\n        elif average_fill < 0.3:\n            priorities[feasible_mask][large_capacity_mask] *= 1.2\n\n        bin_fullness = 1 - bins_remain_cap / np.max(bins_remain_cap)\n        priorities += 0.05 * bin_fullness # Use bins that have been more used\n\n        noise_level = 0.01 * (1 - average_fill)\n        priorities += np.random.normal(0, noise_level, size=len(priorities))\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization and bin utilization with adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_potential_bins = np.sum(potential_bins)\n    noise_scale = 0.01 if num_potential_bins > 0 else 0.05\n    \n    #Exploitation emphasis when there are a lot of bins to choose\n    if num_potential_bins > 3:\n       priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + 0.02 * (bin_utilization**2) + np.random.normal(0, noise_scale, size=len(priorities)) # Add bin utilization with power\n    else:\n        priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + np.random.normal(0, noise_scale, size=len(priorities)) \n\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization and bin utilization with adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_potential_bins = np.sum(potential_bins)\n    noise_scale = 0.01 if num_potential_bins > 0 else 0.05\n    \n    #Exploitation emphasis when there are a lot of bins to choose\n    if num_potential_bins > 3:\n       priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + 0.02 * (bin_utilization**2) + np.random.normal(0, noise_scale, size=len(priorities)) # Add bin utilization with power\n    else:\n        priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + np.random.normal(0, noise_scale, size=len(priorities)) \n\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization and bin utilization with adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_potential_bins = np.sum(potential_bins)\n    noise_scale = 0.01 if num_potential_bins > 0 else 0.05\n    \n    #Exploitation emphasis when there are a lot of bins to choose\n    if num_potential_bins > 3:\n       priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + 0.02 * (bin_utilization**2) + np.random.normal(0, noise_scale, size=len(priorities)) # Add bin utilization with power\n    else:\n        priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + np.random.normal(0, noise_scale, size=len(priorities)) \n\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized waste, bin utilization, and exploration-exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_feasible = np.sum(potential_bins)\n    exploration_prob = 0.3 if num_feasible < 3 else 0.1\n\n    if np.random.rand() < exploration_prob:\n        random_scale = 0.1 * item\n        priorities += np.random.normal(0, random_scale, size=len(priorities))\n    else:\n        exploitation_bonus = 0.05\n        priorities[potential_bins] = waste_normalized[potential_bins] + is_used_bonus[potential_bins] * (1 + exploitation_bonus * bin_utilization[potential_bins])\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities based on waste, utilization,\n    and exploration with small item penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / (bins_remain_cap + 1e-9))\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / (bins_remain_cap + 1e-9)\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus\n\n    num_potential_bins = np.sum(potential_bins)\n    exploration_factor = np.clip(num_potential_bins / len(bins_remain_cap), 0.01, 0.1)\n    priorities += np.random.normal(0, exploration_factor, size=len(priorities))\n\n    small_item_size = np.mean(bins_remain_cap[bins_remain_cap>0])/10 if np.any(bins_remain_cap > 0) else 0.1\n    future_fit_penalty = np.where(waste < small_item_size, -0.1 , 0)\n    priorities[potential_bins] += future_fit_penalty[potential_bins]\n\n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities based on waste, utilization,\n    and exploration with small item penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / (bins_remain_cap + 1e-9))\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / (bins_remain_cap + 1e-9)\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus\n\n    num_potential_bins = np.sum(potential_bins)\n    exploration_factor = np.clip(num_potential_bins / len(bins_remain_cap), 0.01, 0.1)\n    priorities += np.random.normal(0, exploration_factor, size=len(priorities))\n\n    small_item_size = np.mean(bins_remain_cap[bins_remain_cap>0])/10 if np.any(bins_remain_cap > 0) else 0.1\n    future_fit_penalty = np.where(waste < small_item_size, -0.1 , 0)\n    priorities[potential_bins] += future_fit_penalty[potential_bins]\n\n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities based on waste, utilization,\n    and exploration with small item penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / (bins_remain_cap + 1e-9))\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / (bins_remain_cap + 1e-9)\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus\n\n    num_potential_bins = np.sum(potential_bins)\n    exploration_factor = np.clip(num_potential_bins / len(bins_remain_cap), 0.01, 0.1)\n    priorities += np.random.normal(0, exploration_factor, size=len(priorities))\n\n    small_item_size = np.mean(bins_remain_cap[bins_remain_cap>0])/10 if np.any(bins_remain_cap > 0) else 0.1\n    future_fit_penalty = np.where(waste < small_item_size, -0.1 , 0)\n    priorities[potential_bins] += future_fit_penalty[potential_bins]\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratio, waste, and adaptive noise based on # feasible bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bins get a very low priority.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Primary priority: Fill bins well.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Penalize small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.05)\n        priorities[feasible_mask][small_fragment_mask] *= 0.7\n\n        # Incentivize bins that are already relatively full.\n        full_bin_mask = capacity_ratio > 0.7\n        priorities[feasible_mask][full_bin_mask] *= 1.2\n\n        # Adaptive Randomness based on num of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        random_scale = 0.01 * item * max(1, num_feasible)\n        priorities += np.random.normal(0, random_scale, size=len(priorities))\n\n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}