[
  {
    "stdout_filepath": "problem_iter11_response0.txt_stdout.txt",
    "code_path": "problem_iter11_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste, and adaptive noise.\n    Balances exploration/exploitation based on feasible bin count.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    cannot_fit = item > bins_remain_cap\n    priorities[cannot_fit] = -np.inf\n\n    can_fit = ~cannot_fit\n    feasible_bins_remain_cap = bins_remain_cap[can_fit]\n\n    if np.sum(can_fit) == 0:\n        return priorities\n\n    utilization = item / feasible_bins_remain_cap\n    waste = feasible_bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None) # ensure waste is non-negative\n    waste_normalized = waste / feasible_bins_remain_cap\n\n    priorities[can_fit] = utilization - waste_normalized\n\n    num_feasible = np.sum(can_fit)\n    if num_feasible > 5:\n        priorities[can_fit] *= (1 + 0.02 * utilization)  # Exploit\n\n    exploration_rate = 0.01 + 0.03 * (1 - (np.mean(bins_remain_cap[can_fit]) / np.max(bins_remain_cap))) if np.sum(can_fit) > 0 else 0.01\n    priorities += np.random.normal(0, exploration_rate, size=len(priorities))\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.058635819704831,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response1.txt_stdout.txt",
    "code_path": "problem_iter11_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines capacity ratio, waste, and adaptive noise based on # feasible bins.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bins get a very low priority.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Primary priority: Fill bins well.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Penalize small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.05)\n        priorities[feasible_mask][small_fragment_mask] *= 0.7\n\n        # Incentivize bins that are already relatively full.\n        full_bin_mask = capacity_ratio > 0.7\n        priorities[feasible_mask][full_bin_mask] *= 1.2\n\n        # Adaptive Randomness based on num of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        random_scale = 0.01 * item * max(1, num_feasible)\n        priorities += np.random.normal(0, random_scale, size=len(priorities))\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 73.37455125648185,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response2.txt_stdout.txt",
    "code_path": "problem_iter11_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines normalized waste, bin utilization, and exploration-exploitation.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_feasible = np.sum(potential_bins)\n    exploration_prob = 0.3 if num_feasible < 3 else 0.1\n\n    if np.random.rand() < exploration_prob:\n        random_scale = 0.1 * item\n        priorities += np.random.normal(0, random_scale, size=len(priorities))\n    else:\n        exploitation_bonus = 0.05\n        priorities[potential_bins] = waste_normalized[potential_bins] + is_used_bonus[potential_bins] * (1 + exploitation_bonus * bin_utilization[potential_bins])\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.248105305145606,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response3.txt_stdout.txt",
    "code_path": "problem_iter11_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines waste normalization and bin utilization with adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / bins_remain_cap)\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / bins_remain_cap\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    num_potential_bins = np.sum(potential_bins)\n    noise_scale = 0.01 if num_potential_bins > 0 else 0.05\n    \n    #Exploitation emphasis when there are a lot of bins to choose\n    if num_potential_bins > 3:\n       priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + 0.02 * (bin_utilization**2) + np.random.normal(0, noise_scale, size=len(priorities)) # Add bin utilization with power\n    else:\n        priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus + np.random.normal(0, noise_scale, size=len(priorities)) \n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.068607897885915,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response4.txt_stdout.txt",
    "code_path": "problem_iter11_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines fit score, bin utilization, and scaled noise for bin priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities - 1e9\n\n    # Fit score: exp distance\n    fit_score = np.exp(-np.abs(bins_remain_cap - item) / item)\n    priorities[valid_bins] = fit_score[valid_bins]\n\n    # Used bin bonus, scaled by remaining capacity\n    is_used_bonus = (bins_remain_cap < 1).astype(float) * (1 - bins_remain_cap)\n    priorities += is_used_bonus\n\n    # Normalize waste, only for valid bins\n    waste = bins_remain_cap[valid_bins] - item\n    waste_normalized = waste / bins_remain_cap[valid_bins]\n    priorities[valid_bins] -= waste_normalized\n\n    # Dynamic noise injection, scaled by number of valid bins\n    num_valid = np.sum(valid_bins)\n    noise_scale = min(1.0, 1.0 / (num_valid + 1e-6))  # Scale noise down if many bins fit.\n    noise = np.random.normal(0, 0.01 * noise_scale, len(priorities))\n    priorities += noise\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response5.txt_stdout.txt",
    "code_path": "problem_iter11_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines normalized waste and bin utilization with adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities - 1e9\n\n    # Fit score: based on remaining capacity after placing the item\n    remaining_capacity_after_fit = bins_remain_cap[valid_bins] - item\n    fit_score = np.exp(-np.abs(remaining_capacity_after_fit) / item)\n    priorities[valid_bins] = fit_score\n\n    # Bin utilization bonus\n    is_used_bonus = (bins_remain_cap < 1).astype(float) * (1 - bins_remain_cap)\n    priorities += is_used_bonus\n\n    # Normalize waste\n    waste = bins_remain_cap[valid_bins] - item\n    waste_normalized = waste / bins_remain_cap[valid_bins]\n    priorities[valid_bins] -= waste_normalized\n\n    # Dynamic noise injection\n    num_valid = np.sum(valid_bins)\n    noise_scale = min(1.0, 1.0 / (num_valid + 1e-6))\n    noise = np.random.normal(0, 0.01 * noise_scale, len(priorities))\n    priorities += noise\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response6.txt_stdout.txt",
    "code_path": "problem_iter11_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines fit, utilization, and adaptive noise based on feasibility.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities - 1e9\n\n    # Fit score\n    fit_score = np.exp(-np.abs(bins_remain_cap - item) / (item + 1e-9))\n    priorities[valid_bins] = fit_score[valid_bins]\n\n    # Bin utilization bonus\n    utilization = item / bins_remain_cap[valid_bins]\n    priorities[valid_bins] += utilization\n\n    # Adaptive noise based on number of valid bins\n    num_valid_bins = np.sum(valid_bins)\n    noise_scale = 0.005 if num_valid_bins > 3 else 0.05\n    noise = np.random.normal(0, noise_scale, len(priorities))\n    priorities += noise\n\n    # Encourage bins with mid-level remaining capacities\n    remaining_capacity_after_fit = bins_remain_cap[valid_bins] - item\n    medium_capacity_mask = (remaining_capacity_after_fit >= (item * 0.2)) & (remaining_capacity_after_fit <= (item * 1.5))\n    priorities[valid_bins][medium_capacity_mask] *= 1.1  # Slightly favor medium capacity\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response7.txt_stdout.txt",
    "code_path": "problem_iter11_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines utilization, waste, and adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible = item > bins_remain_cap\n    priorities[infeasible] = -np.inf\n\n    feasible = ~infeasible\n    remaining_capacities = bins_remain_cap[feasible]\n\n    if len(remaining_capacities) > 0:\n        utilization = item / remaining_capacities\n        waste = remaining_capacities - item\n        waste = np.clip(waste, a_min=0, a_max=None) #avoid negative values in waste\n        waste_normalized = waste / remaining_capacities\n        priorities[feasible] = utilization - waste_normalized\n\n        # Adaptive noise based on number of bins\n        num_feasible = np.sum(feasible)\n        if num_feasible > 0:\n            noise_level = 1e-6 * np.mean(remaining_capacities) # scale noise\n            noise = np.random.normal(0, noise_level, len(priorities))\n            priorities += noise\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response8.txt_stdout.txt",
    "code_path": "problem_iter11_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines normalized waste, bin utilization, and adaptive noise.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_capacities = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_capacities - item\n        capacity_ratio = item / feasible_capacities\n\n        # Encourage bins that fit the item *relatively* well.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small\n        small_fragment_threshold = 0.1\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity\n        large_capacity_threshold = 2.0\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Adaptive adjustment of exploration vs. exploitation\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio)\n        else:\n            priorities += np.random.normal(0, 0.05, size=len(priorities))\n\n        # Dynamic adjustment based on average bin fill level\n        average_fill = np.mean((1 - bins_remain_cap[feasible_mask] / np.max(bins_remain_cap)) if feasible_capacities.size else 0)\n        if average_fill > 0.7:\n            priorities[feasible_mask][small_fragment_mask] *= 0.3\n            priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio)\n        elif average_fill < 0.3:\n            priorities[feasible_mask][large_capacity_mask] *= 1.2\n\n        bin_fullness = 1 - bins_remain_cap / np.max(bins_remain_cap)\n        priorities += 0.05 * bin_fullness # Use bins that have been more used\n\n        noise_level = 0.01 * (1 - average_fill)\n        priorities += np.random.normal(0, noise_level, size=len(priorities))\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "obj": 4.068607897885915,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter11_response9.txt_stdout.txt",
    "code_path": "problem_iter11_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculates bin priorities based on waste, utilization,\n    and exploration with small item penalty.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    potential_bins = bins_remain_cap >= item\n    priorities[~potential_bins] = -np.inf\n\n    waste = bins_remain_cap - item\n    waste = np.clip(waste, a_min=0, a_max=None)\n    waste_normalized = 1 - (waste / (bins_remain_cap + 1e-9))\n    waste_normalized = np.clip(waste_normalized, a_min=0, a_max=1)\n\n    bin_utilization = (bins_remain_cap - waste) / (bins_remain_cap + 1e-9)\n    is_used_bonus = (bin_utilization > 0).astype(float)\n\n    priorities = 0.7 * waste_normalized + 0.3 * is_used_bonus\n\n    num_potential_bins = np.sum(potential_bins)\n    exploration_factor = np.clip(num_potential_bins / len(bins_remain_cap), 0.01, 0.1)\n    priorities += np.random.normal(0, exploration_factor, size=len(priorities))\n\n    small_item_size = np.mean(bins_remain_cap[bins_remain_cap>0])/10 if np.any(bins_remain_cap > 0) else 0.1\n    future_fit_penalty = np.where(waste < small_item_size, -0.1 , 0)\n    priorities[potential_bins] += future_fit_penalty[potential_bins]\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 6.302353410450742,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response0.txt_stdout.txt",
    "code_path": "problem_iter12_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity to diversify selection.\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Introduce some randomness to break ties and explore the search space more effectively.\n        priorities += np.random.normal(0, 0.01, size=len(priorities))\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.05, size=len(priorities)) # Higher randomness\n\n        # NEW: Dynamic fragment penalty based on item size\n        fragment_penalty_threshold = item * 0.2  # Adjust threshold relative to item size\n\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * 0.3) # Apply a graded penalty\n\n    return priorities",
    "response_id": 0,
    "tryHS": true,
    "obj": 4.008775428799367,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response1.txt_stdout.txt",
    "code_path": "problem_iter12_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity, but with diminishing returns\n        large_capacity_mask = remaining_capacity_after_fit > (item * 2)\n        priorities[feasible_mask][large_capacity_mask] *= (1 + 0.1 * np.tanh(remaining_capacity_after_fit[large_capacity_mask] / item))\n\n        # Introduce some randomness to break ties and explore the search space more effectively.  Scale the randomness based on the number of feasible bins.\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 0:\n            randomness_scale = 0.01 if num_feasible > 5 else 0.05\n            priorities[feasible_mask] += np.random.normal(0, randomness_scale, size=num_feasible)\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins and item size\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.03, size=len(priorities)) # Moderate randomness\n\n        # Prioritize bins with utilization close to a target value. This aims to balance bin utilization across the board.\n        target_utilization = 0.75  # Tunable parameter\n        utilization = (bins_remain_cap[feasible_mask] - remaining_capacity_after_fit) / bins_remain_cap[feasible_mask]\n\n        utilization_diff = np.abs(utilization - target_utilization)\n        priorities[feasible_mask] *= (1 - 0.1 * utilization_diff) # Penalize bins far from target utilization\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response2.txt_stdout.txt",
    "code_path": "problem_iter12_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling: Same as v1\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Primary Priority: Fill Rate - Target Higher Occupancy\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize bins leading to tiny fragments\n        small_fragment_threshold = 0.05  # Reduced threshold to be more aggressive\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold)\n        priorities[feasible_mask][small_fragment_mask] *= 0.25 # Further reduced priority\n\n        # Encourage Perfect Fit (or near perfect): High boost if item fits nearly perfectly\n        perfect_fit_tolerance = 0.05  # Define \"near perfect\"\n        perfect_fit_mask = np.abs(remaining_capacity_after_fit) < (item * perfect_fit_tolerance)\n        priorities[feasible_mask][perfect_fit_mask] *= 2.0  # Significantly boost perfect fit\n\n        # Moderate Remaining Capacity Diversification (Avoid premature overfilling)\n        large_capacity_threshold = 1.5 # Reduce diversification threshold\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold)\n        priorities[feasible_mask][large_capacity_mask] *= 1.05  # Reduced the increase\n\n        # Dynamic Noise Injection: adaptive randomness based on feasibility\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n             priorities += np.random.normal(0, 0.005, size=len(priorities)) # reduced noise\n        else:\n             priorities += np.random.normal(0, 0.02, size=len(priorities)) # reduced noise\n\n        # Bin Balancing Incentive: Favor bins with below average remaining capacity.\n        average_remaining_capacity = np.mean(bins_remain_cap[feasible_mask])\n\n        below_avg_mask = feasible_bins_remain_cap < average_remaining_capacity\n        priorities[feasible_mask][below_avg_mask] *= 1.1\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response3.txt_stdout.txt",
    "code_path": "problem_iter12_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Primary priority: Fill ratio\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment penalty (adaptive)\n        small_fragment_mask = remaining_capacity_after_fit < (item * 0.1)\n        priorities[feasible_mask][small_fragment_mask] *= 0.3  # Reduced penalty\n\n        # Reward larger bins *less*, as we prioritize filling bins tightly, but avoid excessive fragmentation\n        # We will consider bins with capcity > item * 1.5 to be a large bin\n        large_capacity_mask = remaining_capacity_after_fit > (item * 1.5)\n        priorities[feasible_mask][large_capacity_mask] *= 0.9 # slight penalty on large bins to prevent them from hogging items.\n\n        # Introduce randomness - less agressive than v1.\n        priorities += np.random.normal(0, 0.005, size=len(priorities))\n\n        # Bin Diversity using bin utilization rate.\n        bin_utilization = (1 - bins_remain_cap / np.max(bins_remain_cap)) # normalized to largest possible bin size\n        priorities += 0.001 * bin_utilization # bias towards bins that are more utilized.\n        # Adaptive Exploration / Exploitation\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio)  # Stronger fill rate emphasis\n\n        else:\n            priorities += np.random.normal(0, 0.03, size=len(priorities))  # Increased randomness\n\n        # Apply a tiny penalty to bins near full capacity to balance usage and prevent getting stuck.\n        near_full_mask = remaining_capacity_after_fit < np.max(bins_remain_cap)*0.05\n        priorities[feasible_mask][near_full_mask] *= 0.98\n\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.028719585161557,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter12_response4.txt_stdout.txt",
    "code_path": "problem_iter12_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Infeasible bin handling:\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Core Priority: Capacity Ratio (Higher is better)\n        priorities[feasible_mask] = capacity_ratio\n\n        # Fragment Avoidance: Penalize small fragments *relative* to the bin size\n        small_fragment_mask = remaining_capacity_after_fit < (bins_remain_cap[feasible_mask] * 0.05) # relative to original bin size not just item size.\n        priorities[feasible_mask][small_fragment_mask] *= 0.75  # Reduced penalty\n\n        # Favor bins with space slightly larger than the item, but not too large. Encourages filling bins well.\n        optimal_space_mask = (remaining_capacity_after_fit >= (item * 0.1)) & (remaining_capacity_after_fit <= (item * 1.25))\n        priorities[feasible_mask][optimal_space_mask] *= 1.2 # bump up priority\n\n        # Bin Balancing: Discourage excessive empty space in all bins. This encourages using bins more evenly and prevents one bin from becoming excessively full while others are near empty\n        avg_bin_capacity = np.mean(bins_remain_cap[feasible_mask])\n        large_capacity_mask = remaining_capacity_after_fit > (avg_bin_capacity*1.1) # greater than average, penalize\n        priorities[feasible_mask][large_capacity_mask] *= 0.8\n\n        # Adaptive Randomness: Adjust noise based on how full the *most* full bin is.\n        max_capacity_used = np.max(1 - bins_remain_cap / np.max(bins_remain_cap))\n        randomness_scale = 0.01 + (0.04 * max_capacity_used)\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        #Adjust exploration/exploitation based on number of feasible solutions\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            # Apply a slight bonus to bins that are already relatively full.\n             priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) # more exploitation based on current capacity ratio\n        else:\n            # Increase exploration, esp if the average fill level is low\n            avg_fill_level = np.mean(capacity_ratio)\n            exploration_boost = 0.03 + (0.07 * (1 - avg_fill_level)) # less average fill --> increase the exploration boost.\n            priorities += np.random.normal(0, exploration_boost, size=len(priorities))\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.11846828879138,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter13_response4.txt_stdout.txt",
    "code_path": "problem_iter13_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray,\n                small_fragment_threshold_ratio: float = 0.18466643198227367,\n                small_fragment_penalty: float = 0.27731097399017257,\n                large_capacity_threshold_ratio: float = 1.8626060346685316,\n                large_capacity_bonus: float = 1.4940231128487715,\n                base_exploration_noise: float = 0.03615474634440513,\n                num_feasible_bins_threshold: int = 5,\n                exploitation_bonus: float = 0.0388722336961404,\n                exploration_noise: float = 0.05315726236485679,\n                fragment_penalty_threshold_ratio: float = 0.29623738564910945,\n                fragment_penalty_factor: float = 0.6538117438072715) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_fragment_threshold_ratio: Threshold ratio for small fragment penalty.\n        small_fragment_penalty: Penalty factor for small fragments.\n        large_capacity_threshold_ratio: Threshold ratio for large capacity bonus.\n        large_capacity_bonus: Bonus factor for large capacity bins.\n        base_exploration_noise: Base exploration noise.\n        num_feasible_bins_threshold: Threshold for number of feasible bins to switch between exploration and exploitation.\n        exploitation_bonus: Bonus for exploitation when more than num_feasible_bins_threshold bins are feasible.\n        exploration_noise: Exploration noise when fewer than or equal to num_feasible_bins_threshold bins are feasible.\n        fragment_penalty_threshold_ratio: Threshold ratio for fragment penalty.\n        fragment_penalty_factor: Penalty factor for fragments.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold_ratio)\n        priorities[feasible_mask][small_fragment_mask] *= small_fragment_penalty\n\n        # Slightly increase the priority of bins with large remaining capacity to diversify selection.\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold_ratio)\n        priorities[feasible_mask][large_capacity_mask] *= large_capacity_bonus\n\n        # Introduce some randomness to break ties and explore the search space more effectively.\n        priorities += np.random.normal(0, base_exploration_noise, size=len(priorities))\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > num_feasible_bins_threshold:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + exploitation_bonus * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, exploration_noise, size=len(priorities)) # Higher randomness\n\n        # NEW: Dynamic fragment penalty based on item size\n        fragment_penalty_threshold = item * fragment_penalty_threshold_ratio  # Adjust threshold relative to item size\n\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * fragment_penalty_factor) # Apply a graded penalty\n\n    return priorities",
    "response_id": 4,
    "tryHS": true,
    "obj": 3.9988033506182825,
    "exec_success": true
  }
]