{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version considers several factors:\n    1. Remaining capacity compared to item size (closeness to perfect fit).\n    2. Penalty for bins where the item doesn't fit.\n    3. A bonus for bins that were almost full.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Very important: Handle the case where the item doesn't fit!\n    cannot_fit = item > bins_remain_cap\n    priorities[cannot_fit] = -np.inf  # Definitely avoid these bins\n\n    # Now handle bins where the item *can* fit\n    can_fit = ~cannot_fit\n    remaining_capacities_can_fit = bins_remain_cap[can_fit]\n\n    if len(remaining_capacities_can_fit) > 0:  # Only calculate when needed\n        # Proximity to \"perfect fit\" - smaller waste is better\n        waste = remaining_capacities_can_fit - item\n        waste_normalized = waste / remaining_capacities_can_fit # Smaller number means smaller waste as a portion.\n\n        # Encourage packing into bins that were already quite full.  A bit of relative fullness before adding item.\n        relative_fullness = 1 - remaining_capacities_can_fit #Assume bin capacity is 1.\n        priorities[can_fit] = -waste_normalized + relative_fullness\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # First Fit Decreasing inspired: prioritize bins that can fit the item closely\n    fit_mask = bins_remain_cap >= item\n    if np.any(fit_mask):\n      capacities_that_fit = bins_remain_cap[fit_mask]\n      priorities[fit_mask] = capacities_that_fit - item\n      priorities[fit_mask] = -priorities[fit_mask]  #Smaller remaining capacity = higher priority\n      #Prioritize almost-full bins more. We want to fill bins completelly\n      priorities[fit_mask] = priorities[fit_mask] / bins_remain_cap[fit_mask] #Normalizing it\n\n    else: #If nothing fits, try to find a bin to make the overfill as little as possible. This is a last ditch effort\n        priorities = item - bins_remain_cap\n        priorities = -priorities  #minimize wasted space\n        priorities = priorities / np.max(np.abs(priorities)) #Normalize so bins are somewhat close\n\n    return priorities\n\n### Analyze & experience\n- *   Comparing (1st) vs (2nd), we see that the 1st version uses `waste_normalized` (waste as a *portion* of remaining capacity) and `relative_fullness` (a measure of how full the bin *was*), which seems to provide a more nuanced priority than the `fit_score` and `is_used_bonus` of the 2nd, which is based on absolute difference.\n*   Comparing (3rd) vs (4th), the 3rd uses direct inverse of the remaining capacity after the fit, prioritizing snug fits with a specific boost for very tight fits. It uses explicit loops. Whereas, 4th is similar to the 1st and vectorised, normalizing waste.\n*   Comparing (5th) vs (6th), the 5th uses a weighted sum of `fit_priority`, `remaining_cap_penalty`, and `utilization_priority`. The 6th prioritizes based on normalized waste and relative fullness. The key difference is the weighted combination of multiple explicit heuristics versus a more direct calculation.\n*   Comparing (7th) vs (8th), there's no actual difference in the code, just the ranking.\n*   Comparing (9th) vs (10th), there's no actual difference in the code, just the ranking.\n*   Comparing (11th) vs (12th), there's no actual difference in the code, just the ranking.\n*   Comparing (13th) vs (14th), we observe a significant shift in sophistication. The 13th focuses on a basic ratio of item size to bin size, adding a term for waste. In contrast, the 14th employs a more complex approach, including a fragmentation penalty and a random factor.\n*   Comparing (15th) vs (16th), the 15th re-implements the 14th. The 16th uses \"potential energy\" to choose bins and avoids trivial fills.\n*   Comparing (17th) vs (18th), there's no actual difference in the code, just the ranking.\n*   Comparing (19th) vs (20th), there's no actual difference in the code, just the ranking.\n*   Comparing (second worst) vs (worst), we see that (19th) applies scaling factors on different equations that mimic physics to compute priorities, while the (20th) re-implements the (19th).\n* Overall: The better heuristics balance several factors (fit, fullness, waste), and *normalize* values to comparable scales. The use of np.inf to strongly discourage invalid moves (item doesn't fit) is consistently good. The very best heuristics, use `numpy` effectively for vectorized calculations, rather than explicit loops. The worst heuristics either don't handle edge cases, have very complex formulas that don't result in improvements, or apply no penalty for overfill. Simpler, normalized approaches seem to outperform complex, multi-factor equations.\n- \nOkay, let's redefine \"Current Self-Reflection\" to be more effective for designing better heuristics, keeping in mind the goal of avoiding ineffective practices (which are currently undefined, so we'll focus on generally good practices).\n\nHere's a breakdown to guide the redefined self-reflection process:\n\n*   **Keywords:** *Adaptive, Iterative, Modular, Validation, Experimentation.*\n\n*   **Advice:** Focus on iterative refinement through experimentation. Design heuristics as modular components, enabling easy modification and combination. Continuously validate performance against benchmarks and adapt based on results.\n\n*   **Avoid:** Premature optimization, rigid designs, relying solely on intuition without empirical validation, ignoring problem-specific characteristics.\n\n*   **Explanation:** Move beyond static, predefined heuristics. Embrace an adaptive approach where the heuristic's parameters or structure can be adjusted based on feedback from the search process or the problem instance itself. Modular design allows for easier testing and combination of different heuristic components. Rigorous validation is crucial to ensure the heuristic consistently improves performance.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}