{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n                small_fragment_threshold_ratio: float = 0.18466643198227367,\n                small_fragment_penalty: float = 0.27731097399017257,\n                large_capacity_threshold_ratio: float = 1.8626060346685316,\n                large_capacity_bonus: float = 1.4940231128487715,\n                base_exploration_noise: float = 0.03615474634440513,\n                num_feasible_bins_threshold: int = 5,\n                exploitation_bonus: float = 0.0388722336961404,\n                exploration_noise: float = 0.05315726236485679,\n                fragment_penalty_threshold_ratio: float = 0.29623738564910945,\n                fragment_penalty_factor: float = 0.6538117438072715,\n                bin_occupation_exponent: float = 2.0,\n                item_size_relative_importance: float = 0.5) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n        small_fragment_threshold_ratio: Threshold ratio for small fragment penalty.\n        small_fragment_penalty: Penalty factor for small fragments.\n        large_capacity_threshold_ratio: Threshold ratio for large capacity bonus.\n        large_capacity_bonus: Bonus factor for large capacity bins.\n        base_exploration_noise: Base exploration noise.\n        num_feasible_bins_threshold: Threshold for number of feasible bins to switch between exploration and exploitation.\n        exploitation_bonus: Bonus for exploitation when more than num_feasible_bins_threshold bins are feasible.\n        exploration_noise: Exploration noise when fewer than or equal to num_feasible_bins_threshold bins are feasible.\n        fragment_penalty_threshold_ratio: Threshold ratio for fragment penalty.\n        fragment_penalty_factor: Penalty factor for fragments.\n        bin_occupation_exponent: Exponent to adjust the impact of bin occupation ratio.\n        item_size_relative_importance: Relative importance of the item size when calculating the bin score\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n        \n        # Bin occupation ratio, raised to a power\n        bin_occupation_ratio = (feasible_bins_remain_cap - remaining_capacity_after_fit) / feasible_bins_remain_cap\n        priorities[feasible_mask] = np.power(bin_occupation_ratio, bin_occupation_exponent)\n        \n        # Adjust priority based on item size relative to bin capacity\n        priorities[feasible_mask] += item_size_relative_importance * capacity_ratio\n\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold_ratio)\n        priorities[feasible_mask][small_fragment_mask] *= small_fragment_penalty\n\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold_ratio)\n        priorities[feasible_mask][large_capacity_mask] *= large_capacity_bonus\n\n        # Adaptive exploration noise\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > num_feasible_bins_threshold:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + exploitation_bonus * capacity_ratio) # Favor bins that are already relatively full.\n            exploration_noise_level = base_exploration_noise / (1 + exploitation_bonus)\n        else:  # Fewer options, more exploration\n            exploration_noise_level = exploration_noise * (1 + exploitation_bonus)\n        priorities += np.random.normal(0, exploration_noise_level, size=len(priorities))\n\n        # Dynamic fragment penalty\n        fragment_penalty_threshold = item * fragment_penalty_threshold_ratio\n        fragment_penalty_mask = (remaining_capacity_after_fit > 0) & (remaining_capacity_after_fit < fragment_penalty_threshold)\n        priorities[feasible_mask][fragment_penalty_mask] *= (1 - (remaining_capacity_after_fit[fragment_penalty_mask] / fragment_penalty_threshold) * fragment_penalty_factor)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines fit, utilization, and adaptive noise.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    valid_bins = bins_remain_cap >= item\n\n    if not np.any(valid_bins):\n        return priorities - 1e9\n\n    # Fit score\n    fit_score = np.exp(-np.abs(bins_remain_cap - item) / (item + 1e-9))\n    priorities[valid_bins] = fit_score[valid_bins]\n\n    # Bin utilization bonus\n    utilization = item / bins_remain_cap[valid_bins]\n    priorities[valid_bins] += utilization\n\n    # Adaptive noise based on number of valid bins\n    num_valid_bins = np.sum(valid_bins)\n    noise_scale = 0.005 if num_valid_bins > 3 else 0.05\n    noise = np.random.normal(0, noise_scale, len(priorities))\n    priorities += noise\n\n    # Encourage bins with mid-level remaining capacities\n    remaining_capacity_after_fit = bins_remain_cap[valid_bins] - item\n    medium_capacity_mask = (remaining_capacity_after_fit >= (item * 0.2)) & (remaining_capacity_after_fit <= (item * 1.5))\n    priorities[valid_bins][medium_capacity_mask] *= 1.1  # Slightly favor medium capacity\n\n    # Exploration bonus if very few bins\n    exploration_prob = 0.3 if num_valid_bins < 3 else 0.1\n    if np.random.rand() < exploration_prob:\n        random_scale = 0.1 * item\n        priorities += np.random.normal(0, random_scale, size=len(priorities))\n\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see that the 1st includes `bin_utilization_exponent`, `item_size_threshold_ratio`, and `large_item_bonus` whereas 20th does not. The 1st calculates `capacity_ratio = item / feasible_bins_remain_cap`, whereas the 20th uses `capacity_ratio = item / (feasible_bins_remain_cap + item)`. The first utilizes `capacity_ratio**bin_utilization_exponent` to encourage bins that fit items well, providing non-linear scaling. The 20th uses `priorities[feasible_mask] = capacity_ratio`.\n\nComparing (2nd) vs (19th), we see that the 2nd includes `bin_utilization_exponent`, `item_size_threshold_ratio`, and `large_item_bonus` whereas 19th does not. The 2nd calculates `capacity_ratio = item / feasible_bins_remain_cap`, whereas the 19th uses `capacity_ratio = item / (feasible_bins_remain_cap + item)`.\n\nComparing (3rd) vs (4th), the 3rd introduces `well_utilized_threshold` as the threshold for bin utilization and changes the way it is calculated in a bonus condition. 3rd also includes more arguments such as `well_utilized_threshold`.\n\nComparing (2nd worst) vs (worst), 19th and 20th are identical.\n\nOverall: The top performing heuristics include `bin_utilization_exponent`, `item_size_threshold_ratio`, and `large_item_bonus`. They normalize the capacity ratio by `item / feasible_bins_remain_cap`. An important aspect of better heuristics is the non-linear scaling of the capacity ratio with a `bin_utilization_exponent`. The adaptive adjustment of exploration vs. exploitation based on the number of feasible bins is also a critical feature.\n- \nOkay, I'm ready to craft some truly effective self-reflection guidelines for heuristic design! Let's get that $999K!\n\nHere's a redefined \"Current Self-Reflection\" avoiding the pitfalls of \"Ineffective Self-Reflection\":\n\n*   **Keywords:** Simplicity, Adaptivity, Balance, Core Intuition, Iterative Refinement.\n\n*   **Advice:** Begin with a simple, interpretable core heuristic grounded in problem-specific intuition (e.g., normalized waste).\n\n*   **Avoid:** Over-complexity, premature optimization, ignoring edge cases (small fragments, near-full bins).\n\n*   **Explanation:** Prioritize a clear, understandable core. Add complexity gradually, validating each addition. Balance exploration (randomness) and exploitation (greediness) adaptively. Explicitly handle edge cases and parameter tuning should be done carefully, to allow adapting heuristic behavior to new problem instances.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}