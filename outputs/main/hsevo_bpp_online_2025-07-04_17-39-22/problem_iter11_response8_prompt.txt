{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # A bin must have enough capacity to accommodate the item. Otherwise the priority should be very low.\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf  # Make infeasible bins have the lowest priority.\n\n    feasible_mask = ~infeasible_mask\n    feasible_bins_remain_cap = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_bins_remain_cap - item\n        capacity_ratio = item / feasible_bins_remain_cap\n\n        # Encourage bins that fit the item *relatively* well.\n        # The more filled the bin is the higher the priority.\n\n        priorities[feasible_mask] = capacity_ratio\n\n        # Reduce the priority if the remaining space is too small *relative* to the item to avoid creating *very* small fragments.\n        small_fragment_threshold = 0.1  # Tunable parameter\n        small_fragment_mask = remaining_capacity_after_fit < (item * small_fragment_threshold)\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        # Slightly increase the priority of bins with large remaining capacity to diversify selection.\n        large_capacity_threshold = 2.0 # Tunable parameter\n        large_capacity_mask = remaining_capacity_after_fit > (item * large_capacity_threshold)\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        # Introduce some randomness to break ties and explore the search space more effectively.\n        randomness_scale = 0.01 #Tunable parameter\n        priorities += np.random.normal(0, randomness_scale, size=len(priorities))\n\n        # Adaptive adjustment of exploration vs. exploitation\n        # Based on the number of feasible bins\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:  # more options, more exploitation\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio) # Favor bins that are already relatively full.\n        else: # Fewer options, more exploration\n            priorities += np.random.normal(0, 0.05, size=len(priorities)) # Higher randomness\n\n        # Dynamic adjustment based on average bin fill level\n        average_fill = np.mean((1 - bins_remain_cap[feasible_mask] / np.max(bins_remain_cap)) if feasible_bins_remain_cap.size else 0)\n        if average_fill > 0.7:  # Bins are getting full\n            # Increase penalty for small fragments, encourage filling existing bins\n            priorities[feasible_mask][small_fragment_mask] *= 0.3\n            priorities[feasible_mask] *= (1 + 0.02 * capacity_ratio) #Stronger bias towards filling\n        elif average_fill < 0.3:\n            # Encourage using new bins\n            priorities[feasible_mask][large_capacity_mask] *= 1.2\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Combines adaptive strategies for bin packing priority.\"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    infeasible_mask = bins_remain_cap < item\n    priorities[infeasible_mask] = -np.inf\n\n    feasible_mask = ~infeasible_mask\n    feasible_capacities = bins_remain_cap[feasible_mask]\n\n    if np.sum(feasible_mask) > 0:\n        remaining_capacity_after_fit = feasible_capacities - item\n        capacity_ratio = item / feasible_capacities\n\n        priorities[feasible_mask] = capacity_ratio\n\n        small_fragment_threshold = 0.1 * item\n        small_fragment_mask = remaining_capacity_after_fit < small_fragment_threshold\n        priorities[feasible_mask][small_fragment_mask] *= 0.5\n\n        large_capacity_threshold = 2 * item\n        large_capacity_mask = remaining_capacity_after_fit > large_capacity_threshold\n        priorities[feasible_mask][large_capacity_mask] *= 1.1\n\n        num_feasible = np.sum(feasible_mask)\n        if num_feasible > 5:\n            priorities[feasible_mask] *= (1 + 0.01 * capacity_ratio)\n        else:\n            priorities += np.random.normal(0, 0.05, size=len(priorities))\n\n        # Near-full bin penalty\n        near_full_threshold = 0.1 * np.max(bins_remain_cap)\n        near_full_mask = (bins_remain_cap > 0) & (bins_remain_cap < item + near_full_threshold) & (~infeasible_mask)\n        priorities[near_full_mask] = -0.1\n\n        bin_fullness = 1 - bins_remain_cap / np.max(bins_remain_cap)\n        priorities += 0.05 * bin_fullness\n\n        utilization = 1 - np.mean(bins_remain_cap / np.max(bins_remain_cap))\n        noise_level = 0.01 * (1 - utilization)\n        priorities += np.random.normal(0, noise_level, size=len(priorities))\n\n    return priorities\n\n### Analyze & experience\n- Comparing (1st) vs (20th), we see the best heuristic combines normalized waste and bin utilization with dynamic noise, while the worst focuses on capacity ratio, waste, and dynamic noise but adds a penalty for bins becoming nearly full, a bonus for perfect fills, and normalizes priorities.\n(2nd best) vs (2nd worst): the second best balances fit, waste, fullness, and randomness, boosting bins with smaller remaining capacity and using adaptive noise. The second worst, similar to the worst, includes capacity ratio, waste, dynamic noise, near-full bin discouragement, and encourages re-use of nearly full bins.\nComparing (1st) vs (2nd), we see that the best uses a simpler noise scaling strategy based on the number of potential bins, while the second best scales noise by the mean remaining capacity of potential bins.\n(3rd) vs (4th): these two are identical.\nComparing (2nd worst) vs (worst), we see the second worst offers more encouragement toward usage of bins already nearly full while the worst also has near full bin discourage. Overall:\n\n*   **Feature Importance:** The best heuristics prioritize normalized waste and bin utilization. Penalizing future waste and rewarding already utilized bins appears effective.\n*   **Noise Scaling:** Simpler noise scaling mechanisms tied directly to the number of viable bins perform better. Complex scaling with bin capacities may be less effective.\n*   **Over-Engineering:** The worst heuristics attempt too many adjustments (penalties/bonuses) based on fine-grained conditions (e.g., nearly full bins, perfect fills). This can lead to instability.\n*   **Balanced Approach:** Adaptive exploration and exploitation is important, but overemphasizing either can be detrimental. Adaptive randomness, controlled by average bin fill levels or number of feasible bins, are used to break the tie.\n- \nOkay, let's redefine \"Current Self-Reflection\" to design better heuristics, avoiding the pitfalls of \"Ineffective Self-Reflection.\"\n\nHere's a revised approach focusing on iterative improvement and targeted adjustments:\n\n*   **Keywords:** Iterative refinement, targeted complexity, performance analysis, problem-specific adaptation.\n*   **Advice:** Focus on iterative development, rigorously testing each added complexity for tangible performance gains on diverse instances. Analyze performance bottlenecks and tailor adjustments accordingly.\n*   **Avoid:** Premature optimization, over-reliance on intuition without empirical validation, generic solutions failing to adapt to specific problem characteristics.\n*   **Explanation:** Prioritize data-driven decisions. Base heuristic refinement on performance evaluation against specific problem instances rather than assumptions. Complexity should only be added if demonstrably beneficial.\n\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}