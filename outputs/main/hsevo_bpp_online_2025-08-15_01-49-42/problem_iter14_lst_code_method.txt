{"system": "You are an expert in the domain of optimization heuristics. Your task is to provide useful advice based on analysis to design better heuristics.\n", "user": "### List heuristics\nBelow is a list of design heuristics ranked from best to worst.\n[Heuristics 1st]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, using tiered scoring.\n    This heuristic combines the clarity of exact fit prioritization with nuanced differentiation\n    among non-exact fits based on relative space utilization for better packing efficiency.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) == 0:\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    exact_fit_mask = np.abs(eligible_bins_remain_cap - item) < epsilon\n    exact_fit_indices_filtered = np.where(exact_fit_mask)[0]\n    actual_exact_fit_indices = fit_indices[exact_fit_indices_filtered]\n\n    priorities[actual_exact_fit_indices] = 1.0\n\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    non_exact_fit_indices = np.where(non_exact_fit_mask)[0]\n\n    if len(non_exact_fit_indices) > 0:\n        eligible_bins_for_slack_subset = bins_remain_cap[non_exact_fit_indices]\n        \n        remaining_after_fit = eligible_bins_for_slack_subset - item\n        \n        # Prioritize bins that leave minimal space after fitting the item.\n        # Normalize this residual capacity relative to the bin's capacity before fitting.\n        # This captures both 'best fit' and 'normalized slack' ideas.\n        # Higher score for smaller normalized remaining capacity.\n        normalized_remaining_capacity = remaining_after_fit / (eligible_bins_for_slack_subset + epsilon)\n        \n        # Map to scores less than 1.0 (exact fit score) to differentiate.\n        # A score of 1 - normalized_remaining_capacity (scaled) gives higher scores to bins\n        # that have less remaining space after fitting, thus being more \"full\".\n        # Scale to be in a range like [0.5, 0.99] for clear distinction from exact fits.\n        best_fit_scores = 1.0 - normalized_remaining_capacity\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[non_exact_fit_indices] = scaled_best_fit_priorities\n\n    return priorities\n\n[Heuristics 2nd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then Best Fit with a penalty for extreme slack.\n\n    This heuristic assigns the highest priority to bins that perfectly fit the item.\n    For bins that do not offer an exact fit, it prioritizes those with the smallest\n    remaining capacity (Best Fit). Additionally, it penalizes bins that would leave\n    a disproportionately large amount of remaining capacity relative to the item size,\n    promoting more balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    # Define a score for exact fits to give them highest priority\n    exact_fit_score = 1e6  # A very high score for perfect matches\n\n    # Define a penalty factor for large slack relative to the item size.\n    # This encourages more balanced packing by down-weighting bins that become too empty.\n    slack_penalty_factor = 0.1\n    \n    # Define a small epsilon to handle potential division by zero or near-zero item sizes\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Calculate slack for bins that can fit the item\n        slack = bins_remain_cap[available_bins_indices] - item\n        \n        # --- Scoring Logic ---\n        \n        # 1. Prioritize exact fits\n        exact_fit_indices = np.where(slack < epsilon)[0]\n        if exact_fit_indices.size > 0:\n            priorities[available_bins_indices[exact_fit_indices]] = exact_fit_score\n            \n            # Remove exact fits from further consideration for secondary scoring\n            # to ensure they retain the highest priority.\n            non_exact_fit_indices = np.delete(np.arange(len(available_bins_indices)), exact_fit_indices)\n            if non_exact_fit_indices.size == 0:\n                return priorities # All fitting bins were exact fits\n\n            # Update available_bins_indices to only include non-exact fits\n            available_bins_indices = available_bins_indices[non_exact_fit_indices]\n            slack = slack[non_exact_fit_indices]\n        \n        # 2. For non-exact fits, use Best Fit (minimize slack) as primary\n        # Score is proportional to negative slack. Higher value for smaller slack.\n        best_fit_scores = -slack\n        \n        # 3. Add a penalty for large slack relative to item size.\n        # This discourages bins that remain too empty after packing.\n        # Penalty is proportional to `slack / item` (or just `slack` if item is zero)\n        # We use `slack / (item + epsilon)` to avoid division by zero.\n        relative_slack = slack / (item + epsilon)\n        \n        # The penalty reduces the score for bins with high relative slack.\n        # We subtract the penalty term.\n        combined_scores = best_fit_scores - slack_penalty_factor * relative_slack\n        \n        priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n[Heuristics 3rd]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, using tiered scoring.\n    This heuristic combines the clarity of exact fit prioritization with nuanced differentiation\n    among non-exact fits based on relative space utilization for better packing efficiency.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) == 0:\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    exact_fit_mask = np.abs(eligible_bins_remain_cap - item) < epsilon\n    exact_fit_indices_filtered = np.where(exact_fit_mask)[0]\n    actual_exact_fit_indices = fit_indices[exact_fit_indices_filtered]\n\n    priorities[actual_exact_fit_indices] = 1.0\n\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    non_exact_fit_indices = np.where(non_exact_fit_mask)[0]\n\n    if len(non_exact_fit_indices) > 0:\n        eligible_bins_for_slack_subset = bins_remain_cap[non_exact_fit_indices]\n        \n        remaining_after_fit = eligible_bins_for_slack_subset - item\n        \n        # Prioritize bins that leave minimal space after fitting the item.\n        # Normalize this residual capacity relative to the bin's capacity before fitting.\n        # This captures both 'best fit' and 'normalized slack' ideas.\n        # Higher score for smaller normalized remaining capacity.\n        normalized_remaining_capacity = remaining_after_fit / (eligible_bins_for_slack_subset + epsilon)\n        \n        # Map to scores less than 1.0 (exact fit score) to differentiate.\n        # A score of 1 - normalized_remaining_capacity (scaled) gives higher scores to bins\n        # that have less remaining space after fitting, thus being more \"full\".\n        # Scale to be in a range like [0.5, 0.99] for clear distinction from exact fits.\n        best_fit_scores = 1.0 - normalized_remaining_capacity\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[non_exact_fit_indices] = scaled_best_fit_priorities\n\n    return priorities\n\n[Heuristics 4th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then Best Fit with a penalty for extreme slack.\n\n    This heuristic assigns the highest priority to bins that perfectly fit the item.\n    For bins that do not offer an exact fit, it prioritizes those with the smallest\n    remaining capacity (Best Fit). Additionally, it penalizes bins that would leave\n    a disproportionately large amount of remaining capacity relative to the item size,\n    promoting more balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    # Define a score for exact fits to give them highest priority\n    exact_fit_score = 1e6  # A very high score for perfect matches\n\n    # Define a penalty factor for large slack relative to the item size.\n    # This encourages more balanced packing by down-weighting bins that become too empty.\n    slack_penalty_factor = 0.1\n    \n    # Define a small epsilon to handle potential division by zero or near-zero item sizes\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Calculate slack for bins that can fit the item\n        slack = bins_remain_cap[available_bins_indices] - item\n        \n        # --- Scoring Logic ---\n        \n        # 1. Prioritize exact fits\n        exact_fit_indices = np.where(slack < epsilon)[0]\n        if exact_fit_indices.size > 0:\n            priorities[available_bins_indices[exact_fit_indices]] = exact_fit_score\n            \n            # Remove exact fits from further consideration for secondary scoring\n            # to ensure they retain the highest priority.\n            non_exact_fit_indices = np.delete(np.arange(len(available_bins_indices)), exact_fit_indices)\n            if non_exact_fit_indices.size == 0:\n                return priorities # All fitting bins were exact fits\n\n            # Update available_bins_indices to only include non-exact fits\n            available_bins_indices = available_bins_indices[non_exact_fit_indices]\n            slack = slack[non_exact_fit_indices]\n        \n        # 2. For non-exact fits, use Best Fit (minimize slack) as primary\n        # Score is proportional to negative slack. Higher value for smaller slack.\n        best_fit_scores = -slack\n        \n        # 3. Add a penalty for large slack relative to item size.\n        # This discourages bins that remain too empty after packing.\n        # Penalty is proportional to `slack / item` (or just `slack` if item is zero)\n        # We use `slack / (item + epsilon)` to avoid division by zero.\n        relative_slack = slack / (item + epsilon)\n        \n        # The penalty reduces the score for bins with high relative slack.\n        # We subtract the penalty term.\n        combined_scores = best_fit_scores - slack_penalty_factor * relative_slack\n        \n        priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n[Heuristics 5th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then Best Fit with a penalty for extreme slack.\n\n    This heuristic assigns the highest priority to bins that perfectly fit the item.\n    For bins that do not offer an exact fit, it prioritizes those with the smallest\n    remaining capacity (Best Fit). Additionally, it penalizes bins that would leave\n    a disproportionately large amount of remaining capacity relative to the item size,\n    promoting more balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    # Define a score for exact fits to give them highest priority\n    exact_fit_score = 1e6  # A very high score for perfect matches\n\n    # Define a penalty factor for large slack relative to the item size.\n    # This encourages more balanced packing by down-weighting bins that become too empty.\n    slack_penalty_factor = 0.1\n    \n    # Define a small epsilon to handle potential division by zero or near-zero item sizes\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Calculate slack for bins that can fit the item\n        slack = bins_remain_cap[available_bins_indices] - item\n        \n        # --- Scoring Logic ---\n        \n        # 1. Prioritize exact fits\n        exact_fit_indices = np.where(slack < epsilon)[0]\n        if exact_fit_indices.size > 0:\n            priorities[available_bins_indices[exact_fit_indices]] = exact_fit_score\n            \n            # Remove exact fits from further consideration for secondary scoring\n            # to ensure they retain the highest priority.\n            non_exact_fit_indices = np.delete(np.arange(len(available_bins_indices)), exact_fit_indices)\n            if non_exact_fit_indices.size == 0:\n                return priorities # All fitting bins were exact fits\n\n            # Update available_bins_indices to only include non-exact fits\n            available_bins_indices = available_bins_indices[non_exact_fit_indices]\n            slack = slack[non_exact_fit_indices]\n        \n        # 2. For non-exact fits, use Best Fit (minimize slack) as primary\n        # Score is proportional to negative slack. Higher value for smaller slack.\n        best_fit_scores = -slack\n        \n        # 3. Add a penalty for large slack relative to item size.\n        # This discourages bins that remain too empty after packing.\n        # Penalty is proportional to `slack / item` (or just `slack` if item is zero)\n        # We use `slack / (item + epsilon)` to avoid division by zero.\n        relative_slack = slack / (item + epsilon)\n        \n        # The penalty reduces the score for bins with high relative slack.\n        # We subtract the penalty term.\n        combined_scores = best_fit_scores - slack_penalty_factor * relative_slack\n        \n        priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n[Heuristics 6th]\nimport numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, exact_fit_priority: float = 0.8204260464641756, non_exact_fit_min_priority: float = 0.40771058685052436, non_exact_fit_max_priority: float = 0.8437235323156685, epsilon: float = 7.018361641051186e-09) -> np.ndarray:\n    \"\"\"\n    Combines exact fit priority with scaled inverse normalized slack for non-exact fits.\n    Prioritizes exact fits, then bins with minimal normalized slack to promote balanced packing.\n\n    Args:\n        item: The size of the item to be packed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        exact_fit_priority: The priority assigned to bins that are an exact fit.\n        non_exact_fit_min_priority: The minimum priority assigned to non-exact fits.\n        non_exact_fit_max_priority: The maximum priority assigned to non-exact fits.\n        epsilon: A small value for numerical stability in normalization.\n\n    Returns:\n        A numpy array representing the priority for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Mask for bins that can potentially fit the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    # Identify bins that are an exact fit\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    \n    # Assign highest priority to exact fits\n    priorities[exact_fit_mask] = exact_fit_priority\n    \n    # Consider bins that can fit the item but are not exact fits\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    non_exact_indices = np.where(non_exact_fit_mask)[0]\n    \n    if len(non_exact_indices) > 0:\n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = bins_remain_cap[non_exact_indices] - item\n        current_capacities = bins_remain_cap[non_exact_indices]\n        \n        # Calculate normalized slack: (remaining_capacity_after_fit) / (current_bin_capacity)\n        # Smaller normalized slack is better. Add epsilon for numerical stability.\n        normalized_slack = remaining_after_fit / (current_capacities + epsilon)\n        \n        # Assign priorities: higher score for smaller normalized slack.\n        # Use 1.0 - normalized_slack to map smaller slack to higher scores.\n        # Scale these scores to be less than exact_fit_priority, ensuring exact fits are always preferred.\n        # The range [non_exact_fit_min_priority, non_exact_fit_max_priority] effectively differentiates good fits.\n        best_fit_scores = 1.0 - normalized_slack\n        \n        # Clamp scores to ensure they fall within the desired range and are always less than exact_fit_priority\n        # The scaling factor (non_exact_fit_max_priority - non_exact_fit_min_priority) controls the spread of priorities\n        # The offset non_exact_fit_min_priority shifts the range\n        scaled_best_fit_priorities = non_exact_fit_min_priority + np.clip(best_fit_scores, 0.0, 1.0) * (non_exact_fit_max_priority - non_exact_fit_min_priority)\n        \n        # Ensure scaled priorities are strictly less than exact_fit_priority if exact_fit_priority is 1.0\n        if exact_fit_priority == 1.0:\n             scaled_best_fit_priorities = np.clip(scaled_best_fit_priorities, non_exact_fit_min_priority, non_exact_fit_max_priority)\n        else:\n             scaled_best_fit_priorities = np.clip(scaled_best_fit_priorities, non_exact_fit_min_priority, min(non_exact_fit_max_priority, exact_fit_priority - 1e-6)) # ensure less than exact_fit_priority\n        \n        priorities[non_exact_indices] = scaled_best_fit_priorities\n        \n    return priorities\n\n[Heuristics 7th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then Best Fit with a penalty for extreme slack.\n\n    This heuristic assigns the highest priority to bins that perfectly fit the item.\n    For bins that do not offer an exact fit, it prioritizes those with the smallest\n    remaining capacity (Best Fit). Additionally, it penalizes bins that would leave\n    a disproportionately large amount of remaining capacity relative to the item size,\n    promoting more balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    # Define a score for exact fits to give them highest priority\n    exact_fit_score = 1e6  # A very high score for perfect matches\n\n    # Define a penalty factor for large slack relative to the item size.\n    # This encourages more balanced packing by down-weighting bins that become too empty.\n    slack_penalty_factor = 0.1\n    \n    # Define a small epsilon to handle potential division by zero or near-zero item sizes\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Calculate slack for bins that can fit the item\n        slack = bins_remain_cap[available_bins_indices] - item\n        \n        # --- Scoring Logic ---\n        \n        # 1. Prioritize exact fits\n        exact_fit_indices = np.where(slack < epsilon)[0]\n        if exact_fit_indices.size > 0:\n            priorities[available_bins_indices[exact_fit_indices]] = exact_fit_score\n            \n            # Remove exact fits from further consideration for secondary scoring\n            # to ensure they retain the highest priority.\n            non_exact_fit_indices = np.delete(np.arange(len(available_bins_indices)), exact_fit_indices)\n            if non_exact_fit_indices.size == 0:\n                return priorities # All fitting bins were exact fits\n\n            # Update available_bins_indices to only include non-exact fits\n            available_bins_indices = available_bins_indices[non_exact_fit_indices]\n            slack = slack[non_exact_fit_indices]\n        \n        # 2. For non-exact fits, use Best Fit (minimize slack) as primary\n        # Score is proportional to negative slack. Higher value for smaller slack.\n        best_fit_scores = -slack\n        \n        # 3. Add a penalty for large slack relative to item size.\n        # This discourages bins that remain too empty after packing.\n        # Penalty is proportional to `slack / item` (or just `slack` if item is zero)\n        # We use `slack / (item + epsilon)` to avoid division by zero.\n        relative_slack = slack / (item + epsilon)\n        \n        # The penalty reduces the score for bins with high relative slack.\n        # We subtract the penalty term.\n        combined_scores = best_fit_scores - slack_penalty_factor * relative_slack\n        \n        priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n[Heuristics 8th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, offering robust differentiation.\n    Combines exact fit priority with scaled normalized slack for non-exact fits.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # High priority for exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_and_not_exact_mask = bins_remain_cap > item\n    \n    if np.any(can_fit_and_not_exact_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_and_not_exact_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_capacities_if_fit = eligible_bins_remain_cap - item\n        \n        # Normalized slack: remaining capacity / current bin capacity. Smaller is better.\n        # This is similar to \"Normalized Fit\" or \"Normalized Slack\" strategy.\n        epsilon = 1e-9\n        normalized_slack = remaining_capacities_if_fit / (eligible_bins_remain_cap + epsilon)\n        \n        # Priority: Higher score for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher priority (closer to 1.0).\n        # Scale these scores to be distinct from exact fits, e.g., between 0.5 and 0.99.\n        # This strategy provides a good differentiation for non-exact fits.\n        non_exact_priorities = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_and_not_exact_mask] = non_exact_priorities\n\n    return priorities\n\n[Heuristics 9th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, offering robust differentiation.\n    Combines exact fit priority with scaled normalized slack for non-exact fits.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # High priority for exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_and_not_exact_mask = bins_remain_cap > item\n    \n    if np.any(can_fit_and_not_exact_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_and_not_exact_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_capacities_if_fit = eligible_bins_remain_cap - item\n        \n        # Normalized slack: remaining capacity / current bin capacity. Smaller is better.\n        # This is similar to \"Normalized Fit\" or \"Normalized Slack\" strategy.\n        epsilon = 1e-9\n        normalized_slack = remaining_capacities_if_fit / (eligible_bins_remain_cap + epsilon)\n        \n        # Priority: Higher score for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher priority (closer to 1.0).\n        # Scale these scores to be distinct from exact fits, e.g., between 0.5 and 0.99.\n        # This strategy provides a good differentiation for non-exact fits.\n        non_exact_priorities = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_and_not_exact_mask] = non_exact_priorities\n\n    return priorities\n\n[Heuristics 10th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid \"Best Fit\" and \"Worst Fit\" with capacity normalization priority function.\n\n    This strategy aims to improve upon simple \"almost full\" by considering two aspects:\n    1.  Prioritizing bins that are \"tight\" fits (leaving minimal remaining capacity),\n        similar to Best Fit, but without being overly sensitive to small differences.\n    2.  As a secondary consideration, and to avoid clustering items into only the smallest\n        remaining capacity bins (which might lead to fragmentation later), it also\n        slightly favors bins that have more remaining capacity, up to a certain point,\n        acting as a \"soft\" worst fit to spread items better.\n    3.  It normalizes remaining capacities to handle varying bin sizes or item scales more robustly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        eligible_capacities = bins_remain_cap[can_fit_mask]\n        remaining_capacities_if_fit = eligible_capacities - item\n\n        # Primary score component: Prioritize bins that leave minimal remaining capacity.\n        # We use a score that is inversely related to the remaining capacity.\n        # Adding epsilon to denominator for numerical stability and to avoid zero division.\n        # Small remaining capacity -> large positive score.\n        epsilon = 1e-9\n        tight_fit_score = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n        # Secondary score component: Introduce a slight preference for bins with more capacity,\n        # acting as a \"soft\" worst fit to prevent premature clustering.\n        # This component is scaled to be less dominant than the tight fit.\n        # Larger remaining capacity -> larger score.\n        # We can normalize the remaining capacities relative to the *maximum* possible remaining capacity\n        # among eligible bins to make this component more stable.\n        # If all eligible bins have the same remaining capacity, this score will be uniform.\n        if np.max(eligible_capacities) > 0:\n             normalized_slack = eligible_capacities / np.max(eligible_capacities)\n        else:\n             normalized_slack = np.zeros_like(eligible_capacities) # Handle case where max capacity is 0\n\n        # Combine scores: Prioritize tight fits, then spread items.\n        # The weight (0.3) balances tight fitting vs. spreading. Adjust as needed.\n        combined_score = tight_fit_score * 0.7 + normalized_slack * 0.3\n\n        priorities[can_fit_mask] = combined_score\n\n    return priorities\n\n[Heuristics 11th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, offering robust differentiation.\n    Combines exact fit priority with scaled normalized slack for non-exact fits.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # High priority for exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_and_not_exact_mask = bins_remain_cap > item\n    \n    if np.any(can_fit_and_not_exact_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_and_not_exact_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_capacities_if_fit = eligible_bins_remain_cap - item\n        \n        # Normalized slack: remaining capacity / current bin capacity. Smaller is better.\n        # This is similar to \"Normalized Fit\" or \"Normalized Slack\" strategy.\n        epsilon = 1e-9\n        normalized_slack = remaining_capacities_if_fit / (eligible_bins_remain_cap + epsilon)\n        \n        # Priority: Higher score for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher priority (closer to 1.0).\n        # Scale these scores to be distinct from exact fits, e.g., between 0.5 and 0.99.\n        # This strategy provides a good differentiation for non-exact fits.\n        non_exact_priorities = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_and_not_exact_mask] = non_exact_priorities\n\n    return priorities\n\n[Heuristics 12th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid Priority: Best Fit + Differentiated Slack.\n\n    This strategy aims to improve upon \"Almost Full Fit\" by first prioritizing\n    bins that leave a small, specific remaining capacity, and then using\n    a more nuanced approach for bins that don't create an \"almost full\" state.\n    It differentiates between bins that are a perfect fit or leave a very small\n    slack, and bins that have more capacity.\n\n    The core idea is to:\n    1. Prioritize bins that can accommodate the item. Bins that cannot fit get -1.\n    2. Among fitting bins, prioritize those that leave a very small, positive\n       remaining capacity. This is a refined version of \"almost full.\"\n       We can define \"almost full\" as having a remaining capacity between 0 and some threshold,\n       or simply the smallest positive remaining capacity.\n    3. For bins that have larger remaining capacities, we still want to prefer\n       those that leave less residual space, but with a less aggressive scoring.\n       This helps in cases where no \"almost full\" bin exists.\n\n    We will use a scoring system that:\n    - Assigns a high score to bins that fit and leave a small remainder.\n    - Assigns a moderate score to bins that fit and leave a larger remainder.\n    - The scoring function aims to differentiate clearly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        \n        # Define a threshold for \"tight fit\" or \"almost full\".\n        # This could be a small percentage of the bin capacity, or a fixed small value.\n        # For simplicity, let's consider a small absolute remainder as high priority.\n        # Let's say a remainder < 10% of the bin capacity or a fixed small epsilon.\n        # A more robust approach is to normalize the remaining capacity by the original bin capacity\n        # if the original bin capacities were available. Since they are not, we work with remaining.\n        \n        # We want to prioritize minimal positive remaining capacity.\n        # A common way to differentiate is to use a piecewise function or a scaled inverse.\n        # Let's try a score that is higher for smaller remaining capacities.\n        # To ensure differentiation and avoid issues with division by near-zero,\n        # we can use a function like `max(0, threshold - x)` or `1 / (x + epsilon)`.\n        \n        # Approach:\n        # 1. Prioritize bins that leave a very small remainder (e.g., close to 0).\n        # 2. Among these, perhaps prioritize the one with the absolute smallest remainder.\n        # 3. For larger remainders, still prefer smaller ones, but with less impact.\n\n        # Let's assign scores such that smaller `remaining_capacities_if_fit` get higher scores.\n        # We can use a transformation that is strictly decreasing.\n        # For example, `-(remaining_capacity)` as in v1, but maybe scaled or with adjustments.\n        \n        # A key insight from the advice: \"use a hierarchical approach, prioritizing exact matches,\n        # then finely tuned slack minimization\".\n        # Exact matches: remaining_capacity_if_fit == 0.\n        # Finely tuned slack minimization: small positive remaining_capacity_if_fit.\n        \n        # Let's define a base score for fitting bins.\n        # We want to maximize the negative of remaining capacity.\n        # To differentiate, we can add a term that rewards smaller remainders more strongly.\n        \n        # Consider the 'Best Fit' aspect: minimize `bins_remain_cap - item`.\n        # This means maximizing `-(bins_remain_cap - item)`.\n        # Let's enhance this by making the penalty for larger remainders less severe\n        # or by giving a boost to \"almost full\" bins.\n        \n        # A common strategy is to use a score that is inversely related to the remaining capacity.\n        # However, to differentiate, let's consider a score that has a higher gradient for small remainders.\n        \n        # Option 1: Scaled Negative Remainder with a boost for small remainders.\n        # score = -remaining_capacity_if_fit\n        # This is similar to v1.\n\n        # Option 2: Inversely proportional to remaining capacity + a term that penalizes slack.\n        # Let's use a function that decreases rapidly for small remainders and then more slowly.\n        # Example: `1 / (x + epsilon)` or `max_capacity - x` if we knew max_capacity.\n        \n        # A simpler, yet effective strategy is to assign higher priority to bins that leave\n        # a remainder *closer* to zero, and among those, prioritize the one that is *exactly* zero.\n        \n        # Let's try a score based on the inverse of remaining capacity, but capped or scaled\n        # to avoid extreme values and ensure differentiability.\n        \n        # A robust approach is to make the priority score `f(remaining_capacity_if_fit)`\n        # where f is a decreasing function.\n        # To differentiate, we want f'(x) to be \"large negative\" for small x, and\n        # \"small negative\" for large x.\n        \n        # Example: `f(x) = -log(x + epsilon)` or `f(x) = -sqrt(x + epsilon)`.\n        # Or a piecewise linear function.\n        \n        # Let's try a score that is a large positive number minus the remaining capacity,\n        # but scaled.\n        \n        # Let's use a strategy that prioritizes bins that are \"almost full\" (small remainder)\n        # more aggressively than v1.\n        # We can achieve this by using a steeper decreasing function for the priority score.\n        \n        # Consider a scoring function like: `Constant - k * remaining_capacity`.\n        # If k is large, it's very sensitive to small remainders.\n        # If k is small, it's less sensitive.\n        \n        # Let's try to make the priority value higher for smaller remaining capacities.\n        # A common pattern in optimization is to use `1 / (x + epsilon)` for small x,\n        # or `max_val - x`.\n        \n        # Let's use a score that prioritizes smaller remaining capacities.\n        # We can use the negative of the remaining capacity as a base, and then\n        # add a small penalty for larger remainders.\n        \n        # Let's try to assign a score that is inversely proportional to the remaining capacity,\n        # but we need to ensure stability and differentiation.\n        \n        # Consider the objective of making the bins as full as possible.\n        # This means we want to minimize the remaining capacity.\n        # So, a bin with remaining capacity `r` should have a higher priority than one with `r' > r`.\n        \n        # Let's enhance the priority by considering the *relative* remaining capacity if we\n        # had bin capacity information. Since we don't, we focus on absolute remaining capacity.\n        \n        # Hybrid approach:\n        # If remaining_capacity_if_fit is very small (e.g., < 0.1 * item_size or a fixed small value),\n        # give it a high score.\n        # Otherwise, give it a score that is inversely proportional to the remaining capacity.\n        \n        # Let's try a smooth function that is steeper for small remainders.\n        # For example, `score = -remaining_capacity ** 0.5` or `score = -log(remaining_capacity + epsilon)`.\n        # Let's use `-(remaining_capacity_if_fit + epsilon)` for simplicity and stability,\n        # but this is still linear.\n        \n        # To ensure better differentiation for \"almost full\" bins, let's assign\n        # a significantly higher priority to bins that result in a very small remaining capacity.\n        # We can use a threshold to distinguish between \"tight fits\" and \"looser fits\".\n        \n        # Let's define a threshold for what constitutes an \"almost full\" bin.\n        # A simple threshold could be a small fraction of the item size itself, or a fixed value.\n        # For example, `threshold = 0.1 * item`.\n        # Or, more generally, `threshold = min_possible_positive_remainder`.\n        \n        # Let's try a scoring mechanism where:\n        # - Bins with `remaining_capacity_if_fit` close to 0 get a high, distinct score.\n        # - Bins with larger `remaining_capacity_if_fit` get scores that are still decreasing,\n        #   but with a lower magnitude of decrease.\n        \n        # A practical way to achieve this is by using a combination of linear and\n        # inverse proportional scoring, or a power function.\n        \n        # Let's use `score = - (remaining_capacity_if_fit + small_constant)`. This is linear.\n        # To make it more sensitive to small remainders, we can use `score = - (remaining_capacity_if_fit ** 0.5)`.\n        # Or even more aggressive: `score = -log(remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try a robust approach using inverse remaining capacity but with a slight modification\n        # to give a boost to bins that are 'almost full'.\n        \n        # We want to maximize `f(r)` where `f` is decreasing.\n        # Let's use a form that is highly sensitive to small `r`.\n        # `score = M - r` where M is a large constant.\n        # If we want to differentiate \"almost full\" vs \"not so full\", we can add a term.\n        \n        # Consider a piecewise approach for clarity and differentiation:\n        # If `remaining_capacity_if_fit < epsilon_tight`: high priority, proportional to `-remaining_capacity_if_fit`.\n        # Else: medium priority, proportional to `-remaining_capacity_if_fit`.\n        \n        # Let's use a simpler, effective continuous function that prioritizes small remainders.\n        # A form that is highly sensitive to small remainders is `1 / (x + epsilon)`.\n        # However, this can lead to very large values.\n        \n        # Let's refine the idea:\n        # Prioritize bins that minimize `remaining_capacity_if_fit`.\n        # This means we want to maximize `-remaining_capacity_if_fit`.\n        # To differentiate, we can consider a score that is `MAX_SCORE - (remaining_capacity_if_fit / SCALE)`.\n        # A larger `SCALE` means less differentiation. A smaller `SCALE` means more differentiation.\n        \n        # Let's use a score that is strongly decreasing for small `remaining_capacity_if_fit`.\n        # A good candidate is a function like `exp(-k * remaining_capacity_if_fit)` where `k` is positive.\n        # Or `1 / (remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try to combine the \"Best Fit\" (minimal remaining capacity) with a \"First Fit Decreasing\" spirit\n        # by giving a bonus to bins that become \"almost full\".\n        \n        # A more structured scoring:\n        # For bins that can fit:\n        # Calculate `r = bins_remain_cap[can_fit_mask] - item`\n        # Let's define a threshold `T`.\n        # If `r <= T`, the score is `BaseScore + Bonus - r`.\n        # If `r > T`, the score is `BaseScore - r / PenaltyFactor`.\n        \n        # For simplicity and robustness, let's use a score that emphasizes minimal remaining capacity\n        # without creating extreme values.\n        # `score = -remaining_capacity_if_fit` (like v1) is okay but doesn't differentiate much.\n        \n        # Let's try a score that is proportional to the inverse of the remaining capacity,\n        # but ensure stability and prevent extreme values.\n        # A common practice is to use a linear transformation of the inverse.\n        # `score = C - K * remaining_capacity_if_fit` where C and K are constants.\n        # To prioritize smaller remainders, K should be positive.\n        # To differentiate small remainders more, K could be larger for smaller remainders.\n        \n        # Let's try a scoring mechanism that strongly prefers bins with very small remainders.\n        # For example, if `remaining_capacity_if_fit` is 0, it's the best.\n        # If `remaining_capacity_if_fit` is small positive, it's very good.\n        # If `remaining_capacity_if_fit` is larger, it's less good.\n        \n        # Let's use a score that increases as `remaining_capacity_if_fit` decreases.\n        # A simple increasing function is `f(x) = C - x`.\n        # To differentiate more for small x, we could use `f(x) = C - sqrt(x)`.\n        # Or `f(x) = C - log(x + epsilon)`.\n        \n        # Let's implement a scoring function that is inversely related to the remaining capacity,\n        # but scaled to ensure distinct priorities.\n        # We want to maximize `priorities[can_fit_mask]`.\n        \n        # Let `r_vals = remaining_capacities_if_fit`.\n        # A good heuristic priority might be related to `1 / (r_vals + epsilon)`.\n        # To make it more stable and to ensure we don't get excessively large values,\n        # we can normalize or use a different function.\n        \n        # Consider a score like `-(r_vals + epsilon)`. This is v1's core idea.\n        # To improve differentiation:\n        # Let's use a score that is more sensitive to smaller values of `r_vals`.\n        # A power function can achieve this: `-(r_vals**p)` where `0 < p < 1`.\n        # For example, `p = 0.5` (square root).\n        \n        # Let's try `score = - np.sqrt(remaining_capacities_if_fit + 1e-9)`.\n        # This will give higher scores to smaller remaining capacities.\n        # The addition of `1e-9` avoids issues with `sqrt(0)`.\n        \n        # The \"Advice\" section mentions \"finely tuned slack minimization\".\n        # This implies we want to strongly favor bins that leave minimal slack.\n        \n        # Let's try a score that is inversely proportional to the remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + epsilon)`\n        # However, this can lead to extreme values.\n        \n        # A balanced approach:\n        # Prioritize bins with minimal remaining capacity.\n        # `score = -remaining_capacity_if_fit` (similar to v1)\n        \n        # Let's make it more aggressive for \"almost full\" bins.\n        # Consider a score like: `-(remaining_capacity_if_fit + 0.1 * item)`\n        # This penalizes larger remainders more, but we want to reward smaller ones.\n        \n        # Let's try a strategy that assigns a higher value to bins that are \"more full\".\n        # `priority = bins_remain_cap[can_fit_mask] - item` (this is remaining capacity)\n        # We want to minimize this. So, priority should be high for small values.\n        \n        # A simple way to differentiate is to use a large constant minus the remaining capacity.\n        # `score = LargeConstant - remaining_capacity_if_fit`\n        # To make it more sensitive to small remainders, we can make `LargeConstant` dependent on the problem,\n        # or use a non-linear function.\n        \n        # Let's try a robust scoring function that emphasizes minimal remaining capacity.\n        # `score = 1.0 / (remaining_capacity_if_fit + 1e-6)`\n        # This gives higher scores to smaller remainders.\n        # The `1e-6` is a small epsilon for numerical stability.\n        \n        # Let's try a different formulation that differentiates \"tight fits\" better.\n        # Consider a score based on how \"full\" the bin becomes.\n        # If a bin has remaining capacity `C_rem` and we place item `I`, the new remaining capacity is `C_rem - I`.\n        # We want `C_rem - I` to be as small as possible (but non-negative).\n        \n        # Let's use a function that rewards smaller remaining capacities.\n        # A simple way to do this is by using the negative of the remaining capacity itself,\n        # as in v1.\n        # `score = -(remaining_capacities_if_fit)`\n        \n        # To make it \"better\", we need more differentiation.\n        # Consider a scenario:\n        # Item size = 0.5\n        # Bins remaining capacities: [0.6, 0.7, 1.0]\n        # Possible remaining capacities after fitting: [0.1, 0.2, 0.5]\n        # v1 scores: [-0.1, -0.2, -0.5] -> Bin 1 is preferred.\n        \n        # We want to prioritize the bin that leaves 0.1, then 0.2, then 0.5.\n        # This means scores should be decreasing: score(0.1) > score(0.2) > score(0.5).\n        \n        # Let's use a score that is inversely proportional to remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + 1e-6)`\n        # Scores: [1.0/0.1, 1.0/0.2, 1.0/0.5] = [10.0, 5.0, 2.0]\n        # This clearly prioritizes the smallest remainder.\n        \n        # This seems to align well with the goal of prioritizing bins that are \"almost full\".\n        # It provides good differentiation.\n        \n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n    return priorities\n\n[Heuristics 13th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid Priority: Best Fit + Differentiated Slack.\n\n    This strategy aims to improve upon \"Almost Full Fit\" by first prioritizing\n    bins that leave a small, specific remaining capacity, and then using\n    a more nuanced approach for bins that don't create an \"almost full\" state.\n    It differentiates between bins that are a perfect fit or leave a very small\n    slack, and bins that have more capacity.\n\n    The core idea is to:\n    1. Prioritize bins that can accommodate the item. Bins that cannot fit get -1.\n    2. Among fitting bins, prioritize those that leave a very small, positive\n       remaining capacity. This is a refined version of \"almost full.\"\n       We can define \"almost full\" as having a remaining capacity between 0 and some threshold,\n       or simply the smallest positive remaining capacity.\n    3. For bins that have larger remaining capacities, we still want to prefer\n       those that leave less residual space, but with a less aggressive scoring.\n       This helps in cases where no \"almost full\" bin exists.\n\n    We will use a scoring system that:\n    - Assigns a high score to bins that fit and leave a small remainder.\n    - Assigns a moderate score to bins that fit and leave a larger remainder.\n    - The scoring function aims to differentiate clearly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        \n        # Define a threshold for \"tight fit\" or \"almost full\".\n        # This could be a small percentage of the bin capacity, or a fixed small value.\n        # For simplicity, let's consider a small absolute remainder as high priority.\n        # Let's say a remainder < 10% of the bin capacity or a fixed small epsilon.\n        # A more robust approach is to normalize the remaining capacity by the original bin capacity\n        # if the original bin capacities were available. Since they are not, we work with remaining.\n        \n        # We want to prioritize minimal positive remaining capacity.\n        # A common way to differentiate is to use a piecewise function or a scaled inverse.\n        # Let's try a score that is higher for smaller remaining capacities.\n        # To ensure differentiation and avoid issues with division by near-zero,\n        # we can use a function like `max(0, threshold - x)` or `1 / (x + epsilon)`.\n        \n        # Approach:\n        # 1. Prioritize bins that leave a very small remainder (e.g., close to 0).\n        # 2. Among these, perhaps prioritize the one with the absolute smallest remainder.\n        # 3. For larger remainders, still prefer smaller ones, but with less impact.\n\n        # Let's assign scores such that smaller `remaining_capacities_if_fit` get higher scores.\n        # We can use a transformation that is strictly decreasing.\n        # For example, `-(remaining_capacity)` as in v1, but maybe scaled or with adjustments.\n        \n        # A key insight from the advice: \"use a hierarchical approach, prioritizing exact matches,\n        # then finely tuned slack minimization\".\n        # Exact matches: remaining_capacity_if_fit == 0.\n        # Finely tuned slack minimization: small positive remaining_capacity_if_fit.\n        \n        # Let's define a base score for fitting bins.\n        # We want to maximize the negative of remaining capacity.\n        # To differentiate, we can add a term that rewards smaller remainders more strongly.\n        \n        # Consider the 'Best Fit' aspect: minimize `bins_remain_cap - item`.\n        # This means maximizing `-(bins_remain_cap - item)`.\n        # Let's enhance this by making the penalty for larger remainders less severe\n        # or by giving a boost to \"almost full\" bins.\n        \n        # A common strategy is to use a score that is inversely related to the remaining capacity.\n        # However, to differentiate, let's consider a score that has a higher gradient for small remainders.\n        \n        # Option 1: Scaled Negative Remainder with a boost for small remainders.\n        # score = -remaining_capacity_if_fit\n        # This is similar to v1.\n\n        # Option 2: Inversely proportional to remaining capacity + a term that penalizes slack.\n        # Let's use a function that decreases rapidly for small remainders and then more slowly.\n        # Example: `1 / (x + epsilon)` or `max_capacity - x` if we knew max_capacity.\n        \n        # A simpler, yet effective strategy is to assign higher priority to bins that leave\n        # a remainder *closer* to zero, and among those, prioritize the one that is *exactly* zero.\n        \n        # Let's try a score based on the inverse of remaining capacity, but capped or scaled\n        # to avoid extreme values and ensure differentiability.\n        \n        # A robust approach is to make the priority score `f(remaining_capacity_if_fit)`\n        # where f is a decreasing function.\n        # To differentiate, we want f'(x) to be \"large negative\" for small x, and\n        # \"small negative\" for large x.\n        \n        # Example: `f(x) = -log(x + epsilon)` or `f(x) = -sqrt(x + epsilon)`.\n        # Or a piecewise linear function.\n        \n        # Let's try a score that is a large positive number minus the remaining capacity,\n        # but scaled.\n        \n        # Let's use a strategy that prioritizes bins that are \"almost full\" (small remainder)\n        # more aggressively than v1.\n        # We can achieve this by using a steeper decreasing function for the priority score.\n        \n        # Consider a scoring function like: `Constant - k * remaining_capacity`.\n        # If k is large, it's very sensitive to small remainders.\n        # If k is small, it's less sensitive.\n        \n        # Let's try to make the priority value higher for smaller remaining capacities.\n        # A common pattern in optimization is to use `1 / (x + epsilon)` for small x,\n        # or `max_val - x`.\n        \n        # Let's use a score that prioritizes smaller remaining capacities.\n        # We can use the negative of the remaining capacity as a base, and then\n        # add a small penalty for larger remainders.\n        \n        # Let's try to assign a score that is inversely proportional to the remaining capacity,\n        # but we need to ensure stability and differentiation.\n        \n        # Consider the objective of making the bins as full as possible.\n        # This means we want to minimize the remaining capacity.\n        # So, a bin with remaining capacity `r` should have a higher priority than one with `r' > r`.\n        \n        # Let's enhance the priority by considering the *relative* remaining capacity if we\n        # had bin capacity information. Since we don't, we focus on absolute remaining capacity.\n        \n        # Hybrid approach:\n        # If remaining_capacity_if_fit is very small (e.g., < 0.1 * item_size or a fixed small value),\n        # give it a high score.\n        # Otherwise, give it a score that is inversely proportional to the remaining capacity.\n        \n        # Let's try a smooth function that is steeper for small remainders.\n        # For example, `score = -remaining_capacity ** 0.5` or `score = -log(remaining_capacity + epsilon)`.\n        # Let's use `-(remaining_capacity_if_fit + epsilon)` for simplicity and stability,\n        # but this is still linear.\n        \n        # To ensure better differentiation for \"almost full\" bins, let's assign\n        # a significantly higher priority to bins that result in a very small remaining capacity.\n        # We can use a threshold to distinguish between \"tight fits\" and \"looser fits\".\n        \n        # Let's define a threshold for what constitutes an \"almost full\" bin.\n        # A simple threshold could be a small fraction of the item size itself, or a fixed value.\n        # For example, `threshold = 0.1 * item`.\n        # Or, more generally, `threshold = min_possible_positive_remainder`.\n        \n        # Let's try a scoring mechanism where:\n        # - Bins with `remaining_capacity_if_fit` close to 0 get a high, distinct score.\n        # - Bins with larger `remaining_capacity_if_fit` get scores that are still decreasing,\n        #   but with a lower magnitude of decrease.\n        \n        # A practical way to achieve this is by using a combination of linear and\n        # inverse proportional scoring, or a power function.\n        \n        # Let's use `score = - (remaining_capacity_if_fit + small_constant)`. This is linear.\n        # To make it more sensitive to small remainders, we can use `score = - (remaining_capacity_if_fit ** 0.5)`.\n        # Or even more aggressive: `score = -log(remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try a robust approach using inverse remaining capacity but with a slight modification\n        # to give a boost to bins that are 'almost full'.\n        \n        # We want to maximize `f(r)` where `f` is decreasing.\n        # Let's use a form that is highly sensitive to small `r`.\n        # `score = M - r` where M is a large constant.\n        # If we want to differentiate \"almost full\" vs \"not so full\", we can add a term.\n        \n        # Consider a piecewise approach for clarity and differentiation:\n        # If `remaining_capacity_if_fit < epsilon_tight`: high priority, proportional to `-remaining_capacity_if_fit`.\n        # Else: medium priority, proportional to `-remaining_capacity_if_fit`.\n        \n        # Let's use a simpler, effective continuous function that prioritizes small remainders.\n        # A form that is highly sensitive to small remainders is `1 / (x + epsilon)`.\n        # However, this can lead to very large values.\n        \n        # Let's refine the idea:\n        # Prioritize bins that minimize `remaining_capacity_if_fit`.\n        # This means we want to maximize `-remaining_capacity_if_fit`.\n        # To differentiate, we can consider a score that is `MAX_SCORE - (remaining_capacity_if_fit / SCALE)`.\n        # A larger `SCALE` means less differentiation. A smaller `SCALE` means more differentiation.\n        \n        # Let's use a score that is strongly decreasing for small `remaining_capacity_if_fit`.\n        # A good candidate is a function like `exp(-k * remaining_capacity_if_fit)` where `k` is positive.\n        # Or `1 / (remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try to combine the \"Best Fit\" (minimal remaining capacity) with a \"First Fit Decreasing\" spirit\n        # by giving a bonus to bins that become \"almost full\".\n        \n        # A more structured scoring:\n        # For bins that can fit:\n        # Calculate `r = bins_remain_cap[can_fit_mask] - item`\n        # Let's define a threshold `T`.\n        # If `r <= T`, the score is `BaseScore + Bonus - r`.\n        # If `r > T`, the score is `BaseScore - r / PenaltyFactor`.\n        \n        # For simplicity and robustness, let's use a score that emphasizes minimal remaining capacity\n        # without creating extreme values.\n        # `score = -remaining_capacity_if_fit` (like v1) is okay but doesn't differentiate much.\n        \n        # Let's try a score that is proportional to the inverse of the remaining capacity,\n        # but ensure stability and prevent extreme values.\n        # A common practice is to use a linear transformation of the inverse.\n        # `score = C - K * remaining_capacity_if_fit` where C and K are constants.\n        # To prioritize smaller remainders, K should be positive.\n        # To differentiate small remainders more, K could be larger for smaller remainders.\n        \n        # Let's try a scoring mechanism that strongly prefers bins with very small remainders.\n        # For example, if `remaining_capacity_if_fit` is 0, it's the best.\n        # If `remaining_capacity_if_fit` is small positive, it's very good.\n        # If `remaining_capacity_if_fit` is larger, it's less good.\n        \n        # Let's use a score that increases as `remaining_capacity_if_fit` decreases.\n        # A simple increasing function is `f(x) = C - x`.\n        # To differentiate more for small x, we could use `f(x) = C - sqrt(x)`.\n        # Or `f(x) = C - log(x + epsilon)`.\n        \n        # Let's implement a scoring function that is inversely related to the remaining capacity,\n        # but scaled to ensure distinct priorities.\n        # We want to maximize `priorities[can_fit_mask]`.\n        \n        # Let `r_vals = remaining_capacities_if_fit`.\n        # A good heuristic priority might be related to `1 / (r_vals + epsilon)`.\n        # To make it more stable and to ensure we don't get excessively large values,\n        # we can normalize or use a different function.\n        \n        # Consider a score like `-(r_vals + epsilon)`. This is v1's core idea.\n        # To improve differentiation:\n        # Let's use a score that is more sensitive to smaller values of `r_vals`.\n        # A power function can achieve this: `-(r_vals**p)` where `0 < p < 1`.\n        # For example, `p = 0.5` (square root).\n        \n        # Let's try `score = - np.sqrt(remaining_capacities_if_fit + 1e-9)`.\n        # This will give higher scores to smaller remaining capacities.\n        # The addition of `1e-9` avoids issues with `sqrt(0)`.\n        \n        # The \"Advice\" section mentions \"finely tuned slack minimization\".\n        # This implies we want to strongly favor bins that leave minimal slack.\n        \n        # Let's try a score that is inversely proportional to the remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + epsilon)`\n        # However, this can lead to extreme values.\n        \n        # A balanced approach:\n        # Prioritize bins with minimal remaining capacity.\n        # `score = -remaining_capacity_if_fit` (similar to v1)\n        \n        # Let's make it more aggressive for \"almost full\" bins.\n        # Consider a score like: `-(remaining_capacity_if_fit + 0.1 * item)`\n        # This penalizes larger remainders more, but we want to reward smaller ones.\n        \n        # Let's try a strategy that assigns a higher value to bins that are \"more full\".\n        # `priority = bins_remain_cap[can_fit_mask] - item` (this is remaining capacity)\n        # We want to minimize this. So, priority should be high for small values.\n        \n        # A simple way to differentiate is to use a large constant minus the remaining capacity.\n        # `score = LargeConstant - remaining_capacity_if_fit`\n        # To make it more sensitive to small remainders, we can make `LargeConstant` dependent on the problem,\n        # or use a non-linear function.\n        \n        # Let's try a robust scoring function that emphasizes minimal remaining capacity.\n        # `score = 1.0 / (remaining_capacity_if_fit + 1e-6)`\n        # This gives higher scores to smaller remainders.\n        # The `1e-6` is a small epsilon for numerical stability.\n        \n        # Let's try a different formulation that differentiates \"tight fits\" better.\n        # Consider a score based on how \"full\" the bin becomes.\n        # If a bin has remaining capacity `C_rem` and we place item `I`, the new remaining capacity is `C_rem - I`.\n        # We want `C_rem - I` to be as small as possible (but non-negative).\n        \n        # Let's use a function that rewards smaller remaining capacities.\n        # A simple way to do this is by using the negative of the remaining capacity itself,\n        # as in v1.\n        # `score = -(remaining_capacities_if_fit)`\n        \n        # To make it \"better\", we need more differentiation.\n        # Consider a scenario:\n        # Item size = 0.5\n        # Bins remaining capacities: [0.6, 0.7, 1.0]\n        # Possible remaining capacities after fitting: [0.1, 0.2, 0.5]\n        # v1 scores: [-0.1, -0.2, -0.5] -> Bin 1 is preferred.\n        \n        # We want to prioritize the bin that leaves 0.1, then 0.2, then 0.5.\n        # This means scores should be decreasing: score(0.1) > score(0.2) > score(0.5).\n        \n        # Let's use a score that is inversely proportional to remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + 1e-6)`\n        # Scores: [1.0/0.1, 1.0/0.2, 1.0/0.5] = [10.0, 5.0, 2.0]\n        # This clearly prioritizes the smallest remainder.\n        \n        # This seems to align well with the goal of prioritizing bins that are \"almost full\".\n        # It provides good differentiation.\n        \n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n    return priorities\n\n[Heuristics 14th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then employing a scaled\n    Best Fit approach with tie-breaking based on initial bin fullness.\n\n    This strategy assigns the highest priority to bins that perfectly fit the item.\n    For other bins, it prioritizes those that minimize the remaining capacity\n    after packing (Best Fit), using a scaled score. As a tie-breaker, it\n    favors bins that were initially fuller (less remaining capacity).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # --- Primary objective: Exact Fit ---\n        # Assign a very high score for bins that perfectly fit the item.\n        exact_fit_mask = fitting_bins_remain_cap == item\n        \n        # Use a large constant for exact fits to ensure they are always preferred.\n        exact_fit_score = 1e9\n        priorities[can_fit_mask][exact_fit_mask] = exact_fit_score\n        \n        # --- Secondary objective: Best Fit (for non-exact fits) ---\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_fitting_bins_remain_cap = fitting_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate remaining capacity after fitting the item.\n            remaining_after_fit = non_exact_fitting_bins_remain_cap - item\n            \n            # Score for Best Fit: Higher score for smaller `remaining_after_fit`.\n            # We maximize `-remaining_after_fit`.\n            best_fit_score_base = -remaining_after_fit\n            \n            # --- Tertiary objective: Tie-breaking based on initial bin fullness ---\n            # For bins with the same `remaining_after_fit` score (or very close),\n            # prefer the bin that was initially fuller. This means preferring\n            # bins with less `bins_remain_cap`.\n            # We can achieve this by adding a term proportional to `-bins_remain_cap`.\n            # A larger negative value (more initial capacity used) is better.\n            \n            # Combine Best Fit score with tie-breaker. Use a scaling factor\n            # to ensure Best Fit is the dominant criterion.\n            scale_factor = 1e6  # Ensures Best Fit dominates tie-breaking.\n            \n            # Combined score for non-exact fits:\n            # Maximize: (scale_factor * -remaining_after_fit) + (-initial_remaining_capacity)\n            # This prioritizes minimal `remaining_after_fit`, then minimal `initial_remaining_capacity`.\n            combined_priorities = scale_factor * best_fit_score_base - non_exact_fitting_bins_remain_cap\n            \n            priorities[can_fit_mask][non_exact_fit_mask] = combined_priorities\n\n    return priorities\n\n[Heuristics 15th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then employing a scaled\n    Best Fit approach with tie-breaking based on initial bin fullness.\n\n    This strategy assigns the highest priority to bins that perfectly fit the item.\n    For other bins, it prioritizes those that minimize the remaining capacity\n    after packing (Best Fit), using a scaled score. As a tie-breaker, it\n    favors bins that were initially fuller (less remaining capacity).\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # --- Primary objective: Exact Fit ---\n        # Assign a very high score for bins that perfectly fit the item.\n        exact_fit_mask = fitting_bins_remain_cap == item\n        \n        # Use a large constant for exact fits to ensure they are always preferred.\n        exact_fit_score = 1e9\n        priorities[can_fit_mask][exact_fit_mask] = exact_fit_score\n        \n        # --- Secondary objective: Best Fit (for non-exact fits) ---\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_fitting_bins_remain_cap = fitting_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate remaining capacity after fitting the item.\n            remaining_after_fit = non_exact_fitting_bins_remain_cap - item\n            \n            # Score for Best Fit: Higher score for smaller `remaining_after_fit`.\n            # We maximize `-remaining_after_fit`.\n            best_fit_score_base = -remaining_after_fit\n            \n            # --- Tertiary objective: Tie-breaking based on initial bin fullness ---\n            # For bins with the same `remaining_after_fit` score (or very close),\n            # prefer the bin that was initially fuller. This means preferring\n            # bins with less `bins_remain_cap`.\n            # We can achieve this by adding a term proportional to `-bins_remain_cap`.\n            # A larger negative value (more initial capacity used) is better.\n            \n            # Combine Best Fit score with tie-breaker. Use a scaling factor\n            # to ensure Best Fit is the dominant criterion.\n            scale_factor = 1e6  # Ensures Best Fit dominates tie-breaking.\n            \n            # Combined score for non-exact fits:\n            # Maximize: (scale_factor * -remaining_after_fit) + (-initial_remaining_capacity)\n            # This prioritizes minimal `remaining_after_fit`, then minimal `initial_remaining_capacity`.\n            combined_priorities = scale_factor * best_fit_score_base - non_exact_fitting_bins_remain_cap\n            \n            priorities[can_fit_mask][non_exact_fit_mask] = combined_priorities\n\n    return priorities\n\n[Heuristics 16th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit priority with scaled inverse normalized slack for non-exact fits.\n    Prioritizes bins that perfectly fit the item, then bins that leave minimal normalized slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    # Calculate remaining capacities for bins that can fit the item\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign high priority to exact fits (remaining capacity is zero)\n    exact_fit_mask = remaining_after_fit == 0\n    priorities[can_fit_mask][exact_fit_mask] = 1.0\n\n    # Assign scores to non-exact fits based on normalized slack (inverse)\n    # Higher score for smaller remaining capacity relative to original capacity.\n    # Normalized slack = (bin_capacity - item) / bin_capacity\n    # We want to maximize 1 - normalized_slack, which means minimizing normalized slack.\n    # Smaller slack -> higher priority.\n    # Score range for non-exact fits: [0.5, 0.99] to be less than exact fits.\n    non_exact_fit_indices = np.where(can_fit_mask)[0][~exact_fit_mask]\n    \n    if non_exact_fit_indices.size > 0:\n        bins_for_non_exact = bins_remain_cap[can_fit_mask][~exact_fit_mask]\n        remaining_for_non_exact = remaining_after_fit[~exact_fit_mask]\n\n        # Calculate normalized slack: remaining_capacity / original_capacity\n        # Use a small epsilon to avoid division by zero if original capacity was 0 (should not happen if item fits)\n        epsilon = 1e-9\n        normalized_slack = remaining_for_non_exact / (bins_for_non_exact + epsilon)\n        \n        # Scale scores for non-exact fits to be between 0.5 and 0.99\n        # We want to maximize (1 - normalized_slack), so we scale (1 - normalized_slack)\n        # The value (1 - normalized_slack) ranges from approximately 0 (for large slack) to 1 (for very small slack).\n        # We map this to [0.5, 0.99].\n        # Scale factor: 0.49 (range of 0.99 - 0.5)\n        # Offset: 0.5\n        scaled_scores = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_mask][~exact_fit_mask] = scaled_scores\n\n    return priorities\n\n[Heuristics 17th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid prioritization: Exact Fit + Normalized Slack.\n    Prioritizes exact fits, then uses normalized slack to differentiate others.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Score 1: Exact Fit (highest priority)\n        # A bin is an exact fit if remaining capacity equals item size.\n        exact_fit_mask = available_bins_remain_cap == item\n        priorities[can_fit_mask][exact_fit_mask] = 1.0\n        \n        # Score 2: Differentiated Slack for non-exact fits\n        # For bins that don't provide an exact fit, use normalized slack.\n        # Normalized slack = (remaining_cap - item) / bins_remain_cap.\n        # We want to minimize normalized slack, so we maximize its negative.\n        # To ensure these scores are lower than exact fits, we scale them.\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_bins_remain_cap = available_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate normalized slack: (remaining_cap - item) / initial_remaining_cap\n            # Higher score means less slack relative to bin size.\n            # We want to minimize slack, so prioritize bins with smaller slack.\n            # A lower slack value means the bin becomes 'more full' relative to its original capacity.\n            # To map this to higher priority, we can use:\n            # 1. A transformation that makes smaller slack values result in higher scores.\n            # 2. Scale these scores to be less than 1.0 (to ensure exact fits are always preferred).\n            \n            # Calculate slack: remaining_cap - item\n            slack = non_exact_bins_remain_cap - item\n            \n            # Normalize slack: slack / original_remaining_capacity\n            # Using initial remaining capacity of the subset of bins\n            # Add epsilon to avoid division by zero if a bin had 0 capacity initially (though covered by can_fit_mask, good practice)\n            epsilon = 1e-9\n            normalized_slack = slack / (non_exact_bins_remain_cap + epsilon)\n            \n            # Invert and scale: We want to maximize values for smaller normalized slack.\n            # A common range for non-exact fits is [0.5, 0.99].\n            # If normalized_slack is 0 (perfect fit, already handled by exact_fit_mask), it gets high score.\n            # If normalized_slack is close to 1 (item takes up almost no space), it gets low score.\n            # So, we can use 1 - normalized_slack to prioritize smaller normalized slack.\n            # Scale this to a range below 1.0, e.g., [0.5, 0.99].\n            # Let's use a linear mapping:\n            # normalized_slack = 0  -> score = 0.99\n            # normalized_slack = 1  -> score = 0.5\n            # score = m * (1 - normalized_slack) + c\n            # 0.99 = m * 1 + c\n            # 0.5 = m * 0 + c  => c = 0.5\n            # 0.99 = m + 0.5 => m = 0.49\n            # So, score = 0.49 * (1 - normalized_slack) + 0.5\n            \n            scaled_priority = 0.49 * (1.0 - normalized_slack) + 0.5\n            priorities[can_fit_mask][non_exact_fit_mask] = scaled_priority\n            \n    return priorities\n\n[Heuristics 18th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid prioritization: Exact Fit + Normalized Slack.\n    Prioritizes exact fits, then uses normalized slack to differentiate others.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Score 1: Exact Fit (highest priority)\n        # A bin is an exact fit if remaining capacity equals item size.\n        exact_fit_mask = available_bins_remain_cap == item\n        priorities[can_fit_mask][exact_fit_mask] = 1.0\n        \n        # Score 2: Differentiated Slack for non-exact fits\n        # For bins that don't provide an exact fit, use normalized slack.\n        # Normalized slack = (remaining_cap - item) / bins_remain_cap.\n        # We want to minimize normalized slack, so we maximize its negative.\n        # To ensure these scores are lower than exact fits, we scale them.\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_bins_remain_cap = available_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate normalized slack: (remaining_cap - item) / initial_remaining_cap\n            # Higher score means less slack relative to bin size.\n            # We want to minimize slack, so prioritize bins with smaller slack.\n            # A lower slack value means the bin becomes 'more full' relative to its original capacity.\n            # To map this to higher priority, we can use:\n            # 1. A transformation that makes smaller slack values result in higher scores.\n            # 2. Scale these scores to be less than 1.0 (to ensure exact fits are always preferred).\n            \n            # Calculate slack: remaining_cap - item\n            slack = non_exact_bins_remain_cap - item\n            \n            # Normalize slack: slack / original_remaining_capacity\n            # Using initial remaining capacity of the subset of bins\n            # Add epsilon to avoid division by zero if a bin had 0 capacity initially (though covered by can_fit_mask, good practice)\n            epsilon = 1e-9\n            normalized_slack = slack / (non_exact_bins_remain_cap + epsilon)\n            \n            # Invert and scale: We want to maximize values for smaller normalized slack.\n            # A common range for non-exact fits is [0.5, 0.99].\n            # If normalized_slack is 0 (perfect fit, already handled by exact_fit_mask), it gets high score.\n            # If normalized_slack is close to 1 (item takes up almost no space), it gets low score.\n            # So, we can use 1 - normalized_slack to prioritize smaller normalized slack.\n            # Scale this to a range below 1.0, e.g., [0.5, 0.99].\n            # Let's use a linear mapping:\n            # normalized_slack = 0  -> score = 0.99\n            # normalized_slack = 1  -> score = 0.5\n            # score = m * (1 - normalized_slack) + c\n            # 0.99 = m * 1 + c\n            # 0.5 = m * 0 + c  => c = 0.5\n            # 0.99 = m + 0.5 => m = 0.49\n            # So, score = 0.49 * (1 - normalized_slack) + 0.5\n            \n            scaled_priority = 0.49 * (1.0 - normalized_slack) + 0.5\n            priorities[can_fit_mask][non_exact_fit_mask] = scaled_priority\n            \n    return priorities\n\n[Heuristics 19th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit priority with scaled inverse normalized slack for non-exact fits.\n    Prioritizes bins that perfectly fit the item, then bins that leave minimal normalized slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    # Calculate remaining capacities for bins that can fit the item\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign high priority to exact fits (remaining capacity is zero)\n    exact_fit_mask = remaining_after_fit == 0\n    priorities[can_fit_mask][exact_fit_mask] = 1.0\n\n    # Assign scores to non-exact fits based on normalized slack (inverse)\n    # Higher score for smaller remaining capacity relative to original capacity.\n    # Normalized slack = (bin_capacity - item) / bin_capacity\n    # We want to maximize 1 - normalized_slack, which means minimizing normalized slack.\n    # Smaller slack -> higher priority.\n    # Score range for non-exact fits: [0.5, 0.99] to be less than exact fits.\n    non_exact_fit_indices = np.where(can_fit_mask)[0][~exact_fit_mask]\n    \n    if non_exact_fit_indices.size > 0:\n        bins_for_non_exact = bins_remain_cap[can_fit_mask][~exact_fit_mask]\n        remaining_for_non_exact = remaining_after_fit[~exact_fit_mask]\n\n        # Calculate normalized slack: remaining_capacity / original_capacity\n        # Use a small epsilon to avoid division by zero if original capacity was 0 (should not happen if item fits)\n        epsilon = 1e-9\n        normalized_slack = remaining_for_non_exact / (bins_for_non_exact + epsilon)\n        \n        # Scale scores for non-exact fits to be between 0.5 and 0.99\n        # We want to maximize (1 - normalized_slack), so we scale (1 - normalized_slack)\n        # The value (1 - normalized_slack) ranges from approximately 0 (for large slack) to 1 (for very small slack).\n        # We map this to [0.5, 0.99].\n        # Scale factor: 0.49 (range of 0.99 - 0.5)\n        # Offset: 0.5\n        scaled_scores = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_mask][~exact_fit_mask] = scaled_scores\n\n    return priorities\n\n[Heuristics 20th]\nimport numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid prioritization: Exact Fit + Normalized Slack.\n    Prioritizes exact fits, then uses normalized slack to differentiate others.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Score 1: Exact Fit (highest priority)\n        # A bin is an exact fit if remaining capacity equals item size.\n        exact_fit_mask = available_bins_remain_cap == item\n        priorities[can_fit_mask][exact_fit_mask] = 1.0\n        \n        # Score 2: Differentiated Slack for non-exact fits\n        # For bins that don't provide an exact fit, use normalized slack.\n        # Normalized slack = (remaining_cap - item) / bins_remain_cap.\n        # We want to minimize normalized slack, so we maximize its negative.\n        # To ensure these scores are lower than exact fits, we scale them.\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_bins_remain_cap = available_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate normalized slack: (remaining_cap - item) / initial_remaining_cap\n            # Higher score means less slack relative to bin size.\n            # We want to minimize slack, so prioritize bins with smaller slack.\n            # A lower slack value means the bin becomes 'more full' relative to its original capacity.\n            # To map this to higher priority, we can use:\n            # 1. A transformation that makes smaller slack values result in higher scores.\n            # 2. Scale these scores to be less than 1.0 (to ensure exact fits are always preferred).\n            \n            # Calculate slack: remaining_cap - item\n            slack = non_exact_bins_remain_cap - item\n            \n            # Normalize slack: slack / original_remaining_capacity\n            # Using initial remaining capacity of the subset of bins\n            # Add epsilon to avoid division by zero if a bin had 0 capacity initially (though covered by can_fit_mask, good practice)\n            epsilon = 1e-9\n            normalized_slack = slack / (non_exact_bins_remain_cap + epsilon)\n            \n            # Invert and scale: We want to maximize values for smaller normalized slack.\n            # A common range for non-exact fits is [0.5, 0.99].\n            # If normalized_slack is 0 (perfect fit, already handled by exact_fit_mask), it gets high score.\n            # If normalized_slack is close to 1 (item takes up almost no space), it gets low score.\n            # So, we can use 1 - normalized_slack to prioritize smaller normalized slack.\n            # Scale this to a range below 1.0, e.g., [0.5, 0.99].\n            # Let's use a linear mapping:\n            # normalized_slack = 0  -> score = 0.99\n            # normalized_slack = 1  -> score = 0.5\n            # score = m * (1 - normalized_slack) + c\n            # 0.99 = m * 1 + c\n            # 0.5 = m * 0 + c  => c = 0.5\n            # 0.99 = m + 0.5 => m = 0.49\n            # So, score = 0.49 * (1 - normalized_slack) + 0.5\n            \n            scaled_priority = 0.49 * (1.0 - normalized_slack) + 0.5\n            priorities[can_fit_mask][non_exact_fit_mask] = scaled_priority\n            \n    return priorities\n\n\n### Guide\n- Keep in mind, list of design heuristics ranked from best to worst. Meaning the first function in the list is the best and the last function in the list is the worst.\n- The response in Markdown style and nothing else has the following structure:\n\"**Analysis:**\n**Experience:**\"\nIn there:\n+ Meticulously analyze comments, docstrings and source code of several pairs (Better code - Worse code) in List heuristics to fill values for **Analysis:**.\nExample: \"Comparing (best) vs (worst), we see ...;  (second best) vs (second worst) ...; Comparing (1st) vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing (second worst) vs (worst), we see ...; Overall:\"\n\n+ Self-reflect to extract useful experience for design better heuristics and fill to **Experience:** (<60 words).\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}