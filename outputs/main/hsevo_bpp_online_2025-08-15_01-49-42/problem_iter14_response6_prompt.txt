{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Hybrid heuristic prioritizing exact fits and then Best Fit with a penalty for extreme slack.\n\n    This heuristic assigns the highest priority to bins that perfectly fit the item.\n    For bins that do not offer an exact fit, it prioritizes those with the smallest\n    remaining capacity (Best Fit). Additionally, it penalizes bins that would leave\n    a disproportionately large amount of remaining capacity relative to the item size,\n    promoting more balanced utilization.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    # Define a score for exact fits to give them highest priority\n    exact_fit_score = 1e6  # A very high score for perfect matches\n\n    # Define a penalty factor for large slack relative to the item size.\n    # This encourages more balanced packing by down-weighting bins that become too empty.\n    slack_penalty_factor = 0.1\n    \n    # Define a small epsilon to handle potential division by zero or near-zero item sizes\n    epsilon = 1e-9\n\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Calculate slack for bins that can fit the item\n        slack = bins_remain_cap[available_bins_indices] - item\n        \n        # --- Scoring Logic ---\n        \n        # 1. Prioritize exact fits\n        exact_fit_indices = np.where(slack < epsilon)[0]\n        if exact_fit_indices.size > 0:\n            priorities[available_bins_indices[exact_fit_indices]] = exact_fit_score\n            \n            # Remove exact fits from further consideration for secondary scoring\n            # to ensure they retain the highest priority.\n            non_exact_fit_indices = np.delete(np.arange(len(available_bins_indices)), exact_fit_indices)\n            if non_exact_fit_indices.size == 0:\n                return priorities # All fitting bins were exact fits\n\n            # Update available_bins_indices to only include non-exact fits\n            available_bins_indices = available_bins_indices[non_exact_fit_indices]\n            slack = slack[non_exact_fit_indices]\n        \n        # 2. For non-exact fits, use Best Fit (minimize slack) as primary\n        # Score is proportional to negative slack. Higher value for smaller slack.\n        best_fit_scores = -slack\n        \n        # 3. Add a penalty for large slack relative to item size.\n        # This discourages bins that remain too empty after packing.\n        # Penalty is proportional to `slack / item` (or just `slack` if item is zero)\n        # We use `slack / (item + epsilon)` to avoid division by zero.\n        relative_slack = slack / (item + epsilon)\n        \n        # The penalty reduces the score for bins with high relative slack.\n        # We subtract the penalty term.\n        combined_scores = best_fit_scores - slack_penalty_factor * relative_slack\n        \n        priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Hybrid prioritization: Exact Fit + Normalized Slack.\n    Prioritizes exact fits, then uses normalized slack to differentiate others.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Score 1: Exact Fit (highest priority)\n        # A bin is an exact fit if remaining capacity equals item size.\n        exact_fit_mask = available_bins_remain_cap == item\n        priorities[can_fit_mask][exact_fit_mask] = 1.0\n        \n        # Score 2: Differentiated Slack for non-exact fits\n        # For bins that don't provide an exact fit, use normalized slack.\n        # Normalized slack = (remaining_cap - item) / bins_remain_cap.\n        # We want to minimize normalized slack, so we maximize its negative.\n        # To ensure these scores are lower than exact fits, we scale them.\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_bins_remain_cap = available_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate normalized slack: (remaining_cap - item) / initial_remaining_cap\n            # Higher score means less slack relative to bin size.\n            # We want to minimize slack, so prioritize bins with smaller slack.\n            # A lower slack value means the bin becomes 'more full' relative to its original capacity.\n            # To map this to higher priority, we can use:\n            # 1. A transformation that makes smaller slack values result in higher scores.\n            # 2. Scale these scores to be less than 1.0 (to ensure exact fits are always preferred).\n            \n            # Calculate slack: remaining_cap - item\n            slack = non_exact_bins_remain_cap - item\n            \n            # Normalize slack: slack / original_remaining_capacity\n            # Using initial remaining capacity of the subset of bins\n            # Add epsilon to avoid division by zero if a bin had 0 capacity initially (though covered by can_fit_mask, good practice)\n            epsilon = 1e-9\n            normalized_slack = slack / (non_exact_bins_remain_cap + epsilon)\n            \n            # Invert and scale: We want to maximize values for smaller normalized slack.\n            # A common range for non-exact fits is [0.5, 0.99].\n            # If normalized_slack is 0 (perfect fit, already handled by exact_fit_mask), it gets high score.\n            # If normalized_slack is close to 1 (item takes up almost no space), it gets low score.\n            # So, we can use 1 - normalized_slack to prioritize smaller normalized slack.\n            # Scale this to a range below 1.0, e.g., [0.5, 0.99].\n            # Let's use a linear mapping:\n            # normalized_slack = 0  -> score = 0.99\n            # normalized_slack = 1  -> score = 0.5\n            # score = m * (1 - normalized_slack) + c\n            # 0.99 = m * 1 + c\n            # 0.5 = m * 0 + c  => c = 0.5\n            # 0.99 = m + 0.5 => m = 0.49\n            # So, score = 0.49 * (1 - normalized_slack) + 0.5\n            \n            scaled_priority = 0.49 * (1.0 - normalized_slack) + 0.5\n            priorities[can_fit_mask][non_exact_fit_mask] = scaled_priority\n            \n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 and Heuristic 3 (identical):\n*   **Commonality:** Both prioritize exact fits with a score of 1.0. For non-exact fits, they calculate `remaining_after_fit / (eligible_bins_remain_cap + epsilon)` which represents normalized slack. They then use `1.0 - normalized_remaining_capacity` and scale it to `0.5 + best_fit_scores * 0.49`.\n*   **Difference:** No functional difference found.\n\nComparing Heuristic 2, 4, 5, 7 and Heuristic 14, 15:\n*   **Commonality (2, 4, 5, 7):** These heuristics prioritize exact fits with a very high score (`1e6`). For non-exact fits, they calculate slack, then `best_fit_scores = -slack`. They then apply a penalty `slack_penalty_factor * relative_slack` where `relative_slack = slack / (item + epsilon)`. The combined score is `best_fit_scores - slack_penalty_factor * relative_slack`. The penalty factor is 0.1. They initialize priorities to -1.0.\n*   **Commonality (14, 15):** These heuristics prioritize exact fits with a very high score (`1e9`). For non-exact fits, they aim to minimize remaining capacity (`-remaining_after_fit`) and use `-initial_remaining_capacity` as a tie-breaker, scaled by a large factor (`1e6`) to ensure Best Fit dominates. They initialize priorities to -1.0.\n*   **Difference:** Heuristics 2, 4, 5, 7 introduce a *penalty* for large relative slack, actively *reducing* the score for bins that would remain too empty. Heuristics 14, 15 use the initial bin capacity as a *tie-breaker* for bins with similar Best Fit scores, favoring bins that were already fuller. Heuristic 14/15 have a higher exact fit score and a larger scale for their secondary criterion.\n\nComparing Heuristic 1, 3, 8, 9, 11, 16, 17, 18, 19, 20:\n*   **Commonality:** These heuristics prioritize exact fits (score 1.0 or similar high value). For non-exact fits, they use a \"normalized slack\" approach, typically calculating `(remaining_capacity - item) / original_capacity`. They then transform this to create a priority score, often scaling it to a range below the exact fit score (e.g., `[0.5, 0.99]`).\n*   **Heuristic 1/3:** `0.5 + 0.49 * (1.0 - normalized_remaining_capacity)`.\n*   **Heuristic 8/9/11:** Identical to 1/3.\n*   **Heuristic 16/19/20:** Identical to 1/3/8/9/11. The implementation details of `normalized_slack` are slightly different, but the core logic of scaling `(1.0 - normalized_slack)` to `[0.5, 0.99]` is the same.\n*   **Heuristic 17/18:** Identical to 16/19/20.\n*   **Difference:** The primary difference lies in the *exact implementation* of \"normalized slack\" (e.g., `remaining_after_fit / original_capacity` vs. `remaining_after_fit / (original_capacity + epsilon)`) and the scaling range. Heuristic 1/3/8/9/11/16/17/18/19/20 are functionally very similar, using a scaled inverse normalized slack.\n\nComparing Heuristic 10 and Heuristic 12/13:\n*   **Commonality:** Both aim to balance \"tight fits\" with spreading items. They initialize priorities to -1.0 for non-fitting bins.\n*   **Heuristic 10:** Uses a combined score: `tight_fit_score * 0.7 + normalized_slack * 0.3`. `tight_fit_score` is `1.0 / (remaining_capacities_if_fit + epsilon)` (prioritizing small remainder) and `normalized_slack` is `eligible_capacities / np.max(eligible_capacities)` (favoring larger remaining capacity). This attempts to balance minimal remainder with more spread.\n*   **Heuristic 12/13:** Prioritizes minimal remaining capacity by using `1.0 / (remaining_capacities_if_fit + epsilon)`. This is a \"pure\" Best Fit approach based on absolute remaining capacity.\n*   **Difference:** Heuristic 10 explicitly tries to incorporate a \"soft worst fit\" or spreading mechanism by adding a term proportional to the *original* capacity (normalized), whereas 12/13 focuses solely on minimizing the *remaining* capacity after fitting.\n\nComparing Heuristic 6 with others:\n*   **Heuristic 6:** Uses configurable parameters for `exact_fit_priority`, `non_exact_fit_min_priority`, `non_exact_fit_max_priority`. It calculates normalized slack similarly to Heuristics 1/3/8/9/11/16/17/18/19/20, and then scales `1.0 - normalized_slack` to the range defined by `non_exact_fit_min_priority` and `non_exact_fit_max_priority`. It explicitly clips scores to be less than `exact_fit_priority`.\n*   **Difference:** The key difference is the explicit use of *tuned parameters* to define the priority ranges, rather than hardcoded values like 1.0 and `[0.5, 0.99]`. This suggests an approach that might have been optimized or parameterized.\n\n**Ranking Justification:**\n\n*   **Best (1-3):** Heuristics 1, 3, 8, 9, 11, 16, 17, 18, 19, 20 are very similar and represent a strong baseline. They prioritize exact fits and then use a well-defined scaled inverse normalized slack. This approach is robust and provides good differentiation. Heuristic 6 is also in this top tier, essentially offering the same logic but with tunable parameters, which is a good design choice. The exact ranking among these is subtle, but they represent the most refined common strategy.\n*   **Mid-Tier (4-5):** Heuristics 2, 4, 5, 7 introduce a penalty for large relative slack. This is a reasonable extension of Best Fit, aiming to avoid overly empty bins. However, it might be overly aggressive or sensitive to the `slack_penalty_factor`.\n*   **Mid-Tier (10):** Heuristic 10 attempts to balance Best Fit with a \"soft worst fit\" (spreading). This is an interesting idea but the implementation might be less direct than simply prioritizing minimal slack. The combination of inverse remaining capacity and normalized original capacity as scores could lead to unexpected behavior.\n*   **Mid-Tier (12-13):** Heuristics 12, 13 use `1.0 / (remaining_capacity_if_fit + epsilon)`. This is a strong Best Fit heuristic that clearly prioritizes minimal absolute remaining capacity. It's simpler than scaled normalized slack but might be less sensitive to the *proportion* of remaining space.\n*   **Lower-Mid Tier (14-15):** These use a very high exact fit score and a tie-breaker based on initial bin fullness. While prioritizing exact fits is good, the tie-breaker might be secondary to minimizing slack and could lead to less optimal packing if not carefully tuned.\n*   **Worst (None):** All heuristics attempt to address the problem by prioritizing exact fits and then considering some form of slack minimization or differentiation. There isn't a fundamentally flawed heuristic. The ranking is based on common effectiveness and robustness of the chosen strategy.\n\n**Final Ranking (Conceptual, based on observed strategies):**\n\n1.  Heuristic 1, 3, 8, 9, 11, 16, 17, 18, 19, 20 (Scaled Inverse Normalized Slack with clear hierarchy)\n2.  Heuristic 6 (Same as above but with tunable parameters)\n3.  Heuristic 12, 13 (Pure Best Fit on absolute remaining capacity)\n4.  Heuristic 2, 4, 5, 7 (Best Fit with slack penalty)\n5.  Heuristic 10 (Balanced Best Fit + Soft Worst Fit)\n6.  Heuristic 14, 15 (Exact Fit + Tie-breaker on initial fullness)\n\n*(Note: The provided list had duplicates, so the ranking groups similar ones. The original ordering in the prompt implies a ranking that we are trying to reconstruct by analyzing the strategies.)*\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Fit, Slack, Normalization, Differentiation, Hierarchy.\n*   **Advice:** Design scoring mechanisms that clearly distinguish between exact fits and various degrees of near-fits using normalized metrics. Establish a clear hierarchy of criteria, prioritizing exact fits, then minimal normalized slack, with tunable parameters for adaptability.\n*   **Avoid:** Arbitrary penalties, direct division by zero or very small numbers without stabilization (epsilon), overly complex multi-objective functions without justification, and counter-intuitive negative scaling.\n*   **Explanation:** Effective self-reflection in heuristic design centers on creating clear, robust scoring that prioritizes ideal outcomes while robustly differentiating between good-enough alternatives, ensuring numerical stability and intuitive logic.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}