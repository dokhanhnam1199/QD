{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack using a stable, tiered scoring.\n\n    This function assigns the highest priority to exact fits. For other bins that can\n    accommodate the item, it calculates a priority based on normalized slack.\n    The scoring is tiered to ensure clear separation between exact fits and other options,\n    and among different levels of slack.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) == 0:\n        return priorities # No bins can fit the item\n\n    # Separate bins into exact fits and potential fits\n    exact_fit_mask = bins_remain_cap[can_fit_mask] == item\n    exact_fit_indices_filtered = np.where(exact_fit_mask)[0]\n    actual_exact_fit_indices = fit_indices[exact_fit_indices_filtered]\n\n    # Assign highest priority to exact fits\n    priorities[actual_exact_fit_indices] = 1.0\n\n    # Process bins that are not exact fits but can still accommodate the item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    non_exact_fit_indices = np.where(non_exact_fit_mask)[0]\n\n    if len(non_exact_fit_indices) > 0:\n        eligible_bins_for_slack = bins_remain_cap[non_exact_fit_indices]\n        \n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = eligible_bins_for_slack - item\n        \n        # Calculate normalized slack: (remaining_capacity_after_fit) / (current_bin_capacity)\n        # A smaller normalized slack is better. Add epsilon for numerical stability.\n        # Using the current remaining capacity as a proxy for original bin capacity for normalization.\n        normalized_slack = remaining_after_fit / (eligible_bins_for_slack + 1e-9)\n\n        # Assign priorities for non-exact fits.\n        # We want to favor smaller normalized slack. The scoring should be less than 1.0 (exact fit score)\n        # and greater than 0.0 (default for non-fitting bins).\n        # We use 1.0 - normalized_slack to map smaller slack to higher scores.\n        # Scale these scores to be clearly less than 1.0, e.g., into the range [0.5, 0.99].\n        # A linear scaling: 0.5 + (1.0 - normalized_slack) * 0.49\n        # This approach combines the best aspects of prioritizing exact fits and using\n        # normalized slack for fine-grained selection among other options.\n        best_fit_scores = 1.0 - normalized_slack\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[non_exact_fit_indices] = scaled_best_fit_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines exact fit, minimized residual capacity, and normalized slack for robust prioritization.\n    Prioritizes exact fits (score 1.0), then bins with minimal remaining capacity after fit,\n    using normalized slack as a tie-breaker.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_indices = np.where(can_fit_mask)[0]\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        remaining_after_fit = eligible_bins_remain_cap - item\n        \n        # Exact fit: highest priority\n        exact_fit_mask = (remaining_after_fit == 0)\n        priorities[eligible_indices[exact_fit_mask]] = 1.0\n        \n        # Non-exact fits\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_indices = eligible_indices[non_exact_fit_mask]\n            non_exact_remain_caps_after_fit = remaining_after_fit[non_exact_fit_mask]\n            non_exact_current_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n            \n            # Normalize remaining capacity after fit to rank non-exact fits.\n            # Smaller remaining capacity gets higher priority.\n            # Use a score that's inversely related to normalized remaining capacity.\n            # This combines the \"minimize residual\" idea with normalization.\n            epsilon = 1e-9\n            # To avoid division by zero or very small numbers, and to make scores distinct\n            # we can use a transformation that maps smaller values to larger scores.\n            # A simple approach is to use the negative of the normalized remaining capacity\n            # and scale it to be less than 1.0.\n            \n            # Let's try prioritizing bins that leave the *least* amount of space.\n            # This is minimizing `remaining_after_fit`.\n            # We want higher priority for smaller `remaining_after_fit`.\n            \n            # Using a score derived from inverse of remaining capacity after fit.\n            # Higher score for smaller remaining capacity after fit.\n            # Scale to differentiate from exact fits. A range like [0.5, 0.95] is good.\n            \n            # Normalize remaining capacity after fit to a [0, 1] range where 0 is best.\n            max_remaining_after_fit = np.max(non_exact_remain_caps_after_fit)\n            normalized_remaining_after_fit = non_exact_remain_caps_after_fit / (max_remaining_after_fit + epsilon)\n            \n            # Map normalized remaining capacity to a priority score.\n            # 0 (best residual) -> ~0.95, 1 (worst residual) -> ~0.5\n            best_fit_scores = 0.5 + (1.0 - normalized_remaining_after_fit) * 0.45\n            \n            priorities[non_exact_indices] = best_fit_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1, 5, 6, 12, 15 (Exact Fit + Normalized Slack variants) with Heuristics 2, 4, 7 (Almost Full Fit variants) and Heuristic 3 (Hybrid V1), we see that prioritizing exact fits offers a clear, high-priority strategy. Normalized slack provides a good way to differentiate non-exact fits based on relative space utilization. The \"Almost Full Fit\" heuristics (like V1, which is essentially maximizing -slack) are simpler but lack the explicit handling of exact fits or the nuanced scaling of normalized slack. Heuristic 3 attempts a hybrid but its implementation is less clear than the dedicated strategies.\n\nComparing Heuristics 1, 5, 6, 12, 15:\n- Heuristics 1, 5, 15 are very similar, prioritizing exact fits (score 1.0) and then using a scaled version of `1.0 - normalized_slack` for non-exact fits, mapping them to a range like [0.5, 0.99]. This provides a clear hierarchy and differentiated scores.\n- Heuristic 6 is very similar to 1, 5, 15 but uses slightly different scaling (0.5 to 0.99).\n- Heuristic 12 uses `1.0` for exact fits and `0.5 + 0.49 * (1.0 - normalized_slack)` for non-exact fits, which is functionally identical to 1, 5, 15.\n- Heuristic 13 attempts to combine exact fit priority with a Best Fit strategy (minimizing normalized residual) and a tie-breaker based on initial slack. The scoring mechanism (lexicographical preference via scaling) is more complex but aims for a multi-objective optimization.\n- Heuristic 14 also prioritizes exact fits and then uses a normalized remaining capacity (mapped to [0.5, 0.95]) for non-exact fits, similar to 1, 5, 6, 12, 15.\n\nComparing \"Almost Full Fit\" variants (2, 4, 7) with others:\n- Heuristics 2, 4, 7 are consistent in using `-slack` (maximizing negative slack) as the primary scoring mechanism. This prioritizes bins that become most full. They assign -1 to bins that cannot fit. This is a simple and effective \"Best Fit\" approach but doesn't explicitly handle exact fits as a special case.\n\nComparing Heuristics 8, 9, 10, 11, 16, 17, 20:\n- Heuristic 8 attempts to use `-log(slack + epsilon)` for differentiation, which is a novel approach for small slack values.\n- Heuristics 9, 10, 11 combine slack minimization with fit ratio `item / bins_remain_cap`, weighted. This adds a second dimension to the scoring.\n- Heuristic 16 tries to balance tightness with avoiding extreme fills, using `-slack` as a base and adding a penalty for very tight fits (when `slack` is below a threshold relative to `item`). This is a more sophisticated attempt to balance competing goals.\n- Heuristic 17 is identical to 16.\n- Heuristic 20 prioritizes exact fits with a very high score (1e6), then uses a scaled `-remaining_after_fit` (Best Fit) and a penalty based on negative initial remaining capacity as a tie-breaker. This is a strong multi-objective approach.\n\nHeuristics 18 and 19 are incomplete code snippets.\n\nOverall, heuristics that combine a clear, high priority for exact fits with a well-defined scoring for non-exact fits (like normalized slack or scaled best-fit) tend to perform better. Heuristics that attempt multi-objective optimization (e.g., balancing tightness with avoiding fragmentation or using tie-breakers) show promise but can become complex. The \"Almost Full Fit\" (-slack) is a solid baseline but can be improved by explicit exact-fit handling or more nuanced differentiation.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on robust differentiation and avoiding pitfalls:\n\n*   **Keywords:** Objective clarity, Score differentiation, Robustness, Scalability.\n*   **Advice:** Design scores that clearly distinguish between excellent fits and good-enough options. Use a hierarchical approach, prioritizing exact matches, then finely tuned slack minimization (normalized or scaled), and finally, stable secondary metrics.\n*   **Avoid:** Over-reliance on absolute slack, direct division by capacities prone to instability, and complex, unexplainable scoring interactions.\n*   **Explanation:** The goal is to create heuristics that make decisive, understandable choices. Clear scoring ensures that the algorithm consistently favors better solutions without getting lost in minor, unstable numerical variations, promoting robustness across different problem scales.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}