{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack using a scaled approach.\n    Combines the exact fit priority of v0/v9/v10 with the normalized slack approach of v9/v10.\n    The scoring ensures a clear hierarchy: exact fits > best fits (min normalized slack).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Exact Fit: Highest priority (1.0)\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # Best Fit: Prioritize bins with minimal positive remaining capacity after fitting.\n    # Consider bins that can fit the item and are not exact fits.\n    can_fit_mask = (bins_remain_cap >= item) & ~exact_fit_mask\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) > 0:\n        remaining_after_fit = bins_remain_cap[fit_indices] - item\n        current_capacities = bins_remain_cap[fit_indices]\n\n        # Calculate normalized slack: (remaining_capacity_after_fit) / (current_bin_capacity)\n        # Smaller normalized slack is better. Add epsilon for numerical stability.\n        normalized_slack = remaining_after_fit / (current_capacities + 1e-9)\n\n        # Assign priorities that are higher for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher scores.\n        # Scale these scores to be clearly less than 1.0, e.g., into the range [0.5, 0.99].\n        # This mirrors the effective scoring of v9/v10.\n        best_fit_scores = 1.0 - normalized_slack\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[fit_indices] = scaled_best_fit_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines exact fit priority with a scaled Best Fit strategy,\n    using normalized slack and a tie-breaker for bins with less initial slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        eligible_indices = np.where(can_fit_mask)[0]\n\n        # Exact fit has the highest priority\n        exact_fit_mask = (eligible_bins_remain_cap == item)\n        priorities[eligible_indices[exact_fit_mask]] = 1.0\n        \n        # For non-exact fits, use Best Fit by minimizing remaining capacity\n        non_exact_fit_mask = (eligible_bins_remain_cap > item)\n        non_exact_indices = eligible_indices[non_exact_fit_mask]\n        non_exact_remain_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n\n        if len(non_exact_remain_caps) > 0:\n            remaining_after_fit = non_exact_remain_caps - item\n            \n            # Normalize remaining capacity to [0, 1], where 0 is best (min residual)\n            max_residual = np.max(remaining_after_fit)\n            # Add epsilon for stability if all residuals are identical\n            normalized_residual = remaining_after_fit / (max_residual + 1e-9)\n            \n            # Tie-breaking: Prefer bins with less *initial* slack (smaller eligible_bins_remain_cap)\n            # We want to maximize the negative of initial slack.\n            # Scale initial slack to be a secondary factor.\n            # Using the negative of initial capacities ensures smaller initial capacities\n            # get higher (less negative) scores.\n            # The scale should be such that the primary factor (normalized_residual) dominates.\n            # A common approach is to use a large constant.\n            scale_factor = 1e6 \n            tiebreaker_scores = -non_exact_remain_caps * scale_factor\n\n            # Combine primary (Best Fit) and secondary (tie-breaker) scores.\n            # Primary goal: minimize normalized_residual (maximize -(normalized_residual))\n            # Secondary goal: minimize initial slack (maximize -(initial_slack))\n            # Score = PrimaryScore * Scale + SecondaryScore\n            # We want to maximize score, so score = -(normalized_residual) * Scale + -(initial_slack)\n            # Equivalently, for maximization:\n            # Score = (1.0 - normalized_residual) * Scale + (-non_exact_remain_caps)\n            # A larger (1.0 - normalized_residual) is better (less residual).\n            # A larger (-non_exact_remain_caps) is better (less initial slack).\n            \n            # Let's reformulate to ensure maximization:\n            # Maximize: (1 - normalized_residual) -- primary\n            # Maximize: (-non_exact_remain_caps) -- secondary (tie-breaker)\n            # Combined score: (1 - normalized_residual) * large_scale + (-non_exact_remain_caps)\n            \n            # Scores for non-exact fits should be less than 1.0 (exact fit priority)\n            # The range of (1 - normalized_residual) is [0, 1].\n            # To make it distinct and higher than 1.0 for exact fit, we can add 1.0.\n            # Or, we can use a range like [0.5, 0.99] and assign 1.0 to exact fit.\n\n            # Let's aim for scores less than 1.0, with higher meaning better.\n            # Best Fit score: higher for smaller normalized_residual\n            # Using (1 - normalized_residual) gives higher scores for better fits.\n            # To incorporate tie-breaking, we use the scaled negative initial slack.\n            # Combined score = (1 - normalized_residual) * scale_factor + (-non_exact_remain_caps)\n            # This ensures that minimizing normalized residual is primary.\n            # For identical normalized residuals, it picks the bin with less initial slack.\n\n            # However, we need scores to be less than 1.0.\n            # Let's use a base score for non-exact fits and add the tie-breaker.\n            # A simple base score for Best Fit: 0.9 - normalized_residual (maps to [0.8, 0.9])\n            # Then add tie-breaker, scaled down.\n            # A better way to use tiebreaker with scale:\n            # Primary score component: (1.0 - normalized_residual) - Higher is better\n            # Secondary score component: (-non_exact_remain_caps) - Higher is better\n            # Final Score = (PrimaryScore * Scale) + SecondaryScore\n            # To ensure scores are below 1.0, we can scale (1.0 - normalized_residual).\n            # Let's use a range like [0.5, 0.99].\n            \n            # For non-exact fits, map normalized_residual to a score.\n            # Normalized residual is [0, 1] where 0 is best.\n            # We want higher scores for better fits. So, use (1 - normalized_residual).\n            # This gives scores in [0, 1].\n            # Let's map this to [0.5, 0.95] to leave room for exact fit (1.0).\n            # Scale factor for mapping [0, 1] to [0.5, 0.95] is 0.45.\n            # Score = 0.5 + (1.0 - normalized_residual) * 0.45\n\n            best_fit_scores = 0.5 + (1.0 - normalized_residual) * 0.45\n            \n            # Incorporate tie-breaker: penalize bins with more initial slack.\n            # We want to maximize the negative of initial slack.\n            # To avoid interfering with the primary score's range, scale it down significantly.\n            # The tie-breaker should only influence choices when primary scores are equal.\n            # Using the initial remaining capacity directly as a tie-breaker (smaller is better):\n            # We want to maximize the negative of initial remaining capacity.\n            # Add this to the best_fit_scores, scaled appropriately.\n            # Example: scaled_tiebreaker = (-non_exact_remain_caps) / large_constant\n            \n            # A more robust way for lexicographical ordering is:\n            # Score = Primary_Score_Component * Large_Scale + Secondary_Score_Component\n            # Where Primary_Score_Component is (1.0 - normalized_residual).\n            # Secondary_Score_Component is (-non_exact_remain_caps).\n            # To keep scores below 1.0 for non-exact fits:\n            # Score = (1.0 - normalized_residual) * (1.0 - epsilon) + (-non_exact_remain_caps) / scale\n            # Let's directly combine them to reflect the BFD spirit:\n            # Prioritize minimizing residual capacity, then minimizing initial slack.\n            # Score = (-(remaining_after_fit)) * scale_factor + (-non_exact_remain_caps)\n            # This is for maximization.\n\n            # Let's use the approach that maps non-exact fits to [0.5, 0.95] and uses\n            # negative initial slack as a tie-breaker.\n            # The tie-breaker needs to be added in a way that it only affects choices\n            # when the primary scores are very close.\n            \n            # Combining scores for non-exact fits:\n            # Higher priority for smaller remaining_after_fit.\n            # Among those, higher priority for smaller non_exact_remain_caps.\n            \n            # Let's create a composite score:\n            # We want to maximize (1 - normalized_residual)\n            # We want to maximize (-non_exact_remain_caps)\n            # To ensure the first term dominates, scale it.\n            # A common approach is: score = primary * scale_factor + secondary\n            # If primary is in [0,1] and secondary is in [-Max, 0], and scale_factor is large.\n            \n            # Let's stick to a structure that ensures non-exact fits are < 1.0 and exact fits are 1.0.\n            # For non-exact fits:\n            # Primary objective: Minimize normalized_residual -> Score component: (1.0 - normalized_residual)\n            # Secondary objective: Minimize initial slack -> Score component: (-non_exact_remain_caps)\n            \n            # We need to combine these such that the first term has more impact.\n            # Using a weighted sum: alpha * (1.0 - normalized_residual) + beta * (-non_exact_remain_caps)\n            # The weights alpha and beta can be chosen. Alpha should be larger.\n            # Let alpha = 0.9 and beta = 0.1.\n            # This gives scores in a range, but doesn't guarantee the range.\n            \n            # A robust way is to use lexicographical sorting implicitly.\n            # Consider pairs: (primary_score, secondary_score)\n            # Primary: (1.0 - normalized_residual)\n            # Secondary: (-non_exact_remain_caps)\n            # We want to maximize these lexicographically.\n            \n            # Let's simplify: use the mapped Best Fit score and add a scaled tie-breaker.\n            # Score = best_fit_scores + (-non_exact_remain_caps) / 1000.0\n            # This ensures the best_fit_scores component is dominant.\n            \n            scaled_tiebreaker = (-non_exact_remain_caps) / 1000.0 # Scale down tie-breaker\n            combined_non_exact_scores = best_fit_scores + scaled_tiebreaker\n            \n            # Ensure scores are less than 1.0 (exact fit priority)\n            # The current combination could exceed 1.0 if tiebreaker is very positive for some reason\n            # (which it shouldn't be with negative initial capacities).\n            # We can clip or rescale if needed, but given the structure, it should be fine.\n            \n            priorities[non_exact_indices] = combined_non_exact_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1, 5, 6, 12, 15 (Exact Fit + Normalized Slack variants) with Heuristics 2, 4, 7 (Almost Full Fit variants) and Heuristic 3 (Hybrid V1), we see that prioritizing exact fits offers a clear, high-priority strategy. Normalized slack provides a good way to differentiate non-exact fits based on relative space utilization. The \"Almost Full Fit\" heuristics (like V1, which is essentially maximizing -slack) are simpler but lack the explicit handling of exact fits or the nuanced scaling of normalized slack. Heuristic 3 attempts a hybrid but its implementation is less clear than the dedicated strategies.\n\nComparing Heuristics 1, 5, 6, 12, 15:\n- Heuristics 1, 5, 15 are very similar, prioritizing exact fits (score 1.0) and then using a scaled version of `1.0 - normalized_slack` for non-exact fits, mapping them to a range like [0.5, 0.99]. This provides a clear hierarchy and differentiated scores.\n- Heuristic 6 is very similar to 1, 5, 15 but uses slightly different scaling (0.5 to 0.99).\n- Heuristic 12 uses `1.0` for exact fits and `0.5 + 0.49 * (1.0 - normalized_slack)` for non-exact fits, which is functionally identical to 1, 5, 15.\n- Heuristic 13 attempts to combine exact fit priority with a Best Fit strategy (minimizing normalized residual) and a tie-breaker based on initial slack. The scoring mechanism (lexicographical preference via scaling) is more complex but aims for a multi-objective optimization.\n- Heuristic 14 also prioritizes exact fits and then uses a normalized remaining capacity (mapped to [0.5, 0.95]) for non-exact fits, similar to 1, 5, 6, 12, 15.\n\nComparing \"Almost Full Fit\" variants (2, 4, 7) with others:\n- Heuristics 2, 4, 7 are consistent in using `-slack` (maximizing negative slack) as the primary scoring mechanism. This prioritizes bins that become most full. They assign -1 to bins that cannot fit. This is a simple and effective \"Best Fit\" approach but doesn't explicitly handle exact fits as a special case.\n\nComparing Heuristics 8, 9, 10, 11, 16, 17, 20:\n- Heuristic 8 attempts to use `-log(slack + epsilon)` for differentiation, which is a novel approach for small slack values.\n- Heuristics 9, 10, 11 combine slack minimization with fit ratio `item / bins_remain_cap`, weighted. This adds a second dimension to the scoring.\n- Heuristic 16 tries to balance tightness with avoiding extreme fills, using `-slack` as a base and adding a penalty for very tight fits (when `slack` is below a threshold relative to `item`). This is a more sophisticated attempt to balance competing goals.\n- Heuristic 17 is identical to 16.\n- Heuristic 20 prioritizes exact fits with a very high score (1e6), then uses a scaled `-remaining_after_fit` (Best Fit) and a penalty based on negative initial remaining capacity as a tie-breaker. This is a strong multi-objective approach.\n\nHeuristics 18 and 19 are incomplete code snippets.\n\nOverall, heuristics that combine a clear, high priority for exact fits with a well-defined scoring for non-exact fits (like normalized slack or scaled best-fit) tend to perform better. Heuristics that attempt multi-objective optimization (e.g., balancing tightness with avoiding fragmentation or using tie-breakers) show promise but can become complex. The \"Almost Full Fit\" (-slack) is a solid baseline but can be improved by explicit exact-fit handling or more nuanced differentiation.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on robust differentiation and avoiding pitfalls:\n\n*   **Keywords:** Objective clarity, Score differentiation, Robustness, Scalability.\n*   **Advice:** Design scores that clearly distinguish between excellent fits and good-enough options. Use a hierarchical approach, prioritizing exact matches, then finely tuned slack minimization (normalized or scaled), and finally, stable secondary metrics.\n*   **Avoid:** Over-reliance on absolute slack, direct division by capacities prone to instability, and complex, unexplainable scoring interactions.\n*   **Explanation:** The goal is to create heuristics that make decisive, understandable choices. Clear scoring ensures that the algorithm consistently favors better solutions without getting lost in minor, unstable numerical variations, promoting robustness across different problem scales.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}