```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Sigmoid-like function: emphasizes bins with capacities close to item size
    # but also considers those with a bit more remaining space to avoid fragmentation.
    # High values for bins that can fit the item, lower values for those that cannot.
    # We want to penalize bins that are too large as they might leave too much wasted space,
    # and also penalize bins that are too small (cannot fit).
    
    # Capacities where the item fits
    can_fit_mask = bins_remain_cap >= item
    
    # Calculate "fit score" for bins that can fit the item
    # We want to prioritize bins that have just enough space, or slightly more.
    # A simple approach is to use a function that peaks around the item's size.
    # Consider a scaled version of the item size, e.g., item * 1.1 (a little buffer).
    # The sigmoid function takes values from -inf to +inf and maps them to 0 to 1.
    # We'll shift and scale the argument to center it around a desired capacity.
    
    # Let's define a "target" capacity slightly larger than the item for a good fit.
    # A small epsilon can be added to prevent division by zero in case of zero remaining capacity.
    epsilon = 1e-9
    target_capacity = item * 1.1 
    
    # Calculate the difference from the target capacity for bins that can fit.
    # A smaller positive difference is better.
    difference_from_target = bins_remain_cap[can_fit_mask] - target_capacity
    
    # Apply a sigmoid-like function. We want higher scores for smaller positive differences.
    # A steep sigmoid will prioritize very tightly fitting bins.
    # Let's scale the difference to control the steepness. A larger 'k' means steeper.
    k = 10.0  # Steepness parameter
    
    # Sigmoid: 1 / (1 + exp(-k * x)) maps (-inf, inf) to (0, 1).
    # We want to maximize this, so smaller positive difference (closer to 0) is better.
    # If difference_from_target is negative, it means bins_remain_cap < target_capacity
    # but still >= item. We want to give these high scores.
    # If difference_from_target is positive and small, it's good.
    # If difference_from_target is positive and large, it's worse.
    
    # Let's invert the sigmoid behavior or adjust the input to sigmoid.
    # Option 1: Use -sigmoid(-k * difference) which maps to (0,1) and peaks at negative differences.
    # Option 2: Use 1 - sigmoid(k * difference) which maps to (0,1) and peaks at negative differences.
    # Option 3: Use 1 / (1 + exp(k * difference)). This maps (-inf, inf) to (0, 1) and peaks at negative differences.
    
    # Using option 3: 1 / (1 + exp(k * difference))
    # A large negative difference means the bin is much smaller than the target but still fits. Good.
    # A difference close to zero means it's close to the target. Good.
    # A large positive difference means it's much larger than the target. Worse.
    
    # Add epsilon to the denominator to avoid division by zero if exp overflows.
    fit_scores_for_fitting_bins = 1.0 / (1.0 + np.exp(k * difference_from_target))
    
    # Initialize priorities with a low value for bins that cannot fit the item.
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Assign the calculated fit scores to the bins that can fit the item.
    priorities[can_fit_mask] = fit_scores_for_fitting_bins
    
    # To ensure that bins with slightly more space are still considered,
    # we can add a small bonus for bins that are larger, but not excessively larger.
    # Or, simply ensure the sigmoid naturally handles this.
    # The current sigmoid peaks when difference_from_target is negative and approaches 0 as difference_from_target increases positively.
    # This means bins that are slightly too small for the target (but still fit the item) will have highest scores.
    # Then bins that match the target will have moderate scores, and bins much larger will have lower scores.
    
    # If we want to prioritize bins that have *enough* space, but not *too much*,
    # we can think of the ideal space as 'item'.
    # Let's re-evaluate the sigmoid input.
    # We want to maximize `f(x)` where `x` is the remaining capacity.
    # `f(item) = 1`
    # `f(x < item) = 0`
    # `f(x > item) = 0`, but decrease as `x` increases.
    
    # A Gaussian-like function centered at 'item' could work.
    # `exp(-beta * (bins_remain_cap - item)^2)`
    # This peaks at 'item'. Bins smaller than item will have lower values (and 0 if they don't fit).
    # Bins larger than item will also have lower values.
    
    beta = 0.1 # Controls the width of the Gaussian. Smaller beta means wider.
    
    # Initialize scores to zero.
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # For bins that can fit the item:
    can_fit_mask = bins_remain_cap >= item
    
    # Calculate scores for fitting bins.
    # Use a Gaussian-like function centered around `item`.
    # Values further from `item` get lower scores.
    # Bins that are too large will have low scores. Bins that are "just right" will have high scores.
    # Bins that are smaller than item get 0.
    
    # Let's use the difference `bins_remain_cap - item`.
    # We want values close to 0 to have high scores.
    
    # Calculate the squared difference from the item size.
    # We only care about bins that can fit.
    diff_sq = (bins_remain_cap[can_fit_mask] - item)**2
    
    # Gaussian function: exp(-beta * diff_sq)
    # High values for small differences (close to 'item').
    # Lower values for larger differences (much larger than 'item').
    scores_for_fitting = np.exp(-beta * diff_sq)
    
    # Assign these scores to the appropriate bins.
    priorities[can_fit_mask] = scores_for_fitting
    
    return priorities
```
