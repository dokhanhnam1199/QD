{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Hybrid Priority for Online Bin Packing.\n\n    This strategy combines the \"almost full\" concept with a penalty for\n    bins that are too empty, aiming for a more balanced packing.\n    It prioritizes bins that leave a small remaining capacity (similar to V1),\n    but also penalizes bins that would have a very large remaining capacity\n    after packing, thus encouraging fuller bins over very empty ones.\n\n    The scoring is designed to:\n    1. Favor bins where `bins_remain_cap - item` is minimized (most \"full\").\n    2. Penalize bins where `bins_remain_cap - item` is large (too empty).\n    3. Assign a neutral or slightly negative score to bins that perfectly fit\n       the item, to differentiate them from those that leave a small gap.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        \n        # Objective 1: Minimize remaining capacity (maximize negative remaining capacity)\n        # This rewards bins that become \"almost full\".\n        score_almost_full = -remaining_capacities_if_fit\n\n        # Objective 2: Penalize large remaining capacities.\n        # We want to down-weight bins that will still be very empty.\n        # A simple way is to subtract a scaled version of the remaining capacity.\n        # We can use a function like - (remaining_capacity_if_fit)^2 or\n        # a sigmoid-like function to make the penalty grow as remaining capacity increases.\n        # For simplicity and effectiveness, let's use a linear penalty that increases\n        # with remaining capacity, but ensure it doesn't dominate the \"almost full\" score.\n        # A robust approach is to use a scaled version of the inverse:\n        # A bin that becomes very full (small remaining capacity) is good.\n        # A bin that remains very empty (large remaining capacity) is less good.\n\n        # Let's create a score that rewards small `remaining_capacities_if_fit`\n        # and punishes large `remaining_capacities_if_fit`.\n        # We can use a quadratic function that is minimized at 0 remaining capacity,\n        # or a linear function with a slope.\n\n        # Consider a score that is high for small `remaining_capacities_if_fit`\n        # and decreases as `remaining_capacities_if_fit` increases.\n        # e.g., score = 100 - remaining_capacities_if_fit\n        # This is essentially maximizing `100 - remaining_capacities_if_fit`.\n        # This is equivalent to minimizing `remaining_capacities_if_fit`, but with\n        # a large constant that can be adjusted.\n\n        # To differentiate, let's try to reward being \"tight\" but also penalize \"slack\".\n        # A score could be: Max(0, C - (bins_remain_cap - item)) - PenaltyFactor * max(0, (bins_remain_cap - item))\n        # where C is a constant representing \"ideal fullness\" and PenaltyFactor penalizes slack.\n\n        # Let's use a simpler approach that is still a hybrid:\n        # Score = (MaxPossibleScore - remaining_capacity) - penalty_factor * remaining_capacity\n        # This combines a desire for small remaining_capacity with a penalty for it.\n        # Effectively, we want to maximize `(Constant - remaining_capacity) - penalty_factor * remaining_capacity`.\n        # This simplifies to maximizing `Constant - (1 + penalty_factor) * remaining_capacity`.\n        # Which means minimizing `remaining_capacity * (1 + penalty_factor)`.\n        # This is still very close to V1.\n\n        # Let's try to integrate two objectives:\n        # 1. Minimizing slack (rewarding `bins_remain_cap - item` close to 0)\n        # 2. Maximizing bin utilization across all bins (implicitly).\n        # If we prioritize bins that are *almost full*, we might leave many bins\n        # with a moderate amount of remaining capacity.\n\n        # A better hybrid might involve ranking or scaled values.\n        # Consider the value of `bins_remain_cap[can_fit_mask]`.\n        # Bins with lower remaining capacity are generally better.\n        # Let's try to reward bins that are \"just right\" or \"almost full\".\n        # We can use a function that peaks when `bins_remain_cap[can_fit_mask]` is close to `item`.\n        # Or, we can reward small `remaining_capacities_if_fit` and penalize very large ones.\n\n        # Let's try a score based on how \"tight\" the fit is, but with a diminishing return\n        # for extremely tight fits (to avoid fragmentation if item is slightly smaller than bin capacity)\n        # and a penalty for slack.\n        # We can use a function that is high for small positive `remaining_capacities_if_fit`,\n        # goes to zero or negative for larger `remaining_capacities_if_fit`.\n\n        # For \"differentiated scores\", we can assign different levels of priority based on\n        # the magnitude of remaining capacity.\n\n        # Simple hybrid: Maximize `(MaxCapacity - (bins_remain_cap[can_fit_mask] - item))`\n        # This rewards bins that, after packing, have a large capacity. This is counter-intuitive.\n\n        # Let's go back to minimizing `remaining_capacities_if_fit`.\n        # To differentiate, we can add a term that penalizes large remaining capacities.\n        # Consider the target remaining capacity after packing to be small.\n        # Score = -(remaining_capacities_if_fit) - penalty_for_slack * (remaining_capacities_if_fit)^2\n        # Or, more simply: Score = -(remaining_capacities_if_fit) - penalty_for_slack * remaining_capacities_if_fit\n        # This is equivalent to minimizing `remaining_capacities_if_fit * (1 + penalty_for_slack)`.\n        # Still essentially the same structure.\n\n        # Let's consider the \"context\". The context is the set of available bins.\n        # We want to select a bin that is good *in this context*.\n        # A bin that is almost full is good.\n        # A bin that is extremely empty is bad.\n        # A bin that perfectly fits is also good.\n\n        # Let's define a score that is high for small, non-negative remaining capacity.\n        # We can use a function like `1 / (remaining_capacity + 1)` which decreases as remaining_capacity increases.\n        # Or, more linearly: `Constant - remaining_capacity`.\n        # To penalize slack, we can make the score decrease faster.\n        # e.g., Score = K - remaining_capacity - penalty_factor * remaining_capacity\n        # Maximize Score => Minimize `remaining_capacity * (1 + penalty_factor)`\n\n        # A more nuanced approach might be to consider the variance of remaining capacities\n        # or the overall utilization. However, for a per-item priority function, we need\n        # to evaluate based on the item and current bin states.\n\n        # Let's try a score that is high for small `remaining_capacities_if_fit`\n        # and then decays as `remaining_capacities_if_fit` increases.\n        # We can use a piecewise function or a function like `exp(-k * remaining_capacity)`.\n        # A simple linear approach: Score = C - remaining_capacity.\n        # To penalize slack, we can make the \"C\" decrease for larger remaining capacities.\n\n        # Consider the goal: minimize number of bins.\n        # This often implies filling bins as much as possible.\n        # Priority should be given to bins that become \"most full\" without overflowing.\n        # \"Most full\" after packing means `bins_remain_cap - item` is minimal.\n\n        # Let's try to assign scores that are relative to the *average* remaining capacity\n        # among feasible bins. This introduces context.\n        # However, this might be too complex for a simple priority function.\n\n        # Let's stick to a form that enhances V1:\n        # V1: `priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n        # This directly maximizes `bins_remain_cap - item` being minimal.\n\n        # To add differentiation and robustness:\n        # We want to reward bins that are \"tight\" but not too tight, and penalize slack.\n        # A good score might be one that is high for small positive `remaining_capacities_if_fit`,\n        # drops off for larger values, and is perhaps slightly penalized even for zero remaining capacity\n        # if it means we could have used a slightly larger bin. This is getting complicated.\n\n        # Let's try a score that is inversely proportional to the remaining capacity after packing,\n        # but with a penalty that grows faster than linearly with remaining capacity.\n        # Score = 1 / (epsilon + remaining_capacities_if_fit)  # Favors small remaining capacity.\n        # To penalize slack, we can subtract a term that grows with remaining_capacities_if_fit.\n        # Score = 1 / (epsilon + remaining_capacities_if_fit) - penalty_factor * remaining_capacities_if_fit\n\n        # A simpler heuristic that differentiates:\n        # We want to maximize `(BinCapacity - item) - slack_penalty_factor * slack`\n        # where `slack = bins_remain_cap - item`.\n        # So, maximize `(BinCapacity - item) - slack_penalty_factor * (BinCapacity - item)`.\n        # This means we want to minimize `(BinCapacity - item) * (1 + slack_penalty_factor)`.\n\n        # Let's use a scoring function that is high when `remaining_capacities_if_fit` is small,\n        # and decreases more rapidly for larger values.\n        # Example: A negative quadratic function of `remaining_capacities_if_fit` that peaks at 0.\n        # Score = C - k * (remaining_capacities_if_fit)^2\n        # This rewards small remaining capacities, and the penalty increases quadratically.\n\n        # Let's combine a primary goal (minimize remaining capacity) with a secondary goal (penalize slack).\n        # A common technique is to use a weighted sum.\n        # Score = w1 * (1 / (epsilon + remaining_capacities_if_fit)) - w2 * remaining_capacities_if_fit\n        # Or, to avoid division by zero and keep it simple:\n        # Score = w1 * (-remaining_capacities_if_fit) + w2 * (-remaining_capacities_if_fit^2)\n        # This still prioritizes negative remaining capacity.\n\n        # Let's try a simpler form. The core idea of V1 is to pick the bin that becomes \"most full\".\n        # This is equivalent to picking the bin with the smallest `bins_remain_cap - item`.\n        # A common enhancement is to consider the \"waste\" in the bin.\n\n        # Consider a score that reflects \"how well\" the item fits, focusing on minimizing wasted space.\n        # Wasted space in a bin *after* packing: `bins_remain_cap - item`.\n        # We want to minimize this.\n        # Let's use a function that is high for small values of `bins_remain_cap - item`.\n\n        # For differentiation, we can add a term that emphasizes being \"almost full\" over \"moderately full\".\n        # A simple way is to subtract a small penalty for larger remaining capacities.\n        # Score = -(bins_remain_cap[can_fit_mask] - item) - penalty_factor * (bins_remain_cap[can_fit_mask] - item)**2\n\n        # A robust approach is to map remaining capacity to a score.\n        # Low remaining capacity -> high score.\n        # Large remaining capacity -> low score.\n\n        # Let's implement a score that is `1 / (1 + remaining_capacity_if_fit)`.\n        # This gives a score between (0, 1], where 1 is for 0 remaining capacity.\n        # This is similar to V1 but bounded and might be more stable.\n        # To differentiate: we can adjust the steepness.\n        # A function like `exp(-k * remaining_capacity_if_fit)` also works.\n        # A linear function: `C - remaining_capacities_if_fit` is what V1 is doing essentially.\n\n        # Let's try to combine \"tight fit\" and \"avoiding large residual bins\".\n        # Score = 100 - (bins_remain_cap[can_fit_mask] - item)  # Favors minimal remainder.\n        # To penalize slack:\n        # Score = 100 - (bins_remain_cap[can_fit_mask] - item) - penalty_factor * (bins_remain_cap[can_fit_mask] - item)\n        # This is still just minimizing `remaining_capacity * (1 + penalty_factor)`.\n\n        # A key aspect of \"differentiated scores\" is to create a range of scores that clearly rank options.\n        # Let's use a score that is `MaxScore - f(remaining_capacity_if_fit)`, where `f` is an increasing function.\n        # To differentiate, make `f` grow faster than linear.\n        # Let f(x) = x + x^2 / C. Or f(x) = x^2.\n        # Score = MaxScore - (remaining_capacities_if_fit)^2.\n        # This rewards small remaining capacity, but penalizes larger ones more heavily.\n\n        max_possible_score_base = 100.0 # A baseline to ensure positive scores before penalty\n        penalty_factor = 0.5 # Controls how aggressively large remaining capacities are penalized\n\n        # Score: Reward small remaining capacity, penalize larger remaining capacities quadratically.\n        # The idea is to pick a bin that leaves a small gap, but avoid bins that leave a moderately large gap.\n        priorities[can_fit_mask] = max_possible_score_base - remaining_capacities_if_fit - penalty_factor * (remaining_capacities_if_fit ** 2)\n\n        # Ensure that bins with exactly zero remaining capacity get a good score,\n        # but not necessarily the absolute maximum if there are other beneficial properties.\n        # The current formula works well for this: if remaining_capacities_if_fit is 0, score is max_possible_score_base.\n        # If remaining_capacities_if_fit is small positive (e.g., 0.1), score is slightly less.\n        # If remaining_capacities_if_fit is larger (e.g., 1.0), score is significantly less due to the penalty.\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n\n    \"\"\"\n    Multi-objective prioritization for online Bin Packing.\n\n    This strategy aims to find a balance between filling bins as much as possible\n    (to minimize bin count) and ensuring a good fit to avoid fragmentation.\n    It prioritizes bins that can fit the item and assigns scores based on two\n    criteria:\n    1.  How \"tight\" the fit is: Prioritizes bins where the remaining capacity\n        after placing the item is minimized (similar to 'Almost Full Fit').\n    2.  How much capacity is left relative to the item size: Prioritizes bins\n        that have a moderate amount of remaining capacity, to leave room for\n        future items and avoid packing very small items into nearly full bins\n        which might be better utilized by slightly larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Criterion 1: Minimize remaining capacity after fitting (tight fit)\n        # This is the negative of the slack. Higher priority for smaller slack.\n        slack = bins_remain_cap[available_bins_indices] - item\n        tight_fit_score = -slack\n        \n        # Criterion 2: Consider the relative remaining capacity.\n        # We want to avoid bins that become *too* full or *too* empty after fit.\n        # A simple way is to penalize extreme remaining capacities.\n        # For instance, a bin with 0 remaining capacity is not ideal if the item\n        # was much smaller than the bin's original capacity.\n        # Let's try to reward bins that leave a 'reasonable' amount of space.\n        # A common heuristic is to look at the ratio of remaining capacity to bin capacity,\n        # or simply the absolute remaining capacity.\n        # Here, we'll use a score that favors bins with moderate remaining capacity.\n        # A parabolic function centered around some ideal slack could work,\n        # or a simple linear penalty for very small/large remainders.\n        # Let's use a score that is higher for slack values closer to a certain target.\n        # A simpler approach is to reward bins with remaining capacity that is not too small,\n        # but not excessively large either.\n        # Let's prioritize bins where `bins_remain_cap` is around `item` * 2, for example.\n        # This would mean `bins_remain_cap - item` is around `item`.\n        # A gaussian-like function centered at `item` for `bins_remain_cap` could work.\n        # For simplicity, let's consider a function that rewards bins with remaining capacity\n        # that's not excessively large.\n        # We can scale the remaining capacity and take its negative logarithm, or\n        # use a Gaussian-like function centered around an \"ideal\" remaining capacity.\n        # Let's define an \"ideal\" remaining capacity as, say, half of the item size.\n        # ideal_slack = item / 2.0\n        # deviation_from_ideal = np.abs(slack - ideal_slack)\n        # proportional_slack_score = -deviation_from_ideal # Higher score for closer to ideal\n        \n        # A more robust approach: Penalize bins that are almost empty or almost full.\n        # Let's aim for a remaining capacity that is roughly equal to the item size,\n        # suggesting a balanced use of space.\n        # Score = 1 / (1 + (slack - item)^2) for slack around item.\n        # Or, simply, penalize large slacks.\n        # Let's use a score that is inversely proportional to the slack, but with a\n        # floor to avoid extremely high scores for very small slacks.\n        # Also, we don't want to encourage using bins that are almost empty if a tighter\n        # fit is available.\n\n        # Let's combine the two:\n        # 1. Tightness: -slack (maximize)\n        # 2. Moderate remaining capacity: encourage slack not to be too small or too large.\n        #    A common idea is to use capacity utilization. High utilization is good,\n        #    but too high might be bad for fragmentation.\n        #    Let's prioritize bins where the remaining capacity `r` is such that\n        #    `r / bin_original_capacity` is in a good range.\n        #    Since we don't have bin_original_capacity, we can use `r / (r + item)`.\n        #    We want this ratio to be not too close to 0 or 1.\n        #    Let's try a function that rewards slack values that are not extremely small.\n        #    A simple transformation: `slack / (slack + item)` which is the fill percentage.\n        #    We want to avoid fill percentage close to 1 (very tight fit) and close to 0 (very loose fit).\n        #    A score that is high for fill percentage around 0.5 might be good.\n        #    Let's use a score that is inversely proportional to `slack`.\n        \n        # A more refined approach: Prioritize bins that lead to a fill rate\n        # of the *bin* (after placing the item) that is between, say, 0.6 and 0.9.\n        # Fill rate = (original_capacity - remaining_capacity) / original_capacity\n        # We don't have original_capacity.\n        # Let's simplify: use the remaining capacity itself.\n        # Prioritize bins where `slack` is not too small (e.g., `slack > item * 0.1`)\n        # and not too large (e.g., `slack < item * 2.0`).\n        \n        # Let's focus on a score that balances tightness and leaving some space.\n        # A good heuristic might be to prioritize bins that leave a remaining capacity\n        # that is as close as possible to the *average* remaining capacity of bins that can fit the item,\n        # or some other target value.\n        \n        # For this version, let's combine the \"tight fit\" score with a penalty for\n        # very large remaining capacities.\n        # We want to maximize `tight_fit_score` (-slack).\n        # We also want to penalize large `slack` values.\n        # Let's try a function that is `-slack - alpha * slack^2` or `-slack + f(slack)`\n        # where f(slack) is a decreasing function for large slack.\n        \n        # A simpler, robust approach:\n        # Prioritize bins that are \"almost full\" (small slack) but also provide some\n        # buffer. Let's try to reward bins where `slack` is small but positive.\n        # We can use a score like `1.0 / (slack + epsilon)` for small slack, and\n        # perhaps a lower score or even negative for very large slack.\n        \n        # Let's try to achieve a \"balanced\" fill. If an item is size `i`, we prefer\n        # a bin with remaining capacity `r` such that `r` is close to `i`.\n        # This means `slack` (which is `r - i`) should be close to 0.\n        # So, the previous \"Almost Full Fit\" priority of `-slack` is good.\n        # However, it might pick bins that leave only a tiny sliver of space,\n        # which might not be good if a slightly larger item comes next.\n        \n        # Let's introduce a penalty for very small remaining capacities *after* fit (very tight fit).\n        # If `slack` is very small, we might want to penalize it.\n        # Consider the `tight_fit_score = -slack`.\n        # For bins where `slack` is very close to zero (e.g., `slack < epsilon`),\n        # let's reduce their score.\n        \n        # Let's use a score that combines tightness and a preference for not being *too* tight.\n        # Score = `tight_fit_score` - penalty_for_very_tight_fit\n        # Penalty could be `exp(-slack / scale)` where scale is small.\n        # Or, more simply, just `tight_fit_score` but with a cap on how small slack can be to get max priority.\n        \n        # Let's go with a score that rewards tightness, but penalizes excessively large slack.\n        # The \"tight fit\" score is `-slack`. This rewards smaller slacks.\n        # For bins that have very large slack, their `-slack` score will be very negative,\n        # naturally giving them lower priority.\n        # The original v1 function already does this: `priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n        \n        # To improve, we need to differentiate scores more intelligently.\n        # Let's consider the *ratio* of remaining capacity to the item size.\n        # If `slack` is very small compared to `item`, it's a tight fit.\n        # If `slack` is very large compared to `item`, it's a loose fit.\n        # A \"good\" fit might be when `slack` is somewhere in between.\n        # Let's define a \"desirability\" score.\n        # A bin is desirable if `slack` is small, but not *too* small.\n        # Let's try a score that's higher for `slack` values closer to `item * 0.5` (a guess for a balanced state).\n        # Score = `1.0 / (1.0 + (slack - item * 0.5)**2)`\n        # This score is maximized when `slack` is `item * 0.5`.\n        # However, we also want to prioritize tighter fits more.\n        \n        # Let's try a hybrid:\n        # Primary goal: Minimize slack (-slack).\n        # Secondary goal: If multiple bins have very similar small slack,\n        #                 then differentiate based on how \"full\" they become.\n        #                 A bin that becomes 90% full is better than one that becomes 70% full\n        #                 if their remaining capacities are very close.\n        \n        # Let's define the score as:\n        # Score = -slack - alpha * (slack / item)  where alpha is a small positive number.\n        # This prioritizes small slack, but then slightly penalizes bins that have\n        # proportionally larger remaining capacity.\n        # This might encourage tighter packing overall.\n        \n        # Consider the First Fit Decreasing (FFD) approach intuition: sorting items helps.\n        # In online, we don't sort. So, the choice of bin is critical.\n        \n        # Let's try to make the \"almost full\" idea more nuanced.\n        # Instead of just minimizing slack, let's consider the *percentage* of the bin\n        # that the item occupies.\n        # We don't know the bin's original capacity.\n        # So, we can only work with `bins_remain_cap` and `item`.\n        \n        # Let's refine the `tight_fit_score`.\n        # `tight_fit_score = -slack` (higher is better).\n        # We want to differentiate bins that have small slacks.\n        # For bins that can fit the item:\n        # 1. Calculate `slack = bins_remain_cap[available_bins_indices] - item`\n        # 2. We want to prioritize smaller `slack`. So, `-slack` is a good starting point.\n        # 3. Let's add a term that rewards bins that are not \"too empty\" after the item is placed.\n        #    This means `slack` should not be excessively large.\n        #    A simple penalty for large slack: `-slack / item` (if item > 0).\n        #    Or `-(slack / (slack + item))` which is `-(remaining_capacity / original_capacity_estimate)`.\n        #    This is the inverse of fill rate.\n        #    Let's use a score like `-slack - C * (slack / item)` for a constant C.\n        #    If C is positive, it penalizes larger slacks.\n        \n        # Example: item = 0.5, bins_remain_cap = [1.0, 0.6, 0.8]\n        # Bins that fit: [1.0, 0.6, 0.8]\n        # Slacks:        [0.5, 0.1, 0.3]\n        # -slack:        [-0.5, -0.1, -0.3] (Bin 2 is best so far)\n        \n        # Let's add a penalty for large slack, e.g., `slack / item`.\n        # Item = 0.5.\n        # slack/item: [1.0, 0.2, 0.6]\n        # Proposed score: `-slack - 0.1 * (slack / item)`\n        # Bin 1 (cap 1.0): -0.5 - 0.1 * 1.0 = -0.6\n        # Bin 2 (cap 0.6): -0.1 - 0.1 * 0.2 = -0.1 - 0.02 = -0.12 (Still best)\n        # Bin 3 (cap 0.8): -0.3 - 0.1 * 0.6 = -0.3 - 0.06 = -0.36\n        \n        # This approach seems reasonable: prioritize tight fits, but slightly penalize\n        # bins that have a lot of remaining space relative to the item size.\n        # The constant `0.1` is a hyperparameter.\n        \n        # Let's use a more robust calculation that avoids division by zero if item is 0.\n        # if item is very small, slack / item can be unstable.\n        # Better: normalize slack by a typical bin size or a large constant.\n        # Or, simply use `slack` directly and add a term that penalizes its magnitude.\n        \n        # Let's use: `-slack - slack_penalty_factor * slack`\n        # This would amplify the penalty for larger slacks.\n        # E.g., `slack_penalty_factor = 0.1`\n        # Bin 1: -0.5 - 0.1 * 0.5 = -0.55\n        # Bin 2: -0.1 - 0.1 * 0.1 = -0.11\n        # Bin 3: -0.3 - 0.1 * 0.3 = -0.33\n        \n        # This is essentially `-(1 + slack_penalty_factor) * slack`.\n        # It still prioritizes the smallest slack.\n        \n        # Let's try to make the penalty dependent on the *relative* slack.\n        # The ratio `slack / item` captures this.\n        # A robust way to handle `item = 0`: if item is 0, slack is just remaining capacity.\n        # In that case, we'd prefer bins with less remaining capacity (to keep them full).\n        # So, `-slack` is still good.\n        \n        # Let's combine the tight fit score (-slack) with a preference for\n        # remaining capacity `r` such that `r` is not excessively small or large.\n        # Consider a function that is maximized when `slack` is near `item / 2`.\n        # `score = - (slack - item / 2.0)**2`\n        # This rewards bins that leave approximately half the item's size as remaining space.\n        # But this contradicts the \"almost full\" idea.\n        \n        # Let's combine:\n        # 1. Tightness: `-slack` (primary objective)\n        # 2. Avoidance of very tight fits: Penalize small `slack`.\n        #    If `slack < epsilon`, give a higher penalty.\n        #    A simple penalty: `min(0, slack - epsilon)`. This is 0 if slack >= epsilon,\n        #    and `slack - epsilon` if slack < epsilon. It's negative, so it reduces the score.\n        \n        epsilon_tight = 1e-6  # Threshold for \"very tight fit\"\n        \n        # Calculate scores based on tightness: higher is better\n        tight_fit_scores = -slack\n        \n        # Introduce a penalty for very tight fits\n        # If slack is less than epsilon_tight, we subtract a value that increases as slack gets smaller.\n        # The penalty should be proportional to how much slack is *below* epsilon_tight.\n        # So, penalty = `(epsilon_tight - slack)` if `slack < epsilon_tight`, else 0.\n        # To make it a score reduction, we add this penalty multiplied by a factor.\n        penalty_factor_tight = 1.0  # How much to penalize very tight fits\n        \n        very_tight_penalty = np.maximum(0, epsilon_tight - slack) * penalty_factor_tight\n        \n        # The final score is the tight fit score minus the penalty\n        combined_scores = tight_fit_scores - very_tight_penalty\n        \n        priorities[available_bins_indices] = combined_scores\n        \n        # Let's test this logic with an example:\n        # item = 0.5\n        # bins_remain_cap = [0.6, 0.55, 1.0, 0.8]\n        # Can fit: [0.6, 0.55, 1.0, 0.8]\n        # Slacks:  [0.1, 0.05, 0.5, 0.3]\n        # epsilon_tight = 1e-6 (very small)\n        # penalty_factor_tight = 1.0\n        \n        # Bin 1 (cap 0.6): slack = 0.1\n        #   tight_fit_score = -0.1\n        #   very_tight_penalty = max(0, 1e-6 - 0.1) * 1.0 = 0\n        #   combined_score = -0.1 - 0 = -0.1\n        \n        # Bin 2 (cap 0.55): slack = 0.05\n        #   tight_fit_score = -0.05\n        #   very_tight_penalty = max(0, 1e-6 - 0.05) * 1.0 = 0\n        #   combined_score = -0.05 - 0 = -0.05 (This bin would be chosen if only tight fit was used)\n        \n        # Bin 3 (cap 1.0): slack = 0.5\n        #   tight_fit_score = -0.5\n        #   very_tight_penalty = max(0, 1e-6 - 0.5) * 1.0 = 0\n        #   combined_score = -0.5 - 0 = -0.5\n        \n        # Bin 4 (cap 0.8): slack = 0.3\n        #   tight_fit_score = -0.3\n        #   very_tight_penalty = max(0, 1e-6 - 0.3) * 1.0 = 0\n        #   combined_score = -0.3 - 0 = -0.3\n        \n        # Result: [-0.1, -0.05, -0.5, -0.3]. Bin 2 is chosen.\n        # This new heuristic did not change the outcome in this specific case because\n        # the slacks were not small enough to trigger the penalty.\n        \n        # Let's make the epsilon tighter for the penalty.\n        # Or, let's rethink the \"avoiding very tight fits\".\n        # If we have slack = 0.01 and slack = 0.001, we prefer 0.01.\n        # `-slack` makes 0.001 -> -0.001 (better) and 0.01 -> -0.01 (worse).\n        # This is incorrect if we want to penalize very tight fits.\n        \n        # Let's reverse the logic for the penalty:\n        # If `slack` is very small, its score should be *reduced*.\n        # `tight_fit_score = -slack` (higher is better)\n        # If `slack` is very small (e.g., `slack < epsilon`), we want to *lower* its score.\n        # So, we should *subtract* a positive value for small `slack`.\n        # Let the score be `-slack + penalty_for_large_slack`.\n        # Or `-slack - penalty_for_very_small_slack`.\n        \n        # Let's try a score that is higher for slack closer to a moderate value,\n        # and also considers tightness.\n        # Score = `tight_fit_score` + `moderate_slack_preference`\n        # `tight_fit_score = -slack`\n        # `moderate_slack_preference`: A function that is higher when slack is, say,\n        # around `item / 2`.\n        # For example, `gaussian_like(slack, center=item/2, width=item)`\n        # `gaussian_like(x, center, width) = exp(-(x - center)**2 / (2 * width**2))`\n        \n        # Let's try a simpler combination:\n        # The primary goal is `slack` minimization.\n        # Secondary goal: If slacks are very close, pick the one with more remaining capacity.\n        # This is \"Best Fit\" vs \"Almost Full Fit\".\n        \n        # The V1 heuristic is \"almost full fit\" which means minimizing slack.\n        # It's equivalent to maximizing `-slack`.\n        \n        # What if we want to avoid bins that become *too* full?\n        # E.g., if item=0.1 and bin_cap=1.0, remaining is 0.9. This is a bad fit for small items.\n        # The slack is 0.9. `-slack` would be -0.9.\n        # If item=0.9 and bin_cap=1.0, remaining is 0.1. This is a good fit.\n        # The slack is 0.1. `-slack` would be -0.1.\n        \n        # The current `-slack` metric intrinsically prioritizes tighter fits.\n        \n        # Let's consider the context of online BPP. We want to minimize the total number of bins.\n        # Prioritizing tight fits generally leads to higher bin utilization.\n        \n        # What if we use a score that represents \"how full the bin will be\"?\n        # `fill_rate = (original_capacity - slack) / original_capacity`\n        # We don't have original_capacity.\n        # We can approximate it as `item + slack`.\n        # So, `fill_rate_approx = item / (item + slack)`\n        # We want high fill rate. So we want to maximize `item / (item + slack)`.\n        # This is equivalent to minimizing `(item + slack) / item` which is `1 + slack/item`.\n        # Minimizing `1 + slack/item` is equivalent to minimizing `slack/item`.\n        # This is the same as minimizing `slack` (if item > 0).\n        \n        # The core idea remains minimizing slack. The challenge is how to differentiate among small slacks.\n        \n        # Let's propose a score that:\n        # 1. Strongly favors smaller slacks (`-slack`).\n        # 2. Slightly penalizes very large slacks, making them less preferred than medium slacks.\n        #    A bin with slack=5 should be less preferred than a bin with slack=0.5.\n        #    Current `-slack` handles this well: -5 is worse than -0.5.\n        \n        # The prompt asks for \"better than current version\".\n        # The current version `priority_v1` uses `-slack`.\n        # This is effectively \"Best Fit\" if the bins were sorted by remaining capacity.\n        # In online, it means picking the bin that results in the least wasted space *for this item*.\n        \n        # Let's introduce a score that considers the \"quality\" of the remaining space.\n        # A bin that is almost full might be good, but a bin that has a moderate amount of space\n        # might be better for future, potentially larger, items.\n        \n        # Let's try a score that uses a thresholding or weighting based on slack size relative to item size.\n        # Consider slack relative to bin capacity. We don't have bin capacity.\n        \n        # A robust heuristic is often related to \"tightest fit\" but with modifications.\n        # What if we assign higher priority to bins where the slack `s` satisfies:\n        # `s` is small (e.g., `s < bin_capacity * 0.2`), but also not too small (e.g., `s > epsilon`).\n        \n        # Let's consider a score that has two components:\n        # Component 1: Tightness (Maximize `-slack`)\n        # Component 2: Balance (Prefer slack that's not too small or too large)\n        \n        # Let's try to incorporate a penalty for bins that leave a very small residual capacity *relative to the item size*.\n        # Score = `-slack - alpha * exp(-slack / item)` if item > 0, else `-slack`.\n        # This would penalize small slacks when item is small.\n        # Example: item = 0.1\n        # slack = 0.01 => -0.01 - alpha * exp(-0.01 / 0.1) = -0.01 - alpha * exp(-0.1)\n        # slack = 0.1  => -0.1 - alpha * exp(-0.1 / 0.1) = -0.1 - alpha * exp(-1)\n        # If alpha is positive, the small slack gets a more negative score.\n        \n        # Let's define the score more carefully:\n        # We want to maximize `score`.\n        # Primary objective: Minimize slack. Score term: `-slack`.\n        # Secondary objective: Avoid bins that are *too* full after packing.\n        # This means we want to avoid `slack` being extremely small.\n        # Let's penalize `slack` when it's below a certain threshold, say `item * 0.2`.\n        # The penalty should be larger for smaller `slack`.\n        # Penalty = `max(0, (item * 0.2) - slack)`. This is positive if slack is smaller than `item * 0.2`.\n        # Final Score = `-slack - penalty_factor * max(0, (item * 0.2) - slack)`\n        \n        penalty_threshold_ratio = 0.2  # Slack should ideally be at least 20% of item size.\n        penalty_factor = 1.0           # Weight of the penalty.\n        \n        # Handle cases where item is zero or very close to zero.\n        if item < 1e-9:\n            # If item is zero, slack is just the remaining capacity.\n            # We prefer bins with less remaining capacity (tighter fit).\n            # So, `-bins_remain_cap` is the priority.\n            priorities[available_bins_indices] = -bins_remain_cap[available_bins_indices]\n        else:\n            slack = bins_remain_cap[available_bins_indices] - item\n            \n            # Primary score: negative slack (prioritize minimum slack)\n            tight_fit_score = -slack\n            \n            # Secondary score: penalty for slack being too small relative to item size.\n            # We want slack to be at least `item * penalty_threshold_ratio`.\n            # If slack is smaller than this threshold, we apply a penalty.\n            # The penalty is proportional to how much slack is below the threshold.\n            threshold = item * penalty_threshold_ratio\n            slack_penalty = np.maximum(0, threshold - slack)\n            \n            # The final score combines the tight fit score and the penalty.\n            # We subtract the penalty to reduce the score for bins with too little slack.\n            combined_scores = tight_fit_score - penalty_factor * slack_penalty\n            \n            priorities[available_bins_indices] = combined_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1, 5, 6, 12, 15 (Exact Fit + Normalized Slack variants) with Heuristics 2, 4, 7 (Almost Full Fit variants) and Heuristic 3 (Hybrid V1), we see that prioritizing exact fits offers a clear, high-priority strategy. Normalized slack provides a good way to differentiate non-exact fits based on relative space utilization. The \"Almost Full Fit\" heuristics (like V1, which is essentially maximizing -slack) are simpler but lack the explicit handling of exact fits or the nuanced scaling of normalized slack. Heuristic 3 attempts a hybrid but its implementation is less clear than the dedicated strategies.\n\nComparing Heuristics 1, 5, 6, 12, 15:\n- Heuristics 1, 5, 15 are very similar, prioritizing exact fits (score 1.0) and then using a scaled version of `1.0 - normalized_slack` for non-exact fits, mapping them to a range like [0.5, 0.99]. This provides a clear hierarchy and differentiated scores.\n- Heuristic 6 is very similar to 1, 5, 15 but uses slightly different scaling (0.5 to 0.99).\n- Heuristic 12 uses `1.0` for exact fits and `0.5 + 0.49 * (1.0 - normalized_slack)` for non-exact fits, which is functionally identical to 1, 5, 15.\n- Heuristic 13 attempts to combine exact fit priority with a Best Fit strategy (minimizing normalized residual) and a tie-breaker based on initial slack. The scoring mechanism (lexicographical preference via scaling) is more complex but aims for a multi-objective optimization.\n- Heuristic 14 also prioritizes exact fits and then uses a normalized remaining capacity (mapped to [0.5, 0.95]) for non-exact fits, similar to 1, 5, 6, 12, 15.\n\nComparing \"Almost Full Fit\" variants (2, 4, 7) with others:\n- Heuristics 2, 4, 7 are consistent in using `-slack` (maximizing negative slack) as the primary scoring mechanism. This prioritizes bins that become most full. They assign -1 to bins that cannot fit. This is a simple and effective \"Best Fit\" approach but doesn't explicitly handle exact fits as a special case.\n\nComparing Heuristics 8, 9, 10, 11, 16, 17, 20:\n- Heuristic 8 attempts to use `-log(slack + epsilon)` for differentiation, which is a novel approach for small slack values.\n- Heuristics 9, 10, 11 combine slack minimization with fit ratio `item / bins_remain_cap`, weighted. This adds a second dimension to the scoring.\n- Heuristic 16 tries to balance tightness with avoiding extreme fills, using `-slack` as a base and adding a penalty for very tight fits (when `slack` is below a threshold relative to `item`). This is a more sophisticated attempt to balance competing goals.\n- Heuristic 17 is identical to 16.\n- Heuristic 20 prioritizes exact fits with a very high score (1e6), then uses a scaled `-remaining_after_fit` (Best Fit) and a penalty based on negative initial remaining capacity as a tie-breaker. This is a strong multi-objective approach.\n\nHeuristics 18 and 19 are incomplete code snippets.\n\nOverall, heuristics that combine a clear, high priority for exact fits with a well-defined scoring for non-exact fits (like normalized slack or scaled best-fit) tend to perform better. Heuristics that attempt multi-objective optimization (e.g., balancing tightness with avoiding fragmentation or using tie-breakers) show promise but can become complex. The \"Almost Full Fit\" (-slack) is a solid baseline but can be improved by explicit exact-fit handling or more nuanced differentiation.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on robust differentiation and avoiding pitfalls:\n\n*   **Keywords:** Objective clarity, Score differentiation, Robustness, Scalability.\n*   **Advice:** Design scores that clearly distinguish between excellent fits and good-enough options. Use a hierarchical approach, prioritizing exact matches, then finely tuned slack minimization (normalized or scaled), and finally, stable secondary metrics.\n*   **Avoid:** Over-reliance on absolute slack, direct division by capacities prone to instability, and complex, unexplainable scoring interactions.\n*   **Explanation:** The goal is to create heuristics that make decisive, understandable choices. Clear scoring ensures that the algorithm consistently favors better solutions without getting lost in minor, unstable numerical variations, promoting robustness across different problem scales.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}