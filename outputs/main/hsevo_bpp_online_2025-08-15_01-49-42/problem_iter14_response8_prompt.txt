{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Hybrid Priority: Best Fit + Differentiated Slack.\n\n    This strategy aims to improve upon \"Almost Full Fit\" by first prioritizing\n    bins that leave a small, specific remaining capacity, and then using\n    a more nuanced approach for bins that don't create an \"almost full\" state.\n    It differentiates between bins that are a perfect fit or leave a very small\n    slack, and bins that have more capacity.\n\n    The core idea is to:\n    1. Prioritize bins that can accommodate the item. Bins that cannot fit get -1.\n    2. Among fitting bins, prioritize those that leave a very small, positive\n       remaining capacity. This is a refined version of \"almost full.\"\n       We can define \"almost full\" as having a remaining capacity between 0 and some threshold,\n       or simply the smallest positive remaining capacity.\n    3. For bins that have larger remaining capacities, we still want to prefer\n       those that leave less residual space, but with a less aggressive scoring.\n       This helps in cases where no \"almost full\" bin exists.\n\n    We will use a scoring system that:\n    - Assigns a high score to bins that fit and leave a small remainder.\n    - Assigns a moderate score to bins that fit and leave a larger remainder.\n    - The scoring function aims to differentiate clearly.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        \n        # Define a threshold for \"tight fit\" or \"almost full\".\n        # This could be a small percentage of the bin capacity, or a fixed small value.\n        # For simplicity, let's consider a small absolute remainder as high priority.\n        # Let's say a remainder < 10% of the bin capacity or a fixed small epsilon.\n        # A more robust approach is to normalize the remaining capacity by the original bin capacity\n        # if the original bin capacities were available. Since they are not, we work with remaining.\n        \n        # We want to prioritize minimal positive remaining capacity.\n        # A common way to differentiate is to use a piecewise function or a scaled inverse.\n        # Let's try a score that is higher for smaller remaining capacities.\n        # To ensure differentiation and avoid issues with division by near-zero,\n        # we can use a function like `max(0, threshold - x)` or `1 / (x + epsilon)`.\n        \n        # Approach:\n        # 1. Prioritize bins that leave a very small remainder (e.g., close to 0).\n        # 2. Among these, perhaps prioritize the one with the absolute smallest remainder.\n        # 3. For larger remainders, still prefer smaller ones, but with less impact.\n\n        # Let's assign scores such that smaller `remaining_capacities_if_fit` get higher scores.\n        # We can use a transformation that is strictly decreasing.\n        # For example, `-(remaining_capacity)` as in v1, but maybe scaled or with adjustments.\n        \n        # A key insight from the advice: \"use a hierarchical approach, prioritizing exact matches,\n        # then finely tuned slack minimization\".\n        # Exact matches: remaining_capacity_if_fit == 0.\n        # Finely tuned slack minimization: small positive remaining_capacity_if_fit.\n        \n        # Let's define a base score for fitting bins.\n        # We want to maximize the negative of remaining capacity.\n        # To differentiate, we can add a term that rewards smaller remainders more strongly.\n        \n        # Consider the 'Best Fit' aspect: minimize `bins_remain_cap - item`.\n        # This means maximizing `-(bins_remain_cap - item)`.\n        # Let's enhance this by making the penalty for larger remainders less severe\n        # or by giving a boost to \"almost full\" bins.\n        \n        # A common strategy is to use a score that is inversely related to the remaining capacity.\n        # However, to differentiate, let's consider a score that has a higher gradient for small remainders.\n        \n        # Option 1: Scaled Negative Remainder with a boost for small remainders.\n        # score = -remaining_capacity_if_fit\n        # This is similar to v1.\n\n        # Option 2: Inversely proportional to remaining capacity + a term that penalizes slack.\n        # Let's use a function that decreases rapidly for small remainders and then more slowly.\n        # Example: `1 / (x + epsilon)` or `max_capacity - x` if we knew max_capacity.\n        \n        # A simpler, yet effective strategy is to assign higher priority to bins that leave\n        # a remainder *closer* to zero, and among those, prioritize the one that is *exactly* zero.\n        \n        # Let's try a score based on the inverse of remaining capacity, but capped or scaled\n        # to avoid extreme values and ensure differentiability.\n        \n        # A robust approach is to make the priority score `f(remaining_capacity_if_fit)`\n        # where f is a decreasing function.\n        # To differentiate, we want f'(x) to be \"large negative\" for small x, and\n        # \"small negative\" for large x.\n        \n        # Example: `f(x) = -log(x + epsilon)` or `f(x) = -sqrt(x + epsilon)`.\n        # Or a piecewise linear function.\n        \n        # Let's try a score that is a large positive number minus the remaining capacity,\n        # but scaled.\n        \n        # Let's use a strategy that prioritizes bins that are \"almost full\" (small remainder)\n        # more aggressively than v1.\n        # We can achieve this by using a steeper decreasing function for the priority score.\n        \n        # Consider a scoring function like: `Constant - k * remaining_capacity`.\n        # If k is large, it's very sensitive to small remainders.\n        # If k is small, it's less sensitive.\n        \n        # Let's try to make the priority value higher for smaller remaining capacities.\n        # A common pattern in optimization is to use `1 / (x + epsilon)` for small x,\n        # or `max_val - x`.\n        \n        # Let's use a score that prioritizes smaller remaining capacities.\n        # We can use the negative of the remaining capacity as a base, and then\n        # add a small penalty for larger remainders.\n        \n        # Let's try to assign a score that is inversely proportional to the remaining capacity,\n        # but we need to ensure stability and differentiation.\n        \n        # Consider the objective of making the bins as full as possible.\n        # This means we want to minimize the remaining capacity.\n        # So, a bin with remaining capacity `r` should have a higher priority than one with `r' > r`.\n        \n        # Let's enhance the priority by considering the *relative* remaining capacity if we\n        # had bin capacity information. Since we don't, we focus on absolute remaining capacity.\n        \n        # Hybrid approach:\n        # If remaining_capacity_if_fit is very small (e.g., < 0.1 * item_size or a fixed small value),\n        # give it a high score.\n        # Otherwise, give it a score that is inversely proportional to the remaining capacity.\n        \n        # Let's try a smooth function that is steeper for small remainders.\n        # For example, `score = -remaining_capacity ** 0.5` or `score = -log(remaining_capacity + epsilon)`.\n        # Let's use `-(remaining_capacity_if_fit + epsilon)` for simplicity and stability,\n        # but this is still linear.\n        \n        # To ensure better differentiation for \"almost full\" bins, let's assign\n        # a significantly higher priority to bins that result in a very small remaining capacity.\n        # We can use a threshold to distinguish between \"tight fits\" and \"looser fits\".\n        \n        # Let's define a threshold for what constitutes an \"almost full\" bin.\n        # A simple threshold could be a small fraction of the item size itself, or a fixed value.\n        # For example, `threshold = 0.1 * item`.\n        # Or, more generally, `threshold = min_possible_positive_remainder`.\n        \n        # Let's try a scoring mechanism where:\n        # - Bins with `remaining_capacity_if_fit` close to 0 get a high, distinct score.\n        # - Bins with larger `remaining_capacity_if_fit` get scores that are still decreasing,\n        #   but with a lower magnitude of decrease.\n        \n        # A practical way to achieve this is by using a combination of linear and\n        # inverse proportional scoring, or a power function.\n        \n        # Let's use `score = - (remaining_capacity_if_fit + small_constant)`. This is linear.\n        # To make it more sensitive to small remainders, we can use `score = - (remaining_capacity_if_fit ** 0.5)`.\n        # Or even more aggressive: `score = -log(remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try a robust approach using inverse remaining capacity but with a slight modification\n        # to give a boost to bins that are 'almost full'.\n        \n        # We want to maximize `f(r)` where `f` is decreasing.\n        # Let's use a form that is highly sensitive to small `r`.\n        # `score = M - r` where M is a large constant.\n        # If we want to differentiate \"almost full\" vs \"not so full\", we can add a term.\n        \n        # Consider a piecewise approach for clarity and differentiation:\n        # If `remaining_capacity_if_fit < epsilon_tight`: high priority, proportional to `-remaining_capacity_if_fit`.\n        # Else: medium priority, proportional to `-remaining_capacity_if_fit`.\n        \n        # Let's use a simpler, effective continuous function that prioritizes small remainders.\n        # A form that is highly sensitive to small remainders is `1 / (x + epsilon)`.\n        # However, this can lead to very large values.\n        \n        # Let's refine the idea:\n        # Prioritize bins that minimize `remaining_capacity_if_fit`.\n        # This means we want to maximize `-remaining_capacity_if_fit`.\n        # To differentiate, we can consider a score that is `MAX_SCORE - (remaining_capacity_if_fit / SCALE)`.\n        # A larger `SCALE` means less differentiation. A smaller `SCALE` means more differentiation.\n        \n        # Let's use a score that is strongly decreasing for small `remaining_capacity_if_fit`.\n        # A good candidate is a function like `exp(-k * remaining_capacity_if_fit)` where `k` is positive.\n        # Or `1 / (remaining_capacity_if_fit + epsilon)`.\n        \n        # Let's try to combine the \"Best Fit\" (minimal remaining capacity) with a \"First Fit Decreasing\" spirit\n        # by giving a bonus to bins that become \"almost full\".\n        \n        # A more structured scoring:\n        # For bins that can fit:\n        # Calculate `r = bins_remain_cap[can_fit_mask] - item`\n        # Let's define a threshold `T`.\n        # If `r <= T`, the score is `BaseScore + Bonus - r`.\n        # If `r > T`, the score is `BaseScore - r / PenaltyFactor`.\n        \n        # For simplicity and robustness, let's use a score that emphasizes minimal remaining capacity\n        # without creating extreme values.\n        # `score = -remaining_capacity_if_fit` (like v1) is okay but doesn't differentiate much.\n        \n        # Let's try a score that is proportional to the inverse of the remaining capacity,\n        # but ensure stability and prevent extreme values.\n        # A common practice is to use a linear transformation of the inverse.\n        # `score = C - K * remaining_capacity_if_fit` where C and K are constants.\n        # To prioritize smaller remainders, K should be positive.\n        # To differentiate small remainders more, K could be larger for smaller remainders.\n        \n        # Let's try a scoring mechanism that strongly prefers bins with very small remainders.\n        # For example, if `remaining_capacity_if_fit` is 0, it's the best.\n        # If `remaining_capacity_if_fit` is small positive, it's very good.\n        # If `remaining_capacity_if_fit` is larger, it's less good.\n        \n        # Let's use a score that increases as `remaining_capacity_if_fit` decreases.\n        # A simple increasing function is `f(x) = C - x`.\n        # To differentiate more for small x, we could use `f(x) = C - sqrt(x)`.\n        # Or `f(x) = C - log(x + epsilon)`.\n        \n        # Let's implement a scoring function that is inversely related to the remaining capacity,\n        # but scaled to ensure distinct priorities.\n        # We want to maximize `priorities[can_fit_mask]`.\n        \n        # Let `r_vals = remaining_capacities_if_fit`.\n        # A good heuristic priority might be related to `1 / (r_vals + epsilon)`.\n        # To make it more stable and to ensure we don't get excessively large values,\n        # we can normalize or use a different function.\n        \n        # Consider a score like `-(r_vals + epsilon)`. This is v1's core idea.\n        # To improve differentiation:\n        # Let's use a score that is more sensitive to smaller values of `r_vals`.\n        # A power function can achieve this: `-(r_vals**p)` where `0 < p < 1`.\n        # For example, `p = 0.5` (square root).\n        \n        # Let's try `score = - np.sqrt(remaining_capacities_if_fit + 1e-9)`.\n        # This will give higher scores to smaller remaining capacities.\n        # The addition of `1e-9` avoids issues with `sqrt(0)`.\n        \n        # The \"Advice\" section mentions \"finely tuned slack minimization\".\n        # This implies we want to strongly favor bins that leave minimal slack.\n        \n        # Let's try a score that is inversely proportional to the remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + epsilon)`\n        # However, this can lead to extreme values.\n        \n        # A balanced approach:\n        # Prioritize bins with minimal remaining capacity.\n        # `score = -remaining_capacity_if_fit` (similar to v1)\n        \n        # Let's make it more aggressive for \"almost full\" bins.\n        # Consider a score like: `-(remaining_capacity_if_fit + 0.1 * item)`\n        # This penalizes larger remainders more, but we want to reward smaller ones.\n        \n        # Let's try a strategy that assigns a higher value to bins that are \"more full\".\n        # `priority = bins_remain_cap[can_fit_mask] - item` (this is remaining capacity)\n        # We want to minimize this. So, priority should be high for small values.\n        \n        # A simple way to differentiate is to use a large constant minus the remaining capacity.\n        # `score = LargeConstant - remaining_capacity_if_fit`\n        # To make it more sensitive to small remainders, we can make `LargeConstant` dependent on the problem,\n        # or use a non-linear function.\n        \n        # Let's try a robust scoring function that emphasizes minimal remaining capacity.\n        # `score = 1.0 / (remaining_capacity_if_fit + 1e-6)`\n        # This gives higher scores to smaller remainders.\n        # The `1e-6` is a small epsilon for numerical stability.\n        \n        # Let's try a different formulation that differentiates \"tight fits\" better.\n        # Consider a score based on how \"full\" the bin becomes.\n        # If a bin has remaining capacity `C_rem` and we place item `I`, the new remaining capacity is `C_rem - I`.\n        # We want `C_rem - I` to be as small as possible (but non-negative).\n        \n        # Let's use a function that rewards smaller remaining capacities.\n        # A simple way to do this is by using the negative of the remaining capacity itself,\n        # as in v1.\n        # `score = -(remaining_capacities_if_fit)`\n        \n        # To make it \"better\", we need more differentiation.\n        # Consider a scenario:\n        # Item size = 0.5\n        # Bins remaining capacities: [0.6, 0.7, 1.0]\n        # Possible remaining capacities after fitting: [0.1, 0.2, 0.5]\n        # v1 scores: [-0.1, -0.2, -0.5] -> Bin 1 is preferred.\n        \n        # We want to prioritize the bin that leaves 0.1, then 0.2, then 0.5.\n        # This means scores should be decreasing: score(0.1) > score(0.2) > score(0.5).\n        \n        # Let's use a score that is inversely proportional to remaining capacity.\n        # `score = 1.0 / (remaining_capacities_if_fit + 1e-6)`\n        # Scores: [1.0/0.1, 1.0/0.2, 1.0/0.5] = [10.0, 5.0, 2.0]\n        # This clearly prioritizes the smallest remainder.\n        \n        # This seems to align well with the goal of prioritizing bins that are \"almost full\".\n        # It provides good differentiation.\n        \n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Combines exact fit priority with scaled inverse normalized slack for non-exact fits.\n    Prioritizes bins that perfectly fit the item, then bins that leave minimal normalized slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if not np.any(can_fit_mask):\n        return priorities # No bins can fit the item\n\n    # Calculate remaining capacities for bins that can fit the item\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    \n    # Assign high priority to exact fits (remaining capacity is zero)\n    exact_fit_mask = remaining_after_fit == 0\n    priorities[can_fit_mask][exact_fit_mask] = 1.0\n\n    # Assign scores to non-exact fits based on normalized slack (inverse)\n    # Higher score for smaller remaining capacity relative to original capacity.\n    # Normalized slack = (bin_capacity - item) / bin_capacity\n    # We want to maximize 1 - normalized_slack, which means minimizing normalized slack.\n    # Smaller slack -> higher priority.\n    # Score range for non-exact fits: [0.5, 0.99] to be less than exact fits.\n    non_exact_fit_indices = np.where(can_fit_mask)[0][~exact_fit_mask]\n    \n    if non_exact_fit_indices.size > 0:\n        bins_for_non_exact = bins_remain_cap[can_fit_mask][~exact_fit_mask]\n        remaining_for_non_exact = remaining_after_fit[~exact_fit_mask]\n\n        # Calculate normalized slack: remaining_capacity / original_capacity\n        # Use a small epsilon to avoid division by zero if original capacity was 0 (should not happen if item fits)\n        epsilon = 1e-9\n        normalized_slack = remaining_for_non_exact / (bins_for_non_exact + epsilon)\n        \n        # Scale scores for non-exact fits to be between 0.5 and 0.99\n        # We want to maximize (1 - normalized_slack), so we scale (1 - normalized_slack)\n        # The value (1 - normalized_slack) ranges from approximately 0 (for large slack) to 1 (for very small slack).\n        # We map this to [0.5, 0.99].\n        # Scale factor: 0.49 (range of 0.99 - 0.5)\n        # Offset: 0.5\n        scaled_scores = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_mask][~exact_fit_mask] = scaled_scores\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristic 1 and Heuristic 3 (identical):\n*   **Commonality:** Both prioritize exact fits with a score of 1.0. For non-exact fits, they calculate `remaining_after_fit / (eligible_bins_remain_cap + epsilon)` which represents normalized slack. They then use `1.0 - normalized_remaining_capacity` and scale it to `0.5 + best_fit_scores * 0.49`.\n*   **Difference:** No functional difference found.\n\nComparing Heuristic 2, 4, 5, 7 and Heuristic 14, 15:\n*   **Commonality (2, 4, 5, 7):** These heuristics prioritize exact fits with a very high score (`1e6`). For non-exact fits, they calculate slack, then `best_fit_scores = -slack`. They then apply a penalty `slack_penalty_factor * relative_slack` where `relative_slack = slack / (item + epsilon)`. The combined score is `best_fit_scores - slack_penalty_factor * relative_slack`. The penalty factor is 0.1. They initialize priorities to -1.0.\n*   **Commonality (14, 15):** These heuristics prioritize exact fits with a very high score (`1e9`). For non-exact fits, they aim to minimize remaining capacity (`-remaining_after_fit`) and use `-initial_remaining_capacity` as a tie-breaker, scaled by a large factor (`1e6`) to ensure Best Fit dominates. They initialize priorities to -1.0.\n*   **Difference:** Heuristics 2, 4, 5, 7 introduce a *penalty* for large relative slack, actively *reducing* the score for bins that would remain too empty. Heuristics 14, 15 use the initial bin capacity as a *tie-breaker* for bins with similar Best Fit scores, favoring bins that were already fuller. Heuristic 14/15 have a higher exact fit score and a larger scale for their secondary criterion.\n\nComparing Heuristic 1, 3, 8, 9, 11, 16, 17, 18, 19, 20:\n*   **Commonality:** These heuristics prioritize exact fits (score 1.0 or similar high value). For non-exact fits, they use a \"normalized slack\" approach, typically calculating `(remaining_capacity - item) / original_capacity`. They then transform this to create a priority score, often scaling it to a range below the exact fit score (e.g., `[0.5, 0.99]`).\n*   **Heuristic 1/3:** `0.5 + 0.49 * (1.0 - normalized_remaining_capacity)`.\n*   **Heuristic 8/9/11:** Identical to 1/3.\n*   **Heuristic 16/19/20:** Identical to 1/3/8/9/11. The implementation details of `normalized_slack` are slightly different, but the core logic of scaling `(1.0 - normalized_slack)` to `[0.5, 0.99]` is the same.\n*   **Heuristic 17/18:** Identical to 16/19/20.\n*   **Difference:** The primary difference lies in the *exact implementation* of \"normalized slack\" (e.g., `remaining_after_fit / original_capacity` vs. `remaining_after_fit / (original_capacity + epsilon)`) and the scaling range. Heuristic 1/3/8/9/11/16/17/18/19/20 are functionally very similar, using a scaled inverse normalized slack.\n\nComparing Heuristic 10 and Heuristic 12/13:\n*   **Commonality:** Both aim to balance \"tight fits\" with spreading items. They initialize priorities to -1.0 for non-fitting bins.\n*   **Heuristic 10:** Uses a combined score: `tight_fit_score * 0.7 + normalized_slack * 0.3`. `tight_fit_score` is `1.0 / (remaining_capacities_if_fit + epsilon)` (prioritizing small remainder) and `normalized_slack` is `eligible_capacities / np.max(eligible_capacities)` (favoring larger remaining capacity). This attempts to balance minimal remainder with more spread.\n*   **Heuristic 12/13:** Prioritizes minimal remaining capacity by using `1.0 / (remaining_capacities_if_fit + epsilon)`. This is a \"pure\" Best Fit approach based on absolute remaining capacity.\n*   **Difference:** Heuristic 10 explicitly tries to incorporate a \"soft worst fit\" or spreading mechanism by adding a term proportional to the *original* capacity (normalized), whereas 12/13 focuses solely on minimizing the *remaining* capacity after fitting.\n\nComparing Heuristic 6 with others:\n*   **Heuristic 6:** Uses configurable parameters for `exact_fit_priority`, `non_exact_fit_min_priority`, `non_exact_fit_max_priority`. It calculates normalized slack similarly to Heuristics 1/3/8/9/11/16/17/18/19/20, and then scales `1.0 - normalized_slack` to the range defined by `non_exact_fit_min_priority` and `non_exact_fit_max_priority`. It explicitly clips scores to be less than `exact_fit_priority`.\n*   **Difference:** The key difference is the explicit use of *tuned parameters* to define the priority ranges, rather than hardcoded values like 1.0 and `[0.5, 0.99]`. This suggests an approach that might have been optimized or parameterized.\n\n**Ranking Justification:**\n\n*   **Best (1-3):** Heuristics 1, 3, 8, 9, 11, 16, 17, 18, 19, 20 are very similar and represent a strong baseline. They prioritize exact fits and then use a well-defined scaled inverse normalized slack. This approach is robust and provides good differentiation. Heuristic 6 is also in this top tier, essentially offering the same logic but with tunable parameters, which is a good design choice. The exact ranking among these is subtle, but they represent the most refined common strategy.\n*   **Mid-Tier (4-5):** Heuristics 2, 4, 5, 7 introduce a penalty for large relative slack. This is a reasonable extension of Best Fit, aiming to avoid overly empty bins. However, it might be overly aggressive or sensitive to the `slack_penalty_factor`.\n*   **Mid-Tier (10):** Heuristic 10 attempts to balance Best Fit with a \"soft worst fit\" (spreading). This is an interesting idea but the implementation might be less direct than simply prioritizing minimal slack. The combination of inverse remaining capacity and normalized original capacity as scores could lead to unexpected behavior.\n*   **Mid-Tier (12-13):** Heuristics 12, 13 use `1.0 / (remaining_capacity_if_fit + epsilon)`. This is a strong Best Fit heuristic that clearly prioritizes minimal absolute remaining capacity. It's simpler than scaled normalized slack but might be less sensitive to the *proportion* of remaining space.\n*   **Lower-Mid Tier (14-15):** These use a very high exact fit score and a tie-breaker based on initial bin fullness. While prioritizing exact fits is good, the tie-breaker might be secondary to minimizing slack and could lead to less optimal packing if not carefully tuned.\n*   **Worst (None):** All heuristics attempt to address the problem by prioritizing exact fits and then considering some form of slack minimization or differentiation. There isn't a fundamentally flawed heuristic. The ranking is based on common effectiveness and robustness of the chosen strategy.\n\n**Final Ranking (Conceptual, based on observed strategies):**\n\n1.  Heuristic 1, 3, 8, 9, 11, 16, 17, 18, 19, 20 (Scaled Inverse Normalized Slack with clear hierarchy)\n2.  Heuristic 6 (Same as above but with tunable parameters)\n3.  Heuristic 12, 13 (Pure Best Fit on absolute remaining capacity)\n4.  Heuristic 2, 4, 5, 7 (Best Fit with slack penalty)\n5.  Heuristic 10 (Balanced Best Fit + Soft Worst Fit)\n6.  Heuristic 14, 15 (Exact Fit + Tie-breaker on initial fullness)\n\n*(Note: The provided list had duplicates, so the ranking groups similar ones. The original ordering in the prompt implies a ranking that we are trying to reconstruct by analyzing the strategies.)*\n- \nHere's a refined approach to self-reflection for designing better heuristics:\n\n*   **Keywords:** Fit, Slack, Normalization, Differentiation, Hierarchy.\n*   **Advice:** Design scoring mechanisms that clearly distinguish between exact fits and various degrees of near-fits using normalized metrics. Establish a clear hierarchy of criteria, prioritizing exact fits, then minimal normalized slack, with tunable parameters for adaptability.\n*   **Avoid:** Arbitrary penalties, direct division by zero or very small numbers without stabilization (epsilon), overly complex multi-objective functions without justification, and counter-intuitive negative scaling.\n*   **Explanation:** Effective self-reflection in heuristic design centers on creating clear, robust scoring that prioritizes ideal outcomes while robustly differentiating between good-enough alternatives, ensuring numerical stability and intuitive logic.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}