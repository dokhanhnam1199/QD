{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritized Bins using Scaled Remaining Capacity and Slack Minimization.\n\n    This strategy aims to provide more differentiated scores by considering\n    both how \"full\" a bin becomes and how much slack is left. It also\n    introduces a scaling factor to adjust the emphasis on minimizing slack.\n\n    The priority is calculated as:\n    priority = (1 - scaled_remaining_capacity) * weight_slack - scaled_slack * (1 - weight_slack)\n\n    Where:\n    - scaled_remaining_capacity: The remaining capacity after fitting, scaled\n                                  to a [0, 1] range relative to the bin capacity (assumed 1.0 for simplicity).\n                                  A smaller remaining capacity (closer to 0) gets a higher score here.\n    - scaled_slack: The amount of space left in the bin if the item doesn't fit perfectly,\n                   scaled to a [0, 1] range. This term is mainly to penalize bins that are \"too large\".\n                   A smaller slack (closer to 0) gets a lower score here.\n    - weight_slack: A hyperparameter (between 0 and 1) to balance the importance of\n                    minimizing slack vs. maximizing the \"fullness\" of the bin.\n\n    For bins that can fit the item:\n    - If remaining capacity is 0, it gets the highest priority.\n    - Otherwise, we want to minimize remaining capacity.\n\n    The logic:\n    1. Identify bins that can fit the item.\n    2. For these bins, calculate the remaining capacity after placing the item.\n    3. For bins that CANNOT fit, assign a very low priority (-1).\n    4. For bins that CAN fit:\n       - We want to prioritize bins that result in a smaller remaining capacity.\n       - A simple way to get differentiated scores is to use the negative of the remaining capacity.\n       - To make these scores more comparable across different potential remaining capacities,\n         we can normalize them.\n       - However, a direct inverse might not be ideal. A better approach is to create a score\n         that is high when remaining capacity is low.\n\n    Revised Approach:\n    We will aim to create a score that is high for bins that are \"nearly full\"\n    after the item is placed.\n    The priority will be based on how much the *remaining capacity changes* towards zero.\n    A bin that goes from `c` capacity to `c - item` is prioritized if `c - item` is small.\n\n    Let's define priority as a function that is maximized when `bins_remain_cap - item` is minimized.\n    A simple monotonic transformation that achieves this is to use `- (bins_remain_cap - item)`.\n    To make scores more distinct and robust, we can consider a scaled version or a score that\n    rewards bins that are *just* large enough.\n\n    New Strategy: Maximize (1 / (remaining_capacity_after_fit + epsilon)) or minimize remaining_capacity_after_fit.\n    We will prioritize bins that result in the *smallest positive remaining capacity*.\n\n    Let's use a score that rewards bins that are \"just right\" or slightly larger.\n    A bin that is exactly the right size (remaining capacity = 0) should be highly prioritized.\n    Bins that are much larger should be less prioritized than those that are a good fit.\n\n    Priority = -(remaining_capacity_after_fit)\n    To make scores more distinct and avoid very small negative numbers dominating,\n    we can scale them. Or, more simply, we can add a large constant to make them positive\n    and then use the negative of the remaining capacity.\n\n    Let's try a score that emphasizes bins that leave a small, but positive, remainder.\n    The ideal scenario is a bin that leaves 0 remainder.\n    A bin that leaves a very large remainder is less ideal.\n\n    Consider priority = - (bins_remain_cap[can_fit_mask] - item)\n    This means smaller remaining capacity is better (higher score).\n\n    To differentiate more, we can introduce a penalty for very large remaining capacities.\n    A simple way is to use a function that is steep near 0 remaining capacity and less steep for larger capacities.\n    e.g., `- log(remaining_capacity + epsilon)` or `- remaining_capacity`.\n\n    Let's refine `priority_v1` by making the scores more distinct for bins that leave small remainders.\n    Instead of just `-remaining_capacity`, we can use `1 / (remaining_capacity + epsilon)` which\n    gives higher scores to smaller remaining capacities, and the difference between scores\n    is more pronounced.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Use -inf for bins that cannot fit\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate remaining capacities for bins that can fit the item\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n\n        # Strategy: Prioritize bins that leave a small, non-negative remaining capacity.\n        # We want to maximize the score as the remaining capacity approaches 0.\n        # A good function for this is 1 / (remaining_capacity + epsilon).\n        # However, to make scores more distinct, we can use a function that is\n        # sensitive to the magnitude of the remaining capacity.\n        # Let's use a score that is highest for zero remaining capacity and decreases\n        # as remaining capacity increases.\n        # A simple function that achieves this is to assign a score inversely\n        # proportional to the remaining capacity, ensuring a small positive value for the denominator.\n        # A larger value indicates a better fit.\n\n        # We want to maximize `f(remaining_capacity)` where `f(0)` is max and `f(large)` is min.\n        # Options:\n        # 1. `1 / (remaining_capacity + epsilon)`: High scores for small remainders.\n        # 2. `-remaining_capacity`: Also prioritizes small remainders.\n        # 3. `K - remaining_capacity`: Shifts scores but maintains order.\n\n        # To differentiate better, let's consider bins that are *just* large enough.\n        # A bin that is perfectly sized (remaining capacity = 0) is ideal.\n        # Bins that are slightly larger are good. Bins that are much larger are less ideal.\n\n        # Let's use a score that rewards bins that leave minimal capacity,\n        # but also penalizes bins that leave a lot of capacity.\n        # Consider the inverse of the remaining capacity.\n        # To avoid division by zero and create distinct scores, use 1 / (remaining_capacity + epsilon)\n        epsilon = 1e-9\n        scores = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n        # We can also add a term to penalize large remaining capacities more severely.\n        # For example, we could subtract a scaled version of the remaining capacity.\n        # Let's aim for a score that is highest for remaining capacity of 0 and decreases.\n        # A simple linear decrease is `-remaining_capacity`.\n        # A non-linear decrease could be `-log(remaining_capacity + epsilon)`.\n\n        # Let's stick with the inverse relationship, which is good for \"almost full\".\n        # To make scores more \"differentiated\", consider the difference in priorities.\n        # If we have remaining capacities of 0.1 and 0.2, the scores are 10 and 5. Difference is 5.\n        # If we have remaining capacities of 0.01 and 0.02, the scores are 100 and 50. Difference is 50.\n        # This inherently provides differentiation.\n\n        # We can also add a small bonus for bins that are closer to a perfect fit.\n        # Let's use a scaled version of the inverse remaining capacity.\n        # A scaled version of 1 / (remaining_capacity + epsilon) could be:\n        # (1 / (remaining_capacity + epsilon)) * max_possible_score / (1 / epsilon)\n        # This normalizes the scores to be within a certain range.\n\n        # A simpler, more robust approach to differentiation:\n        # Use the negative of the remaining capacity, but shift it so that\n        # perfect fits get the highest scores.\n        # Let's assign a maximum score to a perfect fit and decrease from there.\n        # Max score = 0 (for remaining capacity of 0)\n        # Score = - remaining_capacity\n\n        # To make scores more distinct, especially when many bins have small remainders:\n        # Use `log` scaling or inverse scaling.\n        # `log(1 / (remaining_capacity + epsilon))` or `-log(remaining_capacity + epsilon)`\n        # This makes differences at small values more pronounced.\n\n        # Let's use `-remaining_capacity` as the base, as it directly addresses minimizing slack.\n        # To differentiate, we can add a factor that scales based on how small the remaining capacity is.\n        # Or, more simply, ensure that values are not too close.\n\n        # Let's try the `priority_v1` logic but ensure the values are sufficiently spread.\n        # `priority_v1` uses `-(bins_remain_cap[can_fit_mask] - item)`.\n        # This means smaller remaining capacities lead to higher (less negative) scores.\n        # Example:\n        # Bin A: rem_cap = 0.1, item = 0.9. rem_cap_after = 0.0. Score = 0.\n        # Bin B: rem_cap = 0.2, item = 0.9. rem_cap_after = 0.1. Score = -0.1.\n        # Bin C: rem_cap = 0.3, item = 0.9. rem_cap_after = 0.2. Score = -0.2.\n        # Bin D: rem_cap = 1.0, item = 0.1. rem_cap_after = 0.9. Score = -0.9.\n\n        # This is quite good. The differentiation is there.\n        # To make scores even more distinct for very small remainders, we can use:\n        # `log(1 / (remaining_capacity_after_fit + epsilon))` which is `-log(remaining_capacity_after_fit + epsilon)`\n        # For rem_cap = 0.0, -log(epsilon) is a large positive number.\n        # For rem_cap = 0.1, -log(0.1) approx 2.3\n        # For rem_cap = 0.2, -log(0.2) approx 1.6\n\n        # This seems like a good way to differentiate. Let's use this.\n        # We are maximizing `-log(remaining_capacity_after_fit + epsilon)`.\n\n        # To ensure scores are positive and distinguishable, let's normalize them or\n        # ensure a baseline.\n        # Let's map the best case (remaining_capacity = 0) to a high positive score.\n        # Maximize: `100 - log(remaining_capacity_after_fit + epsilon)`\n        # If remaining_capacity_after_fit is 0, score is `100 - log(epsilon)` (large positive)\n        # If remaining_capacity_after_fit is small, score is slightly less.\n        # If remaining_capacity_after_fit is larger, score is smaller.\n\n        # Consider the range of `remaining_capacity_after_fit`. It's between 0 and `max(bins_remain_cap) - item`.\n        # Let's try a score that is high for 0 remainder and decreases as remainder increases.\n        # A simple linear score: `C - remaining_capacity_after_fit`.\n        # To differentiate, we need a non-linear function.\n\n        # Let's use `1.0 / (remaining_capacity_after_fit + epsilon)` as it's intuitive for \"best fit\".\n        # To ensure differentiation, we can scale this.\n        # If we have many bins with very small remainders, the scores will be very large and close.\n        # We want scores that clearly separate good fits from mediocre ones.\n\n        # Consider a score that combines \"how full\" with \"how much slack\".\n        # Priority = w1 * (1 / (remaining_capacity_after_fit + epsilon)) + w2 * (-remaining_capacity_after_fit)\n        # Where w1 and w2 control the emphasis.\n\n        # Let's go with a score that is designed to be high for small, positive remaining capacities.\n        # A simple way to achieve good differentiation is to use an inverse relationship.\n        # `1 / (remaining_capacity_after_fit + epsilon)` is a good candidate.\n        # To make the values more manageable and perhaps better distributed, we can\n        # scale it.\n\n        # Let's consider the \"value\" of a bin. A bin that leaves 0 space is perfect.\n        # A bin that leaves a tiny bit of space is almost perfect.\n        # A bin that leaves a lot of space is less ideal.\n\n        # We want to maximize `f(remaining_capacity)` where `f` is decreasing.\n        # Let's map the range of `remaining_capacity_after_fit` to a score range.\n        # Let `min_rem = 0` and `max_rem = max(bins_remain_cap[can_fit_mask]) - item`.\n        # We want to map `[min_rem, max_rem]` to `[score_max, score_min]`.\n\n        # Let's use a score that emphasizes bins that are \"just right\".\n        # Score = 1 / (remaining_capacity_after_fit + epsilon)\n        # Example:\n        # rem_cap = [0.0, 0.05, 0.1, 0.5, 1.0]\n        # scores = [1e9, 20, 10, 2, 1] (using epsilon=1e-9)\n        # This provides differentiation.\n\n        # To make it even more robust and perhaps include other factors if needed later,\n        # consider a weighted combination.\n        # For now, let's stick to improving the \"almost full\" heuristic.\n\n        # What if we scale the inverse based on the *original* remaining capacity?\n        # This would give a higher absolute priority to bins that were already full.\n        # Score = (1 / (remaining_capacity_after_fit + epsilon)) * bins_remain_cap[can_fit_mask]\n\n        # Let's refine the `-log(remaining_capacity + epsilon)` approach.\n        # This provides strong differentiation for small remaining capacities.\n        # For robustness, we can add a large constant to make scores positive.\n        # `priority_score = 100.0 - np.log(remaining_capacities_if_fit + epsilon)`\n\n        # This ensures that bins with smaller remaining capacities get higher scores.\n        # The `log` function provides the desired differentiation.\n        # The `100.0` is an arbitrary offset to ensure positive scores.\n\n        priorities[can_fit_mask] = 100.0 - np.log(remaining_capacities_if_fit + epsilon)\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, scaled for clarity.\n\n    Combines exact fit priority with a scaled inverse normalized slack for\n    non-exact fits, ensuring distinct and robust scoring.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # High priority for exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_and_not_exact_mask = bins_remain_cap > item\n    \n    if np.any(can_fit_and_not_exact_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_and_not_exact_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_capacities_if_fit = eligible_bins_remain_cap - item\n        \n        # Normalized slack: remaining capacity / current bin capacity. Smaller is better.\n        epsilon = 1e-9\n        normalized_slack = remaining_capacities_if_fit / (eligible_bins_remain_cap + epsilon)\n        \n        # Priority: Higher score for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher priority (closer to 1.0).\n        # Scale these scores to be distinct from exact fits (e.g., between 0.5 and 0.99).\n        # A simple mapping: 0.5 + 0.49 * (1.0 - normalized_slack)\n        # This ensures non-exact fits are always lower than exact fits and have a good range.\n        non_exact_priorities = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_and_not_exact_mask] = non_exact_priorities\n\n    return priorities\n\n### Analyze & experience\n- Comparing Heuristics 1, 5, 6, 12, 15 (Exact Fit + Normalized Slack variants) with Heuristics 2, 4, 7 (Almost Full Fit variants) and Heuristic 3 (Hybrid V1), we see that prioritizing exact fits offers a clear, high-priority strategy. Normalized slack provides a good way to differentiate non-exact fits based on relative space utilization. The \"Almost Full Fit\" heuristics (like V1, which is essentially maximizing -slack) are simpler but lack the explicit handling of exact fits or the nuanced scaling of normalized slack. Heuristic 3 attempts a hybrid but its implementation is less clear than the dedicated strategies.\n\nComparing Heuristics 1, 5, 6, 12, 15:\n- Heuristics 1, 5, 15 are very similar, prioritizing exact fits (score 1.0) and then using a scaled version of `1.0 - normalized_slack` for non-exact fits, mapping them to a range like [0.5, 0.99]. This provides a clear hierarchy and differentiated scores.\n- Heuristic 6 is very similar to 1, 5, 15 but uses slightly different scaling (0.5 to 0.99).\n- Heuristic 12 uses `1.0` for exact fits and `0.5 + 0.49 * (1.0 - normalized_slack)` for non-exact fits, which is functionally identical to 1, 5, 15.\n- Heuristic 13 attempts to combine exact fit priority with a Best Fit strategy (minimizing normalized residual) and a tie-breaker based on initial slack. The scoring mechanism (lexicographical preference via scaling) is more complex but aims for a multi-objective optimization.\n- Heuristic 14 also prioritizes exact fits and then uses a normalized remaining capacity (mapped to [0.5, 0.95]) for non-exact fits, similar to 1, 5, 6, 12, 15.\n\nComparing \"Almost Full Fit\" variants (2, 4, 7) with others:\n- Heuristics 2, 4, 7 are consistent in using `-slack` (maximizing negative slack) as the primary scoring mechanism. This prioritizes bins that become most full. They assign -1 to bins that cannot fit. This is a simple and effective \"Best Fit\" approach but doesn't explicitly handle exact fits as a special case.\n\nComparing Heuristics 8, 9, 10, 11, 16, 17, 20:\n- Heuristic 8 attempts to use `-log(slack + epsilon)` for differentiation, which is a novel approach for small slack values.\n- Heuristics 9, 10, 11 combine slack minimization with fit ratio `item / bins_remain_cap`, weighted. This adds a second dimension to the scoring.\n- Heuristic 16 tries to balance tightness with avoiding extreme fills, using `-slack` as a base and adding a penalty for very tight fits (when `slack` is below a threshold relative to `item`). This is a more sophisticated attempt to balance competing goals.\n- Heuristic 17 is identical to 16.\n- Heuristic 20 prioritizes exact fits with a very high score (1e6), then uses a scaled `-remaining_after_fit` (Best Fit) and a penalty based on negative initial remaining capacity as a tie-breaker. This is a strong multi-objective approach.\n\nHeuristics 18 and 19 are incomplete code snippets.\n\nOverall, heuristics that combine a clear, high priority for exact fits with a well-defined scoring for non-exact fits (like normalized slack or scaled best-fit) tend to perform better. Heuristics that attempt multi-objective optimization (e.g., balancing tightness with avoiding fragmentation or using tie-breakers) show promise but can become complex. The \"Almost Full Fit\" (-slack) is a solid baseline but can be improved by explicit exact-fit handling or more nuanced differentiation.\n- \nHere's a redefined self-reflection for designing better heuristics, focusing on robust differentiation and avoiding pitfalls:\n\n*   **Keywords:** Objective clarity, Score differentiation, Robustness, Scalability.\n*   **Advice:** Design scores that clearly distinguish between excellent fits and good-enough options. Use a hierarchical approach, prioritizing exact matches, then finely tuned slack minimization (normalized or scaled), and finally, stable secondary metrics.\n*   **Avoid:** Over-reliance on absolute slack, direct division by capacities prone to instability, and complex, unexplainable scoring interactions.\n*   **Explanation:** The goal is to create heuristics that make decisive, understandable choices. Clear scoring ensures that the algorithm consistently favors better solutions without getting lost in minor, unstable numerical variations, promoting robustness across different problem scales.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}