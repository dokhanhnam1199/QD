{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code only and do not add comments into the code. Format your code as a Python code string: \"```python ... ```\".\n", "user": "You are an expert in the domain of optimization heuristics. Your task is to write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n### Better code\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Best Fit Decreasing (BFD) inspired priority for Online Bin Packing.\n\n    This strategy aims to find a bin that can accommodate the item and, among those,\n    selects the one that leaves the minimum remaining capacity (Best Fit principle).\n    To enhance robustness and handle cases where multiple bins might leave the same\n    minimal remaining capacity, a tie-breaking mechanism is introduced by prioritizing\n    bins with less initial slack (i.e., those that were already closer to being full).\n    This encourages packing larger items into bins that can better accommodate them\n    without wasting much space, aligning with the spirit of BFD for improved packing density.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = fitting_bins_remain_cap - item\n        \n        # Priority 1: Minimize remaining capacity after fitting (Best Fit).\n        # We want to maximize the negative of remaining_after_fit.\n        # A large negative number means a small positive remaining capacity.\n        priority_score_bf = -remaining_after_fit\n        \n        # Priority 2: Tie-breaking - prioritize bins that were closer to full *before* fitting.\n        # This means prioritizing bins with smaller *original* remaining capacity\n        # among those that can fit the item and result in the same minimum 'remaining_after_fit'.\n        # To achieve this, we can penalize bins that have a larger original remaining capacity.\n        # We'll subtract the original remaining capacity from a large number, or simply\n        # use its negative value. Using the negative of the original remaining capacity\n        # means smaller original remaining capacities get higher (less negative) scores.\n        priority_score_tiebreaker = -fitting_bins_remain_cap\n\n        # Combine priorities: Primarily Best Fit, secondarily prefer bins with less initial slack.\n        # A simple way to combine is to use a weighted sum or lexicographical ordering.\n        # Lexicographical ordering is often preferred: sort by the primary criterion first,\n        # then by the secondary criterion for ties.\n        # To implement this with a single score, we can use a scaled value.\n        # For example, score = primary_value * large_constant + secondary_value\n        # Here, we want to maximize `priority_score_bf` and then `priority_score_tiebreaker`.\n        # We can achieve this by: score = priority_score_bf + (priority_score_tiebreaker / scale)\n        # where scale is chosen such that priority_score_bf always dominates priority_score_tiebreaker.\n        # A simpler way if we want to assign higher score:\n        # Let's assign a large multiplier to the primary goal (minimizing remaining capacity).\n        # The secondary goal (less initial slack) will be added.\n        # The primary goal is maximizing -(remaining_after_fit).\n        # The secondary goal is maximizing -(fitting_bins_remain_cap).\n\n        # We want to maximize the \"goodness\".\n        # Goodness is primarily determined by minimizing remaining_after_fit.\n        # So, a higher value of -remaining_after_fit is better.\n        # Secondarily, among bins with the same remaining_after_fit, we want to pick\n        # the one with smaller original fitting_bins_remain_cap. This means a higher\n        # value of -fitting_bins_remain_cap is better.\n\n        # To combine these, we can use a scoring system where the primary factor has\n        # a much larger weight.\n        # Let's scale the tie-breaking score to be much smaller than the primary score.\n        # The range of `remaining_after_fit` could be up to the bin capacity.\n        # The range of `fitting_bins_remain_cap` could also be up to the bin capacity.\n        # If we simply add them, they might interfere.\n        # A common technique is to use a large number for scaling.\n        # Let's assume max bin capacity is M.\n        # A score like: `-(remaining_after_fit) * (M + 1) + -(fitting_bins_remain_cap)`\n        # This ensures that minimizing `remaining_after_fit` is the dominant factor.\n        # The tie-breaker `-(fitting_bins_remain_cap)` ensures that among bins with\n        # the same `remaining_after_fit`, we pick the one with the smallest original capacity.\n\n        # To avoid hardcoding M, we can use a relative scaling.\n        # The maximum possible value for `remaining_after_fit` is bounded by the\n        # maximum possible bin capacity. Let's use a sufficiently large constant,\n        # or estimate a maximum possible capacity if known, or use a very large number.\n        # A safe bet is a number larger than any expected `fitting_bins_remain_cap`.\n        # Let's assume a large constant for scaling.\n        # A very large multiplier ensures that the primary objective (minimizing remaining space)\n        # completely dominates the secondary objective (minimizing initial slack).\n        scale_factor = 1e6 # Sufficiently large to dominate any difference in initial capacities\n\n        combined_priorities = priority_score_bf * scale_factor + priority_score_tiebreaker\n        \n        priorities[can_fit_mask] = combined_priorities\n\n    return priorities\n\n### Worse code\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits and then applies a scaled inverse slack for remaining bins.\n\n    This heuristic rewards bins that perfectly accommodate the item with a score of `exact_fit_score`.\n    For other bins, it calculates a priority based on the inverse of the remaining\n    capacity after fitting, ensuring numerical stability and favoring tighter fits.\n\n    Args:\n        item: The size of the item to be placed.\n        bins_remain_cap: A numpy array representing the remaining capacity of each bin.\n        exact_fit_score: The score assigned to bins that perfectly fit the item.\n        slack_offset: A value added to the remaining capacity for inverse calculation,\n                      ensuring bins with zero remaining capacity after fit get a high priority.\n        epsilon: A small value to prevent division by zero and handle floating-point inaccuracies.\n\n    Returns:\n        A numpy array representing the priority score for each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n### Analyze & experience\n- *   **Heuristics 1 vs 2:** Heuristic 1 prioritizes exact fits (score 1.0) and then uses an inverse of remaining capacity for others. Heuristic 2 implements a BFD-inspired approach, maximizing negative remaining capacity and using negative initial slack as a tie-breaker, scaled by a large factor. Heuristic 1's scoring for non-exact fits (1/(remaining+1)) is simpler but might not distinguish as well as Heuristic 2's combined score. Heuristic 2's explicit tie-breaking and scaling seem more robust for BFD.\n\n*   **Heuristics 2 vs 5:** Heuristic 2 uses a scaled combination of minimizing remaining capacity and minimizing initial slack. Heuristic 5 focuses solely on minimizing remaining capacity (equivalent to maximizing negative remaining capacity). Heuristic 2's tie-breaking mechanism (considering initial slack) adds a refinement that Heuristic 5 lacks, potentially leading to better packing in cases of identical minimum remainders.\n\n*   **Heuristics 5 vs 7:** These heuristics appear to be identical in their core logic: prioritizing bins that leave the minimum remaining capacity after fitting an item. Both assign a score as the negative of the remaining capacity after fit.\n\n*   **Heuristics 1 vs 9:** Heuristic 1 rewards exact fits with 1.0 and uses inverse remaining capacity for others. Heuristic 9 prioritizes exact fits with 1.0 and then uses a normalized slack (remaining / current capacity) for non-exact fits, scaled to be between 0.5 and 0.99. Heuristic 9's normalization and scaling for non-exact fits offer a more nuanced approach than Heuristic 1's simple inverse.\n\n*   **Heuristics 9 vs 10:** Heuristics 9 and 10 are identical. They both prioritize exact fits with 1.0 and then use a scaled normalized slack (1 - normalized_slack) for non-exact fits, mapping to [0.5, 0.99].\n\n*   **Heuristics 10 vs 14:** Heuristic 10 prioritizes exact fits with 1.0 and then uses a scaled normalized slack (1 - normalized_slack) for non-exact fits, mapping to [0.5, 0.99]. Heuristic 14 also prioritizes exact fits with 1.0 and uses (1 - normalized slack) for non-exact fits, but these scores are not explicitly scaled to a range like [0.5, 0.99], implying they could be [0, 1). Both use normalized slack, but Heuristic 10's explicit scaling might offer more control.\n\n*   **Heuristics 14 vs 16:** Heuristic 14 prioritizes exact fits with 1.0 and uses `1.0 - normalized_slack` for non-exact fits. Heuristic 16 prioritizes exact fits with 1.0 and uses `0.5 * (1.0 - normalized_slack)` for non-exact fits. Heuristic 16's scaling factor of 0.5 ensures non-exact fits are always distinctly lower than exact fits, potentially offering clearer separation.\n\n*   **Heuristics 16 vs 17:** Heuristic 16 prioritizes exact fits with 1.0 and uses `0.5 * (1.0 - normalized_slack)` for non-exact fits. Heuristic 17 prioritizes exact fits with 1.0 and then uses `1.0 + (1.0 / (normalized_slack + 1e-9))` for non-exact fits. Heuristic 17's scoring for non-exact fits is much higher and seems to invert the normalized slack, making smaller slack yield larger scores. This approach might overvalue non-exact fits.\n\n*   **Heuristics 17 vs 15:** Heuristic 17 prioritizes exact fits with 1.0 and uses `1.0 + (1.0 / (normalized_slack + 1e-9))` for non-exact fits. Heuristic 15 prioritizes exact fits with 1.0 and uses `1.0 + (1.0 / (normalized_slack + 1e-9))` for non-exact fits. These appear identical.\n\n*   **Heuristics 15 vs 18:** Heuristic 15 prioritizes exact fits with 1.0 and uses `1.0 + (1.0 / (normalized_slack + 1e-9))` for non-exact fits. Heuristic 18 prioritizes exact fits with a very high score (1e6) and then uses `-normalized_slack * 1e5` for non-exact fits. Heuristic 18's high score for exact fits and negative scaling for non-exact fits is a different approach. The negative scaling for non-exact fits is unusual for \"higher score = higher priority.\"\n\n*   **Heuristics 18 vs 20:** Heuristics 18 and 20 are identical. They prioritize exact fits with a very high score (1e6) and then use `-normalized_slack * 1e5` for non-exact fits, which appears to have the intention of maximizing the negative normalized slack (minimizing slack).\n\n*   **Heuristics 19 vs 20:** Heuristic 19 is incomplete but aims for exact fits with a specific score and inverse slack. Heuristic 20 prioritizes exact fits with a very high score (1e6) and then uses `-normalized_slack * 1e5` for non-exact fits. Heuristic 20's explicit high score for exact fits and the negative scaling for non-exact fits are concrete implementations.\n\n*   **Overall:** Heuristics 2 and 10/9/14/16 offer robust BFD-like strategies by combining primary (minimizing residual/slack) and secondary (initial slack) objectives or by using normalized slack. Heuristics 18/20 have a clear hierarchy with a very high score for exact fits and negative scaled slack for others, but the negative scaling for priority is counter-intuitive. Heuristics 15/17 use an unusual inversion for non-exact fits. Heuristics 1/5/7 focus on simple inverse remaining capacity.\n- \nHere's a redefined approach to self-reflection for heuristic design:\n\n*   **Keywords:** Differentiated Scores, Scaled Objective Integration, Robustness, Contextual Prioritization.\n*   **Advice:** Design self-reflection to create *distinct, contextually relevant scores* for candidate solutions. Integrate multiple objectives (e.g., fit quality, resource utilization) using scaling or weighted sums that reflect their relative importance.\n*   **Avoid:** Overly simplistic scoring (e.g., direct inverse slack), manual prioritization that's not data-driven, and brittle numerical operations.\n*   **Explanation:** This focuses on creating nuanced, comparable scores that guide the heuristic towards robust, well-reasoned choices by considering the interplay of various factors and ensuring numerical stability.\n\nYour task is to write an improved function `priority_v2` by COMBINING elements of two above heuristics base Analyze & experience.\nOutput the code within a Python code block: ```python ... ```, has comment and docstring (<50 words) to description key idea of heuristics design.\n\nI'm going to tip $999K for a better heuristics! Let's think step by step."}