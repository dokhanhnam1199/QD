```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Combines exact fit priority with a scaled Best Fit strategy,
    using normalized slack and a tie-breaker for bins with less initial slack.
    """
    priorities = np.full_like(bins_remain_cap, -1.0)
    
    can_fit_mask = bins_remain_cap >= item
    
    if np.any(can_fit_mask):
        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]
        eligible_indices = np.where(can_fit_mask)[0]

        # Exact fit has the highest priority
        exact_fit_mask = (eligible_bins_remain_cap == item)
        priorities[eligible_indices[exact_fit_mask]] = 1.0
        
        # For non-exact fits, use Best Fit by minimizing remaining capacity
        non_exact_fit_mask = (eligible_bins_remain_cap > item)
        non_exact_indices = eligible_indices[non_exact_fit_mask]
        non_exact_remain_caps = eligible_bins_remain_cap[non_exact_fit_mask]

        if len(non_exact_remain_caps) > 0:
            remaining_after_fit = non_exact_remain_caps - item
            
            # Normalize remaining capacity to [0, 1], where 0 is best (min residual)
            max_residual = np.max(remaining_after_fit)
            # Add epsilon for stability if all residuals are identical
            normalized_residual = remaining_after_fit / (max_residual + 1e-9)
            
            # Tie-breaking: Prefer bins with less *initial* slack (smaller eligible_bins_remain_cap)
            # We want to maximize the negative of initial slack.
            # Scale initial slack to be a secondary factor.
            # Using the negative of initial capacities ensures smaller initial capacities
            # get higher (less negative) scores.
            # The scale should be such that the primary factor (normalized_residual) dominates.
            # A common approach is to use a large constant.
            scale_factor = 1e6 
            tiebreaker_scores = -non_exact_remain_caps * scale_factor

            # Combine primary (Best Fit) and secondary (tie-breaker) scores.
            # Primary goal: minimize normalized_residual (maximize -(normalized_residual))
            # Secondary goal: minimize initial slack (maximize -(initial_slack))
            # Score = PrimaryScore * Scale + SecondaryScore
            # We want to maximize score, so score = -(normalized_residual) * Scale + -(initial_slack)
            # Equivalently, for maximization:
            # Score = (1.0 - normalized_residual) * Scale + (-non_exact_remain_caps)
            # A larger (1.0 - normalized_residual) is better (less residual).
            # A larger (-non_exact_remain_caps) is better (less initial slack).
            
            # Let's reformulate to ensure maximization:
            # Maximize: (1 - normalized_residual) -- primary
            # Maximize: (-non_exact_remain_caps) -- secondary (tie-breaker)
            # Combined score: (1 - normalized_residual) * large_scale + (-non_exact_remain_caps)
            
            # Scores for non-exact fits should be less than 1.0 (exact fit priority)
            # The range of (1 - normalized_residual) is [0, 1].
            # To make it distinct and higher than 1.0 for exact fit, we can add 1.0.
            # Or, we can use a range like [0.5, 0.99] and assign 1.0 to exact fit.

            # Let's aim for scores less than 1.0, with higher meaning better.
            # Best Fit score: higher for smaller normalized_residual
            # Using (1 - normalized_residual) gives higher scores for better fits.
            # To incorporate tie-breaking, we use the scaled negative initial slack.
            # Combined score = (1 - normalized_residual) * scale_factor + (-non_exact_remain_caps)
            # This ensures that minimizing normalized residual is primary.
            # For identical normalized residuals, it picks the bin with less initial slack.

            # However, we need scores to be less than 1.0.
            # Let's use a base score for non-exact fits and add the tie-breaker.
            # A simple base score for Best Fit: 0.9 - normalized_residual (maps to [0.8, 0.9])
            # Then add tie-breaker, scaled down.
            # A better way to use tiebreaker with scale:
            # Primary score component: (1.0 - normalized_residual) - Higher is better
            # Secondary score component: (-non_exact_remain_caps) - Higher is better
            # Final Score = (PrimaryScore * Scale) + SecondaryScore
            # To ensure scores are below 1.0, we can scale (1.0 - normalized_residual).
            # Let's use a range like [0.5, 0.99].
            
            # For non-exact fits, map normalized_residual to a score.
            # Normalized residual is [0, 1] where 0 is best.
            # We want higher scores for better fits. So, use (1 - normalized_residual).
            # This gives scores in [0, 1].
            # Let's map this to [0.5, 0.95] to leave room for exact fit (1.0).
            # Scale factor for mapping [0, 1] to [0.5, 0.95] is 0.45.
            # Score = 0.5 + (1.0 - normalized_residual) * 0.45

            best_fit_scores = 0.5 + (1.0 - normalized_residual) * 0.45
            
            # Incorporate tie-breaker: penalize bins with more initial slack.
            # We want to maximize the negative of initial slack.
            # To avoid interfering with the primary score's range, scale it down significantly.
            # The tie-breaker should only influence choices when primary scores are equal.
            # Using the initial remaining capacity directly as a tie-breaker (smaller is better):
            # We want to maximize the negative of initial remaining capacity.
            # Add this to the best_fit_scores, scaled appropriately.
            # Example: scaled_tiebreaker = (-non_exact_remain_caps) / large_constant
            
            # A more robust way for lexicographical ordering is:
            # Score = Primary_Score_Component * Large_Scale + Secondary_Score_Component
            # Where Primary_Score_Component is (1.0 - normalized_residual).
            # Secondary_Score_Component is (-non_exact_remain_caps).
            # To keep scores below 1.0 for non-exact fits:
            # Score = (1.0 - normalized_residual) * (1.0 - epsilon) + (-non_exact_remain_caps) / scale
            # Let's directly combine them to reflect the BFD spirit:
            # Prioritize minimizing residual capacity, then minimizing initial slack.
            # Score = (-(remaining_after_fit)) * scale_factor + (-non_exact_remain_caps)
            # This is for maximization.

            # Let's use the approach that maps non-exact fits to [0.5, 0.95] and uses
            # negative initial slack as a tie-breaker.
            # The tie-breaker needs to be added in a way that it only affects choices
            # when the primary scores are very close.
            
            # Combining scores for non-exact fits:
            # Higher priority for smaller remaining_after_fit.
            # Among those, higher priority for smaller non_exact_remain_caps.
            
            # Let's create a composite score:
            # We want to maximize (1 - normalized_residual)
            # We want to maximize (-non_exact_remain_caps)
            # To ensure the first term dominates, scale it.
            # A common approach is: score = primary * scale_factor + secondary
            # If primary is in [0,1] and secondary is in [-Max, 0], and scale_factor is large.
            
            # Let's stick to a structure that ensures non-exact fits are < 1.0 and exact fits are 1.0.
            # For non-exact fits:
            # Primary objective: Minimize normalized_residual -> Score component: (1.0 - normalized_residual)
            # Secondary objective: Minimize initial slack -> Score component: (-non_exact_remain_caps)
            
            # We need to combine these such that the first term has more impact.
            # Using a weighted sum: alpha * (1.0 - normalized_residual) + beta * (-non_exact_remain_caps)
            # The weights alpha and beta can be chosen. Alpha should be larger.
            # Let alpha = 0.9 and beta = 0.1.
            # This gives scores in a range, but doesn't guarantee the range.
            
            # A robust way is to use lexicographical sorting implicitly.
            # Consider pairs: (primary_score, secondary_score)
            # Primary: (1.0 - normalized_residual)
            # Secondary: (-non_exact_remain_caps)
            # We want to maximize these lexicographically.
            
            # Let's simplify: use the mapped Best Fit score and add a scaled tie-breaker.
            # Score = best_fit_scores + (-non_exact_remain_caps) / 1000.0
            # This ensures the best_fit_scores component is dominant.
            
            scaled_tiebreaker = (-non_exact_remain_caps) / 1000.0 # Scale down tie-breaker
            combined_non_exact_scores = best_fit_scores + scaled_tiebreaker
            
            # Ensure scores are less than 1.0 (exact fit priority)
            # The current combination could exceed 1.0 if tiebreaker is very positive for some reason
            # (which it shouldn't be with negative initial capacities).
            # We can clip or rescale if needed, but given the structure, it should be fine.
            
            priorities[non_exact_indices] = combined_non_exact_scores

    return priorities
```
