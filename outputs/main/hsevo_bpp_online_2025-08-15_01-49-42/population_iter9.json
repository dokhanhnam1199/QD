[
  {
    "stdout_filepath": "problem_iter8_response0.txt_stdout.txt",
    "code_path": "problem_iter8_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines exact fit preference with scaled inverse normalized slack for non-exact fits.\"\"\"\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    eligible_bins_indices = np.where(can_fit_mask)[0]\n\n    # Prioritize exact fits with a high score of 1.0\n    exact_fit_mask = np.isclose(eligible_bins_remain_cap, item)\n    priorities[eligible_bins_indices[exact_fit_mask]] = 1.0\n\n    # For non-exact fits, prioritize bins with smaller normalized slack\n    non_exact_fit_mask = ~exact_fit_mask\n    if np.any(non_exact_fit_mask):\n        non_exact_eligible_bins_remain_cap = eligible_bins_remain_cap[non_exact_fit_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_after_fit = non_exact_eligible_bins_remain_cap - item\n        \n        # Original capacities for the eligible non-exact fit bins\n        original_capacities = bins_remain_cap[eligible_bins_indices[non_exact_fit_mask]]\n        \n        # Calculate normalized slack, ensuring stability with epsilon\n        normalized_slack = remaining_after_fit / (original_capacities + 1e-9)\n        \n        # Assign priorities as 1.0 minus normalized slack, scaled to be less than 1.0\n        # This rewards smaller normalized slack with higher priority values closer to 1.0.\n        priorities[eligible_bins_indices[non_exact_fit_mask]] = 0.99 * (1.0 - normalized_slack)\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 95.08241808752197,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response1.txt_stdout.txt",
    "code_path": "problem_iter8_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit, minimized residual capacity, and normalized slack for robust prioritization.\n    Prioritizes exact fits (score 1.0), then bins with minimal remaining capacity after fit,\n    using normalized slack as a tie-breaker.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_indices = np.where(can_fit_mask)[0]\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        remaining_after_fit = eligible_bins_remain_cap - item\n        \n        # Exact fit: highest priority\n        exact_fit_mask = (remaining_after_fit == 0)\n        priorities[eligible_indices[exact_fit_mask]] = 1.0\n        \n        # Non-exact fits\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_indices = eligible_indices[non_exact_fit_mask]\n            non_exact_remain_caps_after_fit = remaining_after_fit[non_exact_fit_mask]\n            non_exact_current_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n            \n            # Normalize remaining capacity after fit to rank non-exact fits.\n            # Smaller remaining capacity gets higher priority.\n            # Use a score that's inversely related to normalized remaining capacity.\n            # This combines the \"minimize residual\" idea with normalization.\n            epsilon = 1e-9\n            # To avoid division by zero or very small numbers, and to make scores distinct\n            # we can use a transformation that maps smaller values to larger scores.\n            # A simple approach is to use the negative of the normalized remaining capacity\n            # and scale it to be less than 1.0.\n            \n            # Let's try prioritizing bins that leave the *least* amount of space.\n            # This is minimizing `remaining_after_fit`.\n            # We want higher priority for smaller `remaining_after_fit`.\n            \n            # Using a score derived from inverse of remaining capacity after fit.\n            # Higher score for smaller remaining capacity after fit.\n            # Scale to differentiate from exact fits. A range like [0.5, 0.95] is good.\n            \n            # Normalize remaining capacity after fit to a [0, 1] range where 0 is best.\n            max_remaining_after_fit = np.max(non_exact_remain_caps_after_fit)\n            normalized_remaining_after_fit = non_exact_remain_caps_after_fit / (max_remaining_after_fit + epsilon)\n            \n            # Map normalized remaining capacity to a priority score.\n            # 0 (best residual) -> ~0.95, 1 (worst residual) -> ~0.5\n            best_fit_scores = 0.5 + (1.0 - normalized_remaining_after_fit) * 0.45\n            \n            priorities[non_exact_indices] = best_fit_scores\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 3.0,
    "halstead": 128.3789500201924,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response2.txt_stdout.txt",
    "code_path": "problem_iter8_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack using a scaled approach.\n    Combines the exact fit priority of v0/v9/v10 with the normalized slack approach of v9/v10.\n    The scoring ensures a clear hierarchy: exact fits > best fits (min normalized slack).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Exact Fit: Highest priority (1.0)\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # Best Fit: Prioritize bins with minimal positive remaining capacity after fitting.\n    # Consider bins that can fit the item and are not exact fits.\n    can_fit_mask = (bins_remain_cap >= item) & ~exact_fit_mask\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) > 0:\n        remaining_after_fit = bins_remain_cap[fit_indices] - item\n        current_capacities = bins_remain_cap[fit_indices]\n\n        # Calculate normalized slack: (remaining_capacity_after_fit) / (current_bin_capacity)\n        # Smaller normalized slack is better. Add epsilon for numerical stability.\n        normalized_slack = remaining_after_fit / (current_capacities + 1e-9)\n\n        # Assign priorities that are higher for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher scores.\n        # Scale these scores to be clearly less than 1.0, e.g., into the range [0.5, 0.99].\n        # This mirrors the effective scoring of v9/v10.\n        best_fit_scores = 1.0 - normalized_slack\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[fit_indices] = scaled_best_fit_priorities\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 136.3127518260917,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response3.txt_stdout.txt",
    "code_path": "problem_iter8_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack using a stable, tiered scoring.\n\n    This function assigns the highest priority to exact fits. For other bins that can\n    accommodate the item, it calculates a priority based on normalized slack.\n    The scoring is tiered to ensure clear separation between exact fits and other options,\n    and among different levels of slack.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Mask for bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n    fit_indices = np.where(can_fit_mask)[0]\n\n    if len(fit_indices) == 0:\n        return priorities # No bins can fit the item\n\n    # Separate bins into exact fits and potential fits\n    exact_fit_mask = bins_remain_cap[can_fit_mask] == item\n    exact_fit_indices_filtered = np.where(exact_fit_mask)[0]\n    actual_exact_fit_indices = fit_indices[exact_fit_indices_filtered]\n\n    # Assign highest priority to exact fits\n    priorities[actual_exact_fit_indices] = 1.0\n\n    # Process bins that are not exact fits but can still accommodate the item\n    non_exact_fit_mask = can_fit_mask & ~exact_fit_mask\n    non_exact_fit_indices = np.where(non_exact_fit_mask)[0]\n\n    if len(non_exact_fit_indices) > 0:\n        eligible_bins_for_slack = bins_remain_cap[non_exact_fit_indices]\n        \n        # Calculate remaining capacity after fitting the item\n        remaining_after_fit = eligible_bins_for_slack - item\n        \n        # Calculate normalized slack: (remaining_capacity_after_fit) / (current_bin_capacity)\n        # A smaller normalized slack is better. Add epsilon for numerical stability.\n        # Using the current remaining capacity as a proxy for original bin capacity for normalization.\n        normalized_slack = remaining_after_fit / (eligible_bins_for_slack + 1e-9)\n\n        # Assign priorities for non-exact fits.\n        # We want to favor smaller normalized slack. The scoring should be less than 1.0 (exact fit score)\n        # and greater than 0.0 (default for non-fitting bins).\n        # We use 1.0 - normalized_slack to map smaller slack to higher scores.\n        # Scale these scores to be clearly less than 1.0, e.g., into the range [0.5, 0.99].\n        # A linear scaling: 0.5 + (1.0 - normalized_slack) * 0.49\n        # This approach combines the best aspects of prioritizing exact fits and using\n        # normalized slack for fine-grained selection among other options.\n        best_fit_scores = 1.0 - normalized_slack\n        scaled_best_fit_priorities = 0.5 + best_fit_scores * 0.49\n\n        priorities[non_exact_fit_indices] = scaled_best_fit_priorities\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 168.25742227201613,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response4.txt_stdout.txt",
    "code_path": "problem_iter8_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Combines Best Fit with a penalty for large initial slacks, prioritizing exact fits.\n\n    This heuristic prioritizes exact fits with a high score. For non-exact fits,\n    it favors bins with minimal remaining capacity, applying a penalty based on\n    the bin's original slack to encourage using bins that were already fuller.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Score 1: Prioritize exact fits with a very high score.\n        exact_fit_mask = fitting_bins_remain_cap == item\n        priorities[can_fit_mask][exact_fit_mask] = 1e6 \n\n        # Score 2: For non-exact fits, prioritize minimizing remaining capacity (Best Fit).\n        # We want to maximize -(remaining_after_fit).\n        non_exact_fit_mask = ~exact_fit_mask\n        if np.any(non_exact_fit_mask):\n            non_exact_fitting_bins_remain_cap = fitting_bins_remain_cap[non_exact_fit_mask]\n            remaining_after_fit = non_exact_fitting_bins_remain_cap - item\n            \n            # Score 3: Tie-breaking - penalize bins with large initial slack.\n            # This encourages using bins that were already closer to full.\n            # We use the negative of original remaining capacity as a secondary scoring metric.\n            # A smaller initial remaining capacity (larger negative value) is better.\n            \n            # Combine Best Fit score and initial slack penalty.\n            # Maximize -(remaining_after_fit) for Best Fit.\n            # Maximize -(fitting_bins_remain_cap) for secondary tie-breaking.\n            # Use a large scale factor to ensure Best Fit dominates.\n            scale_factor = 1e6  # Ensures Best Fit objective is dominant\n            \n            # Priorities for non-exact fits: max(-remaining_after_fit * scale_factor - fitting_bins_remain_cap)\n            combined_priorities = -remaining_after_fit * scale_factor - non_exact_fitting_bins_remain_cap\n            \n            priorities[can_fit_mask][non_exact_fit_mask] = combined_priorities\n\n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 3.0,
    "halstead": 84.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response5.txt_stdout.txt",
    "code_path": "problem_iter8_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit priority with a scaled Best Fit strategy,\n    using normalized slack and a tie-breaker for bins with less initial slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        eligible_indices = np.where(can_fit_mask)[0]\n\n        # Exact fit has the highest priority\n        exact_fit_mask = (eligible_bins_remain_cap == item)\n        priorities[eligible_indices[exact_fit_mask]] = 1.0\n        \n        # For non-exact fits, use Best Fit by minimizing remaining capacity\n        non_exact_fit_mask = (eligible_bins_remain_cap > item)\n        non_exact_indices = eligible_indices[non_exact_fit_mask]\n        non_exact_remain_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n\n        if len(non_exact_remain_caps) > 0:\n            remaining_after_fit = non_exact_remain_caps - item\n            \n            # Normalize remaining capacity to [0, 1], where 0 is best (min residual)\n            max_residual = np.max(remaining_after_fit)\n            # Add epsilon for stability if all residuals are identical\n            normalized_residual = remaining_after_fit / (max_residual + 1e-9)\n            \n            # Tie-breaking: Prefer bins with less *initial* slack (smaller eligible_bins_remain_cap)\n            # We want to maximize the negative of initial slack.\n            # Scale initial slack to be a secondary factor.\n            # Using the negative of initial capacities ensures smaller initial capacities\n            # get higher (less negative) scores.\n            # The scale should be such that the primary factor (normalized_residual) dominates.\n            # A common approach is to use a large constant.\n            scale_factor = 1e6 \n            tiebreaker_scores = -non_exact_remain_caps * scale_factor\n\n            # Combine primary (Best Fit) and secondary (tie-breaker) scores.\n            # Primary goal: minimize normalized_residual (maximize -(normalized_residual))\n            # Secondary goal: minimize initial slack (maximize -(initial_slack))\n            # Score = PrimaryScore * Scale + SecondaryScore\n            # We want to maximize score, so score = -(normalized_residual) * Scale + -(initial_slack)\n            # Equivalently, for maximization:\n            # Score = (1.0 - normalized_residual) * Scale + (-non_exact_remain_caps)\n            # A larger (1.0 - normalized_residual) is better (less residual).\n            # A larger (-non_exact_remain_caps) is better (less initial slack).\n            \n            # Let's reformulate to ensure maximization:\n            # Maximize: (1 - normalized_residual) -- primary\n            # Maximize: (-non_exact_remain_caps) -- secondary (tie-breaker)\n            # Combined score: (1 - normalized_residual) * large_scale + (-non_exact_remain_caps)\n            \n            # Scores for non-exact fits should be less than 1.0 (exact fit priority)\n            # The range of (1 - normalized_residual) is [0, 1].\n            # To make it distinct and higher than 1.0 for exact fit, we can add 1.0.\n            # Or, we can use a range like [0.5, 0.99] and assign 1.0 to exact fit.\n\n            # Let's aim for scores less than 1.0, with higher meaning better.\n            # Best Fit score: higher for smaller normalized_residual\n            # Using (1 - normalized_residual) gives higher scores for better fits.\n            # To incorporate tie-breaking, we use the scaled negative initial slack.\n            # Combined score = (1 - normalized_residual) * scale_factor + (-non_exact_remain_caps)\n            # This ensures that minimizing normalized residual is primary.\n            # For identical normalized residuals, it picks the bin with less initial slack.\n\n            # However, we need scores to be less than 1.0.\n            # Let's use a base score for non-exact fits and add the tie-breaker.\n            # A simple base score for Best Fit: 0.9 - normalized_residual (maps to [0.8, 0.9])\n            # Then add tie-breaker, scaled down.\n            # A better way to use tiebreaker with scale:\n            # Primary score component: (1.0 - normalized_residual) - Higher is better\n            # Secondary score component: (-non_exact_remain_caps) - Higher is better\n            # Final Score = (PrimaryScore * Scale) + SecondaryScore\n            # To ensure scores are below 1.0, we can scale (1.0 - normalized_residual).\n            # Let's use a range like [0.5, 0.99].\n            \n            # For non-exact fits, map normalized_residual to a score.\n            # Normalized residual is [0, 1] where 0 is best.\n            # We want higher scores for better fits. So, use (1 - normalized_residual).\n            # This gives scores in [0, 1].\n            # Let's map this to [0.5, 0.95] to leave room for exact fit (1.0).\n            # Scale factor for mapping [0, 1] to [0.5, 0.95] is 0.45.\n            # Score = 0.5 + (1.0 - normalized_residual) * 0.45\n\n            best_fit_scores = 0.5 + (1.0 - normalized_residual) * 0.45\n            \n            # Incorporate tie-breaker: penalize bins with more initial slack.\n            # We want to maximize the negative of initial slack.\n            # To avoid interfering with the primary score's range, scale it down significantly.\n            # The tie-breaker should only influence choices when primary scores are equal.\n            # Using the initial remaining capacity directly as a tie-breaker (smaller is better):\n            # We want to maximize the negative of initial remaining capacity.\n            # Add this to the best_fit_scores, scaled appropriately.\n            # Example: scaled_tiebreaker = (-non_exact_remain_caps) / large_constant\n            \n            # A more robust way for lexicographical ordering is:\n            # Score = Primary_Score_Component * Large_Scale + Secondary_Score_Component\n            # Where Primary_Score_Component is (1.0 - normalized_residual).\n            # Secondary_Score_Component is (-non_exact_remain_caps).\n            # To keep scores below 1.0 for non-exact fits:\n            # Score = (1.0 - normalized_residual) * (1.0 - epsilon) + (-non_exact_remain_caps) / scale\n            # Let's directly combine them to reflect the BFD spirit:\n            # Prioritize minimizing residual capacity, then minimizing initial slack.\n            # Score = (-(remaining_after_fit)) * scale_factor + (-non_exact_remain_caps)\n            # This is for maximization.\n\n            # Let's use the approach that maps non-exact fits to [0.5, 0.95] and uses\n            # negative initial slack as a tie-breaker.\n            # The tie-breaker needs to be added in a way that it only affects choices\n            # when the primary scores are very close.\n            \n            # Combining scores for non-exact fits:\n            # Higher priority for smaller remaining_after_fit.\n            # Among those, higher priority for smaller non_exact_remain_caps.\n            \n            # Let's create a composite score:\n            # We want to maximize (1 - normalized_residual)\n            # We want to maximize (-non_exact_remain_caps)\n            # To ensure the first term dominates, scale it.\n            # A common approach is: score = primary * scale_factor + secondary\n            # If primary is in [0,1] and secondary is in [-Max, 0], and scale_factor is large.\n            \n            # Let's stick to a structure that ensures non-exact fits are < 1.0 and exact fits are 1.0.\n            # For non-exact fits:\n            # Primary objective: Minimize normalized_residual -> Score component: (1.0 - normalized_residual)\n            # Secondary objective: Minimize initial slack -> Score component: (-non_exact_remain_caps)\n            \n            # We need to combine these such that the first term has more impact.\n            # Using a weighted sum: alpha * (1.0 - normalized_residual) + beta * (-non_exact_remain_caps)\n            # The weights alpha and beta can be chosen. Alpha should be larger.\n            # Let alpha = 0.9 and beta = 0.1.\n            # This gives scores in a range, but doesn't guarantee the range.\n            \n            # A robust way is to use lexicographical sorting implicitly.\n            # Consider pairs: (primary_score, secondary_score)\n            # Primary: (1.0 - normalized_residual)\n            # Secondary: (-non_exact_remain_caps)\n            # We want to maximize these lexicographically.\n            \n            # Let's simplify: use the mapped Best Fit score and add a scaled tie-breaker.\n            # Score = best_fit_scores + (-non_exact_remain_caps) / 1000.0\n            # This ensures the best_fit_scores component is dominant.\n            \n            scaled_tiebreaker = (-non_exact_remain_caps) / 1000.0 # Scale down tie-breaker\n            combined_non_exact_scores = best_fit_scores + scaled_tiebreaker\n            \n            # Ensure scores are less than 1.0 (exact fit priority)\n            # The current combination could exceed 1.0 if tiebreaker is very positive for some reason\n            # (which it shouldn't be with negative initial capacities).\n            # We can clip or rescale if needed, but given the structure, it should be fine.\n            \n            priorities[non_exact_indices] = combined_non_exact_scores\n\n    return priorities",
    "response_id": 5,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 3.0,
    "halstead": 220.81007680238335,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response6.txt_stdout.txt",
    "code_path": "problem_iter8_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Prioritizes exact fits with a high score, then uses scaled inverse slack for others.\n\n    Combines the exact-fit emphasis of v0 with a more nuanced inverse slack calculation\n    inspired by the spirit of other heuristics, ensuring distinct scores and robustness.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    suitable_bins_indices = np.where(can_fit_mask)[0]\n    suitable_bins_remain_cap = bins_remain_cap[suitable_bins_indices]\n\n    # Prioritize exact fits with a high score\n    exact_fit_mask = suitable_bins_remain_cap == item\n    priorities[suitable_bins_indices[exact_fit_mask]] = 1.0\n\n    # For non-exact fits, calculate a scaled inverse slack\n    non_exact_fit_mask = ~exact_fit_mask\n    if np.any(non_exact_fit_mask):\n        non_exact_bins_remain_cap = suitable_bins_remain_cap[non_exact_fit_mask]\n        remaining_after_fit = non_exact_bins_remain_cap - item\n\n        # Use a scaled inverse of (remaining_after_fit + 1) to give higher priority to tighter fits.\n        # Adding 1 to remaining_after_fit ensures that a remaining capacity of 0 (after fitting)\n        # gets a score of 1.0, and smaller positive remaining capacities get scores < 1.0.\n        # A small epsilon prevents division by zero.\n        priorities[suitable_bins_indices[non_exact_fit_mask]] = 0.5 * (1.0 / (remaining_after_fit + 1.0 + 1e-9))\n\n    return priorities",
    "response_id": 6,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 3.0,
    "halstead": 109.80793556946902,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response7.txt_stdout.txt",
    "code_path": "problem_iter8_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid heuristic: exact fit, then best-fit by minimizing remaining capacity,\n    with a secondary preference for bins with less initial slack.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        eligible_indices = np.where(can_fit_mask)[0]\n\n        # Exact fit: highest priority\n        exact_fit_mask = (eligible_bins_remain_cap == item)\n        if np.any(exact_fit_mask):\n            priorities[eligible_indices[exact_fit_mask]] = 1.0\n        \n        # For non-exact fits, prioritize bins that leave the minimum remaining capacity.\n        # This is equivalent to maximizing -(remaining_capacity - item).\n        # To ensure these scores are distinct from exact fits (1.0) and\n        # to provide a meaningful range for best-fit, we scale this value.\n        non_exact_fit_mask = (eligible_bins_remain_cap > item)\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_indices = eligible_indices[non_exact_fit_mask]\n            non_exact_remain_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n            \n            # Calculate scores for non-exact fits: maximize ( -(remaining_capacity - item) )\n            # Score is higher for smaller remaining capacity after fitting.\n            # Scale these scores to be less than 1.0, e.g., in the range [0.5, 0.95].\n            \n            remaining_after_fit = non_exact_remain_caps - item\n            \n            # Normalize residuals to [0, 1], where 0 is best (min residual)\n            max_remaining_after_fit = np.max(remaining_after_fit)\n            # Add epsilon for stability if all residuals are the same\n            normalized_remaining = remaining_after_fit / (max_remaining_after_fit + 1e-9)\n            \n            # Scale to a range below 1.0, e.g., [0.5, 0.95].\n            # This maps best-fit (normalized_remaining near 0) to ~0.95\n            # and worst-fit (normalized_remaining near 1) to ~0.5.\n            best_fit_scores = 0.5 + (1.0 - normalized_remaining) * 0.45\n            \n            # Assign these scores to the non-exact fits\n            priorities[non_exact_indices] = best_fit_scores\n\n    return priorities",
    "response_id": 7,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 4.0,
    "halstead": 129.32351694048162,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter8_response8.txt_stdout.txt",
    "code_path": "problem_iter8_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray, bin_original_caps: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Combines exact fit, minimized normalized slack, and a tie-breaker.\n    Prioritizes exact fits, then bins with minimal normalized slack, using\n    remaining capacity as a tie-breaker.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # Mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        eligible_indices = np.where(can_fit_mask)[0]\n        eligible_bins_remain_cap = bins_remain_cap[eligible_indices]\n        eligible_bins_original_caps = bin_original_caps[eligible_indices]\n\n        # 1. Exact Fit: Highest Priority\n        exact_fit_mask = (eligible_bins_remain_cap == item)\n        if np.any(exact_fit_mask):\n            priorities[eligible_indices[exact_fit_mask]] = 1e9  # Very high score for exact fits\n\n        # 2. Non-Exact Fits: Minimize Normalized Slack\n        # Normalized slack = (remaining_capacity - item) / original_bin_capacity\n        # We want to maximize -(normalized_slack) to prioritize smaller slack.\n        non_exact_fit_mask = ~exact_fit_mask\n        \n        if np.any(non_exact_fit_mask):\n            non_exact_indices = eligible_indices[non_exact_fit_mask]\n            current_remain_caps = eligible_bins_remain_cap[non_exact_fit_mask]\n            original_caps = eligible_bins_original_caps[non_exact_fit_mask]\n\n            # Calculate normalized slack. Add epsilon to denominator for stability.\n            normalized_slack = (current_remain_caps - item) / (original_caps + 1e-9)\n            \n            # To make scores distinct from exact fits and order them,\n            # we use a scaled negative normalized slack.\n            # The scale factor ensures they are lower than exact fits.\n            # We also add a term based on remaining capacity as a tie-breaker:\n            # Larger remaining capacity after fit is worse, so we use negative remaining capacity.\n            # The overall score for non-exact fits: -normalized_slack - (remaining_after_fit / original_capacity)\n            # We want to maximize this score.\n            \n            remaining_after_fit = current_remain_caps - item\n            \n            # Score combines minimizing normalized slack and minimizing residual capacity.\n            # The goal is to maximize the combined score.\n            # Higher priority for lower normalized slack and lower residual capacity after fit.\n            # Using negative values to maximize.\n            score_normalized_slack = -normalized_slack\n            score_residual = -(remaining_after_fit / (original_caps + 1e-9))\n\n            # Combine scores: Prioritize normalized slack more, then residual capacity.\n            # Scale normalized slack to have a larger impact.\n            combined_score = score_normalized_slack * 100 + score_residual\n            \n            priorities[non_exact_indices] = combined_score\n\n    return priorities",
    "response_id": 8,
    "tryHS": false,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\nTypeError: priority_v2() missing 1 required positional argument: 'bin_original_caps'\n4\n178.61670928936152\n"
  },
  {
    "stdout_filepath": "problem_iter8_response9.txt_stdout.txt",
    "code_path": "problem_iter8_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritizes exact fits, then bins with minimal normalized slack, scaled for clarity.\n\n    Combines exact fit priority with a scaled inverse normalized slack for\n    non-exact fits, ensuring distinct and robust scoring.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    # High priority for exact fits\n    exact_fit_mask = np.isclose(bins_remain_cap, item)\n    priorities[exact_fit_mask] = 1.0\n\n    # For bins that can fit but are not exact fits\n    can_fit_and_not_exact_mask = bins_remain_cap > item\n    \n    if np.any(can_fit_and_not_exact_mask):\n        eligible_bins_remain_cap = bins_remain_cap[can_fit_and_not_exact_mask]\n        \n        # Calculate remaining capacity after placing the item\n        remaining_capacities_if_fit = eligible_bins_remain_cap - item\n        \n        # Normalized slack: remaining capacity / current bin capacity. Smaller is better.\n        epsilon = 1e-9\n        normalized_slack = remaining_capacities_if_fit / (eligible_bins_remain_cap + epsilon)\n        \n        # Priority: Higher score for smaller normalized slack.\n        # We use 1.0 - normalized_slack to map smaller slack to higher priority (closer to 1.0).\n        # Scale these scores to be distinct from exact fits (e.g., between 0.5 and 0.99).\n        # A simple mapping: 0.5 + 0.49 * (1.0 - normalized_slack)\n        # This ensures non-exact fits are always lower than exact fits and have a good range.\n        non_exact_priorities = 0.5 + 0.49 * (1.0 - normalized_slack)\n        \n        priorities[can_fit_and_not_exact_mask] = non_exact_priorities\n\n    return priorities",
    "response_id": 9,
    "tryHS": false,
    "obj": 4.198244914240141,
    "cyclomatic_complexity": 2.0,
    "halstead": 95.90827503317318,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response0.txt_stdout.txt",
    "code_path": "problem_iter9_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Multi-objective prioritization for online Bin Packing.\n\n    This strategy aims to find a balance between filling bins as much as possible\n    (to minimize bin count) and ensuring a good fit to avoid fragmentation.\n    It prioritizes bins that can fit the item and assigns scores based on two\n    criteria:\n    1.  How \"tight\" the fit is: Prioritizes bins where the remaining capacity\n        after placing the item is minimized (similar to 'Almost Full Fit').\n    2.  How much capacity is left relative to the item size: Prioritizes bins\n        that have a moderate amount of remaining capacity, to leave room for\n        future items and avoid packing very small items into nearly full bins\n        which might be better utilized by slightly larger items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0, dtype=float)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_indices = np.where(can_fit_mask)[0]\n        \n        # Criterion 1: Minimize remaining capacity after fitting (tight fit)\n        # This is the negative of the slack. Higher priority for smaller slack.\n        slack = bins_remain_cap[available_bins_indices] - item\n        tight_fit_score = -slack\n        \n        # Criterion 2: Consider the relative remaining capacity.\n        # We want to avoid bins that become *too* full or *too* empty after fit.\n        # A simple way is to penalize extreme remaining capacities.\n        # For instance, a bin with 0 remaining capacity is not ideal if the item\n        # was much smaller than the bin's original capacity.\n        # Let's try to reward bins that leave a 'reasonable' amount of space.\n        # A common heuristic is to look at the ratio of remaining capacity to bin capacity,\n        # or simply the absolute remaining capacity.\n        # Here, we'll use a score that favors bins with moderate remaining capacity.\n        # A parabolic function centered around some ideal slack could work,\n        # or a simple linear penalty for very small/large remainders.\n        # Let's use a score that is higher for slack values closer to a certain target.\n        # A simpler approach is to reward bins with remaining capacity that is not too small,\n        # but not excessively large either.\n        # Let's prioritize bins where `bins_remain_cap` is around `item` * 2, for example.\n        # This would mean `bins_remain_cap - item` is around `item`.\n        # A gaussian-like function centered at `item` for `bins_remain_cap` could work.\n        # For simplicity, let's consider a function that rewards bins with remaining capacity\n        # that's not excessively large.\n        # We can scale the remaining capacity and take its negative logarithm, or\n        # use a Gaussian-like function centered around an \"ideal\" remaining capacity.\n        # Let's define an \"ideal\" remaining capacity as, say, half of the item size.\n        # ideal_slack = item / 2.0\n        # deviation_from_ideal = np.abs(slack - ideal_slack)\n        # proportional_slack_score = -deviation_from_ideal # Higher score for closer to ideal\n        \n        # A more robust approach: Penalize bins that are almost empty or almost full.\n        # Let's aim for a remaining capacity that is roughly equal to the item size,\n        # suggesting a balanced use of space.\n        # Score = 1 / (1 + (slack - item)^2) for slack around item.\n        # Or, simply, penalize large slacks.\n        # Let's use a score that is inversely proportional to the slack, but with a\n        # floor to avoid extremely high scores for very small slacks.\n        # Also, we don't want to encourage using bins that are almost empty if a tighter\n        # fit is available.\n\n        # Let's combine the two:\n        # 1. Tightness: -slack (maximize)\n        # 2. Moderate remaining capacity: encourage slack not to be too small or too large.\n        #    A common idea is to use capacity utilization. High utilization is good,\n        #    but too high might be bad for fragmentation.\n        #    Let's prioritize bins where the remaining capacity `r` is such that\n        #    `r / bin_original_capacity` is in a good range.\n        #    Since we don't have bin_original_capacity, we can use `r / (r + item)`.\n        #    We want this ratio to be not too close to 0 or 1.\n        #    Let's try a function that rewards slack values that are not extremely small.\n        #    A simple transformation: `slack / (slack + item)` which is the fill percentage.\n        #    We want to avoid fill percentage close to 1 (very tight fit) and close to 0 (very loose fit).\n        #    A score that is high for fill percentage around 0.5 might be good.\n        #    Let's use a score that is inversely proportional to `slack`.\n        \n        # A more refined approach: Prioritize bins that lead to a fill rate\n        # of the *bin* (after placing the item) that is between, say, 0.6 and 0.9.\n        # Fill rate = (original_capacity - remaining_capacity) / original_capacity\n        # We don't have original_capacity.\n        # Let's simplify: use the remaining capacity itself.\n        # Prioritize bins where `slack` is not too small (e.g., `slack > item * 0.1`)\n        # and not too large (e.g., `slack < item * 2.0`).\n        \n        # Let's focus on a score that balances tightness and leaving some space.\n        # A good heuristic might be to prioritize bins that leave a remaining capacity\n        # that is as close as possible to the *average* remaining capacity of bins that can fit the item,\n        # or some other target value.\n        \n        # For this version, let's combine the \"tight fit\" score with a penalty for\n        # very large remaining capacities.\n        # We want to maximize `tight_fit_score` (-slack).\n        # We also want to penalize large `slack` values.\n        # Let's try a function that is `-slack - alpha * slack^2` or `-slack + f(slack)`\n        # where f(slack) is a decreasing function for large slack.\n        \n        # A simpler, robust approach:\n        # Prioritize bins that are \"almost full\" (small slack) but also provide some\n        # buffer. Let's try to reward bins where `slack` is small but positive.\n        # We can use a score like `1.0 / (slack + epsilon)` for small slack, and\n        # perhaps a lower score or even negative for very large slack.\n        \n        # Let's try to achieve a \"balanced\" fill. If an item is size `i`, we prefer\n        # a bin with remaining capacity `r` such that `r` is close to `i`.\n        # This means `slack` (which is `r - i`) should be close to 0.\n        # So, the previous \"Almost Full Fit\" priority of `-slack` is good.\n        # However, it might pick bins that leave only a tiny sliver of space,\n        # which might not be good if a slightly larger item comes next.\n        \n        # Let's introduce a penalty for very small remaining capacities *after* fit (very tight fit).\n        # If `slack` is very small, we might want to penalize it.\n        # Consider the `tight_fit_score = -slack`.\n        # For bins where `slack` is very close to zero (e.g., `slack < epsilon`),\n        # let's reduce their score.\n        \n        # Let's use a score that combines tightness and a preference for not being *too* tight.\n        # Score = `tight_fit_score` - penalty_for_very_tight_fit\n        # Penalty could be `exp(-slack / scale)` where scale is small.\n        # Or, more simply, just `tight_fit_score` but with a cap on how small slack can be to get max priority.\n        \n        # Let's go with a score that rewards tightness, but penalizes excessively large slack.\n        # The \"tight fit\" score is `-slack`. This rewards smaller slacks.\n        # For bins that have very large slack, their `-slack` score will be very negative,\n        # naturally giving them lower priority.\n        # The original v1 function already does this: `priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n        \n        # To improve, we need to differentiate scores more intelligently.\n        # Let's consider the *ratio* of remaining capacity to the item size.\n        # If `slack` is very small compared to `item`, it's a tight fit.\n        # If `slack` is very large compared to `item`, it's a loose fit.\n        # A \"good\" fit might be when `slack` is somewhere in between.\n        # Let's define a \"desirability\" score.\n        # A bin is desirable if `slack` is small, but not *too* small.\n        # Let's try a score that's higher for `slack` values closer to `item * 0.5` (a guess for a balanced state).\n        # Score = `1.0 / (1.0 + (slack - item * 0.5)**2)`\n        # This score is maximized when `slack` is `item * 0.5`.\n        # However, we also want to prioritize tighter fits more.\n        \n        # Let's try a hybrid:\n        # Primary goal: Minimize slack (-slack).\n        # Secondary goal: If multiple bins have very similar small slack,\n        #                 then differentiate based on how \"full\" they become.\n        #                 A bin that becomes 90% full is better than one that becomes 70% full\n        #                 if their remaining capacities are very close.\n        \n        # Let's define the score as:\n        # Score = -slack - alpha * (slack / item)  where alpha is a small positive number.\n        # This prioritizes small slack, but then slightly penalizes bins that have\n        # proportionally larger remaining capacity.\n        # This might encourage tighter packing overall.\n        \n        # Consider the First Fit Decreasing (FFD) approach intuition: sorting items helps.\n        # In online, we don't sort. So, the choice of bin is critical.\n        \n        # Let's try to make the \"almost full\" idea more nuanced.\n        # Instead of just minimizing slack, let's consider the *percentage* of the bin\n        # that the item occupies.\n        # We don't know the bin's original capacity.\n        # So, we can only work with `bins_remain_cap` and `item`.\n        \n        # Let's refine the `tight_fit_score`.\n        # `tight_fit_score = -slack` (higher is better).\n        # We want to differentiate bins that have small slacks.\n        # For bins that can fit the item:\n        # 1. Calculate `slack = bins_remain_cap[available_bins_indices] - item`\n        # 2. We want to prioritize smaller `slack`. So, `-slack` is a good starting point.\n        # 3. Let's add a term that rewards bins that are not \"too empty\" after the item is placed.\n        #    This means `slack` should not be excessively large.\n        #    A simple penalty for large slack: `-slack / item` (if item > 0).\n        #    Or `-(slack / (slack + item))` which is `-(remaining_capacity / original_capacity_estimate)`.\n        #    This is the inverse of fill rate.\n        #    Let's use a score like `-slack - C * (slack / item)` for a constant C.\n        #    If C is positive, it penalizes larger slacks.\n        \n        # Example: item = 0.5, bins_remain_cap = [1.0, 0.6, 0.8]\n        # Bins that fit: [1.0, 0.6, 0.8]\n        # Slacks:        [0.5, 0.1, 0.3]\n        # -slack:        [-0.5, -0.1, -0.3] (Bin 2 is best so far)\n        \n        # Let's add a penalty for large slack, e.g., `slack / item`.\n        # Item = 0.5.\n        # slack/item: [1.0, 0.2, 0.6]\n        # Proposed score: `-slack - 0.1 * (slack / item)`\n        # Bin 1 (cap 1.0): -0.5 - 0.1 * 1.0 = -0.6\n        # Bin 2 (cap 0.6): -0.1 - 0.1 * 0.2 = -0.1 - 0.02 = -0.12 (Still best)\n        # Bin 3 (cap 0.8): -0.3 - 0.1 * 0.6 = -0.3 - 0.06 = -0.36\n        \n        # This approach seems reasonable: prioritize tight fits, but slightly penalize\n        # bins that have a lot of remaining space relative to the item size.\n        # The constant `0.1` is a hyperparameter.\n        \n        # Let's use a more robust calculation that avoids division by zero if item is 0.\n        # if item is very small, slack / item can be unstable.\n        # Better: normalize slack by a typical bin size or a large constant.\n        # Or, simply use `slack` directly and add a term that penalizes its magnitude.\n        \n        # Let's use: `-slack - slack_penalty_factor * slack`\n        # This would amplify the penalty for larger slacks.\n        # E.g., `slack_penalty_factor = 0.1`\n        # Bin 1: -0.5 - 0.1 * 0.5 = -0.55\n        # Bin 2: -0.1 - 0.1 * 0.1 = -0.11\n        # Bin 3: -0.3 - 0.1 * 0.3 = -0.33\n        \n        # This is essentially `-(1 + slack_penalty_factor) * slack`.\n        # It still prioritizes the smallest slack.\n        \n        # Let's try to make the penalty dependent on the *relative* slack.\n        # The ratio `slack / item` captures this.\n        # A robust way to handle `item = 0`: if item is 0, slack is just remaining capacity.\n        # In that case, we'd prefer bins with less remaining capacity (to keep them full).\n        # So, `-slack` is still good.\n        \n        # Let's combine the tight fit score (-slack) with a preference for\n        # remaining capacity `r` such that `r` is not excessively small or large.\n        # Consider a function that is maximized when `slack` is near `item / 2`.\n        # `score = - (slack - item / 2.0)**2`\n        # This rewards bins that leave approximately half the item's size as remaining space.\n        # But this contradicts the \"almost full\" idea.\n        \n        # Let's combine:\n        # 1. Tightness: `-slack` (primary objective)\n        # 2. Avoidance of very tight fits: Penalize small `slack`.\n        #    If `slack < epsilon`, give a higher penalty.\n        #    A simple penalty: `min(0, slack - epsilon)`. This is 0 if slack >= epsilon,\n        #    and `slack - epsilon` if slack < epsilon. It's negative, so it reduces the score.\n        \n        epsilon_tight = 1e-6  # Threshold for \"very tight fit\"\n        \n        # Calculate scores based on tightness: higher is better\n        tight_fit_scores = -slack\n        \n        # Introduce a penalty for very tight fits\n        # If slack is less than epsilon_tight, we subtract a value that increases as slack gets smaller.\n        # The penalty should be proportional to how much slack is *below* epsilon_tight.\n        # So, penalty = `(epsilon_tight - slack)` if `slack < epsilon_tight`, else 0.\n        # To make it a score reduction, we add this penalty multiplied by a factor.\n        penalty_factor_tight = 1.0  # How much to penalize very tight fits\n        \n        very_tight_penalty = np.maximum(0, epsilon_tight - slack) * penalty_factor_tight\n        \n        # The final score is the tight fit score minus the penalty\n        combined_scores = tight_fit_scores - very_tight_penalty\n        \n        priorities[available_bins_indices] = combined_scores\n        \n        # Let's test this logic with an example:\n        # item = 0.5\n        # bins_remain_cap = [0.6, 0.55, 1.0, 0.8]\n        # Can fit: [0.6, 0.55, 1.0, 0.8]\n        # Slacks:  [0.1, 0.05, 0.5, 0.3]\n        # epsilon_tight = 1e-6 (very small)\n        # penalty_factor_tight = 1.0\n        \n        # Bin 1 (cap 0.6): slack = 0.1\n        #   tight_fit_score = -0.1\n        #   very_tight_penalty = max(0, 1e-6 - 0.1) * 1.0 = 0\n        #   combined_score = -0.1 - 0 = -0.1\n        \n        # Bin 2 (cap 0.55): slack = 0.05\n        #   tight_fit_score = -0.05\n        #   very_tight_penalty = max(0, 1e-6 - 0.05) * 1.0 = 0\n        #   combined_score = -0.05 - 0 = -0.05 (This bin would be chosen if only tight fit was used)\n        \n        # Bin 3 (cap 1.0): slack = 0.5\n        #   tight_fit_score = -0.5\n        #   very_tight_penalty = max(0, 1e-6 - 0.5) * 1.0 = 0\n        #   combined_score = -0.5 - 0 = -0.5\n        \n        # Bin 4 (cap 0.8): slack = 0.3\n        #   tight_fit_score = -0.3\n        #   very_tight_penalty = max(0, 1e-6 - 0.3) * 1.0 = 0\n        #   combined_score = -0.3 - 0 = -0.3\n        \n        # Result: [-0.1, -0.05, -0.5, -0.3]. Bin 2 is chosen.\n        # This new heuristic did not change the outcome in this specific case because\n        # the slacks were not small enough to trigger the penalty.\n        \n        # Let's make the epsilon tighter for the penalty.\n        # Or, let's rethink the \"avoiding very tight fits\".\n        # If we have slack = 0.01 and slack = 0.001, we prefer 0.01.\n        # `-slack` makes 0.001 -> -0.001 (better) and 0.01 -> -0.01 (worse).\n        # This is incorrect if we want to penalize very tight fits.\n        \n        # Let's reverse the logic for the penalty:\n        # If `slack` is very small, its score should be *reduced*.\n        # `tight_fit_score = -slack` (higher is better)\n        # If `slack` is very small (e.g., `slack < epsilon`), we want to *lower* its score.\n        # So, we should *subtract* a positive value for small `slack`.\n        # Let the score be `-slack + penalty_for_large_slack`.\n        # Or `-slack - penalty_for_very_small_slack`.\n        \n        # Let's try a score that is higher for slack closer to a moderate value,\n        # and also considers tightness.\n        # Score = `tight_fit_score` + `moderate_slack_preference`\n        # `tight_fit_score = -slack`\n        # `moderate_slack_preference`: A function that is higher when slack is, say,\n        # around `item / 2`.\n        # For example, `gaussian_like(slack, center=item/2, width=item)`\n        # `gaussian_like(x, center, width) = exp(-(x - center)**2 / (2 * width**2))`\n        \n        # Let's try a simpler combination:\n        # The primary goal is `slack` minimization.\n        # Secondary goal: If slacks are very close, pick the one with more remaining capacity.\n        # This is \"Best Fit\" vs \"Almost Full Fit\".\n        \n        # The V1 heuristic is \"almost full fit\" which means minimizing slack.\n        # It's equivalent to maximizing `-slack`.\n        \n        # What if we want to avoid bins that become *too* full?\n        # E.g., if item=0.1 and bin_cap=1.0, remaining is 0.9. This is a bad fit for small items.\n        # The slack is 0.9. `-slack` would be -0.9.\n        # If item=0.9 and bin_cap=1.0, remaining is 0.1. This is a good fit.\n        # The slack is 0.1. `-slack` would be -0.1.\n        \n        # The current `-slack` metric intrinsically prioritizes tighter fits.\n        \n        # Let's consider the context of online BPP. We want to minimize the total number of bins.\n        # Prioritizing tight fits generally leads to higher bin utilization.\n        \n        # What if we use a score that represents \"how full the bin will be\"?\n        # `fill_rate = (original_capacity - slack) / original_capacity`\n        # We don't have original_capacity.\n        # We can approximate it as `item + slack`.\n        # So, `fill_rate_approx = item / (item + slack)`\n        # We want high fill rate. So we want to maximize `item / (item + slack)`.\n        # This is equivalent to minimizing `(item + slack) / item` which is `1 + slack/item`.\n        # Minimizing `1 + slack/item` is equivalent to minimizing `slack/item`.\n        # This is the same as minimizing `slack` (if item > 0).\n        \n        # The core idea remains minimizing slack. The challenge is how to differentiate among small slacks.\n        \n        # Let's propose a score that:\n        # 1. Strongly favors smaller slacks (`-slack`).\n        # 2. Slightly penalizes very large slacks, making them less preferred than medium slacks.\n        #    A bin with slack=5 should be less preferred than a bin with slack=0.5.\n        #    Current `-slack` handles this well: -5 is worse than -0.5.\n        \n        # The prompt asks for \"better than current version\".\n        # The current version `priority_v1` uses `-slack`.\n        # This is effectively \"Best Fit\" if the bins were sorted by remaining capacity.\n        # In online, it means picking the bin that results in the least wasted space *for this item*.\n        \n        # Let's introduce a score that considers the \"quality\" of the remaining space.\n        # A bin that is almost full might be good, but a bin that has a moderate amount of space\n        # might be better for future, potentially larger, items.\n        \n        # Let's try a score that uses a thresholding or weighting based on slack size relative to item size.\n        # Consider slack relative to bin capacity. We don't have bin capacity.\n        \n        # A robust heuristic is often related to \"tightest fit\" but with modifications.\n        # What if we assign higher priority to bins where the slack `s` satisfies:\n        # `s` is small (e.g., `s < bin_capacity * 0.2`), but also not too small (e.g., `s > epsilon`).\n        \n        # Let's consider a score that has two components:\n        # Component 1: Tightness (Maximize `-slack`)\n        # Component 2: Balance (Prefer slack that's not too small or too large)\n        \n        # Let's try to incorporate a penalty for bins that leave a very small residual capacity *relative to the item size*.\n        # Score = `-slack - alpha * exp(-slack / item)` if item > 0, else `-slack`.\n        # This would penalize small slacks when item is small.\n        # Example: item = 0.1\n        # slack = 0.01 => -0.01 - alpha * exp(-0.01 / 0.1) = -0.01 - alpha * exp(-0.1)\n        # slack = 0.1  => -0.1 - alpha * exp(-0.1 / 0.1) = -0.1 - alpha * exp(-1)\n        # If alpha is positive, the small slack gets a more negative score.\n        \n        # Let's define the score more carefully:\n        # We want to maximize `score`.\n        # Primary objective: Minimize slack. Score term: `-slack`.\n        # Secondary objective: Avoid bins that are *too* full after packing.\n        # This means we want to avoid `slack` being extremely small.\n        # Let's penalize `slack` when it's below a certain threshold, say `item * 0.2`.\n        # The penalty should be larger for smaller `slack`.\n        # Penalty = `max(0, (item * 0.2) - slack)`. This is positive if slack is smaller than `item * 0.2`.\n        # Final Score = `-slack - penalty_factor * max(0, (item * 0.2) - slack)`\n        \n        penalty_threshold_ratio = 0.2  # Slack should ideally be at least 20% of item size.\n        penalty_factor = 1.0           # Weight of the penalty.\n        \n        # Handle cases where item is zero or very close to zero.\n        if item < 1e-9:\n            # If item is zero, slack is just the remaining capacity.\n            # We prefer bins with less remaining capacity (tighter fit).\n            # So, `-bins_remain_cap` is the priority.\n            priorities[available_bins_indices] = -bins_remain_cap[available_bins_indices]\n        else:\n            slack = bins_remain_cap[available_bins_indices] - item\n            \n            # Primary score: negative slack (prioritize minimum slack)\n            tight_fit_score = -slack\n            \n            # Secondary score: penalty for slack being too small relative to item size.\n            # We want slack to be at least `item * penalty_threshold_ratio`.\n            # If slack is smaller than this threshold, we apply a penalty.\n            # The penalty is proportional to how much slack is below the threshold.\n            threshold = item * penalty_threshold_ratio\n            slack_penalty = np.maximum(0, threshold - slack)\n            \n            # The final score combines the tight fit score and the penalty.\n            # We subtract the penalty to reduce the score for bins with too little slack.\n            combined_scores = tight_fit_score - penalty_factor * slack_penalty\n            \n            priorities[available_bins_indices] = combined_scores\n\n    return priorities",
    "response_id": 0,
    "tryHS": false,
    "obj": 4.337854008775429,
    "cyclomatic_complexity": 3.0,
    "halstead": 197.15338753100974,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response1.txt_stdout.txt",
    "code_path": "problem_iter9_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Prioritized Bins using Scaled Remaining Capacity and Slack Minimization.\n\n    This strategy aims to provide more differentiated scores by considering\n    both how \"full\" a bin becomes and how much slack is left. It also\n    introduces a scaling factor to adjust the emphasis on minimizing slack.\n\n    The priority is calculated as:\n    priority = (1 - scaled_remaining_capacity) * weight_slack - scaled_slack * (1 - weight_slack)\n\n    Where:\n    - scaled_remaining_capacity: The remaining capacity after fitting, scaled\n                                  to a [0, 1] range relative to the bin capacity (assumed 1.0 for simplicity).\n                                  A smaller remaining capacity (closer to 0) gets a higher score here.\n    - scaled_slack: The amount of space left in the bin if the item doesn't fit perfectly,\n                   scaled to a [0, 1] range. This term is mainly to penalize bins that are \"too large\".\n                   A smaller slack (closer to 0) gets a lower score here.\n    - weight_slack: A hyperparameter (between 0 and 1) to balance the importance of\n                    minimizing slack vs. maximizing the \"fullness\" of the bin.\n\n    For bins that can fit the item:\n    - If remaining capacity is 0, it gets the highest priority.\n    - Otherwise, we want to minimize remaining capacity.\n\n    The logic:\n    1. Identify bins that can fit the item.\n    2. For these bins, calculate the remaining capacity after placing the item.\n    3. For bins that CANNOT fit, assign a very low priority (-1).\n    4. For bins that CAN fit:\n       - We want to prioritize bins that result in a smaller remaining capacity.\n       - A simple way to get differentiated scores is to use the negative of the remaining capacity.\n       - To make these scores more comparable across different potential remaining capacities,\n         we can normalize them.\n       - However, a direct inverse might not be ideal. A better approach is to create a score\n         that is high when remaining capacity is low.\n\n    Revised Approach:\n    We will aim to create a score that is high for bins that are \"nearly full\"\n    after the item is placed.\n    The priority will be based on how much the *remaining capacity changes* towards zero.\n    A bin that goes from `c` capacity to `c - item` is prioritized if `c - item` is small.\n\n    Let's define priority as a function that is maximized when `bins_remain_cap - item` is minimized.\n    A simple monotonic transformation that achieves this is to use `- (bins_remain_cap - item)`.\n    To make scores more distinct and robust, we can consider a scaled version or a score that\n    rewards bins that are *just* large enough.\n\n    New Strategy: Maximize (1 / (remaining_capacity_after_fit + epsilon)) or minimize remaining_capacity_after_fit.\n    We will prioritize bins that result in the *smallest positive remaining capacity*.\n\n    Let's use a score that rewards bins that are \"just right\" or slightly larger.\n    A bin that is exactly the right size (remaining capacity = 0) should be highly prioritized.\n    Bins that are much larger should be less prioritized than those that are a good fit.\n\n    Priority = -(remaining_capacity_after_fit)\n    To make scores more distinct and avoid very small negative numbers dominating,\n    we can scale them. Or, more simply, we can add a large constant to make them positive\n    and then use the negative of the remaining capacity.\n\n    Let's try a score that emphasizes bins that leave a small, but positive, remainder.\n    The ideal scenario is a bin that leaves 0 remainder.\n    A bin that leaves a very large remainder is less ideal.\n\n    Consider priority = - (bins_remain_cap[can_fit_mask] - item)\n    This means smaller remaining capacity is better (higher score).\n\n    To differentiate more, we can introduce a penalty for very large remaining capacities.\n    A simple way is to use a function that is steep near 0 remaining capacity and less steep for larger capacities.\n    e.g., `- log(remaining_capacity + epsilon)` or `- remaining_capacity`.\n\n    Let's refine `priority_v1` by making the scores more distinct for bins that leave small remainders.\n    Instead of just `-remaining_capacity`, we can use `1 / (remaining_capacity + epsilon)` which\n    gives higher scores to smaller remaining capacities, and the difference between scores\n    is more pronounced.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Use -inf for bins that cannot fit\n\n    can_fit_mask = bins_remain_cap >= item\n\n    if np.any(can_fit_mask):\n        # Calculate remaining capacities for bins that can fit the item\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n\n        # Strategy: Prioritize bins that leave a small, non-negative remaining capacity.\n        # We want to maximize the score as the remaining capacity approaches 0.\n        # A good function for this is 1 / (remaining_capacity + epsilon).\n        # However, to make scores more distinct, we can use a function that is\n        # sensitive to the magnitude of the remaining capacity.\n        # Let's use a score that is highest for zero remaining capacity and decreases\n        # as remaining capacity increases.\n        # A simple function that achieves this is to assign a score inversely\n        # proportional to the remaining capacity, ensuring a small positive value for the denominator.\n        # A larger value indicates a better fit.\n\n        # We want to maximize `f(remaining_capacity)` where `f(0)` is max and `f(large)` is min.\n        # Options:\n        # 1. `1 / (remaining_capacity + epsilon)`: High scores for small remainders.\n        # 2. `-remaining_capacity`: Also prioritizes small remainders.\n        # 3. `K - remaining_capacity`: Shifts scores but maintains order.\n\n        # To differentiate better, let's consider bins that are *just* large enough.\n        # A bin that is perfectly sized (remaining capacity = 0) is ideal.\n        # Bins that are slightly larger are good. Bins that are much larger are less ideal.\n\n        # Let's use a score that rewards bins that leave minimal capacity,\n        # but also penalizes bins that leave a lot of capacity.\n        # Consider the inverse of the remaining capacity.\n        # To avoid division by zero and create distinct scores, use 1 / (remaining_capacity + epsilon)\n        epsilon = 1e-9\n        scores = 1.0 / (remaining_capacities_if_fit + epsilon)\n\n        # We can also add a term to penalize large remaining capacities more severely.\n        # For example, we could subtract a scaled version of the remaining capacity.\n        # Let's aim for a score that is highest for remaining capacity of 0 and decreases.\n        # A simple linear decrease is `-remaining_capacity`.\n        # A non-linear decrease could be `-log(remaining_capacity + epsilon)`.\n\n        # Let's stick with the inverse relationship, which is good for \"almost full\".\n        # To make scores more \"differentiated\", consider the difference in priorities.\n        # If we have remaining capacities of 0.1 and 0.2, the scores are 10 and 5. Difference is 5.\n        # If we have remaining capacities of 0.01 and 0.02, the scores are 100 and 50. Difference is 50.\n        # This inherently provides differentiation.\n\n        # We can also add a small bonus for bins that are closer to a perfect fit.\n        # Let's use a scaled version of the inverse remaining capacity.\n        # A scaled version of 1 / (remaining_capacity + epsilon) could be:\n        # (1 / (remaining_capacity + epsilon)) * max_possible_score / (1 / epsilon)\n        # This normalizes the scores to be within a certain range.\n\n        # A simpler, more robust approach to differentiation:\n        # Use the negative of the remaining capacity, but shift it so that\n        # perfect fits get the highest scores.\n        # Let's assign a maximum score to a perfect fit and decrease from there.\n        # Max score = 0 (for remaining capacity of 0)\n        # Score = - remaining_capacity\n\n        # To make scores more distinct, especially when many bins have small remainders:\n        # Use `log` scaling or inverse scaling.\n        # `log(1 / (remaining_capacity + epsilon))` or `-log(remaining_capacity + epsilon)`\n        # This makes differences at small values more pronounced.\n\n        # Let's use `-remaining_capacity` as the base, as it directly addresses minimizing slack.\n        # To differentiate, we can add a factor that scales based on how small the remaining capacity is.\n        # Or, more simply, ensure that values are not too close.\n\n        # Let's try the `priority_v1` logic but ensure the values are sufficiently spread.\n        # `priority_v1` uses `-(bins_remain_cap[can_fit_mask] - item)`.\n        # This means smaller remaining capacities lead to higher (less negative) scores.\n        # Example:\n        # Bin A: rem_cap = 0.1, item = 0.9. rem_cap_after = 0.0. Score = 0.\n        # Bin B: rem_cap = 0.2, item = 0.9. rem_cap_after = 0.1. Score = -0.1.\n        # Bin C: rem_cap = 0.3, item = 0.9. rem_cap_after = 0.2. Score = -0.2.\n        # Bin D: rem_cap = 1.0, item = 0.1. rem_cap_after = 0.9. Score = -0.9.\n\n        # This is quite good. The differentiation is there.\n        # To make scores even more distinct for very small remainders, we can use:\n        # `log(1 / (remaining_capacity_after_fit + epsilon))` which is `-log(remaining_capacity_after_fit + epsilon)`\n        # For rem_cap = 0.0, -log(epsilon) is a large positive number.\n        # For rem_cap = 0.1, -log(0.1) approx 2.3\n        # For rem_cap = 0.2, -log(0.2) approx 1.6\n\n        # This seems like a good way to differentiate. Let's use this.\n        # We are maximizing `-log(remaining_capacity_after_fit + epsilon)`.\n\n        # To ensure scores are positive and distinguishable, let's normalize them or\n        # ensure a baseline.\n        # Let's map the best case (remaining_capacity = 0) to a high positive score.\n        # Maximize: `100 - log(remaining_capacity_after_fit + epsilon)`\n        # If remaining_capacity_after_fit is 0, score is `100 - log(epsilon)` (large positive)\n        # If remaining_capacity_after_fit is small, score is slightly less.\n        # If remaining_capacity_after_fit is larger, score is smaller.\n\n        # Consider the range of `remaining_capacity_after_fit`. It's between 0 and `max(bins_remain_cap) - item`.\n        # Let's try a score that is high for 0 remainder and decreases as remainder increases.\n        # A simple linear score: `C - remaining_capacity_after_fit`.\n        # To differentiate, we need a non-linear function.\n\n        # Let's use `1.0 / (remaining_capacity_after_fit + epsilon)` as it's intuitive for \"best fit\".\n        # To ensure differentiation, we can scale this.\n        # If we have many bins with very small remainders, the scores will be very large and close.\n        # We want scores that clearly separate good fits from mediocre ones.\n\n        # Consider a score that combines \"how full\" with \"how much slack\".\n        # Priority = w1 * (1 / (remaining_capacity_after_fit + epsilon)) + w2 * (-remaining_capacity_after_fit)\n        # Where w1 and w2 control the emphasis.\n\n        # Let's go with a score that is designed to be high for small, positive remaining capacities.\n        # A simple way to achieve good differentiation is to use an inverse relationship.\n        # `1 / (remaining_capacity_after_fit + epsilon)` is a good candidate.\n        # To make the values more manageable and perhaps better distributed, we can\n        # scale it.\n\n        # Let's consider the \"value\" of a bin. A bin that leaves 0 space is perfect.\n        # A bin that leaves a tiny bit of space is almost perfect.\n        # A bin that leaves a lot of space is less ideal.\n\n        # We want to maximize `f(remaining_capacity)` where `f` is decreasing.\n        # Let's map the range of `remaining_capacity_after_fit` to a score range.\n        # Let `min_rem = 0` and `max_rem = max(bins_remain_cap[can_fit_mask]) - item`.\n        # We want to map `[min_rem, max_rem]` to `[score_max, score_min]`.\n\n        # Let's use a score that emphasizes bins that are \"just right\".\n        # Score = 1 / (remaining_capacity_after_fit + epsilon)\n        # Example:\n        # rem_cap = [0.0, 0.05, 0.1, 0.5, 1.0]\n        # scores = [1e9, 20, 10, 2, 1] (using epsilon=1e-9)\n        # This provides differentiation.\n\n        # To make it even more robust and perhaps include other factors if needed later,\n        # consider a weighted combination.\n        # For now, let's stick to improving the \"almost full\" heuristic.\n\n        # What if we scale the inverse based on the *original* remaining capacity?\n        # This would give a higher absolute priority to bins that were already full.\n        # Score = (1 / (remaining_capacity_after_fit + epsilon)) * bins_remain_cap[can_fit_mask]\n\n        # Let's refine the `-log(remaining_capacity + epsilon)` approach.\n        # This provides strong differentiation for small remaining capacities.\n        # For robustness, we can add a large constant to make scores positive.\n        # `priority_score = 100.0 - np.log(remaining_capacities_if_fit + epsilon)`\n\n        # This ensures that bins with smaller remaining capacities get higher scores.\n        # The `log` function provides the desired differentiation.\n        # The `100.0` is an arbitrary offset to ensure positive scores.\n\n        priorities[can_fit_mask] = 100.0 - np.log(remaining_capacities_if_fit + epsilon)\n\n    return priorities",
    "response_id": 1,
    "tryHS": false,
    "obj": 4.058635819704831,
    "cyclomatic_complexity": 2.0,
    "halstead": 78.13781191217038,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response2.txt_stdout.txt",
    "code_path": "problem_iter9_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Hybrid Priority for Online Bin Packing.\n\n    This strategy combines the \"almost full\" concept with a penalty for\n    bins that are too empty, aiming for a more balanced packing.\n    It prioritizes bins that leave a small remaining capacity (similar to V1),\n    but also penalizes bins that would have a very large remaining capacity\n    after packing, thus encouraging fuller bins over very empty ones.\n\n    The scoring is designed to:\n    1. Favor bins where `bins_remain_cap - item` is minimized (most \"full\").\n    2. Penalize bins where `bins_remain_cap - item` is large (too empty).\n    3. Assign a neutral or slightly negative score to bins that perfectly fit\n       the item, to differentiate them from those that leave a small gap.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        remaining_capacities_if_fit = bins_remain_cap[can_fit_mask] - item\n        \n        # Objective 1: Minimize remaining capacity (maximize negative remaining capacity)\n        # This rewards bins that become \"almost full\".\n        score_almost_full = -remaining_capacities_if_fit\n\n        # Objective 2: Penalize large remaining capacities.\n        # We want to down-weight bins that will still be very empty.\n        # A simple way is to subtract a scaled version of the remaining capacity.\n        # We can use a function like - (remaining_capacity_if_fit)^2 or\n        # a sigmoid-like function to make the penalty grow as remaining capacity increases.\n        # For simplicity and effectiveness, let's use a linear penalty that increases\n        # with remaining capacity, but ensure it doesn't dominate the \"almost full\" score.\n        # A robust approach is to use a scaled version of the inverse:\n        # A bin that becomes very full (small remaining capacity) is good.\n        # A bin that remains very empty (large remaining capacity) is less good.\n\n        # Let's create a score that rewards small `remaining_capacities_if_fit`\n        # and punishes large `remaining_capacities_if_fit`.\n        # We can use a quadratic function that is minimized at 0 remaining capacity,\n        # or a linear function with a slope.\n\n        # Consider a score that is high for small `remaining_capacities_if_fit`\n        # and decreases as `remaining_capacities_if_fit` increases.\n        # e.g., score = 100 - remaining_capacities_if_fit\n        # This is essentially maximizing `100 - remaining_capacities_if_fit`.\n        # This is equivalent to minimizing `remaining_capacities_if_fit`, but with\n        # a large constant that can be adjusted.\n\n        # To differentiate, let's try to reward being \"tight\" but also penalize \"slack\".\n        # A score could be: Max(0, C - (bins_remain_cap - item)) - PenaltyFactor * max(0, (bins_remain_cap - item))\n        # where C is a constant representing \"ideal fullness\" and PenaltyFactor penalizes slack.\n\n        # Let's use a simpler approach that is still a hybrid:\n        # Score = (MaxPossibleScore - remaining_capacity) - penalty_factor * remaining_capacity\n        # This combines a desire for small remaining_capacity with a penalty for it.\n        # Effectively, we want to maximize `(Constant - remaining_capacity) - penalty_factor * remaining_capacity`.\n        # This simplifies to maximizing `Constant - (1 + penalty_factor) * remaining_capacity`.\n        # Which means minimizing `remaining_capacity * (1 + penalty_factor)`.\n        # This is still very close to V1.\n\n        # Let's try to integrate two objectives:\n        # 1. Minimizing slack (rewarding `bins_remain_cap - item` close to 0)\n        # 2. Maximizing bin utilization across all bins (implicitly).\n        # If we prioritize bins that are *almost full*, we might leave many bins\n        # with a moderate amount of remaining capacity.\n\n        # A better hybrid might involve ranking or scaled values.\n        # Consider the value of `bins_remain_cap[can_fit_mask]`.\n        # Bins with lower remaining capacity are generally better.\n        # Let's try to reward bins that are \"just right\" or \"almost full\".\n        # We can use a function that peaks when `bins_remain_cap[can_fit_mask]` is close to `item`.\n        # Or, we can reward small `remaining_capacities_if_fit` and penalize very large ones.\n\n        # Let's try a score based on how \"tight\" the fit is, but with a diminishing return\n        # for extremely tight fits (to avoid fragmentation if item is slightly smaller than bin capacity)\n        # and a penalty for slack.\n        # We can use a function that is high for small positive `remaining_capacities_if_fit`,\n        # goes to zero or negative for larger `remaining_capacities_if_fit`.\n\n        # For \"differentiated scores\", we can assign different levels of priority based on\n        # the magnitude of remaining capacity.\n\n        # Simple hybrid: Maximize `(MaxCapacity - (bins_remain_cap[can_fit_mask] - item))`\n        # This rewards bins that, after packing, have a large capacity. This is counter-intuitive.\n\n        # Let's go back to minimizing `remaining_capacities_if_fit`.\n        # To differentiate, we can add a term that penalizes large remaining capacities.\n        # Consider the target remaining capacity after packing to be small.\n        # Score = -(remaining_capacities_if_fit) - penalty_for_slack * (remaining_capacities_if_fit)^2\n        # Or, more simply: Score = -(remaining_capacities_if_fit) - penalty_for_slack * remaining_capacities_if_fit\n        # This is equivalent to minimizing `remaining_capacities_if_fit * (1 + penalty_for_slack)`.\n        # Still essentially the same structure.\n\n        # Let's consider the \"context\". The context is the set of available bins.\n        # We want to select a bin that is good *in this context*.\n        # A bin that is almost full is good.\n        # A bin that is extremely empty is bad.\n        # A bin that perfectly fits is also good.\n\n        # Let's define a score that is high for small, non-negative remaining capacity.\n        # We can use a function like `1 / (remaining_capacity + 1)` which decreases as remaining_capacity increases.\n        # Or, more linearly: `Constant - remaining_capacity`.\n        # To penalize slack, we can make the score decrease faster.\n        # e.g., Score = K - remaining_capacity - penalty_factor * remaining_capacity\n        # Maximize Score => Minimize `remaining_capacity * (1 + penalty_factor)`\n\n        # A more nuanced approach might be to consider the variance of remaining capacities\n        # or the overall utilization. However, for a per-item priority function, we need\n        # to evaluate based on the item and current bin states.\n\n        # Let's try a score that is high for small `remaining_capacities_if_fit`\n        # and then decays as `remaining_capacities_if_fit` increases.\n        # We can use a piecewise function or a function like `exp(-k * remaining_capacity)`.\n        # A simple linear approach: Score = C - remaining_capacity.\n        # To penalize slack, we can make the \"C\" decrease for larger remaining capacities.\n\n        # Consider the goal: minimize number of bins.\n        # This often implies filling bins as much as possible.\n        # Priority should be given to bins that become \"most full\" without overflowing.\n        # \"Most full\" after packing means `bins_remain_cap - item` is minimal.\n\n        # Let's try to assign scores that are relative to the *average* remaining capacity\n        # among feasible bins. This introduces context.\n        # However, this might be too complex for a simple priority function.\n\n        # Let's stick to a form that enhances V1:\n        # V1: `priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)`\n        # This directly maximizes `bins_remain_cap - item` being minimal.\n\n        # To add differentiation and robustness:\n        # We want to reward bins that are \"tight\" but not too tight, and penalize slack.\n        # A good score might be one that is high for small positive `remaining_capacities_if_fit`,\n        # drops off for larger values, and is perhaps slightly penalized even for zero remaining capacity\n        # if it means we could have used a slightly larger bin. This is getting complicated.\n\n        # Let's try a score that is inversely proportional to the remaining capacity after packing,\n        # but with a penalty that grows faster than linearly with remaining capacity.\n        # Score = 1 / (epsilon + remaining_capacities_if_fit)  # Favors small remaining capacity.\n        # To penalize slack, we can subtract a term that grows with remaining_capacities_if_fit.\n        # Score = 1 / (epsilon + remaining_capacities_if_fit) - penalty_factor * remaining_capacities_if_fit\n\n        # A simpler heuristic that differentiates:\n        # We want to maximize `(BinCapacity - item) - slack_penalty_factor * slack`\n        # where `slack = bins_remain_cap - item`.\n        # So, maximize `(BinCapacity - item) - slack_penalty_factor * (BinCapacity - item)`.\n        # This means we want to minimize `(BinCapacity - item) * (1 + slack_penalty_factor)`.\n\n        # Let's use a scoring function that is high when `remaining_capacities_if_fit` is small,\n        # and decreases more rapidly for larger values.\n        # Example: A negative quadratic function of `remaining_capacities_if_fit` that peaks at 0.\n        # Score = C - k * (remaining_capacities_if_fit)^2\n        # This rewards small remaining capacities, and the penalty increases quadratically.\n\n        # Let's combine a primary goal (minimize remaining capacity) with a secondary goal (penalize slack).\n        # A common technique is to use a weighted sum.\n        # Score = w1 * (1 / (epsilon + remaining_capacities_if_fit)) - w2 * remaining_capacities_if_fit\n        # Or, to avoid division by zero and keep it simple:\n        # Score = w1 * (-remaining_capacities_if_fit) + w2 * (-remaining_capacities_if_fit^2)\n        # This still prioritizes negative remaining capacity.\n\n        # Let's try a simpler form. The core idea of V1 is to pick the bin that becomes \"most full\".\n        # This is equivalent to picking the bin with the smallest `bins_remain_cap - item`.\n        # A common enhancement is to consider the \"waste\" in the bin.\n\n        # Consider a score that reflects \"how well\" the item fits, focusing on minimizing wasted space.\n        # Wasted space in a bin *after* packing: `bins_remain_cap - item`.\n        # We want to minimize this.\n        # Let's use a function that is high for small values of `bins_remain_cap - item`.\n\n        # For differentiation, we can add a term that emphasizes being \"almost full\" over \"moderately full\".\n        # A simple way is to subtract a small penalty for larger remaining capacities.\n        # Score = -(bins_remain_cap[can_fit_mask] - item) - penalty_factor * (bins_remain_cap[can_fit_mask] - item)**2\n\n        # A robust approach is to map remaining capacity to a score.\n        # Low remaining capacity -> high score.\n        # Large remaining capacity -> low score.\n\n        # Let's implement a score that is `1 / (1 + remaining_capacity_if_fit)`.\n        # This gives a score between (0, 1], where 1 is for 0 remaining capacity.\n        # This is similar to V1 but bounded and might be more stable.\n        # To differentiate: we can adjust the steepness.\n        # A function like `exp(-k * remaining_capacity_if_fit)` also works.\n        # A linear function: `C - remaining_capacities_if_fit` is what V1 is doing essentially.\n\n        # Let's try to combine \"tight fit\" and \"avoiding large residual bins\".\n        # Score = 100 - (bins_remain_cap[can_fit_mask] - item)  # Favors minimal remainder.\n        # To penalize slack:\n        # Score = 100 - (bins_remain_cap[can_fit_mask] - item) - penalty_factor * (bins_remain_cap[can_fit_mask] - item)\n        # This is still just minimizing `remaining_capacity * (1 + penalty_factor)`.\n\n        # A key aspect of \"differentiated scores\" is to create a range of scores that clearly rank options.\n        # Let's use a score that is `MaxScore - f(remaining_capacity_if_fit)`, where `f` is an increasing function.\n        # To differentiate, make `f` grow faster than linear.\n        # Let f(x) = x + x^2 / C. Or f(x) = x^2.\n        # Score = MaxScore - (remaining_capacities_if_fit)^2.\n        # This rewards small remaining capacity, but penalizes larger ones more heavily.\n\n        max_possible_score_base = 100.0 # A baseline to ensure positive scores before penalty\n        penalty_factor = 0.5 # Controls how aggressively large remaining capacities are penalized\n\n        # Score: Reward small remaining capacity, penalize larger remaining capacities quadratically.\n        # The idea is to pick a bin that leaves a small gap, but avoid bins that leave a moderately large gap.\n        priorities[can_fit_mask] = max_possible_score_base - remaining_capacities_if_fit - penalty_factor * (remaining_capacities_if_fit ** 2)\n\n        # Ensure that bins with exactly zero remaining capacity get a good score,\n        # but not necessarily the absolute maximum if there are other beneficial properties.\n        # The current formula works well for this: if remaining_capacities_if_fit is 0, score is max_possible_score_base.\n        # If remaining_capacities_if_fit is small positive (e.g., 0.1), score is slightly less.\n        # If remaining_capacities_if_fit is larger (e.g., 1.0), score is significantly less due to the penalty.\n\n    return priorities",
    "response_id": 2,
    "tryHS": false,
    "obj": 4.048663741523748,
    "cyclomatic_complexity": 2.0,
    "halstead": 88.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response3.txt_stdout.txt",
    "code_path": "problem_iter9_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Adaptive Nearly Full Fit with Robustness and Scaled Objective.\n\n    This strategy aims to improve upon \"Almost Full Fit\" by introducing\n    a more nuanced scoring that balances the \"almost full\" objective with\n    robustness and a consideration for bins that might be slightly larger\n    but still offer a good fit.\n\n    The core idea is to prioritize bins that leave a small remaining capacity\n    (similar to v1), but with modifications:\n    1.  **Differentiated Scores:** Instead of a simple linear inverse, we use a\n        sigmoid-like function or a piecewise function to differentiate scores\n        more effectively. Bins that are *very* close to fitting get a high score,\n        but bins that are slightly further away but still good fits don't get\n        penalized too harshly.\n    2.  **Scaled Objective Integration:** The \"almost full\" metric (remaining capacity)\n        is scaled to be within a reasonable range, preventing extreme values\n        from dominating the priority. We can also integrate a secondary objective,\n        like the proportion of remaining capacity relative to the bin's original\n        capacity (if original capacity was available, which it isn't here, so we\n        use remaining capacity itself as a proxy for \"how much space is left in general\").\n        A simpler approach here is to focus on the *quality of fit* as a primary driver.\n    3.  **Robustness:** Handle edge cases and potential numerical instabilities.\n        The primary concern is minimizing `remaining_capacity = bins_remain_cap - item`.\n        A very small positive `remaining_capacity` is ideal.\n        Large positive `remaining_capacity` is less ideal.\n        Negative `remaining_capacity` is not allowed (handled by the mask).\n\n    We will prioritize bins that leave the smallest *positive* remaining capacity.\n    To achieve this, we want to maximize a score that is high for small positive\n    remainders and decreases as the remainder increases.\n\n    A function like `exp(-k * remaining_capacity)` where k is a scaling factor\n    could work, but a simpler linear approach is often effective and more interpretable.\n    Let's refine the negative linear approach from v1 to be more robust and\n    potentially offer better discrimination.\n\n    Instead of `- (remaining_capacity)`, consider a function that emphasizes\n    very small positive remainders.\n\n    Let `slack = bins_remain_cap - item`. We want to maximize a function `f(slack)`\n    where `slack >= 0`.\n    We want `f(slack)` to be high when `slack` is close to 0, and decrease as `slack` grows.\n\n    Let's use a transformation:\n    `priority = max_possible_slack - slack` where `max_possible_slack` is some\n    large value (e.g., bin capacity, if known, or a heuristic large value).\n    This effectively turns minimization of slack into maximization.\n\n    A more sophisticated approach: Map slack to a priority score.\n    Consider a function that maps `slack` to a score in a fixed range, e.g., [0, 1].\n    If slack is 0, score is 1. If slack is very large, score approaches 0.\n\n    Let's try a simple but effective approach: Maximize `1.0 / (slack + epsilon)`\n    for small positive slacks, but cap the priority or use a softer decay.\n    A common heuristic is to use a penalty function that is small for small slacks\n    and increases more rapidly for larger slacks.\n\n    The original v1 used `-(bins_remain_cap - item)`. This means it prioritizes\n    the bin that leaves the *least negative* remaining capacity (or largest positive)\n    if we were to consider negative values. However, since we only consider bins\n    where `bins_remain_cap >= item`, `bins_remain_cap - item` is always non-negative.\n    So `-(bins_remain_cap - item)` means we want to *minimize* `bins_remain_cap - item`.\n    This is equivalent to minimizing the remaining capacity.\n\n    Let's refine this to make the difference between \"almost full\" and \"moderately full\"\n    more pronounced.\n    A potential improvement is to scale the priority based on the *relative* remaining capacity\n    or use a non-linear transformation that emphasizes near-zero remainders.\n\n    Let's try prioritizing bins with the smallest *absolute* difference between\n    `bins_remain_cap` and `item`. This is equivalent to minimizing `abs(bins_remain_cap - item)`.\n    Since we only consider `bins_remain_cap >= item`, this simplifies to minimizing\n    `bins_remain_cap - item`.\n\n    To create *differentiated scores* and *robustness*, we can normalize or scale\n    the slack.\n    If `slack = bins_remain_cap - item`, we want to maximize `f(slack)`.\n    A good candidate for `f(slack)` would be something that decays as `slack` increases.\n    A simple inverse `1/(slack + epsilon)` might be too aggressive.\n\n    Let's try a scaled approach:\n    Prioritize bins that leave a slack `s`. We want to maximize `g(s)`.\n    We want `g(0)` to be maximum.\n    Consider `g(s) = C - s`, where `C` is a large constant. This is what v1 implicitly does.\n\n    How to differentiate?\n    If we have bins with slacks [0.1, 0.2, 0.3], v1 gives priorities [-0.1, -0.2, -0.3]. Max is -0.1.\n    If we have slacks [0.01, 0.1, 0.5], v1 gives [-0.01, -0.1, -0.5]. Max is -0.01.\n\n    Let's introduce a non-linearity or scaling.\n    Consider `priority = 1.0 / (slack + epsilon)` capped at some value.\n    Or, transform slack: `transformed_slack = slack / max_possible_slack`.\n    Then `priority = 1.0 - transformed_slack`. This is linear.\n\n    Let's focus on the 'almost full' aspect by rewarding minimal slack.\n    The key is to make the scores *comparable* and *meaningful*.\n    A bin that leaves 0.1 remaining capacity should be significantly better than\n    one leaving 0.5, but perhaps not infinitely better.\n\n    Let's define priority score `P` for a bin `i` with remaining capacity `R_i` for item `I`:\n    `slack_i = R_i - I`\n    If `slack_i < 0`, priority is invalid (-1).\n    If `slack_i >= 0`, we want to maximize `P_i`.\n\n    Proposed scoring function:\n    `P_i = exp(-k * slack_i)` where `k` is a tuning parameter.\n    If `k=1`, `P_i = exp(-slack_i)`.\n    slacks [0.1, 0.2, 0.3] -> exp(-0.1) ~ 0.905, exp(-0.2) ~ 0.819, exp(-0.3) ~ 0.741.\n    slacks [0.01, 0.1, 0.5] -> exp(-0.01) ~ 0.990, exp(-0.1) ~ 0.905, exp(-0.5) ~ 0.606.\n    This gives good differentiation.\n\n    We need to select a reasonable `k`.\n    The range of `slack` can vary greatly depending on the bin capacity and item sizes.\n    If `slack` can be large, `exp(-k * slack)` can become very small.\n    To make it more robust, we can normalize slack.\n    Let `max_slack_possible` be an estimate of the maximum reasonable slack.\n    This is tricky without knowing bin capacity.\n    Alternatively, we can compute slack relative to the *item size* itself, or relative to\n    the *smallest bin capacity that fits the item*.\n\n    Let's assume we only have `bins_remain_cap`.\n    We can use the smallest non-negative slack as a reference for normalization.\n    However, this requires an initial pass to find the minimum slack.\n\n    Simpler approach: Use a capped inverse or a sigmoid-like function.\n    Let's stick to the exponential decay but consider a scaling factor derived from the\n    range of possible slacks IF we had more context.\n    Without it, a fixed `k` is a starting point.\n\n    Let's try a piecewise linear function or a modified inverse.\n    Consider a linear function for small slacks and a different one for larger slacks.\n\n    The core idea is to maximize a score that is high for small `slack = R_i - I` and\n    decreases as `slack` increases.\n\n    Let's try prioritizing bins that have a high \"occupancy\" after placing the item.\n    Occupancy = `item_size / (original_capacity)` if original_capacity was known.\n    Here, we only have remaining capacity.\n    The quality of fit is measured by minimizing `slack`.\n\n    Revised approach for `priority_v2`:\n    Use a score that is high for bins with small `slack = bins_remain_cap - item`.\n    The score should decay smoothly.\n    We want to maximize `f(slack)`.\n    Let's define `f(slack)` such that `f(0) = 1`, `f(large) = 0`.\n\n    Option 1: `f(slack) = 1.0 / (1.0 + slack * scale_factor)`\n    This is a form of logistic function which maps to [0, 1].\n    If `slack` is 0, score is 1. If `slack` is large, score approaches 0.\n    `scale_factor` controls how quickly the score drops.\n    A smaller `scale_factor` means slower decay (less preference for \"almost full\").\n    A larger `scale_factor` means faster decay (stronger preference for \"almost full\").\n\n    Let's choose a `scale_factor` that is adaptive or empirically set.\n    For online BPP, we don't have global information to set an optimal `scale_factor`.\n    A reasonable heuristic might be to scale by the average remaining capacity of bins\n    that can fit the item, or by the item size itself.\n\n    Let `scale_factor = 1.0 / avg_slack_of_fitting_bins` or `scale_factor = 1.0 / item_size`.\n    This could be unstable if `item_size` is very small or average slack is 0.\n\n    Let's try a simpler, robust approach.\n    We want to maximize `-(slack)` as in v1.\n    To differentiate better, let's apply a non-linear transformation that amplifies\n    small differences.\n    Consider `priority = -(slack^2)` or `priority = -(sqrt(slack))`.\n    This would penalize larger slacks more heavily than v1.\n\n    The key problem with `-(slack)` is that the difference between slack=0.1 and slack=0.2\n    is 0.1. The difference between slack=0.01 and slack=0.02 is also 0.1.\n    We want the difference between 0.01 and 0.02 to be *more significant* than the\n    difference between 0.1 and 0.2.\n\n    This suggests a function that grows faster for small slacks.\n    Consider `priority = -exp(slack)`. This would have higher priority for smaller slack.\n    slacks [0.1, 0.2, 0.3] -> -exp(0.1) ~ -1.105, -exp(0.2) ~ -1.221, -exp(0.3) ~ -1.350. Max is -1.105.\n    slacks [0.01, 0.1, 0.5] -> -exp(0.01) ~ -1.010, -exp(0.1) ~ -1.105, -exp(0.5) ~ -1.648. Max is -1.010.\n\n    This seems promising. It prioritizes bins that are \"closer\" to being full.\n    The \"almost full\" concept is captured by minimizing `slack`.\n    `exp(slack)` grows exponentially, so small changes in `slack` at small values\n    result in larger changes in the score compared to linear or `exp(-slack)`.\n\n    Let's ensure the scores are robust. The range of `exp(slack)` can be large.\n    We can shift the scores by subtracting a constant to make them negative,\n    or normalize.\n\n    Let's use `priority = -exp(slack)`.\n    We still need to handle bins that cannot fit the item.\n    `priorities = np.full_like(bins_remain_cap, -np.inf)` (or a very small number).\n    Then for bins that fit: `priorities[can_fit_mask] = -np.exp(slack[can_fit_mask])`.\n\n    We can also add a small epsilon to slack to avoid `exp(0)` if needed, but `exp(0)=1` is fine.\n    The primary requirement is to return higher scores for better bins.\n\n    Final consideration: \"Scaled Objective Integration\" and \"Differentiated Scores\".\n    The `-exp(slack)` function provides differentiated scores.\n    To scale, we can consider the distribution of slacks.\n    If `slack` can be very large, `exp(slack)` can overflow.\n    We need to cap slack or use a different function if this is a concern.\n    A robust alternative to `exp(slack)` might be `1.0 / (slack + epsilon)` with some clipping.\n    Or `priority = 1.0 - tanh(slack * scale_factor)`.\n\n    Let's refine `-exp(slack)` by scaling slack relative to a typical slack.\n    Or, let's try a function that gives a higher priority to bins with smaller remaining capacity.\n    This is equivalent to maximizing `f(remaining_capacity_after_fit)`.\n    We want `f(x)` to be decreasing for `x >= 0`.\n\n    Let's reconsider v1's `-(bins_remain_cap[can_fit_mask] - item)`.\n    This is equivalent to `item - bins_remain_cap[can_fit_mask]`.\n    It prioritizes bins with the smallest `bins_remain_cap` that can still fit the item.\n    This is the \"Best Fit\" heuristic.\n\n    The prompt asks for \"almost full fit\".\n    This usually means minimizing the slack.\n    So, v1 implements \"Best Fit\" in a sense of minimizing slack.\n\n    Let's try to differentiate more.\n    A common way to differentiate scores for \"best fit\" is to reward smaller slacks.\n    Consider `priority = MaxPossibleSlack - slack`.\n    If we don't know `MaxPossibleSlack`, we can use a large constant or a value related to the data.\n\n    Let's try a robust scoring based on the inverse of slack, but with a non-linear transformation\n    that handles small slacks effectively.\n    `priority = 1 / (1 + slack^alpha)` where alpha > 1.\n    If `alpha = 2`: `priority = 1 / (1 + slack^2)`.\n    slacks [0.1, 0.2, 0.3] -> 1/(1+0.01) ~ 0.99, 1/(1+0.04) ~ 0.96, 1/(1+0.09) ~ 0.917. Max is 0.99.\n    slacks [0.01, 0.1, 0.5] -> 1/(1+0.0001) ~ 0.9999, 1/(1+0.01) ~ 0.99, 1/(1+0.25) ~ 0.8. Max is 0.9999.\n    This works. It prioritizes smaller slacks. The `slack^2` term penalizes larger slacks more.\n\n    To make it more \"almost full\" specific, let's focus on small slacks.\n    Consider a function that is close to 1 for very small slacks and decays.\n    `priority = exp(-slack / scale_factor)` might be better than `exp(slack)`.\n    If `scale_factor` is small, it penalizes larger slacks more.\n    Let `scale_factor` be related to the typical slack values.\n    A simple robust choice for `scale_factor` could be `1.0` or `item_size`.\n\n    Let's use `priority = exp(-slack)`. This is simple and effective.\n    To ensure robustness and prevent potential underflow/overflow if slack is extreme,\n    we can clip slack or use a robust variant.\n\n    Consider the goal: \"prioritize bins that will be 'almost full'\".\n    This implies minimizing `slack = R_i - I`.\n    So we want to maximize `f(slack)` where `f` is a decreasing function.\n\n    Let's try `priority = 1.0 / (slack + epsilon)`. This is simple and often effective.\n    To differentiate scores:\n    The problem is when slacks are very small (e.g., 1e-5, 1e-4). `1/slack` can be huge.\n    We can cap the priority score, or shift the slack.\n\n    Alternative: Use a function that maps slack to a score in a bounded range, e.g., [0, 1].\n    Let `slack = bins_remain_cap - item`.\n    `priorities = np.full_like(bins_remain_cap, -1.0)`\n    `can_fit_mask = bins_remain_cap >= item`\n    `fitting_slacks = bins_remain_cap[can_fit_mask] - item`\n\n    If we want to prioritize small slacks, we want a high score for small slack.\n    `score = 1 / (slack + epsilon)` works conceptually.\n    To make it robust and differentiated, we can consider scaling or non-linearity.\n\n    Let's try a robust inverse squared function that maps to [0, 1].\n    `priority = 1.0 / (1.0 + slack^2)`\n    This gives scores between 0 and 1.\n    `slack = 0` -> `priority = 1`\n    `slack = 1` -> `priority = 0.5`\n    `slack = 10` -> `priority = 1/101 ~ 0.01`\n\n    This approach:\n    1.  **Differentiated Scores:** `slack^2` penalizes larger slacks quadratically,\n        leading to more distinct scores for bins with small slacks.\n    2.  **Robustness:** The addition of `1.0` prevents division by zero and keeps scores\n        within a predictable range [0, 1]. Using `slack^2` is more stable than `1/slack`.\n    3.  **Scaled Objective Integration:** The objective (minimizing slack) is mapped\n        to a score in [0, 1]. The quadratic penalty can be seen as a form of scaling\n        that emphasizes \"almost full\" more than a linear penalty.\n\n    Let's implement this.\n    We'll set priority to -1 for bins that cannot fit.\n    For bins that can fit, the priority will be `1.0 / (1.0 + slack**2)`.\n    Higher scores are better.\n    \"\"\"\n\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        slacks = fitting_bins_remain_cap - item\n        \n        # Calculate priority score: 1.0 / (1.0 + slack^2)\n        # This function gives higher scores to bins with smaller slacks.\n        # slack = 0 -> priority = 1.0\n        # slack increases -> priority decreases towards 0.\n        # The slack^2 term provides a quadratic penalty, differentiating scores more\n        # for small slack values.\n        \n        epsilon = 1e-9  # To prevent potential issues if slack is extremely close to zero and squared.\n                        # Though 1.0 + 0^2 is 1.0, it's good practice.\n        priorities[can_fit_mask] = 1.0 / (1.0 + slacks**2 + epsilon)\n        \n        # Ensure scores are within a reasonable range if needed, though [0, 1] is good.\n        # No explicit clipping needed here as the function naturally stays in [0, 1].\n\n    return priorities",
    "response_id": 3,
    "tryHS": false,
    "obj": 4.487435181491823,
    "cyclomatic_complexity": 2.0,
    "halstead": 80.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter9_response4.txt_stdout.txt",
    "code_path": "problem_iter9_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Differentiated and Scaled Fit priority function.\n\n    This strategy aims to provide more differentiated priority scores by considering\n    both how \"full\" a bin becomes and how well the item \"fits\" relative to the bin's\n    current remaining capacity. It also scales the contribution of these factors.\n\n    The priority is calculated based on two components:\n    1.  **Slack Minimization:** Similar to \"Almost Full Fit\", we want to minimize\n        the remaining capacity after placing the item. This is captured by\n        `bins_remain_cap - item`. A smaller value here is better.\n    2.  **Fit Ratio:** We also consider how \"tight\" the fit is relative to the\n        bin's current remaining capacity. A bin with much more capacity might\n        be less preferable even if it leaves a small absolute remainder,\n        if another bin with less capacity can also accommodate the item snugly.\n        This is captured by `item / bins_remain_cap`. A smaller value here is better.\n\n    These two components are combined with scaling factors to create a single priority score.\n    The function prioritizes bins that can fit the item (remaining capacity >= item).\n    Bins that cannot fit receive a priority of -1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of priority scores for each bin. Bins that cannot accommodate the item\n        receive a priority of -1. Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -1.0)\n    \n    can_fit_mask = bins_remain_cap >= item\n    \n    if np.any(can_fit_mask):\n        available_bins_remain_cap = bins_remain_cap[can_fit_mask]\n        \n        # Component 1: Minimize remaining capacity after placing the item (Slack)\n        # We want to maximize the negative of slack, so smaller slack -> higher priority.\n        slack = available_bins_remain_cap - item\n        slack_priority = -slack\n        \n        # Component 2: Minimize the ratio of item size to bin remaining capacity (Fit Ratio)\n        # This penalizes using a large bin for a small item if a tighter fit is available.\n        # We want to maximize the negative of the ratio.\n        # Add a small epsilon to avoid division by zero if an item perfectly fills a bin\n        # that had 0 remaining capacity (though this case is handled by can_fit_mask,\n        # it's good practice for robustness if logic were to change).\n        epsilon = 1e-9\n        fit_ratio = item / (available_bins_remain_cap + epsilon)\n        fit_ratio_priority = -fit_ratio\n        \n        # Combine components with scaling.\n        # We can assign weights based on desired behavior.\n        # For example, give more weight to minimizing slack (e.g., 0.7) and less to fit ratio (e.g., 0.3).\n        # The specific weights can be tuned based on empirical performance.\n        weight_slack = 0.7\n        weight_fit_ratio = 0.3\n        \n        # A simple linear combination. Normalize the priorities if their scales differ significantly\n        # or if we want to ensure they contribute to a bounded range.\n        # For now, we combine them directly.\n        combined_priority = (weight_slack * slack_priority) + (weight_fit_ratio * fit_ratio_priority)\n        \n        # Assign the calculated priorities to the bins that can fit the item.\n        priorities[can_fit_mask] = combined_priority\n        \n    return priorities",
    "response_id": 4,
    "tryHS": false,
    "obj": 4.098524132429212,
    "cyclomatic_complexity": 2.0,
    "halstead": 116.69205856195879,
    "exec_success": true
  }
]