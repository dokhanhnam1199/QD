[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit\n    with emphasis on favoring tighter fits by shifting the sigmoid.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a modified sigmoid function to provide a smooth ranking, strongly favoring\n    tighter fits by mapping perfect fits to scores higher than 0.5.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 0.5 + 0.5 * sigmoid(steepness * (item - remaining_capacity)).\n    This formulation means that a perfect fit (remaining_capacity - item = 0) results\n    in a score of 0.75, while looser fits get scores between 0.5 and 0.75, and very\n    loose fits approach 0.5. Tighter fits (if possible, i.e., remaining_capacity < item)\n    would result in scores greater than 0.75.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit) upwards, with tighter fits receiving higher scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 10.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the negative surplus for bins where the item can fit.\n    # Negative surplus = item - remaining_capacity\n    # A smaller remaining capacity (tighter fit) leads to a larger negative surplus.\n    neg_surplus_valid = item - bins_remain_cap[can_fit_mask]\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher scores for smaller `remaining_capacity` (i.e., larger `neg_surplus`).\n    # The function `sigmoid(steepness * x)` increases with `x`.\n    # Thus, `sigmoid(steepness * neg_surplus)` increases as `neg_surplus` increases.\n    sigmoid_argument = steepness * neg_surplus_valid\n\n    # Clip the sigmoid argument for numerical stability.\n    # The range [-30, 30] is generally safe for np.exp.\n    clipped_sigmoid_argument = np.clip(sigmoid_argument, -30.0, 30.0)\n\n    # Calculate the sigmoid values.\n    # For valid bins, neg_surplus_valid <= 0, so sigmoid_argument <= 0.\n    # This means sigmoid_values will be <= 0.5 for valid bins.\n    sigmoid_values = 1.0 / (1.0 + np.exp(-clipped_sigmoid_argument))\n\n    # Transform sigmoid values to prioritize tighter fits more strongly.\n    # We shift and scale the sigmoid output: 0.5 + 0.5 * sigmoid_values.\n    # This maps:\n    # - Perfect fit (neg_surplus=0): sigmoid_values=0.5 -> priority = 0.5 + 0.5*0.5 = 0.75\n    # - Loose fit (neg_surplus<0): sigmoid_values<0.5 -> priority < 0.75 (closer to 0.5 for very loose fits)\n    # - Tighter fit (neg_surplus>0, not applicable with mask): sigmoid_values>0.5 -> priority > 0.75\n    priorities[can_fit_mask] = 0.5 + 0.5 * sigmoid_values\n\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a modified Best Fit.\n\n    This version implements a modified \"Best Fit\" heuristic. It prioritizes bins\n    that have the least remaining capacity *after* the item is placed,\n    but with a twist: it adds a small penalty for bins that would be *exactly* full.\n    This encourages leaving a tiny bit of space if possible, which can sometimes\n    be beneficial for subsequent items. Bins that cannot fit the item are given\n    a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the remaining capacity after placing the item.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Prioritize bins with the smallest remaining capacity.\n    # We use the negative of the remaining capacity.\n    # Add a small epsilon to the priority for bins that would be exactly full,\n    # making them slightly less preferable than bins with a tiny bit of leftover space.\n    # This encourages leaving a small gap (e.g., 0.001) rather than filling completely.\n    epsilon = 1e-6\n    priorities[can_fit_mask] = -remaining_after_fit - (remaining_after_fit == 0) * epsilon\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed,\n    non-linear function that strongly favors tighter fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a transformation of the negative post-placement remaining capacity,\n    emphasizing preference for near-perfect fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated using `0.5 + 0.5 * sigmoid(steepness * (item - remaining_capacity))`.\n    This formulation maps a perfect fit (surplus 0) to a score of 0.75, a loose fit\n    to scores between 0.5 and 0.75, and a tight fit (if possible) to scores greater than 0.75,\n    thus strongly favoring tighter fits and better bin utilization.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to values approaching 1\n        (for very tight fits).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Tunable parameter for controlling the \"steepness\" or emphasis on tighter fits.\n    # A higher value means stronger preference for very small remaining capacities.\n    steepness = 10.0  \n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the \"negative surplus\" for bins that can fit.\n    # Negative surplus = item - remaining_capacity.\n    # A larger negative surplus (i.e., smaller positive surplus or negative surplus) indicates a tighter fit.\n    neg_surplus_valid = item - bins_remain_cap[can_fit_mask]\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher scores for larger `neg_surplus_valid`.\n    # `sigmoid(x)` increases with `x`. So, we use `steepness * neg_surplus_valid`.\n    sigmoid_argument = steepness * neg_surplus_valid\n\n    # Clip the sigmoid argument for numerical stability.\n    # Values outside [-30, 30] can cause overflow/underflow in exp.\n    clipped_sigmoid_argument = np.clip(sigmoid_argument, -30.0, 30.0)\n\n    # Calculate the sigmoid values.\n    # For valid bins, `neg_surplus_valid` is <= 0, so `clipped_sigmoid_argument` is <= 0.\n    # Thus, `sigmoid_values` will be <= 0.5.\n    sigmoid_values = 1.0 / (1.0 + np.exp(-clipped_sigmoid_argument))\n\n    # Transform sigmoid values to prioritize tighter fits more strongly.\n    # The transformation `0.5 + 0.5 * sigmoid_values` maps:\n    # - A perfect fit (surplus=0, neg_surplus=0) -> sigmoid_values=0.5 -> score = 0.5 + 0.5*0.5 = 0.75.\n    # - A loose fit (surplus > 0, neg_surplus < 0) -> sigmoid_values < 0.5 -> score < 0.75.\n    # - A tight fit (surplus < 0, neg_surplus > 0) -> sigmoid_values > 0.5 -> score > 0.75.\n    # This ensures that tighter fits receive higher priority scores.\n    priorities[can_fit_mask] = 0.5 + 0.5 * sigmoid_values\n\n    return priorities",
    "response_id": 2,
    "obj": 4.108496210610296,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a tunable\n    exponential decay function that favors tighter fits but allows looser fits\n    to retain some priority.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it strongly favors bins with minimal remaining capacity after packing (tight fits).\n    However, it allows the priority score to decay more slowly for bins with\n    larger remaining capacities (looser fits), ensuring that these bins are not\n    completely disregarded and can still be considered.\n\n    The priority score is calculated using a function of the form:\n    `score = exp(-k_high * gap / (1 + alpha * gap))`,\n    where `gap` is the remaining capacity after packing (`bins_remain_cap - item`).\n\n    - `k_high` (controlled by `tight_fit_steepness`): Determines how sharply the\n      priority drops for small gaps (tight fits). A higher value means a stronger\n      preference for very tight fits.\n    - `alpha` (controlled by `loose_fit_decay_rate`): Influences how the decay\n      rate slows down for larger gaps (looser fits). A higher value means the\n      priority decays more slowly for looser fits, giving them more consideration.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated as described above. The score is 1 for a perfect fit\n    (gap=0) and decreases as the gap increases. The `alpha` parameter ensures\n    this decrease is slower for larger gaps.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from approximately 0 to 1.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Tunable parameters:\n    # `tight_fit_steepness`: Controls how rapidly priority drops for small gaps.\n    # Higher value means stronger preference for very tight fits.\n    tight_fit_steepness = 10.0  \n    \n    # `loose_fit_decay_rate`: Controls how much the decay slows down for larger gaps.\n    # Higher value means looser fits retain priority for longer.\n    loose_fit_decay_rate = 0.5  \n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity (gap) for bins that can fit the item.\n    # gap = remaining_capacity - item\n    gaps_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the score using the tunable exponential decay function.\n    # Score = exp(-k_high * gap / (1 + alpha * gap))\n    \n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the denominator: 1 + alpha * gap.\n    # For gap >= 0 and alpha >= 0, denominator is always >= 1.0.\n    denominator = 1.0 + alpha * gaps_valid\n\n    # Calculate the exponent argument: -k_high * gap / denominator\n    # This argument will be 0 for gap=0 and become increasingly negative as gap increases.\n    exponent_args = -k_high * gaps_valid / denominator\n\n    # Clip the exponent arguments for numerical stability to avoid exp overflow/underflow.\n    # Values between -30 and 30 are generally safe for np.exp.\n    # The minimum value of exponent_args occurs as gap -> infinity, which is -k_high / alpha.\n    # Clipping ensures that exp() operates on values within a stable range.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores.\n    # The score is exp(clipped_exponent_args).\n    # For gap=0, score=exp(0)=1.0.\n    # For gap>0, score < 1.0 and decays. The decay rate is influenced by k_high and alpha.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined\n    Sigmoid Best Fit approach that also considers the \"Worst Fit\" aspect for\n    bins that are not a tight fit.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it favors bins that result in a smaller remaining capacity (Best Fit).\n    However, to avoid creating too many nearly-full bins prematurely, it also\n    gives a moderate score to bins that are a significantly looser fit,\n    preventing them from being completely ignored but still prioritizing\n    tighter fits.\n\n    The priority is calculated using a modified sigmoid-like function that has a\n    steeper increase for near-perfect fits and a slower decrease for looser fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated based on the remaining capacity `g = bins_remain_cap[i] - item`.\n    The function used is `exp(-k_high * g / (1 + alpha * g))`:\n    - `k_high` (tight_fit_steepness): Controls how sharply the priority drops for gaps close to zero.\n    - `alpha` (loose_fit_decay_rate): Controls how quickly the decay rate slows down for larger gaps.\n      A higher `alpha` means slower decay for loose fits, giving them more priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are designed to be non-negative, with higher scores indicating\n        higher priority. Scores range from 0 (cannot fit) up to 1 (perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Parameters to tune the behavior:\n    # `tight_fit_steepness`: Controls how quickly the priority drops for near-perfect fits.\n    # `loose_fit_decay_rate`: Controls how quickly the priority decays for looser fits.\n    #                         Higher values give more preference to looser fits.\n    tight_fit_steepness = 10.0  # Higher value means stronger preference for very tight fits\n    loose_fit_decay_rate = 0.5  # Lower value means slower decay for looser fits (higher priority for loose fits)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # g = remaining_capacity_after_packing\n    gaps = bins_remain_cap[can_fit_mask] - item\n\n    # Use the modified sigmoid-like function: exp(-k_high * g / (1 + alpha * g))\n    # k_high = tight_fit_steepness\n    # alpha = loose_fit_decay_rate\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent term.\n    # Denominator is (1 + alpha * g). For g>=0 and alpha>=0, this is always >= 1.\n    denominator = 1.0 + alpha * gaps\n    exponent_args = -k_high * gaps / denominator\n\n    # Clip the exponent arguments for numerical stability.\n    # Values between -30 and 30 are generally safe for np.exp.\n    # As g -> infinity, exponent_args -> -k_high / alpha.\n    # For k_high=10, alpha=0.5, this is -20, which is safe.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate priorities for the bins that can fit the item.\n    # At g=0 (perfect fit), exponent_args=0, score=exp(0)=1.\n    # As g increases, the exponent becomes more negative, and the score decreases.\n    # The `alpha` parameter slows down this decrease for larger `g`.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a balanced\n    non-linear function that favors tight fits but allows looser fits to be\n    considered moderately.\n\n    This heuristic aims to combine the \"Best Fit\" tendency (prioritizing bins\n    with minimal remaining capacity after packing) with a \"Worst Fit\" aspect\n    (not completely discarding bins with large remaining capacity).\n\n    The priority is calculated using a function that decays smoothly but\n    less aggressively for larger remaining capacities. The proposed function is\n    `exp(-k_high * g / (1 + alpha * g))`, where `g` is the remaining capacity\n    after packing the item.\n\n    - `k_high`: Controls the steepness of the priority drop for small remaining capacities (tight fits).\n                A higher value strongly favors very tight fits.\n    - `alpha`: Controls how much the decay rate slows down for larger remaining capacities (looser fits).\n               A higher value means looser fits retain more priority.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit:\n    - If `g` is close to 0 (tight fit), the score is close to `exp(0) = 1`.\n    - As `g` increases, the term `g / (1 + alpha * g)` increases slower than `g` itself,\n      making the exponent less negative and the score decay more slowly than a simple exponential.\n    - The effective decay rate transitions from `k_high` to `k_high / (1 + alpha * g)`. As `g` becomes large,\n      this approaches `k_high / (alpha * g)`, effectively slowing down the decay.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are non-negative, with higher scores indicating higher priority.\n        Scores range from near 0 for very loose fits up to 1 for perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Tunable parameters:\n    # `tight_fit_steepness`: Controls initial decay rate for tight fits. Higher means stronger preference.\n    # `loose_fit_decay_rate`: Controls how much the decay rate slows for looser fits. Higher means looser fits are more considered.\n    tight_fit_steepness = 10.0  # Corresponds to k_high\n    loose_fit_decay_rate = 0.5  # Corresponds to alpha\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # This represents the \"gap\" or \"slack\" after packing.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # Use the proposed function: exp(-k_high * g / (1 + alpha * g))\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent argument. Ensure it's numerically stable.\n    # The denominator (1 + alpha * g) is always >= 1 for non-negative g and alpha.\n    denominator = 1.0 + alpha * remaining_capacity_after_packing\n    exponent_args = -k_high * remaining_capacity_after_packing / denominator\n\n    # Clip exponent arguments to prevent overflow/underflow in np.exp.\n    # Values between -30 and 30 are generally safe for exp.\n    # The minimum value of the exponent is -k_high / alpha (as remaining_capacity_after_packing -> infinity).\n    # For k_high=10, alpha=0.5, this is -20, which is well within safe limits.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed\n    non-linear function favoring tighter fits, with a stronger emphasis on near-perfect fits.\n\n    This heuristic prioritizes bins that can accommodate the item. The priority is\n    calculated using a modified sigmoid function that gives higher scores to bins\n    with minimal remaining capacity after packing (Best Fit strategy).\n\n    The priority for a bin is calculated as:\n    1 / (1 + exp(steepness * (remaining_capacity - item)))\n\n    This function has the property that:\n    - If remaining_capacity - item is large positive (loose fit), the score approaches 0.\n    - If remaining_capacity - item is 0 (perfect fit), the score is 0.5.\n    - If remaining_capacity - item is negative (item is smaller than remaining cap), the score approaches 1.\n\n    To further emphasize near-perfect fits and distinguish them from very loose fits,\n    we can introduce a penalty for overly loose fits, effectively pushing their scores\n    closer to zero more aggressively. This can be achieved by scaling the exponent\n    by a factor that grows with the surplus.\n\n    A potential approach: `priority = sigmoid(steepness * (item - remaining_capacity))`\n    This way, smaller `remaining_capacity` (tighter fit) leads to larger `item - remaining_capacity`,\n    resulting in a higher priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) up to 1 (very tight fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Steepness controls how quickly the priority drops as the fit becomes looser.\n    # A higher steepness means a stronger preference for tighter fits.\n    steepness = 8.0  # Tunable parameter, increased steepness from v1\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the 'tightness' factor for bins that can fit the item.\n    # A smaller positive value indicates a tighter fit.\n    # A larger positive value indicates a looser fit.\n    # For a perfect fit, this value is 0.\n    tightness_factor = bins_remain_cap[can_fit_mask] - item\n\n    # We want higher priority for smaller tightness_factor.\n    # Using `np.exp(-steepness * tightness_factor)` will give high values for small tightness_factor.\n    # However, we want scores to be bounded. The sigmoid function is a good candidate.\n    # Let's define the sigmoid argument `x` such that `sigmoid(x)` is high when `tightness_factor` is small.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If we set `x = steepness * (item - bins_remain_cap[can_fit_mask])`\n    # then `x` is large and positive for tight fits, leading to score ~ 1.\n    # `x` is large and negative for loose fits, leading to score ~ 0.\n    # This is opposite of what we want for Best Fit with higher priority for *smaller* remaining capacity.\n    #\n    # Let's re-evaluate v1's logic: `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # This indeed gives 0.5 for perfect fit, and approaches 1 for negative `remaining_capacity - item`\n    # and approaches 0 for positive `remaining_capacity - item`. This favors *very tight* fits over perfect fits.\n    #\n    # To create a \"smoother non-linear function favoring tighter fits\" and distinguish\n    # better between near-perfect fits and moderately loose fits, we can consider\n    # a function that is steeper around the perfect fit point, or penalizes looseness more.\n    #\n    # A simple modification could be to amplify the exponent for loose fits.\n    # Consider `sigmoid(steepness * (remaining_capacity - item) * penalty_factor)`\n    # where `penalty_factor` is 1 for tight fits and increases for loose fits.\n    #\n    # Alternative: Use a function like `1 - tanh(steepness * (remaining_capacity - item))`\n    # `tanh(x)` ranges from -1 to 1.\n    # If `remaining_capacity - item` is very negative (tight fit), `tanh` is ~-1, score ~ 1 - (-1) = 2 (problematic, need scaling)\n    # If `remaining_capacity - item` is 0 (perfect fit), `tanh` is 0, score ~ 1 - 0 = 1.\n    # If `remaining_capacity - item` is very positive (loose fit), `tanh` is ~1, score ~ 1 - 1 = 0.\n    #\n    # Let's stick with sigmoid but adjust the argument to prioritize smaller `tightness_factor`.\n    # We want score to be high when `tightness_factor` is small.\n    # `sigmoid(S * (C - x))` where `C` is capacity, `x` is item size.\n    # `sigmoid(S * (target - actual))`\n    # Let's use `sigmoid(steepness * (1.0 - (bins_remain_cap[can_fit_mask] / item)))` if item is not zero?\n    # This might be unstable if item is small.\n    #\n    # Back to `1 / (1 + exp(X))`. We want X to be small (negative) for good fits.\n    # `X = steepness * (bins_remain_cap[can_fit_mask] - item)`. This is what v1 does.\n    # Let's try to modify the `steepness` based on the surplus.\n    # If surplus `s = bins_remain_cap[can_fit_mask] - item`, we want the exponent to be `steepness * s`.\n    # We want to penalize large `s`.\n    # Maybe `steepness * (s + s^2)`? Or `steepness * s * (1 + s)`?\n    #\n    # Let's try scaling the `steepness` by the `tightness_factor` itself.\n    # If `tightness_factor` is negative (item larger than capacity, should not happen), `steepness` becomes very negative, exp -> large.\n    # If `tightness_factor` is 0 (perfect fit), exponent is 0, score is 0.5.\n    # If `tightness_factor` is positive (loose fit), `steepness` becomes positive, and it increases with `tightness_factor`.\n    # This means the exponent grows faster for looser fits.\n    #\n    # Consider exponent: `steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This amplifies the exponent for positive surpluses.\n\n    # Let's try a more direct approach that emphasizes smaller non-negative surpluses.\n    # We want a function that is high for `tightness_factor` close to 0.\n    # Consider `1 / (1 + (tightness_factor / some_scale)^2)` or `exp(-(tightness_factor / some_scale)^2)`.\n    # The latter gives 1 for perfect fit, and decreases towards 0 for loose fits.\n    # This might be simpler and more direct for favoring near-perfect fits.\n\n    # Let's refine the v1 idea: `sigmoid(steepness * (item - remaining_capacity))`\n    # `item - bins_remain_cap[can_fit_mask]`\n    # For tight fits: `item` is close to `bins_remain_cap`, difference is small positive. `sigmoid(steepness * small_positive)` -> score near 0.5.\n    # For perfect fits: `item == bins_remain_cap`, difference is 0. `sigmoid(0)` -> 0.5.\n    # For very tight fits (item < remaining): `item - remaining` is negative. `sigmoid(steepness * negative)` -> score near 1.\n    # For loose fits: `item` much smaller than `bins_remain_cap`, difference is large positive. `sigmoid(steepness * large_positive)` -> score near 0.\n\n    # This interpretation seems to favor *very tight* fits over perfect fits.\n    # The prompt says \"favoring tighter fits\". This implies that a bin that has *just enough* space\n    # should get a higher priority than a bin with *lots of extra* space.\n    #\n    # Let's go back to the prompt's \"Better code\" and its interpretation:\n    # `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # Perfect fit: `rem - item = 0`, score = 0.5\n    # Tighter fit (rem < item conceptually, but masked out): `rem - item` negative, score > 0.5\n    # Looser fit (rem > item): `rem - item` positive, score < 0.5.\n    # This function *does* favor tighter fits, with perfect fit being exactly in the middle.\n    # The issue might be that the drop-off for looser fits isn't steep enough.\n    #\n    # To emphasize *near-perfect* fits more, we can increase the steepness.\n    # Let's try a function that is very sensitive to small positive surpluses.\n    #\n    # Consider `exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This gives 1 for perfect fit, and decays exponentially for loose fits.\n    # For tight fits (negative surplus), this would give `exp(0)` which is 1.\n    # This would score perfect and very tight fits equally high.\n    #\n    # Let's try to combine the best of both. Prioritize bins that fit,\n    # and among those, prefer those with minimal *non-negative* remaining capacity.\n    #\n    # `priority = exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This gives 1.0 for perfect fits and tight fits.\n    # For loose fits, it drops off.\n    #\n    # How to differentiate between perfect and \"very tight but still fits\"?\n    # The previous interpretation of v1 favoring negative `rem - item` is interesting.\n    # \"scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative).\"\n    # This means the prompt implies `rem < item` is possible and gets score near 1. This is confusing if `rem` is remaining capacity.\n    # The mask `bins_remain_cap >= item` ensures `remaining_capacity - item >= 0`.\n    # So `post_placement_remain_cap` is always non-negative.\n    # The scores from v1 would thus be between 0 and 0.5, with 0.5 for perfect fits.\n    # This implies v1 actually favors *looser* fits if interpreted strictly.\n    #\n    # Let's assume the intention of v1 was to favor bins where `remaining_capacity - item` is smallest (closest to 0).\n    # In that case, `1 / (1 + exp(steepness * (remaining_capacity - item)))` where `remaining_capacity - item >= 0`.\n    # This function *decreases* as `remaining_capacity - item` increases.\n    # So, smaller `remaining_capacity - item` (tighter fit) gives *higher* scores.\n    #\n    # The \"Better code\" description: \"scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative)\"\n    # This contradicts the code `remaining_capacity - item` which is always non-negative due to the mask.\n    # If `post_placement_remain_cap` is always `>= 0`, then `steepness * post_placement_remain_cap` is always `>= 0`.\n    # `exp(...)` is always `>= 1`. `1 / (1 + exp(...))` is always `< 0.5`.\n    # So, v1 actually gives scores between 0 and 0.5, with 0.5 for perfect fits, and scores approach 0 for loose fits.\n    # This means v1 *does* favor tighter fits.\n    #\n    # To make it \"improved\", we need to make the drop-off for loose fits more pronounced.\n    # We can use a higher `steepness`, or modify the exponent.\n    #\n    # Let's consider a quadratic penalty for surplus:\n    # `priority = 1 / (1 + exp(steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + bins_remain_cap[can_fit_mask] - item)))`\n    # This amplifies the exponent for larger surpluses.\n\n    # Let's use a simpler approach: exponential decay for positive surplus, and a constant high value for perfect/negative surplus.\n    # Since we are masked to `bins_remain_cap >= item`, the `tightness_factor` is always >= 0.\n    # We want scores to be high for small `tightness_factor`.\n    #\n    # How about: `exp(-steepness * tightness_factor)`\n    # `tightness_factor = bins_remain_cap[can_fit_mask] - item`\n    # This gives 1 for perfect fit.\n    # For loose fit, it drops off.\n    #\n    # To differentiate better, let's make it more sensitive to small values.\n    # `exp(-steepness * tightness_factor)` where `steepness` is high.\n    #\n    # Let's try `exp(-steepness * tightness_factor^2)`?\n    # This would peak at 1 for perfect fit, and decay quickly.\n    #\n    # Consider this approach:\n    # For bins that can fit, calculate `surplus = bins_remain_cap[can_fit_mask] - item`.\n    # Priority score = `exp(-steepness * surplus)`\n    # This gives a score of 1 for a perfect fit (surplus=0), and scores decrease towards 0 as surplus increases.\n    # This directly favors tighter fits.\n\n    steepness = 10.0  # Increased steepness to emphasize tighter fits more.\n\n    # Calculate the surplus for bins that can fit the item.\n    # surplus = remaining_capacity - item\n    # We want smaller surplus to have higher priority.\n    surplus = bins_remain_cap[can_fit_mask] - item\n\n    # Use an exponential decay function. A perfect fit (surplus=0) gets a score of 1.\n    # Looser fits (positive surplus) get scores less than 1, decaying exponentially.\n    # We want to use a scale that makes the decay noticeable but not too rapid.\n    # The `steepness` parameter controls this decay rate.\n    # A higher `steepness` means faster decay for surplus.\n    #\n    # To make it more \"smoothed non-linear\" and perhaps distinguish better than pure exponential:\n    # Use a logistic function again, but let's ensure it favors small *non-negative* surpluses.\n    # The v1 function `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # actually works as intended if we interpret `remaining_capacity - item` as the \"slack\".\n    # Smaller slack -> higher priority.\n    #\n    # The \"reflection\" stated \"Smooth non-linear functions, like sigmoid, can yield better results than linear ones.\"\n    # v1 already uses sigmoid. The improvement might come from tuning `steepness` or changing the input to sigmoid.\n    #\n    # Let's try to make the function more sensitive to small surpluses by squaring the term inside the exponent.\n    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(steepness * (surplus**2)))`\n    # This would make perfect fit give 0.5.\n    # Small surplus: `surplus^2` is very small positive. `exp` is slightly > 1. Score is slightly < 0.5.\n    # Larger surplus: `surplus^2` is larger positive. `exp` is larger. Score is smaller.\n    # This actually favors looser fits or perfect fits!\n    #\n    # Let's reverse the input to sigmoid: `1 / (1 + exp(-steepness * (surplus)))`\n    # This gives score of 0.5 for surplus=0.\n    # Small positive surplus: exp is slightly > 1. Score slightly < 0.5.\n    # Large positive surplus: exp is large. Score is small.\n    # This *still* favors tighter fits, with perfect fit at 0.5.\n    #\n    # What if we want a score of 1 for perfect fit and decreasing from there?\n    # Consider `exp(-steepness * surplus)`.\n    # Perfect fit (surplus=0): score = 1.\n    # Small surplus: score < 1.\n    # Large surplus: score approaches 0.\n    # This function directly favors the tightest possible fit.\n\n    # Let's use this exponential decay as the improved heuristic.\n    # The steepness parameter controls how quickly the priority drops off for larger surpluses.\n    # A higher steepness means that only bins with very small surpluses will receive high scores.\n\n    # Calculate priorities using exponential decay of surplus.\n    # Clipping the argument to exp to prevent overflow/underflow.\n    # If surplus is very large, exp(-steepness * surplus) can underflow to 0.\n    # If surplus is negative (should not happen here), exp could grow large.\n    # We are interested in surplus >= 0.\n    # Max surplus can be bin_capacity - min_item_size.\n    # If bin_capacity=100 and min_item=1, max surplus ~ 99.\n    # With steepness=10, exp(-10 * 99) is very close to 0.\n    # A safe range for exp argument is [-30, 30].\n    # -steepness * surplus. We want this to be between -30 and 0 ideally.\n    # So, surplus should be between 0 and 3.\n    # This implies the steepness needs tuning or the function needs adjustment for larger surpluses.\n\n    # Let's rescale the surplus to a more manageable range.\n    # Max possible surplus could be the bin capacity itself.\n    # Let's normalize surplus by a reasonable upper bound, e.g., the bin capacity or average item size.\n    # For simplicity, let's use the `steepness` to control the decay.\n    # We want the score to drop significantly if surplus is, say, 10% of bin capacity.\n    # If bin capacity is 100, a surplus of 10.\n    # exp(-steepness * 10). If steepness=1, exp(-10) ~ 0.\n    # If steepness=0.1, exp(-1) ~ 0.36.\n    #\n    # Let's reconsider the sigmoid function from v1 and boost the steepness.\n    # The problem description implies v1 is good, just needs improvement.\n    # The description of v1's score range was confusing, but the *logic* favors tighter fits.\n    # Higher steepness in `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # means the score drops more sharply as `remaining_capacity - item` increases.\n    # This directly improves the preference for tighter fits.\n\n    steepness = 15.0  # Significantly increased steepness\n\n    # Calculate the post-placement remaining capacity (slack) for bins that can fit the item.\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument. Larger slack results in a larger exponent.\n    exponent_args = steepness * slack\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args (not possible here due to mask), exp -> 0, score -> 1.\n    # For very positive args (loose fits), exp -> inf, score -> 0.\n    # Perfect fit (slack=0) gives exponent=0, score=0.5.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores. Scores range from 0 (loose fit) to 0.5 (perfect fit).\n    # Higher steepness makes the transition from 0.5 to 0 much faster for any positive slack.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 6,
    "obj": 4.198244914240141,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tailored inverse mapping with a scaling factor.\n\n    This heuristic prioritizes bins where placing the item leaves the least amount\n    of remaining capacity. It calculates the \"resulting remaining capacity\" for each\n    bin that can fit the item and then applies a transformation that maps smaller\n    resulting remaining capacities to higher priority scores. The transformation\n    involves taking the inverse of the resulting remaining capacity, adding a\n    small epsilon to prevent division by zero, and then scaling this value.\n    A sensitivity parameter `k` controls how strongly tighter fits are favored.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to zero for all bins. Bins that cannot fit the item will retain this zero priority.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this resulting remaining capacity is minimized.\n    resulting_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize smaller resulting remaining capacities, we use a transformation.\n    # A common approach is to use the inverse: 1 / (resulting_remaining_cap).\n    # This naturally gives higher values for smaller resulting capacities.\n    # Add a small epsilon to avoid division by zero or extremely large values when resulting_remaining_cap is very close to zero.\n    epsilon = 1e-9\n    \n    # Scale the inverse by a factor 'k'. A larger 'k' amplifies the difference between bins,\n    # making the priority more sensitive to smaller resulting remaining capacities.\n    # This is an alternative to the sigmoid in v1, aiming for a more direct mapping of tightness.\n    k = 20.0  # Sensitivity parameter: Controls how strongly tighter fits are favored.\n    \n    # The transformed score directly reflects how \"tight\" the fit is.\n    # A resulting_remaining_cap of 0 gives k / epsilon, which is a very high score.\n    # As resulting_remaining_cap increases, the score decreases.\n    transformed_scores = k / (resulting_remaining_cap + epsilon)\n\n    # Assign the calculated transformed scores to the bins that can fit the item.\n    priorities[can_fit_mask] = transformed_scores\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version implements a \"Best Fit\" heuristic by prioritizing bins that\n    minimize the remaining capacity *after* packing the item (i.e., minimize slack).\n    Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority (i.e., a bin that results in\n        less remaining capacity after packing).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, the priority is the negative of the slack\n    # (remaining capacity after placing the item). This way, smaller slack\n    # (better fit) results in a higher priority score.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and item/bin proportion.\n\n    This heuristic prioritizes bins that can accommodate the item and result in a\n    small remaining capacity after packing (tight fit). It also incorporates a\n    secondary factor that favors packing the item into bins where it represents\n    a larger proportion of the *current* remaining capacity. This dual objective\n    aims to minimize wasted space by finding tight fits and by encouraging\n    larger items to occupy relatively larger available slots.\n\n    The score for a bin is calculated using a sigmoid function applied to the\n    \"fit gap\" (remaining capacity after packing). A smaller fit gap (tighter fit)\n    results in a higher score. The sigmoid function `1 / (1 + exp(-k * gap))`\n    maps smaller gaps to scores closer to 1.\n\n    The secondary factor is `item / (initial_remaining_capacity + epsilon)`,\n    which is high when the item is large relative to the bin's current capacity.\n\n    The final priority is the product of these two scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit) to approximately 1 (excellent fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value to avoid division by zero.\n    steepness = 15.0 # Tunable parameter: higher values mean stronger preference for tighter fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # The 'fit gap' is the remaining capacity *after* placing the item.\n    # We want to minimize this gap for tight fits.\n    fit_gaps = bins_remain_cap[can_fit_mask] - item\n    initial_remaining_caps = bins_remain_cap[can_fit_mask]\n\n    # Primary criterion: Tight fit using a sigmoid function.\n    # We use `1 / (1 + exp(-steepness * fit_gap))` which results in scores\n    # approaching 1 for smaller `fit_gaps` (tighter fits).\n    # A perfect fit (fit_gap = 0) gives a score of 0.5.\n    # Scores increase as `fit_gap` becomes smaller (approaching 0).\n    exponent_args_tight_fit = -steepness * fit_gaps\n    # Clip to prevent overflow/underflow in np.exp.\n    clipped_exponent_args = np.clip(exponent_args_tight_fit, -30.0, 30.0)\n    tight_fit_score = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    # Secondary criterion: Item proportion of current bin capacity.\n    # This encourages packing larger items into relatively larger available bins.\n    # A higher score is given if the item is a larger fraction of the current bin capacity.\n    proportion_score = item / (initial_remaining_caps + epsilon)\n\n    # Combine scores by multiplication.\n    # This prioritizes bins that are both a tight fit AND where the item\n    # represents a significant portion of the available space.\n    priorities[can_fit_mask] = tight_fit_score * proportion_score\n\n    return priorities",
    "response_id": 9,
    "obj": 5.045871559633042,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response0.txt_stdout.txt",
    "code_path": "problem_iter7_code0.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function to provide a smooth ranking, strongly favoring tighter fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).\n    This function is monotonically decreasing with respect to (remaining_capacity - item),\n    meaning smaller non-negative remaining capacities get higher scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # If a bin can fit, the remaining capacity after packing is: bins_remain_cap[i] - item\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `potential_remaining_cap_valid`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it decrease as `potential_remaining_cap_valid` increases, we set `x = steepness * potential_remaining_cap_valid`.\n    # A small `potential_remaining_cap_valid` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `potential_remaining_cap_valid` (loose fit) results in a larger `x`, thus a lower score.\n    exponent_args = steepness * potential_remaining_cap_valid\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # priorities[can_fit_mask] will be populated with scores between ~0.5 (for perfect fit) and ~0 (for very loose fits).\n    # Scores for bins that cannot fit remain 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 0,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\n"
  },
  {
    "stdout_filepath": "problem_iter7_response1.txt_stdout.txt",
    "code_path": "problem_iter7_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a tuned Sigmoid Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function, with parameters tuned for a more balanced preference between\n    tight fits and general fit. It also incorporates a small base priority for any bin\n    that can fit, ensuring that even loose fits are considered if no tight fits are available.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as base_priority + (1 - base_priority) * (1 / (1 + exp(steepness * (remaining_capacity - item)))).\n    This ensures scores are between base_priority and 1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit) to 1 (best possible fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 7.0  # Increased steepness for stronger preference for tighter fits.\n    base_priority = 0.1  # Small base priority for any bin that can fit.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `potential_remaining_cap_valid`.\n    # A small `potential_remaining_cap_valid` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `potential_remaining_cap_valid` (loose fit) results in a larger `x`, thus a lower score.\n    exponent_args = steepness * potential_remaining_cap_valid\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the core best-fit score using the sigmoid function.\n    # Scores range from ~0.5 (for perfect fit) to ~0 (for very loose fits).\n    best_fit_scores = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    # Combine with base priority: higher scores for tighter fits, but all fitting bins have at least base_priority.\n    # The final score is base_priority + (1 - base_priority) * best_fit_score.\n    # This scales the best_fit_scores to be between base_priority and 1.\n    priorities[can_fit_mask] = base_priority + (1.0 - base_priority) * best_fit_scores\n\n    return priorities",
    "response_id": 1,
    "obj": 3.9988033506182825,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response2.txt_stdout.txt",
    "code_path": "problem_iter7_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a modified Sigmoid Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item. It favors tighter fits\n    using a non-linear function on the surplus capacity. The priority is calculated\n    using a sigmoid function where the \"steepness\" is dynamically adjusted based on the\n    ratio of the item size to the bin's remaining capacity. This aims to balance\n    the preference for tight fits with the magnitude of the item and bin.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 1 / (1 + exp(dynamic_steepness * (remaining_capacity - item))).\n    The dynamic_steepness is proportional to (item / remaining_capacity) to emphasize\n    tighter fits more when the item is relatively large compared to the remaining capacity.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    base_steepness = 3.0  # Base steepness parameter, tunable.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate a dynamic steepness. We want steeper preference for tight fits.\n    # A higher ratio of `item / bins_remain_cap[can_fit_mask]` suggests that the item\n    # is a significant portion of the bin's current capacity, making a tight fit more impactful.\n    # We add a small epsilon to the denominator to avoid division by zero and to ensure\n    # the ratio is not overly dominant when remaining_capacity is very small.\n    epsilon = 1e-6\n    dynamic_steepness_factors = item / (bins_remain_cap[can_fit_mask] + epsilon)\n\n    # Combine base steepness with the dynamic factor.\n    # The `+ 1` in the exponent argument scaling ensures that even for perfect fits (remaining_after_fit=0),\n    # the exponential term doesn't become too extreme and the sigmoid doesn't saturate too quickly.\n    # The goal is that `steepness * (remaining_capacity - item)` is positive for loose fits\n    # and small/zero for tight fits.\n    # We scale remaining_after_fit by `(1 + dynamic_steepness_factors)` to increase the penalty\n    # for loose fits when the item is proportionally large for the bin.\n    scaled_surplus = remaining_after_fit * (1 + dynamic_steepness_factors)\n    \n    # The exponent argument for the sigmoid function: `steepness * surplus`.\n    # A smaller surplus should lead to a higher priority.\n    # For perfect fit (surplus=0), the argument is 0, sigmoid is 0.5.\n    # For loose fit (surplus>0), argument is positive, sigmoid < 0.5.\n    # To get higher priority for tighter fits, we want the argument to be small or negative for tight fits.\n    # So we use `steepness * (-remaining_after_fit)`.\n    # To dynamically adjust based on item size, we can make the steepness itself vary.\n    # Let's reconsider the sigmoid structure: `1 / (1 + exp(x))`. For higher priority, `x` should be smaller.\n    # `x = steepness * (remaining_capacity - item)`.\n    # To favor tight fits more when item is large relative to bin capacity, we want `steepness` to be larger.\n    # So `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`\n    # This is equivalent to `base_steepness * (item - item^2 / bins_remain_cap[can_fit_mask])`\n    # Let's use the inverse of remaining capacity, as that directly relates to \"tightness\".\n    # Higher `1/remaining_capacity` implies a tighter fit is more likely.\n    \n    # Let's try prioritizing bins with smaller `remaining_after_fit` in a non-linear way.\n    # We want the function `f(remaining_after_fit)` to be decreasing.\n    # Original: `exp(steepness * remaining_after_fit)` -> `1 / (1 + exp(steepness * remaining_after_fit))` (lower value is better)\n    # We want to make `steepness` higher when `item` is large relative to `bins_remain_cap`.\n    # Let `factor = item / bins_remain_cap[can_fit_mask]`.\n    # `dynamic_steepness = base_steepness * factor`\n    # `x = dynamic_steepness * remaining_after_fit`\n    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`\n    \n    # This expression means for a given `remaining_after_fit`, if `item` is large relative to `bins_remain_cap`,\n    # `x` becomes larger, `exp(x)` becomes larger, and the score `1/(1+exp(x))` becomes smaller. This is the opposite of what we want.\n    \n    # Let's rethink the argument `x`. We want smaller `remaining_after_fit` to result in a higher score.\n    # `1 / (1 + exp(x))`. Higher score when `x` is smaller (more negative).\n    # So we want `x` to be `steepness * (-remaining_after_fit)`.\n    # For tighter fits (smaller `remaining_after_fit`), we want `x` to be smaller (more negative).\n    # Let's use `steepness` itself to be adjusted.\n    # `dynamic_steepness = base_steepness * (1.0 - item / bins_remain_cap[can_fit_mask])` (This favors bins where item is large)\n    # Or `dynamic_steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])` (This favors bins where item is large)\n    \n    # Let's consider a different approach: prioritize bins where `remaining_capacity - item` is small, but scale the penalty based on item size relative to bin capacity.\n    # Score = 1.0 if perfect fit.\n    # Score = `(bins_remain_cap[i] - item)` if item fits.\n    # We want to invert this and apply a non-linearity.\n    # Consider `1 - (item / bins_remain_cap[can_fit_mask])`. This is higher when the item is smaller relative to capacity.\n    # Consider `(bins_remain_cap[can_fit_mask] - item)`. This is smaller for tighter fits.\n    \n    # Let's stick to the sigmoid form but adjust its behavior.\n    # We want higher scores for smaller `remaining_after_fit`.\n    # `1 / (1 + exp(steepness * surplus))` means smaller surplus -> smaller exponent arg -> higher score.\n    # To make this *more* sensitive to surplus when `item` is large relative to `bins_remain_cap`, we need to increase `steepness`.\n    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    \n    # Let `surplus = bins_remain_cap[can_fit_mask] - item`.\n    # The exponent argument is `steepness * surplus`.\n    # `x = (base_steepness * (item / bins_remain_cap[can_fit_mask])) * surplus`\n    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`\n    \n    # For a fixed `surplus`, if `item / bins_remain_cap` is large, `x` is large, score is low.\n    # This implies looser fits are more penalized when the item is large relative to bin capacity.\n    # This matches the reflection's idea: \"Balance fit with item/bin proportions.\"\n    # A large item that fits snugly should have high priority. A small item that fits loosely should have low priority.\n    \n    # Let's re-evaluate the sigmoid function's meaning.\n    # `1 / (1 + exp(k * (target - value)))` where `target` is the ideal value.\n    # Here, ideal `remaining_capacity - item` is 0.\n    # So `1 / (1 + exp(k * (0 - (bins_remain_cap[i] - item))))`\n    # `1 / (1 + exp(k * (item - bins_remain_cap[i])))`\n    # To get higher priority, we want `k * (item - bins_remain_cap[i])` to be small (negative).\n    # This means `k` should be positive, and `item - bins_remain_cap[i]` should be negative (which is true for valid fits).\n    # The smaller `bins_remain_cap[i] - item`, the more negative `item - bins_remain_cap[i]` is, the higher the priority.\n    \n    # Now, let's make `k` (steepness) depend on item/bin proportions.\n    # If `item` is large relative to `bins_remain_cap[can_fit_mask]`, we want to be more sensitive to the surplus.\n    # So, `k = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    \n    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (item - (bins_remain_cap[can_fit_mask] + epsilon))\n    \n    # Clip the exponent arguments to prevent potential overflow/underflow.\n    # The range of exponent_args can be wide. A safe range for exp is typically [-30, 30].\n    # The term `(item / bins_remain_cap[can_fit_mask])` can be > 1.\n    # The term `(item - bins_remain_cap[can_fit_mask])` is <= 0.\n    # So `exponent_args` is negative or zero.\n    # If `item` is very close to `bins_remain_cap`, `exponent_args` is close to 0. Score ~0.5.\n    # If `item` is much smaller than `bins_remain_cap`, `exponent_args` is large negative. Score close to 1.\n    # If `item` is large relative to `bins_remain_cap` (e.g., `item = 0.9 * bin_cap`), `item - bin_cap` is negative.\n    # `exponent_args = steepness * 0.9 * (0.9*bin_cap - bin_cap) = steepness * 0.9 * (-0.1*bin_cap)`\n    # `exponent_args = -steepness * 0.09 * bin_cap`.\n    # If `base_steepness` is large, this becomes very negative.\n    # This is still yielding high scores for tight fits.\n    \n    # Let's go back to the original formulation of `priority_v1` and modify the argument to sigmoid:\n    # `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # Here, `steepness` is positive, so smaller `(remaining_capacity - item)` gives higher priority.\n    # To increase sensitivity when `item` is large relative to `bins_remain_cap`:\n    # `dynamic_steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # So the argument becomes `(base_steepness * (item / bins_remain_cap[can_fit_mask])) * (bins_remain_cap[can_fit_mask] - item)`\n    \n    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (bins_remain_cap[can_fit_mask] - item)\n\n    # Clip the exponent arguments. The range of `exponent_args` can be problematic.\n    # If `item / bins_remain_cap` is very large (e.g., item slightly larger than capacity), this gets positive.\n    # But we already filtered `can_fit_mask`. So `bins_remain_cap >= item`.\n    # Thus, `bins_remain_cap - item >= 0`.\n    # The ratio `item / bins_remain_cap` is between approximately 0 and 1 (or slightly more if item=capacity).\n    # The term `bins_remain_cap - item` is between 0 and `bins_remain_cap`.\n    # The product `(item / bins_remain_cap) * (bins_remain_cap - item)`\n    # = `item - item^2 / bins_remain_cap`.\n    # This term is maximized when `bins_remain_cap = 2*item` (derivative of x - x^2/C).\n    # Max value is `2*item - (2*item)^2 / (2*item) = 2*item - 2*item = 0`. This is not right.\n    # Let's check derivative of `f(x) = a*x - b*x^2`. f'(x) = a - 2*b*x = 0 => x = a/(2b).\n    # Here `a = item`, `b = item^2 / bins_remain_cap`.\n    # So `x = item / (2 * item^2 / bins_remain_cap) = bins_remain_cap / (2*item)`.\n    # If `bins_remain_cap = 2*item`, then `x = 1`. The value is `item - item^2/(2*item) = item - item/2 = item/2`.\n    # So the exponent arg can range from 0 (perfect fit, surplus 0) up to potentially something if `item/bins_remain_cap` is large and surplus is large.\n    # Example: item=0.9*C, bin_cap=C => surplus=0.1*C. ratio = 0.9. arg = 3.0 * 0.9 * 0.1*C.\n    # This can still grow large. Let's consider `item=0.1*C`, `bin_cap=C` => surplus=0.9*C. ratio=0.1. arg = 3.0 * 0.1 * 0.9*C = 0.27*C.\n    # The problem is the `C` is not normalized. Let's assume C is always 1.\n    # Item size `s`, Bin Capacity `C`.\n    # `exp_arg = base_steepness * (s / R) * (R - s)` where R is remaining capacity.\n    # If `s` is large and `R` is just slightly larger than `s`, then `s/R` is close to 1, `R-s` is small. `exp_arg` is small. Score is high.\n    # If `s` is small and `R` is large, then `s/R` is small, `R-s` is large. `exp_arg` is small. Score is high.\n    # This seems to favor smaller items into larger bins.\n    \n    # Let's try to directly penalize loose fits more when item is large.\n    # `priority = 1 - (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item) / bins_remain_cap[can_fit_mask]`\n    # This is getting complicated.\n    \n    # Back to `1 / (1 + exp(x))`. We want `x` to be small for good fits.\n    # `x = steepness * surplus`.\n    # We want higher steepness when `item` is large relative to `bins_remain_cap`.\n    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # So, `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`\n    # This correctly makes `x` smaller (more negative) for tighter fits (smaller surplus).\n    # And it makes `steepness` larger for larger items relative to bin capacity.\n    # Consider the range of `item / bins_remain_cap[can_fit_mask]`. If `item` is small, this ratio is small. If `item` is large (close to capacity), this ratio is close to 1.\n    # The term `bins_remain_cap[can_fit_mask] - item` ranges from 0 to `bins_remain_cap[can_fit_mask]`.\n    # The product `(item / R) * (R - s) = s - s^2/R`.\n    # This is bounded. If `R` is the capacity `C`, then `s <= C`.\n    # The term `s - s^2/C` is maximized at `s = C/2`. Value is `C/2 - (C/2)^2/C = C/2 - C/4 = C/4`.\n    # So the argument `x` is `base_steepness * (s - s^2/C)`.\n    # The maximum value of `s - s^2/C` is `C/4` (when `s=C/2`).\n    # So the argument is at most `base_steepness * C/4`.\n    # If `C` is normalized to 1, then argument is at most `base_steepness/4`.\n    # This seems manageable.\n    \n    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (bins_remain_cap[can_fit_mask] - item)\n    \n    # Clip to prevent numerical issues, considering the maximum possible argument.\n    # Let's assume `base_steepness = 5.0`. Max arg could be around `5.0 / 4.0 = 1.25`. This is very small.\n    # The issue might be when `bins_remain_cap` is very small, close to `item`.\n    # Example: `item = 0.9`, `bins_remain_cap = 1.0`. `surplus = 0.1`. `ratio = 0.9`.\n    # `arg = 5.0 * 0.9 * 0.1 = 0.45`. Score = `1 / (1 + exp(0.45))` approx 0.38.\n    # Example: `item = 0.1`, `bins_remain_cap = 1.0`. `surplus = 0.9`. `ratio = 0.1`.\n    # `arg = 5.0 * 0.1 * 0.9 = 0.45`. Score = `1 / (1 + exp(0.45))` approx 0.38.\n    # This means the priority is similar for a tight fit of a large item and a loose fit of a small item.\n    # This isn't quite right. We want the tight fit of a large item to have HIGHER priority.\n    \n    # Let's invert the sigmoid logic.\n    # `1 - (1 / (1 + exp(x))) = exp(x) / (1 + exp(x)) = 1 / (1 + exp(-x))`.\n    # If we want higher priority for tighter fits, we want `x` to be small (negative).\n    # Let's use the form `1 / (1 + exp(-k * surplus))`.\n    # `k` is steepness.\n    # `surplus = R - s`.\n    # `arg = -k * (R - s) = k * (s - R)`.\n    # For tight fits `s-R` is small negative, so arg is small negative, score close to 0.5.\n    # For perfect fits `s=R`, arg is 0, score is 0.5.\n    # For loose fits `s < R`, `s-R` is negative. arg is negative. score is < 0.5.\n    # This is also not what we want. Higher score for tighter fit.\n    \n    # The original formulation `1 / (1 + exp(steepness * surplus))` assigns higher score to smaller surplus.\n    # So we need to make `steepness` larger for the cases we want to prioritize MORE.\n    # We want to prioritize cases where `item` is large relative to `bins_remain_cap`.\n    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # `arg = (base_steepness * (item / R)) * (R - s)`\n    # Let's reconsider the objective. We want to prioritize bins that are \"almost full\" with the current item.\n    # This means `R` should be close to `s`.\n    # A good metric for \"almost full\" could be `s / R`. We want to prioritize high `s / R`.\n    # But we also want to penalize loose fits.\n    \n    # Let's try a simpler heuristic inspired by the reflection:\n    # Prioritize tight fits using non-linear functions on surplus.\n    # `priority = f(bins_remain_cap[i] - item)` where `f` is decreasing.\n    # Let's make the \"decrease\" steeper for larger items.\n    # `f(x) = 1 / (1 + x^p)` where `p` is large? No, `x` is surplus.\n    # How about `f(x) = exp(-k * x)`? Higher priority for smaller `x`.\n    # Let `k` depend on `item`.\n    # `k = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # So, `priority = exp(-(base_steepness * (item / R)) * (R - s))`\n    # `priority = exp(-base_steepness * (s - s^2/R))`\n    # This is `exp(-something_positive)`.\n    # If `s` is close to `R`, `s-R` is small, arg is small negative. `exp` is close to 1.\n    # If `s` is much smaller than `R`, `s-R` is large negative, `s - s^2/R` is large positive. `exp(-large_positive)` is close to 0.\n    # This assigns higher priority to tighter fits.\n    # Now, let's check the dynamic steepness part:\n    # If `item` is large relative to `bins_remain_cap` (e.g., `s=0.9R`), then `s/R = 0.9`.\n    # `exp_arg = -base_steepness * (0.9R - (0.9R)^2/R) = -base_steepness * (0.9R - 0.81R^2/R) = -base_steepness * (0.9R - 0.81R) = -base_steepness * 0.09R`.\n    # If `item` is small relative to `bins_remain_cap` (e.g., `s=0.1R`), then `s/R = 0.1`.\n    # `exp_arg = -base_steepness * (0.1R - (0.1R)^2/R) = -base_steepness * (0.1R - 0.01R) = -base_steepness * 0.09R`.\n    # The exponent argument calculation seems to be yielding similar values, which is not ideal.\n    \n    # The core idea: \"Prioritize tight fits using non-linear functions on surplus. Tune steepness/parameters for balanced preference.\"\n    # And \"Balance fit with item/bin proportions.\"\n    # A tight fit means `bins_remain_cap - item` is small.\n    # A balanced proportion could be related to `item / bins_remain_cap`.\n    # Let's try a simple modification: penalize loose fits more severely when the item is large.\n    # Score = `1.0` for perfect fit.\n    # Score = `1.0 - (bins_remain_cap[i] - item) / bins_remain_cap[can_fit_mask]` for other fits.\n    # This is linear. Let's make it non-linear.\n    # Score = `1.0 - ((bins_remain_cap[i] - item) / bins_remain_cap[can_fit_mask]) ** power`\n    # If `power > 1`, this increases penalty for larger surplus.\n    # Let `power` depend on item size relative to bin capacity.\n    # `power = 1 + alpha * (item / bins_remain_cap[can_fit_mask])` where alpha is a tuning param.\n    \n    # Let's refine the sigmoid approach.\n    # `1 / (1 + exp(steepness * (surplus)))`\n    # `surplus = R - s`.\n    # We want higher priority for smaller surplus.\n    # We want `steepness` to be higher when `s/R` is high (item is large relative to bin).\n    # `steepness = base_steepness * (item / R)`\n    # `arg = (base_steepness * (s/R)) * (R-s)`\n    # `arg = base_steepness * (s/R) * R * (1 - s/R)`\n    # `arg = base_steepness * s * (1 - s/R)`\n    \n    # Let's test this. Assume `base_steepness = 5`.\n    # Case 1: `s=0.9`, `R=1.0`. `arg = 5 * 0.9 * (1 - 0.9/1.0) = 5 * 0.9 * 0.1 = 0.45`. Score = `1/(1+exp(0.45))` = 0.38.\n    # Case 2: `s=0.1`, `R=1.0`. `arg = 5 * 0.1 * (1 - 0.1/1.0) = 5 * 0.1 * 0.9 = 0.45`. Score = `1/(1+exp(0.45))` = 0.38.\n    # This still gives same scores. The problem might be how `s/R` and `R-s` interact.\n    \n    # What if the steepness is related to the *ideal* remaining capacity if the item was the bottleneck?\n    # Or, what if we transform the surplus?\n    # Let `f(surplus) = surplus / item`. This is smaller for tighter fits relative to item size.\n    # `steepness = base_steepness / (1 + f(surplus))`\n    # `f(surplus) = (R-s) / s = R/s - 1`.\n    # `steepness = base_steepness / (1 + R/s - 1) = base_steepness / (R/s) = base_steepness * s/R`.\n    # This leads to the same formula again.\n    \n    # Let's try a simpler, more direct approach related to the reflection:\n    # \"Prioritize tight fits using non-linear functions on surplus.\"\n    # `priority = 1.0 / (1.0 + (bins_remain_cap[i] - item) ** steepness)`\n    # This gives higher priority for smaller surplus.\n    # We want `steepness` to be larger when `item` is large relative to `bins_remain_cap`.\n    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    \n    surplus = bins_remain_cap[can_fit_mask] - item\n    # Ensure surplus is non-negative due to floating point inaccuracies\n    surplus = np.maximum(0, surplus) \n    \n    # Dynamic steepness: higher when item is a larger fraction of the bin's *original* capacity (or current remaining capacity)\n    # Using bins_remain_cap[can_fit_mask] for ratio calculation.\n    ratio_item_to_bin = item / (bins_remain_cap[can_fit_mask] + epsilon)\n    dynamic_steepness = base_steepness * ratio_item_to_bin\n    \n    # Calculate exponent argument. We want smaller surplus to have smaller exponent arg.\n    # So, `exponent_arg = dynamic_steepness * surplus`\n    # Test:\n    # Case 1: `item = 0.9`, `R = 1.0`. `surplus = 0.1`. `ratio = 0.9`. `steepness = 3 * 0.9 = 2.7`. `arg = 2.7 * 0.1 = 0.27`. Score `1/(1+0.27)` = 0.78.\n    # Case 2: `item = 0.1`, `R = 1.0`. `surplus = 0.9`. `ratio = 0.1`. `steepness = 3 * 0.1 = 0.3`. `arg = 0.3 * 0.9 = 0.27`. Score `1/(1+0.27)` = 0.78.\n    # Still same scores. The multiplication is commutative in this way.\n\n    # Let's try making the steepness influence the *surplus* term before the sigmoid.\n    # `sigmoid(x) = 1 / (1 + exp(x))`\n    # We want `x` to be small for good fits.\n    # `x = steepness * transformed_surplus`\n    # If `item` is large, we want to penalize `surplus` more.\n    # `transformed_surplus = surplus / item` (smaller for tighter fits relative to item size)\n    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`\n    # `x = (base_steepness * (s/R)) * ((R-s)/s)`\n    # `x = base_steepness * (s/R) * (R/s - 1)`\n    # `x = base_steepness * (1 - s/R)`\n    # This means the argument only depends on the ratio `s/R`. Higher ratio means lower argument (closer to 0).\n    # This would prioritize items that are a large fraction of the bin capacity.\n    # Let's test:\n    # Case 1: `s=0.9`, `R=1.0`. `ratio = 0.9`. `arg = 3 * (1 - 0.9) = 0.3`. Score = `1/(1+exp(0.3))` = 0.42.\n    # Case 2: `s=0.1`, `R=1.0`. `ratio = 0.1`. `arg = 3 * (1 - 0.1) = 2.7`. Score = `1/(1+exp(2.7))` = 0.06.\n    # This seems better. It strongly prioritizes items that fill the bin more.\n\n    # Let's adjust to make tight fits get higher scores.\n    # The formula `1/(1+exp(x))` gives higher score when `x` is smaller (more negative).\n    # So we want `x = -base_steepness * (1 - s/R)`?\n    # `x = base_steepness * (s/R - 1)`.\n    # Case 1: `s=0.9`, `R=1.0`. `ratio = 0.9`. `arg = 3 * (0.9 - 1) = -0.3`. Score = `1/(1+exp(-0.3))` = 0.57.\n    # Case 2: `s=0.1`, `R=1.0`. `ratio = 0.1`. `arg = 3 * (0.1 - 1) = -2.7`. Score = `1/(1+exp(-2.7))` = 0.93.\n    # This prioritizes small items fitting into large bins. Not ideal.\n    \n    # Let's go back to the idea of dynamically scaling the surplus itself.\n    # We want to penalize `surplus` more if `item` is large relative to `bins_remain_cap`.\n    # Consider `priority = 1.0 / (1.0 + exp(base_steepness * (surplus / item)))`\n    # This maps `surplus/item` (smaller is better) to `[0, inf)`.\n    # If `item` is large, `surplus/item` is smaller for the same absolute `surplus`.\n    # Example: `s=0.9`, `R=1.0`. `surplus=0.1`. `surplus/item = 0.1/0.9 = 0.11`. `arg = 3 * 0.11 = 0.33`. Score = 0.42.\n    # Example: `s=0.1`, `R=1.0`. `surplus=0.9`. `surplus/item = 0.9/0.1 = 9`. `arg = 3 * 9 = 27`. Score = near 0.\n    # This prioritizes items that leave less *relative* surplus compared to their own size.\n    # This favors items that are close to the bin's remaining capacity, where \"close\" is measured in proportion to item size.\n    \n    # Let's use this: `arg = base_steepness * (surplus / (item + epsilon))`\n    \n    surplus = bins_remain_cap[can_fit_mask] - item\n    surplus = np.maximum(0, surplus) # Ensure non-negative surplus\n    \n    # Calculate priority using a sigmoid function that emphasizes tighter fits,\n    # with sensitivity increasing for larger items relative to remaining capacity.\n    # The argument to the sigmoid is `steepness * (surplus / item)`\n    # A smaller `surplus / item` means a tighter fit relative to item size.\n    # We want smaller arguments to yield higher priorities (e.g., `1 / (1 + exp(arg))`).\n    # `steepness` is `base_steepness` potentially modified.\n    # If we want to prioritize larger items that fit tightly, we might need `steepness` to be larger for larger items.\n    # However, the reflection says \"Balance fit with item/bin proportions.\"\n    # Let's try `steepness = base_steepness`.\n    # The term `surplus / item` effectively normalizes the surplus by the item size.\n    # A small surplus relative to item size is prioritized.\n    # This means a large item fitting into a nearly full bin gets high priority.\n    # A small item fitting into a nearly full bin also gets high priority.\n    # A large item fitting into a mostly empty bin gets low priority.\n    # A small item fitting into a mostly empty bin gets very low priority.\n    \n    exponent_args = base_steepness * (surplus / (item + epsilon))\n    \n    # Clip to avoid overflow in np.exp.\n    # `surplus / item` can be large if `item` is very small.\n    # e.g., `item=0.001`, `R=1.0`, `surplus=0.999`. `surplus/item = 999`.\n    # `base_steepness * 999` can be large.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 70.0) # Allow larger positive values for exp\n\n    # Calculate priorities: 1 / (1 + exp(arg)). Higher priority for smaller arg.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 21.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response3.txt_stdout.txt",
    "code_path": "problem_iter7_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a tuned Sigmoid Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function with a tunable steepness and an offset. The offset\n    introduces a preference for already \"good\" fits to be even more prioritized,\n    especially when considering the proportion of item size to bin capacity.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item - offset))).\n    This function is monotonically decreasing with respect to (remaining_capacity - item - offset).\n    A smaller (remaining_capacity - item - offset) leads to a higher score.\n    The offset is designed to slightly favor tighter fits that are still relatively generous,\n    preventing extremely tight fits from dominating to the exclusion of all else.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (ideal fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 7.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n    # Offset to slightly favor fits that are tight but not excessively so.\n    # This can be related to item size or bin capacity proportions. Here, a fixed offset is used.\n    offset = 0.1 * item # Offset scales with item size, encouraging tighter fits for larger items.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the effective remaining capacity for bins that can fit the item.\n    # This adjusted value includes the offset.\n    effective_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item - offset\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `effective_remaining_cap_valid`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # We set `x = steepness * effective_remaining_cap_valid`.\n    # A small `effective_remaining_cap_valid` (tight fit relative to offset) results in a smaller `x`, thus a higher score.\n    # A large `effective_remaining_cap_valid` (loose fit relative to offset) results in a larger `x`, thus a lower score.\n    exponent_args = steepness * effective_remaining_cap_valid\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # A wider range might be acceptable depending on the exact sigmoid behavior desired.\n    # [-30, 30] is generally safe for standard float precision.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # Scores for bins that cannot fit remain 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 3,
    "obj": 3.9589150378939015,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter7_response4.txt_stdout.txt",
    "code_path": "problem_iter7_code4.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function to provide a smooth ranking, strongly favoring tighter fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).\n    This function is monotonically decreasing with respect to (remaining_capacity - item),\n    meaning smaller non-negative remaining capacities get higher scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # If a bin can fit, the remaining capacity after packing is: bins_remain_cap[i] - item\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `potential_remaining_cap_valid`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it decrease as `potential_remaining_cap_valid` increases, we set `x = steepness * potential_remaining_cap_valid`.\n    # A small `potential_remaining_cap_valid` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `potential_remaining_cap_valid` (loose fit) results in a larger `x`, thus a lower score.\n    exponent_args = steepness * potential_remaining_cap_valid\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # priorities[can_fit_mask] will be populated with scores between ~0.5 (for perfect fit) and ~0 (for very loose fits).\n    # Scores for bins that cannot fit remain 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 4,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\nImportError: cannot import name 'priority_v2' from 'gpt' (/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py)\n"
  }
]