```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a refined
    Sigmoid Best Fit approach that also considers the "Worst Fit" aspect for
    bins that are not a tight fit.

    This heuristic prioritizes bins that can accommodate the item. Among those,
    it favors bins that result in a smaller remaining capacity (Best Fit).
    However, to avoid creating too many nearly-full bins prematurely, it also
    gives a moderate score to bins that are a significantly looser fit,
    preventing them from being completely ignored but still prioritizing
    tighter fits.

    The priority is calculated using a sigmoid-like function that has a steeper
    increase for near-fits and a slower decrease for looser fits.

    The score for a bin is 0 if the item cannot fit. For bins that can fit,
    the score is calculated based on the remaining capacity `rc = bins_remain_cap[i] - item`.
    The function is designed such that:
    - A perfect fit (rc=0) gets a high score.
    - A slightly loose fit (small positive rc) gets a slightly lower score.
    - A significantly loose fit (larger positive rc) gets a score that decays
      slower than a simple inverse, allowing these bins to still be considered
      without dominating the selection.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores are designed to be non-negative, with higher scores indicating
        higher priority.
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    
    # Parameters to tune the behavior:
    # `tight_fit_steepness`: Controls how quickly the priority drops for near-perfect fits.
    # `loose_fit_decay_rate`: Controls how quickly the priority drops for looser fits.
    # `mid_point_shift`: Shifts the "middle" of the sigmoid-like curve to favor
    #                    slightly looser fits being considered more than a strict best fit.
    tight_fit_steepness = 10.0  # Higher value means stronger preference for very tight fits
    loose_fit_decay_rate = 0.5  # Lower value means slower decay for looser fits
    mid_point_shift = 0.2       # Positive shift makes the curve respond to larger gaps

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # Calculate the remaining capacity for bins that can fit the item.
    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item

    # Calculate scores. We want higher scores for smaller `potential_remaining_cap_valid`.
    # A simple approach could be exp(-k * rc), but we want to differentiate between
    # tight and loose fits.
    # We can use a combination, or a shifted sigmoid.
    # Let's use a sigmoid-like function that can be tuned.
    # We want `f(rc)` to be high for small `rc` and decrease as `rc` increases.
    # Consider a function like `1 / (1 + exp(a * (rc - b)))` where `a` is steepness
    # and `b` is a shift.
    # To achieve the desired behavior (stronger preference for tight fits, slower decay for loose):
    # We can use a sigmoid that has a steeper slope around 0 and a shallower slope later.
    # An alternative is to use a function that rewards small gaps more, and moderately
    # rewards larger gaps.
    # Let's try to model this:
    # For rc near 0: prioritize highly.
    # For rc moderately larger: still prioritize, but less so.
    # For rc very large: still give some priority to avoid discarding bins entirely.

    # Let's use a transformation that maps remaining capacity to priority.
    # A function like `exp(-k1 * rc) * (1 + k2 * rc)` or a modified sigmoid.

    # Using a sigmoid with adjusted parameters and potentially an additive term
    # for looser fits to prevent their scores from dropping too quickly.
    # We'll map `potential_remaining_cap_valid` to an exponent argument.
    # For tighter fits, we want the exponent argument to be small (highly negative for sigmoid).
    # For looser fits, we want the exponent argument to be less negative or even positive,
    # leading to lower scores but not zero.

    # Let's use a two-part strategy or a more complex sigmoid formulation.
    # A common approach for this is to use a function that saturates or decays
    # slowly.
    # Let's try a function that resembles `1 / (1 + exp(steepness * (gap - shift)))`
    # but we want to tune it.
    # We can achieve a varying steepness by making the exponent argument a function of `rc`.

    # A common heuristic for "balanced" packing is "First Fit Decreasing" (FFD)
    # for offline, but for online, "Best Fit" (BF) and "Worst Fit" (WF) are common.
    # BF aims for tight fits. WF aims for loose fits.
    # A good heuristic might combine these.

    # Let's consider `f(rc) = exp(-tight_fit_steepness * rc)` for tight fits (rc close to 0)
    # and then transition to a slower decay.
    # `f(rc) = C * exp(-loose_fit_decay_rate * rc)` for larger `rc`.

    # A simpler way to achieve a smooth transition and non-zero scores for loose fits
    # without immediate zeroing could be a scaled inverse or exponential decay.
    # Let's try a formulation inspired by sigmoid but adjusted:
    # Score = exp(-k * (rc - shift))  - for tight fits
    # Score = C * exp(-k2 * rc)      - for loose fits

    # Let's use a single function that interpolates or combines these behaviors.
    # A sigmoid function: `1 / (1 + exp(x))` where `x = steepness * (rc - shift)`
    # We can tune `steepness` and `shift`.
    # To make scores decay slower for loose fits, we can make the function less sensitive
    # to larger `rc`.

    # Let's try mapping `rc` to `exp_arg`.
    # If `rc` is small, `exp_arg` should be very negative.
    # If `rc` is larger, `exp_arg` should be less negative, approaching zero or positive.
    # `exp_arg = tight_fit_steepness * (potential_remaining_cap_valid - mid_point_shift)`
    # This makes very small `rc` highly negative, leading to scores near 1.
    # Larger `rc` values lead to less negative `exp_arg`, reducing the score.
    # We can further control the decay of loose fits by adjusting `tight_fit_steepness`
    # and `mid_point_shift`.

    # Let's refine the sigmoid approach.
    # `score = 1 / (1 + exp(steepness * (gap - offset)))`
    # To make it decay slower for loose fits, we can also add a small baseline score
    # or ensure the exponent doesn't grow too large too quickly.

    # Consider the function: `f(gap) = exp(-k * gap)`. This decays exponentially.
    # For tight fits, `k` should be large. For loose fits, `k` should be small.
    # We can combine this:
    # `f(gap) = exp(-k_tight * gap)` for `gap < threshold`
    # `f(gap) = C * exp(-k_loose * gap)` for `gap >= threshold`

    # A single function that achieves a similar effect might be:
    # `score = exp(-k1 * gap) - k2 * gap` or similar additive adjustments.

    # Let's re-evaluate the sigmoid `1 / (1 + exp(x))`.
    # If `x = steepness * (gap - shift)`, for small `gap`, `x` is very negative, score is ~1.
    # For large `gap`, `x` is positive, score approaches 0.
    # To make loose fits have higher scores, we can shift the curve to the right
    # (`shift` positive) or reduce `steepness`.
    # Let's try increasing `steepness` but also adding a term that keeps the score
    # from dropping too fast for larger gaps.

    # Alternative approach: Rank based on `rc`.
    # Smallest `rc` gets rank 1, next gets rank 2, etc.
    # Then transform ranks to scores. A non-linear transformation is better.

    # Let's try to modify the sigmoid exponent argument.
    # We want the exponent to grow less rapidly for larger `rc`.
    # Instead of `steepness * rc`, consider `steepness * (rc^p)` where `p < 1` or `steepness * log(rc)`.

    # Let's try: `exponent_arg = steepness * (potential_remaining_cap_valid ** power)`
    # where `power` is less than 1 to slow down the growth for larger `rc`.
    # Or, even simpler, directly adjust the `steepness` for different ranges of `rc`.

    # Let's consider a heuristic that assigns priority based on the *relative* gap.
    # `relative_gap = potential_remaining_cap_valid / item` (if item > 0)
    # Or `relative_gap = potential_remaining_cap_valid / bin_capacity` (if bin_capacity is known).
    # Since we only have `bins_remain_cap`, `item` is the best reference.

    # Let's go back to the sigmoid and tune it for the desired behavior.
    # `score = 1 / (1 + exp(steepness * (rc - shift)))`
    # - To favor tight fits strongly: increase `steepness`.
    # - To give looser fits more chance: increase `shift` (this effectively moves the steep part to the right).
    #   Or, use a softer decay.

    # Let's use a function that directly rewards small gaps and decays slowly.
    # `score = exp(-k * gap)` is a starting point.
    # To make the decay slower for larger gaps, we can use a form like:
    # `score = exp(-k * gap) * (1 + alpha * gap)`
    # Or `score = exp(-k * gap) / (1 + alpha * gap)` (inverse relationship for alpha)
    # Or a piecewise approach.

    # Let's try a combination of tight fit reward and loose fit "grace".
    # `tight_priority = exp(-tight_fit_steepness * potential_remaining_cap_valid)`
    # `loose_priority = exp(-loose_fit_decay_rate * potential_remaining_cap_valid)`
    # We want to transition from `tight_priority` to `loose_priority`.

    # A simpler sigmoid adjustment:
    # Let `g = potential_remaining_cap_valid`.
    # We want to compute a score `s(g)`.
    # `s(0)` should be high. `s(large_g)` should be low but non-zero.
    # A function like `exp(-k * g)` is good but decays quickly.
    # `s(g) = 1.0 / (1.0 + exp(steepness * (g - shift)))`
    # If we want looser fits to have higher scores, we need to reduce the exponent's
    # positive values. This can be done by increasing `shift` or decreasing `steepness`.
    # Let's try increasing `shift` to push the high-priority region further.

    # `shift_value = 0.3` # A small positive gap might still be considered "good enough".
    # `exponent_args = steepness * (potential_remaining_cap_valid - shift_value)`
    # This would make a gap of `shift_value` have an exponent argument of 0, score 0.5.
    # Gaps smaller than `shift_value` get scores > 0.5. Gaps larger get scores < 0.5.
    # This is still best-fit like.

    # To allow looser fits to have higher priority than just a decaying sigmoid score:
    # we can consider the *inverse* of the remaining capacity as a factor,
    # or give a baseline score.

    # Let's try a formulation that rewards small gaps more strongly and then
    # gradually rewards larger gaps with a slower decay.
    # `score = exp(-k1 * g) * (1 + k2 * g)` seems promising.
    # If `k1` is large, it's a tight fit preference. `k2` controls the boost for larger gaps.
    # `k1 = tight_fit_steepness`
    # `k2 = loose_fit_decay_rate` (inverse relationship, so `k2` should be small if `gap` is large)

    # Let's use a function of the form: `score = exp(-k * g) * (1 + alpha * g)`.
    # `k` controls initial decay, `alpha` controls the "boost" for larger gaps.
    # For small `g`, `score ~ exp(-k * g)`.
    # For large `g`, `score ~ (exp(-k * g) * alpha * g)`.
    # This term `alpha * g * exp(-k * g)` has a maximum and then decays.
    # We want `k` to be related to `tight_fit_steepness` and `alpha` to `loose_fit_decay_rate`.

    # Let `k = steepness` and `alpha = decay_rate`.
    # `score = np.exp(-steepness * potential_remaining_cap_valid) * (1 + decay_rate * potential_remaining_cap_valid)`

    # Let's test this:
    # If `g=0`: `score = exp(0) * (1 + 0) = 1` (perfect fit).
    # If `g` is small positive: `score = exp(-steepness*g) * (1 + decay_rate*g)`.
    # The `exp(-steepness*g)` term dominates and reduces the score.
    # If `g` is larger: `exp(-steepness*g)` drops, but `(1 + decay_rate*g)` increases.
    # We need to ensure the overall score is reasonable.

    # Let's adjust the functional form to be more in line with sigmoid properties but with tuning.
    # Consider a function that has a 'knee' rather than a smooth decay.
    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`
    # We can make the decay slower for larger gaps by making the exponent argument grow slower.
    # Use `steepness * sqrt(gap)` or `steepness * log(gap + 1)`.

    # Let's try a robust sigmoid modification that balances tight fits and allows looser fits.
    # `score = 1.0 / (1.0 + exp(steepness * (g - g0)))` where `g0` is a target gap.
    # For looser fits to be favored slightly:
    # We can normalize the gap by item size: `g_norm = g / item`
    # And use a sigmoid on `g_norm`: `1.0 / (1.0 + exp(steepness * (g_norm - shift_norm)))`

    # Let's use the `exp(-k*g) * (1 + alpha*g)` form as it directly models
    # strong initial decay and slower decay for larger values.
    # `k` (steepness) should be large for tight fits.
    # `alpha` (decay_rate) should be small if we want scores to drop fast for loose fits,
    # and larger if we want loose fits to retain priority.
    # So, `alpha` should be positively correlated with the preference for looser fits.
    # If we want to prioritize tight fits but not ignore loose ones, `alpha` should be small.

    # Let `steepness = tight_fit_steepness`
    # Let `alpha = loose_fit_decay_rate` (This parameter name is a bit counter-intuitive, let's call it `loose_fit_boost_factor`).
    # `boost_factor` controls how much larger gaps are "boosted" relative to strict exponential decay.
    # Higher `boost_factor` means slower decay for loose fits.

    boost_factor = loose_fit_decay_rate # Renaming for clarity
    
    # Calculate scores: exp(-steepness * g) * (1 + boost_factor * g)
    # Need to handle potential overflows/underflows in exp.
    # The term `steepness * g` can become very large.
    # `exp(-large)` -> 0.
    # `(1 + boost_factor * g)` can also grow.
    # If `steepness` is large and `g` is small, `exp` dominates.
    # If `boost_factor` is non-zero and `g` is large, `(1 + boost_factor * g)` grows.
    # The product can still be problematic.

    # Let's try to normalize the gaps or use a more stable function.
    # Consider `score = exp(-k*g)`. This is stable.
    # To make it decay slower for larger gaps, we can cap the exponent value or use a different function.

    # Let's try a piecewise definition or a modified sigmoid that has varying steepness.
    # `score = 1.0 / (1.0 + exp(steepness * (g - shift)))`
    # To make it decay slower, increase `shift`. This moves the steep part to the right.
    # `shift` can be thought of as a "tolerance" for gaps.

    # Let's re-evaluate the reflection: "Smoothness, non-linearity, and strategic parameter tuning improve heuristic performance."
    # The sigmoid function is smooth and non-linear. Parameter tuning is key.
    # The problem statement for `v1` suggests strong preference for tight fits.
    # `v2` should potentially offer a balance.

    # Let's try to create a scoring function that:
    # 1. Gives high scores to tight fits (small `g`).
    # 2. Gives moderate scores to slightly looser fits.
    # 3. Gives a small but non-zero score to very loose fits, ensuring they are not entirely ignored.

    # Consider `score = exp(-k1 * g)` for `g` up to a certain threshold `T`, and
    # `score = C * exp(-k2 * g)` for `g > T` with `k2 < k1`.
    # This is a piecewise approach.
    # For a single function:
    # `score = exp(-k * g)` where `k` itself is a function of `g`.
    # e.g., `k = k_tight` for small `g`, `k = k_loose` for large `g`.

    # Let's use a clamped exponential decay with an additive bonus for larger gaps.
    # `score = exp(-steepness * potential_remaining_cap_valid) + bonus_factor * min(potential_remaining_cap_valid, bonus_cap)`
    # The `bonus_factor` should be small, and `bonus_cap` defines the range for bonus.

    # A simple modification to the sigmoid from v1:
    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`
    # Let `steepness` control the initial drop.
    # Let `shift` control the point where scores become less than 0.5.
    # To make scores decay slower: increase `shift`. This pushes the significant drop to larger gaps.

    # Let's use a formulation inspired by logistic functions but with tunable rates.
    # `score = 1.0 / (1.0 + exp(k * (g - g_mid)))`
    # `k` = steepness
    # `g_mid` = gap at which score is 0.5

    # Let's try to shape the decay rate directly.
    # For small `g`, we want `exp(-k_high * g)`.
    # For large `g`, we want `exp(-k_low * g)`.
    # A function like `exp(-k_high * g / (1 + alpha * g))` can transition between these.
    # When `g` is small, `1 + alpha*g ~ 1`, so `exp(-k_high * g)`.
    # When `g` is large, `1 + alpha*g` grows, making the exponent argument smaller (less negative),
    # effectively slowing down the decay.

    # `k_high = tight_fit_steepness`
    # `k_low` (implicit) should be smaller.
    # `alpha` = `loose_fit_decay_rate`
    # `score = exp(-k_high * g / (1 + alpha * g))`

    # This form seems promising. It's smooth, non-linear, and tunable.
    # `tight_fit_steepness` controls how sharply the score drops for gaps close to zero.
    # `loose_fit_decay_rate` controls how quickly the decay rate slows down for larger gaps.
    # A higher `loose_fit_decay_rate` means slower decay for loose fits.

    # Let's set parameters:
    # `steepness` (k_high) = 10.0: Very sharp drop for small gaps.
    # `decay_rate` (alpha) = 0.5: Makes the decay significantly slower for larger gaps.

    # We need to clip the exponent argument to avoid `exp` overflow/underflow.
    # The exponent is `-k_high * g / (1 + alpha * g)`.
    # As `g` -> infinity, exponent -> `-k_high / alpha`.
    # We need to ensure `-k_high / alpha` is not too large negative.
    # If `k_high = 10`, `alpha = 0.5`, then `-k_high / alpha = -20`.
    # This is fine for `exp`.
    # If `g` is very close to 0, exponent is close to 0.
    # If `g` is small positive, exponent is slightly negative.

    k_high = tight_fit_steepness
    alpha = loose_fit_decay_rate

    # Calculate the exponent term: -k_high * g / (1 + alpha * g)
    # Avoid division by zero if g is negative (which shouldn't happen here, as g >= 0)
    # Ensure denominator is at least 1 to avoid issues with very small alpha and g.
    denominator = 1.0 + alpha * potential_remaining_cap_valid
    # If denominator is extremely close to zero (e.g., alpha is tiny and g is tiny negative, though g is >=0), it could be an issue.
    # For g >= 0, alpha >= 0, denominator is always >= 1.0.
    
    exponent_args = -k_high * potential_remaining_cap_valid / denominator

    # Clip the exponent arguments for numerical stability.
    # Values between -30 and 30 are generally safe.
    # For `exponent_args = -k_high * g / (1 + alpha * g)`:
    # Min value when g -> infinity: -k_high / alpha. If this is < -30, we should clip.
    # Max value when g = 0: 0.
    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)

    # Calculate priorities
    priorities[can_fit_mask] = np.exp(clipped_exponent_args)

    # Normalization consideration: The scores are not guaranteed to sum to a specific value or be in a fixed range [0, 1].
    # The current function `exp(-k*g / (1+a*g))` ranges from 1 (at g=0) down to `exp(-k/a)`.
    # This is fine as we only care about relative priorities.

    return priorities
```
