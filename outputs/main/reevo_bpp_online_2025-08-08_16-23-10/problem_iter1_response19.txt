```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    The priority is calculated based on how well the item fits into the bin.
    Bins with remaining capacity equal to or slightly larger than the item
    will have higher probabilities. The softmax function is used to convert
    these fit scores into probabilities (priorities).

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Initialize priorities to a small negative number to avoid log(0) or zero division issues
    # if no bin can fit the item. This ensures that bins that cannot fit the item
    # will have very low probabilities after softmax.
    fit_scores = np.full_like(bins_remain_cap, -np.inf)

    # Consider only bins that have enough remaining capacity for the item
    can_fit_mask = bins_remain_cap >= item

    # For bins that can fit the item, calculate a 'fit score'.
    # A good heuristic is to favor bins where the remaining capacity is
    # just enough for the item (minimizing wasted space).
    # We can use the negative of the difference as a score,
    # so smaller difference means a larger (better) score.
    # Alternatively, we can use 1 / (remaining_capacity - item + epsilon)
    # to reward smaller remaining capacities.
    # Let's use a score that emphasizes bins with very little remaining space
    # after placing the item.
    if np.any(can_fit_mask):
        # Calculate the remaining capacity after placing the item
        remaining_after_fit = bins_remain_cap[can_fit_mask] - item
        # A higher score for smaller remaining space. Adding a small epsilon
        # to the denominator to avoid division by zero if remaining_after_fit is 0.
        # The '-1' exponent is because we want to maximize the score for smaller gaps.
        # The magnitude of the score can be scaled to influence the softmax behavior.
        # Here, a larger negative exponent might amplify differences,
        # but for simplicity, we'll use a direct relationship with inverse remaining space.
        # A score inversely proportional to the wasted space: 1 / (wasted_space + epsilon)
        # to prioritize tighter fits.
        epsilon = 1e-6
        fit_scores[can_fit_mask] = 1.0 / (remaining_after_fit + epsilon)


    # Apply softmax to convert fit scores into priorities (probabilities)
    # Softmax: exp(score) / sum(exp(scores))
    # Adding a small constant to the scores can help stabilize softmax if all scores are -inf.
    # However, if some bins *can* fit, those will have finite positive scores,
    # and the -inf will correctly result in near-zero probabilities.
    exp_scores = np.exp(fit_scores)
    sum_exp_scores = np.sum(exp_scores)

    if sum_exp_scores == 0:
        # If no bin can fit the item, or if all fit scores resulted in exp(score) being 0 (unlikely with positive values)
        # return a uniform distribution, or a distribution favoring the first available bin.
        # For this problem, if nothing fits, no bin should be chosen. A uniform low probability is reasonable.
        # A truly optimal strategy would signal that no bin is suitable.
        # Here, we'll return near-zero probabilities.
        return np.ones_like(bins_remain_cap) / len(bins_remain_cap) if len(bins_remain_cap) > 0 else np.array([])
    else:
        priorities = exp_scores / sum_exp_scores
        return priorities
```
