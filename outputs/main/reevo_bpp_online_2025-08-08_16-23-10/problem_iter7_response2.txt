```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a modified Sigmoid Best Fit.

    This heuristic prioritizes bins that can accommodate the item. It favors tighter fits
    using a non-linear function on the surplus capacity. The priority is calculated
    using a sigmoid function where the "steepness" is dynamically adjusted based on the
    ratio of the item size to the bin's remaining capacity. This aims to balance
    the preference for tight fits with the magnitude of the item and bin.

    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score
    is calculated as 1 / (1 + exp(dynamic_steepness * (remaining_capacity - item))).
    The dynamic_steepness is proportional to (item / remaining_capacity) to emphasize
    tighter fits more when the item is relatively large compared to the remaining capacity.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    base_steepness = 3.0  # Base steepness parameter, tunable.

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # Calculate the remaining capacity for bins that can fit the item.
    remaining_after_fit = bins_remain_cap[can_fit_mask] - item

    # Calculate a dynamic steepness. We want steeper preference for tight fits.
    # A higher ratio of `item / bins_remain_cap[can_fit_mask]` suggests that the item
    # is a significant portion of the bin's current capacity, making a tight fit more impactful.
    # We add a small epsilon to the denominator to avoid division by zero and to ensure
    # the ratio is not overly dominant when remaining_capacity is very small.
    epsilon = 1e-6
    dynamic_steepness_factors = item / (bins_remain_cap[can_fit_mask] + epsilon)

    # Combine base steepness with the dynamic factor.
    # The `+ 1` in the exponent argument scaling ensures that even for perfect fits (remaining_after_fit=0),
    # the exponential term doesn't become too extreme and the sigmoid doesn't saturate too quickly.
    # The goal is that `steepness * (remaining_capacity - item)` is positive for loose fits
    # and small/zero for tight fits.
    # We scale remaining_after_fit by `(1 + dynamic_steepness_factors)` to increase the penalty
    # for loose fits when the item is proportionally large for the bin.
    scaled_surplus = remaining_after_fit * (1 + dynamic_steepness_factors)
    
    # The exponent argument for the sigmoid function: `steepness * surplus`.
    # A smaller surplus should lead to a higher priority.
    # For perfect fit (surplus=0), the argument is 0, sigmoid is 0.5.
    # For loose fit (surplus>0), argument is positive, sigmoid < 0.5.
    # To get higher priority for tighter fits, we want the argument to be small or negative for tight fits.
    # So we use `steepness * (-remaining_after_fit)`.
    # To dynamically adjust based on item size, we can make the steepness itself vary.
    # Let's reconsider the sigmoid structure: `1 / (1 + exp(x))`. For higher priority, `x` should be smaller.
    # `x = steepness * (remaining_capacity - item)`.
    # To favor tight fits more when item is large relative to bin capacity, we want `steepness` to be larger.
    # So `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`
    # This is equivalent to `base_steepness * (item - item^2 / bins_remain_cap[can_fit_mask])`
    # Let's use the inverse of remaining capacity, as that directly relates to "tightness".
    # Higher `1/remaining_capacity` implies a tighter fit is more likely.
    
    # Let's try prioritizing bins with smaller `remaining_after_fit` in a non-linear way.
    # We want the function `f(remaining_after_fit)` to be decreasing.
    # Original: `exp(steepness * remaining_after_fit)` -> `1 / (1 + exp(steepness * remaining_after_fit))` (lower value is better)
    # We want to make `steepness` higher when `item` is large relative to `bins_remain_cap`.
    # Let `factor = item / bins_remain_cap[can_fit_mask]`.
    # `dynamic_steepness = base_steepness * factor`
    # `x = dynamic_steepness * remaining_after_fit`
    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`
    
    # This expression means for a given `remaining_after_fit`, if `item` is large relative to `bins_remain_cap`,
    # `x` becomes larger, `exp(x)` becomes larger, and the score `1/(1+exp(x))` becomes smaller. This is the opposite of what we want.
    
    # Let's rethink the argument `x`. We want smaller `remaining_after_fit` to result in a higher score.
    # `1 / (1 + exp(x))`. Higher score when `x` is smaller (more negative).
    # So we want `x` to be `steepness * (-remaining_after_fit)`.
    # For tighter fits (smaller `remaining_after_fit`), we want `x` to be smaller (more negative).
    # Let's use `steepness` itself to be adjusted.
    # `dynamic_steepness = base_steepness * (1.0 - item / bins_remain_cap[can_fit_mask])` (This favors bins where item is large)
    # Or `dynamic_steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])` (This favors bins where item is large)
    
    # Let's consider a different approach: prioritize bins where `remaining_capacity - item` is small, but scale the penalty based on item size relative to bin capacity.
    # Score = 1.0 if perfect fit.
    # Score = `(bins_remain_cap[i] - item)` if item fits.
    # We want to invert this and apply a non-linearity.
    # Consider `1 - (item / bins_remain_cap[can_fit_mask])`. This is higher when the item is smaller relative to capacity.
    # Consider `(bins_remain_cap[can_fit_mask] - item)`. This is smaller for tighter fits.
    
    # Let's stick to the sigmoid form but adjust its behavior.
    # We want higher scores for smaller `remaining_after_fit`.
    # `1 / (1 + exp(steepness * surplus))` means smaller surplus -> smaller exponent arg -> higher score.
    # To make this *more* sensitive to surplus when `item` is large relative to `bins_remain_cap`, we need to increase `steepness`.
    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    
    # Let `surplus = bins_remain_cap[can_fit_mask] - item`.
    # The exponent argument is `steepness * surplus`.
    # `x = (base_steepness * (item / bins_remain_cap[can_fit_mask])) * surplus`
    # `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`
    
    # For a fixed `surplus`, if `item / bins_remain_cap` is large, `x` is large, score is low.
    # This implies looser fits are more penalized when the item is large relative to bin capacity.
    # This matches the reflection's idea: "Balance fit with item/bin proportions."
    # A large item that fits snugly should have high priority. A small item that fits loosely should have low priority.
    
    # Let's re-evaluate the sigmoid function's meaning.
    # `1 / (1 + exp(k * (target - value)))` where `target` is the ideal value.
    # Here, ideal `remaining_capacity - item` is 0.
    # So `1 / (1 + exp(k * (0 - (bins_remain_cap[i] - item))))`
    # `1 / (1 + exp(k * (item - bins_remain_cap[i])))`
    # To get higher priority, we want `k * (item - bins_remain_cap[i])` to be small (negative).
    # This means `k` should be positive, and `item - bins_remain_cap[i]` should be negative (which is true for valid fits).
    # The smaller `bins_remain_cap[i] - item`, the more negative `item - bins_remain_cap[i]` is, the higher the priority.
    
    # Now, let's make `k` (steepness) depend on item/bin proportions.
    # If `item` is large relative to `bins_remain_cap[can_fit_mask]`, we want to be more sensitive to the surplus.
    # So, `k = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    
    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (item - (bins_remain_cap[can_fit_mask] + epsilon))
    
    # Clip the exponent arguments to prevent potential overflow/underflow.
    # The range of exponent_args can be wide. A safe range for exp is typically [-30, 30].
    # The term `(item / bins_remain_cap[can_fit_mask])` can be > 1.
    # The term `(item - bins_remain_cap[can_fit_mask])` is <= 0.
    # So `exponent_args` is negative or zero.
    # If `item` is very close to `bins_remain_cap`, `exponent_args` is close to 0. Score ~0.5.
    # If `item` is much smaller than `bins_remain_cap`, `exponent_args` is large negative. Score close to 1.
    # If `item` is large relative to `bins_remain_cap` (e.g., `item = 0.9 * bin_cap`), `item - bin_cap` is negative.
    # `exponent_args = steepness * 0.9 * (0.9*bin_cap - bin_cap) = steepness * 0.9 * (-0.1*bin_cap)`
    # `exponent_args = -steepness * 0.09 * bin_cap`.
    # If `base_steepness` is large, this becomes very negative.
    # This is still yielding high scores for tight fits.
    
    # Let's go back to the original formulation of `priority_v1` and modify the argument to sigmoid:
    # `1 / (1 + exp(steepness * (remaining_capacity - item)))`
    # Here, `steepness` is positive, so smaller `(remaining_capacity - item)` gives higher priority.
    # To increase sensitivity when `item` is large relative to `bins_remain_cap`:
    # `dynamic_steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # So the argument becomes `(base_steepness * (item / bins_remain_cap[can_fit_mask])) * (bins_remain_cap[can_fit_mask] - item)`
    
    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (bins_remain_cap[can_fit_mask] - item)

    # Clip the exponent arguments. The range of `exponent_args` can be problematic.
    # If `item / bins_remain_cap` is very large (e.g., item slightly larger than capacity), this gets positive.
    # But we already filtered `can_fit_mask`. So `bins_remain_cap >= item`.
    # Thus, `bins_remain_cap - item >= 0`.
    # The ratio `item / bins_remain_cap` is between approximately 0 and 1 (or slightly more if item=capacity).
    # The term `bins_remain_cap - item` is between 0 and `bins_remain_cap`.
    # The product `(item / bins_remain_cap) * (bins_remain_cap - item)`
    # = `item - item^2 / bins_remain_cap`.
    # This term is maximized when `bins_remain_cap = 2*item` (derivative of x - x^2/C).
    # Max value is `2*item - (2*item)^2 / (2*item) = 2*item - 2*item = 0`. This is not right.
    # Let's check derivative of `f(x) = a*x - b*x^2`. f'(x) = a - 2*b*x = 0 => x = a/(2b).
    # Here `a = item`, `b = item^2 / bins_remain_cap`.
    # So `x = item / (2 * item^2 / bins_remain_cap) = bins_remain_cap / (2*item)`.
    # If `bins_remain_cap = 2*item`, then `x = 1`. The value is `item - item^2/(2*item) = item - item/2 = item/2`.
    # So the exponent arg can range from 0 (perfect fit, surplus 0) up to potentially something if `item/bins_remain_cap` is large and surplus is large.
    # Example: item=0.9*C, bin_cap=C => surplus=0.1*C. ratio = 0.9. arg = 3.0 * 0.9 * 0.1*C.
    # This can still grow large. Let's consider `item=0.1*C`, `bin_cap=C` => surplus=0.9*C. ratio=0.1. arg = 3.0 * 0.1 * 0.9*C = 0.27*C.
    # The problem is the `C` is not normalized. Let's assume C is always 1.
    # Item size `s`, Bin Capacity `C`.
    # `exp_arg = base_steepness * (s / R) * (R - s)` where R is remaining capacity.
    # If `s` is large and `R` is just slightly larger than `s`, then `s/R` is close to 1, `R-s` is small. `exp_arg` is small. Score is high.
    # If `s` is small and `R` is large, then `s/R` is small, `R-s` is large. `exp_arg` is small. Score is high.
    # This seems to favor smaller items into larger bins.
    
    # Let's try to directly penalize loose fits more when item is large.
    # `priority = 1 - (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item) / bins_remain_cap[can_fit_mask]`
    # This is getting complicated.
    
    # Back to `1 / (1 + exp(x))`. We want `x` to be small for good fits.
    # `x = steepness * surplus`.
    # We want higher steepness when `item` is large relative to `bins_remain_cap`.
    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # So, `x = base_steepness * (item / bins_remain_cap[can_fit_mask]) * (bins_remain_cap[can_fit_mask] - item)`
    # This correctly makes `x` smaller (more negative) for tighter fits (smaller surplus).
    # And it makes `steepness` larger for larger items relative to bin capacity.
    # Consider the range of `item / bins_remain_cap[can_fit_mask]`. If `item` is small, this ratio is small. If `item` is large (close to capacity), this ratio is close to 1.
    # The term `bins_remain_cap[can_fit_mask] - item` ranges from 0 to `bins_remain_cap[can_fit_mask]`.
    # The product `(item / R) * (R - s) = s - s^2/R`.
    # This is bounded. If `R` is the capacity `C`, then `s <= C`.
    # The term `s - s^2/C` is maximized at `s = C/2`. Value is `C/2 - (C/2)^2/C = C/2 - C/4 = C/4`.
    # So the argument `x` is `base_steepness * (s - s^2/C)`.
    # The maximum value of `s - s^2/C` is `C/4` (when `s=C/2`).
    # So the argument is at most `base_steepness * C/4`.
    # If `C` is normalized to 1, then argument is at most `base_steepness/4`.
    # This seems manageable.
    
    exponent_args = (base_steepness * (item / (bins_remain_cap[can_fit_mask] + epsilon))) * (bins_remain_cap[can_fit_mask] - item)
    
    # Clip to prevent numerical issues, considering the maximum possible argument.
    # Let's assume `base_steepness = 5.0`. Max arg could be around `5.0 / 4.0 = 1.25`. This is very small.
    # The issue might be when `bins_remain_cap` is very small, close to `item`.
    # Example: `item = 0.9`, `bins_remain_cap = 1.0`. `surplus = 0.1`. `ratio = 0.9`.
    # `arg = 5.0 * 0.9 * 0.1 = 0.45`. Score = `1 / (1 + exp(0.45))` approx 0.38.
    # Example: `item = 0.1`, `bins_remain_cap = 1.0`. `surplus = 0.9`. `ratio = 0.1`.
    # `arg = 5.0 * 0.1 * 0.9 = 0.45`. Score = `1 / (1 + exp(0.45))` approx 0.38.
    # This means the priority is similar for a tight fit of a large item and a loose fit of a small item.
    # This isn't quite right. We want the tight fit of a large item to have HIGHER priority.
    
    # Let's invert the sigmoid logic.
    # `1 - (1 / (1 + exp(x))) = exp(x) / (1 + exp(x)) = 1 / (1 + exp(-x))`.
    # If we want higher priority for tighter fits, we want `x` to be small (negative).
    # Let's use the form `1 / (1 + exp(-k * surplus))`.
    # `k` is steepness.
    # `surplus = R - s`.
    # `arg = -k * (R - s) = k * (s - R)`.
    # For tight fits `s-R` is small negative, so arg is small negative, score close to 0.5.
    # For perfect fits `s=R`, arg is 0, score is 0.5.
    # For loose fits `s < R`, `s-R` is negative. arg is negative. score is < 0.5.
    # This is also not what we want. Higher score for tighter fit.
    
    # The original formulation `1 / (1 + exp(steepness * surplus))` assigns higher score to smaller surplus.
    # So we need to make `steepness` larger for the cases we want to prioritize MORE.
    # We want to prioritize cases where `item` is large relative to `bins_remain_cap`.
    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # `arg = (base_steepness * (item / R)) * (R - s)`
    # Let's reconsider the objective. We want to prioritize bins that are "almost full" with the current item.
    # This means `R` should be close to `s`.
    # A good metric for "almost full" could be `s / R`. We want to prioritize high `s / R`.
    # But we also want to penalize loose fits.
    
    # Let's try a simpler heuristic inspired by the reflection:
    # Prioritize tight fits using non-linear functions on surplus.
    # `priority = f(bins_remain_cap[i] - item)` where `f` is decreasing.
    # Let's make the "decrease" steeper for larger items.
    # `f(x) = 1 / (1 + x^p)` where `p` is large? No, `x` is surplus.
    # How about `f(x) = exp(-k * x)`? Higher priority for smaller `x`.
    # Let `k` depend on `item`.
    # `k = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # So, `priority = exp(-(base_steepness * (item / R)) * (R - s))`
    # `priority = exp(-base_steepness * (s - s^2/R))`
    # This is `exp(-something_positive)`.
    # If `s` is close to `R`, `s-R` is small, arg is small negative. `exp` is close to 1.
    # If `s` is much smaller than `R`, `s-R` is large negative, `s - s^2/R` is large positive. `exp(-large_positive)` is close to 0.
    # This assigns higher priority to tighter fits.
    # Now, let's check the dynamic steepness part:
    # If `item` is large relative to `bins_remain_cap` (e.g., `s=0.9R`), then `s/R = 0.9`.
    # `exp_arg = -base_steepness * (0.9R - (0.9R)^2/R) = -base_steepness * (0.9R - 0.81R^2/R) = -base_steepness * (0.9R - 0.81R) = -base_steepness * 0.09R`.
    # If `item` is small relative to `bins_remain_cap` (e.g., `s=0.1R`), then `s/R = 0.1`.
    # `exp_arg = -base_steepness * (0.1R - (0.1R)^2/R) = -base_steepness * (0.1R - 0.01R) = -base_steepness * 0.09R`.
    # The exponent argument calculation seems to be yielding similar values, which is not ideal.
    
    # The core idea: "Prioritize tight fits using non-linear functions on surplus. Tune steepness/parameters for balanced preference."
    # And "Balance fit with item/bin proportions."
    # A tight fit means `bins_remain_cap - item` is small.
    # A balanced proportion could be related to `item / bins_remain_cap`.
    # Let's try a simple modification: penalize loose fits more severely when the item is large.
    # Score = `1.0` for perfect fit.
    # Score = `1.0 - (bins_remain_cap[i] - item) / bins_remain_cap[can_fit_mask]` for other fits.
    # This is linear. Let's make it non-linear.
    # Score = `1.0 - ((bins_remain_cap[i] - item) / bins_remain_cap[can_fit_mask]) ** power`
    # If `power > 1`, this increases penalty for larger surplus.
    # Let `power` depend on item size relative to bin capacity.
    # `power = 1 + alpha * (item / bins_remain_cap[can_fit_mask])` where alpha is a tuning param.
    
    # Let's refine the sigmoid approach.
    # `1 / (1 + exp(steepness * (surplus)))`
    # `surplus = R - s`.
    # We want higher priority for smaller surplus.
    # We want `steepness` to be higher when `s/R` is high (item is large relative to bin).
    # `steepness = base_steepness * (item / R)`
    # `arg = (base_steepness * (s/R)) * (R-s)`
    # `arg = base_steepness * (s/R) * R * (1 - s/R)`
    # `arg = base_steepness * s * (1 - s/R)`
    
    # Let's test this. Assume `base_steepness = 5`.
    # Case 1: `s=0.9`, `R=1.0`. `arg = 5 * 0.9 * (1 - 0.9/1.0) = 5 * 0.9 * 0.1 = 0.45`. Score = `1/(1+exp(0.45))` = 0.38.
    # Case 2: `s=0.1`, `R=1.0`. `arg = 5 * 0.1 * (1 - 0.1/1.0) = 5 * 0.1 * 0.9 = 0.45`. Score = `1/(1+exp(0.45))` = 0.38.
    # This still gives same scores. The problem might be how `s/R` and `R-s` interact.
    
    # What if the steepness is related to the *ideal* remaining capacity if the item was the bottleneck?
    # Or, what if we transform the surplus?
    # Let `f(surplus) = surplus / item`. This is smaller for tighter fits relative to item size.
    # `steepness = base_steepness / (1 + f(surplus))`
    # `f(surplus) = (R-s) / s = R/s - 1`.
    # `steepness = base_steepness / (1 + R/s - 1) = base_steepness / (R/s) = base_steepness * s/R`.
    # This leads to the same formula again.
    
    # Let's try a simpler, more direct approach related to the reflection:
    # "Prioritize tight fits using non-linear functions on surplus."
    # `priority = 1.0 / (1.0 + (bins_remain_cap[i] - item) ** steepness)`
    # This gives higher priority for smaller surplus.
    # We want `steepness` to be larger when `item` is large relative to `bins_remain_cap`.
    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    
    surplus = bins_remain_cap[can_fit_mask] - item
    # Ensure surplus is non-negative due to floating point inaccuracies
    surplus = np.maximum(0, surplus) 
    
    # Dynamic steepness: higher when item is a larger fraction of the bin's *original* capacity (or current remaining capacity)
    # Using bins_remain_cap[can_fit_mask] for ratio calculation.
    ratio_item_to_bin = item / (bins_remain_cap[can_fit_mask] + epsilon)
    dynamic_steepness = base_steepness * ratio_item_to_bin
    
    # Calculate exponent argument. We want smaller surplus to have smaller exponent arg.
    # So, `exponent_arg = dynamic_steepness * surplus`
    # Test:
    # Case 1: `item = 0.9`, `R = 1.0`. `surplus = 0.1`. `ratio = 0.9`. `steepness = 3 * 0.9 = 2.7`. `arg = 2.7 * 0.1 = 0.27`. Score `1/(1+0.27)` = 0.78.
    # Case 2: `item = 0.1`, `R = 1.0`. `surplus = 0.9`. `ratio = 0.1`. `steepness = 3 * 0.1 = 0.3`. `arg = 0.3 * 0.9 = 0.27`. Score `1/(1+0.27)` = 0.78.
    # Still same scores. The multiplication is commutative in this way.

    # Let's try making the steepness influence the *surplus* term before the sigmoid.
    # `sigmoid(x) = 1 / (1 + exp(x))`
    # We want `x` to be small for good fits.
    # `x = steepness * transformed_surplus`
    # If `item` is large, we want to penalize `surplus` more.
    # `transformed_surplus = surplus / item` (smaller for tighter fits relative to item size)
    # `steepness = base_steepness * (item / bins_remain_cap[can_fit_mask])`
    # `x = (base_steepness * (s/R)) * ((R-s)/s)`
    # `x = base_steepness * (s/R) * (R/s - 1)`
    # `x = base_steepness * (1 - s/R)`
    # This means the argument only depends on the ratio `s/R`. Higher ratio means lower argument (closer to 0).
    # This would prioritize items that are a large fraction of the bin capacity.
    # Let's test:
    # Case 1: `s=0.9`, `R=1.0`. `ratio = 0.9`. `arg = 3 * (1 - 0.9) = 0.3`. Score = `1/(1+exp(0.3))` = 0.42.
    # Case 2: `s=0.1`, `R=1.0`. `ratio = 0.1`. `arg = 3 * (1 - 0.1) = 2.7`. Score = `1/(1+exp(2.7))` = 0.06.
    # This seems better. It strongly prioritizes items that fill the bin more.

    # Let's adjust to make tight fits get higher scores.
    # The formula `1/(1+exp(x))` gives higher score when `x` is smaller (more negative).
    # So we want `x = -base_steepness * (1 - s/R)`?
    # `x = base_steepness * (s/R - 1)`.
    # Case 1: `s=0.9`, `R=1.0`. `ratio = 0.9`. `arg = 3 * (0.9 - 1) = -0.3`. Score = `1/(1+exp(-0.3))` = 0.57.
    # Case 2: `s=0.1`, `R=1.0`. `ratio = 0.1`. `arg = 3 * (0.1 - 1) = -2.7`. Score = `1/(1+exp(-2.7))` = 0.93.
    # This prioritizes small items fitting into large bins. Not ideal.
    
    # Let's go back to the idea of dynamically scaling the surplus itself.
    # We want to penalize `surplus` more if `item` is large relative to `bins_remain_cap`.
    # Consider `priority = 1.0 / (1.0 + exp(base_steepness * (surplus / item)))`
    # This maps `surplus/item` (smaller is better) to `[0, inf)`.
    # If `item` is large, `surplus/item` is smaller for the same absolute `surplus`.
    # Example: `s=0.9`, `R=1.0`. `surplus=0.1`. `surplus/item = 0.1/0.9 = 0.11`. `arg = 3 * 0.11 = 0.33`. Score = 0.42.
    # Example: `s=0.1`, `R=1.0`. `surplus=0.9`. `surplus/item = 0.9/0.1 = 9`. `arg = 3 * 9 = 27`. Score = near 0.
    # This prioritizes items that leave less *relative* surplus compared to their own size.
    # This favors items that are close to the bin's remaining capacity, where "close" is measured in proportion to item size.
    
    # Let's use this: `arg = base_steepness * (surplus / (item + epsilon))`
    
    surplus = bins_remain_cap[can_fit_mask] - item
    surplus = np.maximum(0, surplus) # Ensure non-negative surplus
    
    # Calculate priority using a sigmoid function that emphasizes tighter fits,
    # with sensitivity increasing for larger items relative to remaining capacity.
    # The argument to the sigmoid is `steepness * (surplus / item)`
    # A smaller `surplus / item` means a tighter fit relative to item size.
    # We want smaller arguments to yield higher priorities (e.g., `1 / (1 + exp(arg))`).
    # `steepness` is `base_steepness` potentially modified.
    # If we want to prioritize larger items that fit tightly, we might need `steepness` to be larger for larger items.
    # However, the reflection says "Balance fit with item/bin proportions."
    # Let's try `steepness = base_steepness`.
    # The term `surplus / item` effectively normalizes the surplus by the item size.
    # A small surplus relative to item size is prioritized.
    # This means a large item fitting into a nearly full bin gets high priority.
    # A small item fitting into a nearly full bin also gets high priority.
    # A large item fitting into a mostly empty bin gets low priority.
    # A small item fitting into a mostly empty bin gets very low priority.
    
    exponent_args = base_steepness * (surplus / (item + epsilon))
    
    # Clip to avoid overflow in np.exp.
    # `surplus / item` can be large if `item` is very small.
    # e.g., `item=0.001`, `R=1.0`, `surplus=0.999`. `surplus/item = 999`.
    # `base_steepness * 999` can be large.
    clipped_exponent_args = np.clip(exponent_args, -30.0, 70.0) # Allow larger positive values for exp

    # Calculate priorities: 1 / (1 + exp(arg)). Higher priority for smaller arg.
    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))

    return priorities
```
