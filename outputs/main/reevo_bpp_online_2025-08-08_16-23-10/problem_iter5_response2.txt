```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a smoothed
    non-linear function favoring tighter fits.

    This heuristic prioritizes bins that can accommodate the item and have the smallest
    remaining capacity after packing (Best Fit strategy). The priority is calculated
    using a softened version of the "smallest surplus" idea. Specifically, for bins
    that can fit the item, the priority is determined by `sigmoid(-steepness * (remaining_capacity - item))`.
    This function is monotonically decreasing with respect to `(remaining_capacity - item)`,
    meaning smaller non-negative remaining capacities (tighter fits) get higher scores.

    The score for a bin is 0 if the item cannot fit. For bins that can fit,
    the score is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).
    This formulation ensures that a perfect fit (remaining_capacity - item = 0) results
    in a score of 0.5, tighter fits (negative surplus) result in scores > 0.5, and
    looser fits (positive surplus) result in scores < 0.5.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores range from 0 (cannot fit or very loose fit) up to 1 (perfect or very tight fit).
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # Calculate the post-placement remaining capacity for bins that can fit the item.
    post_placement_remain_cap = bins_remain_cap[can_fit_mask] - item

    # Calculate the exponent argument for the sigmoid function.
    # We want to prioritize smaller `post_placement_remain_cap`.
    # The function `1 / (1 + exp(x))` is decreasing in `x`.
    # To make it decrease as `post_placement_remain_cap` increases, we set `x = steepness * post_placement_remain_cap`.
    # A small `post_placement_remain_cap` (tight fit) results in a smaller `x`, thus a higher score.
    # A large `post_placement_remain_cap` (loose fit) results in a larger `x`, thus a lower score.
    # For a perfect fit (post_placement_remain_cap = 0), x=0, score=0.5.
    # For negative post_placement_remain_cap (item is smaller than remaining cap), x is negative, exp(x) is small, score > 0.5.
    # For positive post_placement_remain_cap (item is larger than remaining cap, which should not happen due to can_fit_mask,
    # but conceptually if it did, x would be positive, score < 0.5).
    exponent_args = steepness * post_placement_remain_cap

    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.
    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.
    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.
    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)

    # Calculate the priority scores for the valid bins using the sigmoid function.
    # This results in scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative).
    # Scores for bins that cannot fit remain 0.
    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))

    return priorities
```
