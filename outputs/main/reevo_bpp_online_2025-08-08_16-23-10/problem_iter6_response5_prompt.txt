{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined\n    Sigmoid Best Fit approach that also considers the \"Worst Fit\" aspect for\n    bins that are not a tight fit.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it favors bins that result in a smaller remaining capacity (Best Fit).\n    However, to avoid creating too many nearly-full bins prematurely, it also\n    gives a moderate score to bins that are a significantly looser fit,\n    preventing them from being completely ignored but still prioritizing\n    tighter fits.\n\n    The priority is calculated using a sigmoid-like function that has a steeper\n    increase for near-fits and a slower decrease for looser fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated based on the remaining capacity `rc = bins_remain_cap[i] - item`.\n    The function is designed such that:\n    - A perfect fit (rc=0) gets a high score.\n    - A slightly loose fit (small positive rc) gets a slightly lower score.\n    - A significantly loose fit (larger positive rc) gets a score that decays\n      slower than a simple inverse, allowing these bins to still be considered\n      without dominating the selection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are designed to be non-negative, with higher scores indicating\n        higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Parameters to tune the behavior:\n    # `tight_fit_steepness`: Controls how quickly the priority drops for near-perfect fits.\n    # `loose_fit_decay_rate`: Controls how quickly the priority drops for looser fits.\n    # `mid_point_shift`: Shifts the \"middle\" of the sigmoid-like curve to favor\n    #                    slightly looser fits being considered more than a strict best fit.\n    tight_fit_steepness = 10.0  # Higher value means stronger preference for very tight fits\n    loose_fit_decay_rate = 0.5  # Lower value means slower decay for looser fits\n    mid_point_shift = 0.2       # Positive shift makes the curve respond to larger gaps\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate scores. We want higher scores for smaller `potential_remaining_cap_valid`.\n    # A simple approach could be exp(-k * rc), but we want to differentiate between\n    # tight and loose fits.\n    # We can use a combination, or a shifted sigmoid.\n    # Let's use a sigmoid-like function that can be tuned.\n    # We want `f(rc)` to be high for small `rc` and decrease as `rc` increases.\n    # Consider a function like `1 / (1 + exp(a * (rc - b)))` where `a` is steepness\n    # and `b` is a shift.\n    # To achieve the desired behavior (stronger preference for tight fits, slower decay for loose):\n    # We can use a sigmoid that has a steeper slope around 0 and a shallower slope later.\n    # An alternative is to use a function that rewards small gaps more, and moderately\n    # rewards larger gaps.\n    # Let's try to model this:\n    # For rc near 0: prioritize highly.\n    # For rc moderately larger: still prioritize, but less so.\n    # For rc very large: still give some priority to avoid discarding bins entirely.\n\n    # Let's use a transformation that maps remaining capacity to priority.\n    # A function like `exp(-k1 * rc) * (1 + k2 * rc)` or a modified sigmoid.\n\n    # Using a sigmoid with adjusted parameters and potentially an additive term\n    # for looser fits to prevent their scores from dropping too quickly.\n    # We'll map `potential_remaining_cap_valid` to an exponent argument.\n    # For tighter fits, we want the exponent argument to be small (highly negative for sigmoid).\n    # For looser fits, we want the exponent argument to be less negative or even positive,\n    # leading to lower scores but not zero.\n\n    # Let's use a two-part strategy or a more complex sigmoid formulation.\n    # A common approach for this is to use a function that saturates or decays\n    # slowly.\n    # Let's try a function that resembles `1 / (1 + exp(steepness * (gap - shift)))`\n    # but we want to tune it.\n    # We can achieve a varying steepness by making the exponent argument a function of `rc`.\n\n    # A common heuristic for \"balanced\" packing is \"First Fit Decreasing\" (FFD)\n    # for offline, but for online, \"Best Fit\" (BF) and \"Worst Fit\" (WF) are common.\n    # BF aims for tight fits. WF aims for loose fits.\n    # A good heuristic might combine these.\n\n    # Let's consider `f(rc) = exp(-tight_fit_steepness * rc)` for tight fits (rc close to 0)\n    # and then transition to a slower decay.\n    # `f(rc) = C * exp(-loose_fit_decay_rate * rc)` for larger `rc`.\n\n    # A simpler way to achieve a smooth transition and non-zero scores for loose fits\n    # without immediate zeroing could be a scaled inverse or exponential decay.\n    # Let's try a formulation inspired by sigmoid but adjusted:\n    # Score = exp(-k * (rc - shift))  - for tight fits\n    # Score = C * exp(-k2 * rc)      - for loose fits\n\n    # Let's use a single function that interpolates or combines these behaviors.\n    # A sigmoid function: `1 / (1 + exp(x))` where `x = steepness * (rc - shift)`\n    # We can tune `steepness` and `shift`.\n    # To make scores decay slower for loose fits, we can make the function less sensitive\n    # to larger `rc`.\n\n    # Let's try mapping `rc` to `exp_arg`.\n    # If `rc` is small, `exp_arg` should be very negative.\n    # If `rc` is larger, `exp_arg` should be less negative, approaching zero or positive.\n    # `exp_arg = tight_fit_steepness * (potential_remaining_cap_valid - mid_point_shift)`\n    # This makes very small `rc` highly negative, leading to scores near 1.\n    # Larger `rc` values lead to less negative `exp_arg`, reducing the score.\n    # We can further control the decay of loose fits by adjusting `tight_fit_steepness`\n    # and `mid_point_shift`.\n\n    # Let's refine the sigmoid approach.\n    # `score = 1 / (1 + exp(steepness * (gap - offset)))`\n    # To make it decay slower for loose fits, we can also add a small baseline score\n    # or ensure the exponent doesn't grow too large too quickly.\n\n    # Consider the function: `f(gap) = exp(-k * gap)`. This decays exponentially.\n    # For tight fits, `k` should be large. For loose fits, `k` should be small.\n    # We can combine this:\n    # `f(gap) = exp(-k_tight * gap)` for `gap < threshold`\n    # `f(gap) = C * exp(-k_loose * gap)` for `gap >= threshold`\n\n    # A single function that achieves a similar effect might be:\n    # `score = exp(-k1 * gap) - k2 * gap` or similar additive adjustments.\n\n    # Let's re-evaluate the sigmoid `1 / (1 + exp(x))`.\n    # If `x = steepness * (gap - shift)`, for small `gap`, `x` is very negative, score is ~1.\n    # For large `gap`, `x` is positive, score approaches 0.\n    # To make loose fits have higher scores, we can shift the curve to the right\n    # (`shift` positive) or reduce `steepness`.\n    # Let's try increasing `steepness` but also adding a term that keeps the score\n    # from dropping too fast for larger gaps.\n\n    # Alternative approach: Rank based on `rc`.\n    # Smallest `rc` gets rank 1, next gets rank 2, etc.\n    # Then transform ranks to scores. A non-linear transformation is better.\n\n    # Let's try to modify the sigmoid exponent argument.\n    # We want the exponent to grow less rapidly for larger `rc`.\n    # Instead of `steepness * rc`, consider `steepness * (rc^p)` where `p < 1` or `steepness * log(rc)`.\n\n    # Let's try: `exponent_arg = steepness * (potential_remaining_cap_valid ** power)`\n    # where `power` is less than 1 to slow down the growth for larger `rc`.\n    # Or, even simpler, directly adjust the `steepness` for different ranges of `rc`.\n\n    # Let's consider a heuristic that assigns priority based on the *relative* gap.\n    # `relative_gap = potential_remaining_cap_valid / item` (if item > 0)\n    # Or `relative_gap = potential_remaining_cap_valid / bin_capacity` (if bin_capacity is known).\n    # Since we only have `bins_remain_cap`, `item` is the best reference.\n\n    # Let's go back to the sigmoid and tune it for the desired behavior.\n    # `score = 1 / (1 + exp(steepness * (rc - shift)))`\n    # - To favor tight fits strongly: increase `steepness`.\n    # - To give looser fits more chance: increase `shift` (this effectively moves the steep part to the right).\n    #   Or, use a softer decay.\n\n    # Let's use a function that directly rewards small gaps and decays slowly.\n    # `score = exp(-k * gap)` is a starting point.\n    # To make the decay slower for larger gaps, we can use a form like:\n    # `score = exp(-k * gap) * (1 + alpha * gap)`\n    # Or `score = exp(-k * gap) / (1 + alpha * gap)` (inverse relationship for alpha)\n    # Or a piecewise approach.\n\n    # Let's try a combination of tight fit reward and loose fit \"grace\".\n    # `tight_priority = exp(-tight_fit_steepness * potential_remaining_cap_valid)`\n    # `loose_priority = exp(-loose_fit_decay_rate * potential_remaining_cap_valid)`\n    # We want to transition from `tight_priority` to `loose_priority`.\n\n    # A simpler sigmoid adjustment:\n    # Let `g = potential_remaining_cap_valid`.\n    # We want to compute a score `s(g)`.\n    # `s(0)` should be high. `s(large_g)` should be low but non-zero.\n    # A function like `exp(-k * g)` is good but decays quickly.\n    # `s(g) = 1.0 / (1.0 + exp(steepness * (g - shift)))`\n    # If we want looser fits to have higher scores, we need to reduce the exponent's\n    # positive values. This can be done by increasing `shift` or decreasing `steepness`.\n    # Let's try increasing `shift` to push the high-priority region further.\n\n    # `shift_value = 0.3` # A small positive gap might still be considered \"good enough\".\n    # `exponent_args = steepness * (potential_remaining_cap_valid - shift_value)`\n    # This would make a gap of `shift_value` have an exponent argument of 0, score 0.5.\n    # Gaps smaller than `shift_value` get scores > 0.5. Gaps larger get scores < 0.5.\n    # This is still best-fit like.\n\n    # To allow looser fits to have higher priority than just a decaying sigmoid score:\n    # we can consider the *inverse* of the remaining capacity as a factor,\n    # or give a baseline score.\n\n    # Let's try a formulation that rewards small gaps more strongly and then\n    # gradually rewards larger gaps with a slower decay.\n    # `score = exp(-k1 * g) * (1 + k2 * g)` seems promising.\n    # If `k1` is large, it's a tight fit preference. `k2` controls the boost for larger gaps.\n    # `k1 = tight_fit_steepness`\n    # `k2 = loose_fit_decay_rate` (inverse relationship, so `k2` should be small if `gap` is large)\n\n    # Let's use a function of the form: `score = exp(-k * g) * (1 + alpha * g)`.\n    # `k` controls initial decay, `alpha` controls the \"boost\" for larger gaps.\n    # For small `g`, `score ~ exp(-k * g)`.\n    # For large `g`, `score ~ (exp(-k * g) * alpha * g)`.\n    # This term `alpha * g * exp(-k * g)` has a maximum and then decays.\n    # We want `k` to be related to `tight_fit_steepness` and `alpha` to `loose_fit_decay_rate`.\n\n    # Let `k = steepness` and `alpha = decay_rate`.\n    # `score = np.exp(-steepness * potential_remaining_cap_valid) * (1 + decay_rate * potential_remaining_cap_valid)`\n\n    # Let's test this:\n    # If `g=0`: `score = exp(0) * (1 + 0) = 1` (perfect fit).\n    # If `g` is small positive: `score = exp(-steepness*g) * (1 + decay_rate*g)`.\n    # The `exp(-steepness*g)` term dominates and reduces the score.\n    # If `g` is larger: `exp(-steepness*g)` drops, but `(1 + decay_rate*g)` increases.\n    # We need to ensure the overall score is reasonable.\n\n    # Let's adjust the functional form to be more in line with sigmoid properties but with tuning.\n    # Consider a function that has a 'knee' rather than a smooth decay.\n    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`\n    # We can make the decay slower for larger gaps by making the exponent argument grow slower.\n    # Use `steepness * sqrt(gap)` or `steepness * log(gap + 1)`.\n\n    # Let's try a robust sigmoid modification that balances tight fits and allows looser fits.\n    # `score = 1.0 / (1.0 + exp(steepness * (g - g0)))` where `g0` is a target gap.\n    # For looser fits to be favored slightly:\n    # We can normalize the gap by item size: `g_norm = g / item`\n    # And use a sigmoid on `g_norm`: `1.0 / (1.0 + exp(steepness * (g_norm - shift_norm)))`\n\n    # Let's use the `exp(-k*g) * (1 + alpha*g)` form as it directly models\n    # strong initial decay and slower decay for larger values.\n    # `k` (steepness) should be large for tight fits.\n    # `alpha` (decay_rate) should be small if we want scores to drop fast for loose fits,\n    # and larger if we want loose fits to retain priority.\n    # So, `alpha` should be positively correlated with the preference for looser fits.\n    # If we want to prioritize tight fits but not ignore loose ones, `alpha` should be small.\n\n    # Let `steepness = tight_fit_steepness`\n    # Let `alpha = loose_fit_decay_rate` (This parameter name is a bit counter-intuitive, let's call it `loose_fit_boost_factor`).\n    # `boost_factor` controls how much larger gaps are \"boosted\" relative to strict exponential decay.\n    # Higher `boost_factor` means slower decay for loose fits.\n\n    boost_factor = loose_fit_decay_rate # Renaming for clarity\n    \n    # Calculate scores: exp(-steepness * g) * (1 + boost_factor * g)\n    # Need to handle potential overflows/underflows in exp.\n    # The term `steepness * g` can become very large.\n    # `exp(-large)` -> 0.\n    # `(1 + boost_factor * g)` can also grow.\n    # If `steepness` is large and `g` is small, `exp` dominates.\n    # If `boost_factor` is non-zero and `g` is large, `(1 + boost_factor * g)` grows.\n    # The product can still be problematic.\n\n    # Let's try to normalize the gaps or use a more stable function.\n    # Consider `score = exp(-k*g)`. This is stable.\n    # To make it decay slower for larger gaps, we can cap the exponent value or use a different function.\n\n    # Let's try a piecewise definition or a modified sigmoid that has varying steepness.\n    # `score = 1.0 / (1.0 + exp(steepness * (g - shift)))`\n    # To make it decay slower, increase `shift`. This moves the steep part to the right.\n    # `shift` can be thought of as a \"tolerance\" for gaps.\n\n    # Let's re-evaluate the reflection: \"Smoothness, non-linearity, and strategic parameter tuning improve heuristic performance.\"\n    # The sigmoid function is smooth and non-linear. Parameter tuning is key.\n    # The problem statement for `v1` suggests strong preference for tight fits.\n    # `v2` should potentially offer a balance.\n\n    # Let's try to create a scoring function that:\n    # 1. Gives high scores to tight fits (small `g`).\n    # 2. Gives moderate scores to slightly looser fits.\n    # 3. Gives a small but non-zero score to very loose fits, ensuring they are not entirely ignored.\n\n    # Consider `score = exp(-k1 * g)` for `g` up to a certain threshold `T`, and\n    # `score = C * exp(-k2 * g)` for `g > T` with `k2 < k1`.\n    # This is a piecewise approach.\n    # For a single function:\n    # `score = exp(-k * g)` where `k` itself is a function of `g`.\n    # e.g., `k = k_tight` for small `g`, `k = k_loose` for large `g`.\n\n    # Let's use a clamped exponential decay with an additive bonus for larger gaps.\n    # `score = exp(-steepness * potential_remaining_cap_valid) + bonus_factor * min(potential_remaining_cap_valid, bonus_cap)`\n    # The `bonus_factor` should be small, and `bonus_cap` defines the range for bonus.\n\n    # A simple modification to the sigmoid from v1:\n    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`\n    # Let `steepness` control the initial drop.\n    # Let `shift` control the point where scores become less than 0.5.\n    # To make scores decay slower: increase `shift`. This pushes the significant drop to larger gaps.\n\n    # Let's use a formulation inspired by logistic functions but with tunable rates.\n    # `score = 1.0 / (1.0 + exp(k * (g - g_mid)))`\n    # `k` = steepness\n    # `g_mid` = gap at which score is 0.5\n\n    # Let's try to shape the decay rate directly.\n    # For small `g`, we want `exp(-k_high * g)`.\n    # For large `g`, we want `exp(-k_low * g)`.\n    # A function like `exp(-k_high * g / (1 + alpha * g))` can transition between these.\n    # When `g` is small, `1 + alpha*g ~ 1`, so `exp(-k_high * g)`.\n    # When `g` is large, `1 + alpha*g` grows, making the exponent argument smaller (less negative),\n    # effectively slowing down the decay.\n\n    # `k_high = tight_fit_steepness`\n    # `k_low` (implicit) should be smaller.\n    # `alpha` = `loose_fit_decay_rate`\n    # `score = exp(-k_high * g / (1 + alpha * g))`\n\n    # This form seems promising. It's smooth, non-linear, and tunable.\n    # `tight_fit_steepness` controls how sharply the score drops for gaps close to zero.\n    # `loose_fit_decay_rate` controls how quickly the decay rate slows down for larger gaps.\n    # A higher `loose_fit_decay_rate` means slower decay for loose fits.\n\n    # Let's set parameters:\n    # `steepness` (k_high) = 10.0: Very sharp drop for small gaps.\n    # `decay_rate` (alpha) = 0.5: Makes the decay significantly slower for larger gaps.\n\n    # We need to clip the exponent argument to avoid `exp` overflow/underflow.\n    # The exponent is `-k_high * g / (1 + alpha * g)`.\n    # As `g` -> infinity, exponent -> `-k_high / alpha`.\n    # We need to ensure `-k_high / alpha` is not too large negative.\n    # If `k_high = 10`, `alpha = 0.5`, then `-k_high / alpha = -20`.\n    # This is fine for `exp`.\n    # If `g` is very close to 0, exponent is close to 0.\n    # If `g` is small positive, exponent is slightly negative.\n\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent term: -k_high * g / (1 + alpha * g)\n    # Avoid division by zero if g is negative (which shouldn't happen here, as g >= 0)\n    # Ensure denominator is at least 1 to avoid issues with very small alpha and g.\n    denominator = 1.0 + alpha * potential_remaining_cap_valid\n    # If denominator is extremely close to zero (e.g., alpha is tiny and g is tiny negative, though g is >=0), it could be an issue.\n    # For g >= 0, alpha >= 0, denominator is always >= 1.0.\n    \n    exponent_args = -k_high * potential_remaining_cap_valid / denominator\n\n    # Clip the exponent arguments for numerical stability.\n    # Values between -30 and 30 are generally safe.\n    # For `exponent_args = -k_high * g / (1 + alpha * g)`:\n    # Min value when g -> infinity: -k_high / alpha. If this is < -30, we should clip.\n    # Max value when g = 0: 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate priorities\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    # Normalization consideration: The scores are not guaranteed to sum to a specific value or be in a fixed range [0, 1].\n    # The current function `exp(-k*g / (1+a*g))` ranges from 1 (at g=0) down to `exp(-k/a)`.\n    # This is fine as we only care about relative priorities.\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed\n    non-linear function favoring tighter fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a softened version of the \"smallest surplus\" idea. Specifically, for bins\n    that can fit the item, the priority is determined by `sigmoid(-steepness * (remaining_capacity - item))`.\n    This function is monotonically decreasing with respect to `(remaining_capacity - item)`,\n    meaning smaller non-negative remaining capacities (tighter fits) get higher scores.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).\n    This formulation ensures that a perfect fit (remaining_capacity - item = 0) results\n    in a score of 0.5, tighter fits (negative surplus) result in scores > 0.5, and\n    looser fits (positive surplus) result in scores < 0.5.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) up to 1 (perfect or very tight fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the post-placement remaining capacity for bins that can fit the item.\n    post_placement_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `post_placement_remain_cap`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it decrease as `post_placement_remain_cap` increases, we set `x = steepness * post_placement_remain_cap`.\n    # A small `post_placement_remain_cap` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `post_placement_remain_cap` (loose fit) results in a larger `x`, thus a lower score.\n    # For a perfect fit (post_placement_remain_cap = 0), x=0, score=0.5.\n    # For negative post_placement_remain_cap (item is smaller than remaining cap), x is negative, exp(x) is small, score > 0.5.\n    # For positive post_placement_remain_cap (item is larger than remaining cap, which should not happen due to can_fit_mask,\n    # but conceptually if it did, x would be positive, score < 0.5).\n    exponent_args = steepness * post_placement_remain_cap\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # This results in scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative).\n    # Scores for bins that cannot fit remain 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities\n\n[Reflection]\nSmooth, non-linear functions with tunable parameters for balanced packing.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}