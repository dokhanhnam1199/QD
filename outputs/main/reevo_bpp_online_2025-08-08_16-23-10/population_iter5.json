[
  {
    "stdout_filepath": "problem_iter4_response0.txt_stdout.txt",
    "code_path": "problem_iter4_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    This version implements the \"Best Fit\" heuristic by prioritizing bins\n    that have the least remaining capacity *after* the item is placed.\n    This aims to minimize wasted space. Bins that cannot fit the item are\n    given a priority of negative infinity to ensure they are never selected.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Identify bins that have enough capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # The \"best fit\" is the bin with the smallest remaining capacity.\n    # To maximize priority, we want to minimize this remaining capacity.\n    # Therefore, we can assign the negative of the remaining capacity as the priority.\n    # A smaller positive remaining capacity will result in a larger negative value,\n    # which will be correctly selected as the maximum by a max-priority queue or similar selection mechanism.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 0,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response1.txt_stdout.txt",
    "code_path": "problem_iter4_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a temperature-controlled exponential fit.\n\n    This priority function aims to prioritize bins with less remaining capacity that can still fit the item.\n    It uses an exponential function with a temperature parameter to smooth the priority distribution.\n    Bins that cannot fit the item are assigned a priority of zero.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Define a temperature parameter. Lower temperature makes the priority sharper,\n    # favoring bins that are a closer fit. Higher temperature smooths the distribution.\n    # A value of 1.0 can be a good starting point, similar to Softmax.\n    temperature = 1.0\n\n    # Initialize priorities to zero. Bins that cannot fit the item will retain this zero priority.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a score.\n    # We want to prioritize bins with less remaining capacity (closer fit).\n    # A good heuristic is to use the negative of the remaining capacity.\n    # However, to favor bins that are *just* enough, we can use the negative of the *surplus* capacity.\n    # Surplus capacity = bins_remain_cap - item.\n    # So, score = -(bins_remain_cap - item).\n    # This makes a bin with exactly `item` capacity have a score of 0,\n    # a bin with `item + 1` capacity have a score of -1, and so on.\n\n    surplus_capacity = bins_remain_cap[can_fit_mask] - item\n    \n    # We want higher priority for smaller surplus capacity.\n    # The function `exp(-surplus_capacity / temperature)` achieves this.\n    # When surplus_capacity is 0, exp(0) = 1.\n    # When surplus_capacity is positive, exp(-positive/temperature) < 1.\n    # As surplus_capacity increases, the score decreases exponentially.\n\n    scores = np.exp(-surplus_capacity / temperature)\n\n    # Assign the calculated scores to the priorities array for the bins that can fit the item.\n    priorities[can_fit_mask] = scores\n\n    return priorities",
    "response_id": 1,
    "obj": 4.198244914240141,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response2.txt_stdout.txt",
    "code_path": "problem_iter4_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    This version implements the \"Best Fit\" heuristic. It prioritizes bins\n    that have the least remaining capacity *after* the item is placed,\n    thereby minimizing wasted space. Bins that cannot fit the item are given\n    a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the remaining capacity after placing the item.\n    # The goal is to minimize this remaining capacity to achieve the \"best fit\".\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize bins with the smallest remaining capacity, we can assign\n    # the negative of this remaining capacity as the priority. A smaller\n    # (more negative) value means a tighter fit, and thus a higher priority\n    # when we seek the maximum priority score.\n    priorities[can_fit_mask] = -remaining_after_fit\n\n    return priorities",
    "response_id": 2,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response3.txt_stdout.txt",
    "code_path": "problem_iter4_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Sigmoid Fit Score strategy that emphasizes minimal resulting remaining capacity.\n\n    This heuristic prioritizes bins where placing the item leaves the least amount\n    of remaining capacity. It calculates the \"resulting remaining capacity\" for each\n    bin that can fit the item and then uses a sigmoid function to normalize these\n    values into priority scores between 0 and 1. A smaller resulting remaining\n    capacity leads to a higher priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to zero for all bins. Bins that cannot fit the item will retain this zero priority.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this resulting remaining capacity is minimized.\n    resulting_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize smaller resulting remaining capacities, we can use the inverse\n    # of the resulting remaining capacity as a measure of \"fit tightness\".\n    # A smaller resulting capacity means a larger inverse, hence a tighter fit.\n    # Add a small epsilon to avoid division by zero or extremely large values when resulting_remaining_cap is close to zero.\n    epsilon = 1e-9\n    fit_tightness = 1.0 / (resulting_remaining_cap + epsilon)\n\n    # Apply a sigmoid function to map the tightness scores to a normalized priority between 0 and 1.\n    # The sigmoid function `1 / (1 + exp(-x))` maps larger x to values closer to 1.\n    # We scale `fit_tightness` by a sensitivity parameter `k`. A larger `k` means\n    # a steeper increase in priority as the fit becomes tighter (resulting_remaining_cap decreases).\n    k = 10.0  # Sensitivity parameter: Controls how strongly tighter fits are favored.\n    scaled_tightness = k * fit_tightness\n    sigmoid_scores = 1 / (1 + np.exp(-scaled_tightness))\n\n    # Assign the calculated sigmoid scores to the bins that can fit the item.\n    priorities[can_fit_mask] = sigmoid_scores\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response4.txt_stdout.txt",
    "code_path": "problem_iter4_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined\n    Sigmoid Best Fit approach that also considers the \"Worst Fit\" aspect for\n    bins that are not a tight fit.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it favors bins that result in a smaller remaining capacity (Best Fit).\n    However, to avoid creating too many nearly-full bins prematurely, it also\n    gives a moderate score to bins that are a significantly looser fit,\n    preventing them from being completely ignored but still prioritizing\n    tighter fits.\n\n    The priority is calculated using a sigmoid-like function that has a steeper\n    increase for near-fits and a slower decrease for looser fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated based on the remaining capacity `rc = bins_remain_cap[i] - item`.\n    The function is designed such that:\n    - A perfect fit (rc=0) gets a high score.\n    - A slightly loose fit (small positive rc) gets a slightly lower score.\n    - A significantly loose fit (larger positive rc) gets a score that decays\n      slower than a simple inverse, allowing these bins to still be considered\n      without dominating the selection.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are designed to be non-negative, with higher scores indicating\n        higher priority.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Parameters to tune the behavior:\n    # `tight_fit_steepness`: Controls how quickly the priority drops for near-perfect fits.\n    # `loose_fit_decay_rate`: Controls how quickly the priority drops for looser fits.\n    # `mid_point_shift`: Shifts the \"middle\" of the sigmoid-like curve to favor\n    #                    slightly looser fits being considered more than a strict best fit.\n    tight_fit_steepness = 10.0  # Higher value means stronger preference for very tight fits\n    loose_fit_decay_rate = 0.5  # Lower value means slower decay for looser fits\n    mid_point_shift = 0.2       # Positive shift makes the curve respond to larger gaps\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate scores. We want higher scores for smaller `potential_remaining_cap_valid`.\n    # A simple approach could be exp(-k * rc), but we want to differentiate between\n    # tight and loose fits.\n    # We can use a combination, or a shifted sigmoid.\n    # Let's use a sigmoid-like function that can be tuned.\n    # We want `f(rc)` to be high for small `rc` and decrease as `rc` increases.\n    # Consider a function like `1 / (1 + exp(a * (rc - b)))` where `a` is steepness\n    # and `b` is a shift.\n    # To achieve the desired behavior (stronger preference for tight fits, slower decay for loose):\n    # We can use a sigmoid that has a steeper slope around 0 and a shallower slope later.\n    # An alternative is to use a function that rewards small gaps more, and moderately\n    # rewards larger gaps.\n    # Let's try to model this:\n    # For rc near 0: prioritize highly.\n    # For rc moderately larger: still prioritize, but less so.\n    # For rc very large: still give some priority to avoid discarding bins entirely.\n\n    # Let's use a transformation that maps remaining capacity to priority.\n    # A function like `exp(-k1 * rc) * (1 + k2 * rc)` or a modified sigmoid.\n\n    # Using a sigmoid with adjusted parameters and potentially an additive term\n    # for looser fits to prevent their scores from dropping too quickly.\n    # We'll map `potential_remaining_cap_valid` to an exponent argument.\n    # For tighter fits, we want the exponent argument to be small (highly negative for sigmoid).\n    # For looser fits, we want the exponent argument to be less negative or even positive,\n    # leading to lower scores but not zero.\n\n    # Let's use a two-part strategy or a more complex sigmoid formulation.\n    # A common approach for this is to use a function that saturates or decays\n    # slowly.\n    # Let's try a function that resembles `1 / (1 + exp(steepness * (gap - shift)))`\n    # but we want to tune it.\n    # We can achieve a varying steepness by making the exponent argument a function of `rc`.\n\n    # A common heuristic for \"balanced\" packing is \"First Fit Decreasing\" (FFD)\n    # for offline, but for online, \"Best Fit\" (BF) and \"Worst Fit\" (WF) are common.\n    # BF aims for tight fits. WF aims for loose fits.\n    # A good heuristic might combine these.\n\n    # Let's consider `f(rc) = exp(-tight_fit_steepness * rc)` for tight fits (rc close to 0)\n    # and then transition to a slower decay.\n    # `f(rc) = C * exp(-loose_fit_decay_rate * rc)` for larger `rc`.\n\n    # A simpler way to achieve a smooth transition and non-zero scores for loose fits\n    # without immediate zeroing could be a scaled inverse or exponential decay.\n    # Let's try a formulation inspired by sigmoid but adjusted:\n    # Score = exp(-k * (rc - shift))  - for tight fits\n    # Score = C * exp(-k2 * rc)      - for loose fits\n\n    # Let's use a single function that interpolates or combines these behaviors.\n    # A sigmoid function: `1 / (1 + exp(x))` where `x = steepness * (rc - shift)`\n    # We can tune `steepness` and `shift`.\n    # To make scores decay slower for loose fits, we can make the function less sensitive\n    # to larger `rc`.\n\n    # Let's try mapping `rc` to `exp_arg`.\n    # If `rc` is small, `exp_arg` should be very negative.\n    # If `rc` is larger, `exp_arg` should be less negative, approaching zero or positive.\n    # `exp_arg = tight_fit_steepness * (potential_remaining_cap_valid - mid_point_shift)`\n    # This makes very small `rc` highly negative, leading to scores near 1.\n    # Larger `rc` values lead to less negative `exp_arg`, reducing the score.\n    # We can further control the decay of loose fits by adjusting `tight_fit_steepness`\n    # and `mid_point_shift`.\n\n    # Let's refine the sigmoid approach.\n    # `score = 1 / (1 + exp(steepness * (gap - offset)))`\n    # To make it decay slower for loose fits, we can also add a small baseline score\n    # or ensure the exponent doesn't grow too large too quickly.\n\n    # Consider the function: `f(gap) = exp(-k * gap)`. This decays exponentially.\n    # For tight fits, `k` should be large. For loose fits, `k` should be small.\n    # We can combine this:\n    # `f(gap) = exp(-k_tight * gap)` for `gap < threshold`\n    # `f(gap) = C * exp(-k_loose * gap)` for `gap >= threshold`\n\n    # A single function that achieves a similar effect might be:\n    # `score = exp(-k1 * gap) - k2 * gap` or similar additive adjustments.\n\n    # Let's re-evaluate the sigmoid `1 / (1 + exp(x))`.\n    # If `x = steepness * (gap - shift)`, for small `gap`, `x` is very negative, score is ~1.\n    # For large `gap`, `x` is positive, score approaches 0.\n    # To make loose fits have higher scores, we can shift the curve to the right\n    # (`shift` positive) or reduce `steepness`.\n    # Let's try increasing `steepness` but also adding a term that keeps the score\n    # from dropping too fast for larger gaps.\n\n    # Alternative approach: Rank based on `rc`.\n    # Smallest `rc` gets rank 1, next gets rank 2, etc.\n    # Then transform ranks to scores. A non-linear transformation is better.\n\n    # Let's try to modify the sigmoid exponent argument.\n    # We want the exponent to grow less rapidly for larger `rc`.\n    # Instead of `steepness * rc`, consider `steepness * (rc^p)` where `p < 1` or `steepness * log(rc)`.\n\n    # Let's try: `exponent_arg = steepness * (potential_remaining_cap_valid ** power)`\n    # where `power` is less than 1 to slow down the growth for larger `rc`.\n    # Or, even simpler, directly adjust the `steepness` for different ranges of `rc`.\n\n    # Let's consider a heuristic that assigns priority based on the *relative* gap.\n    # `relative_gap = potential_remaining_cap_valid / item` (if item > 0)\n    # Or `relative_gap = potential_remaining_cap_valid / bin_capacity` (if bin_capacity is known).\n    # Since we only have `bins_remain_cap`, `item` is the best reference.\n\n    # Let's go back to the sigmoid and tune it for the desired behavior.\n    # `score = 1 / (1 + exp(steepness * (rc - shift)))`\n    # - To favor tight fits strongly: increase `steepness`.\n    # - To give looser fits more chance: increase `shift` (this effectively moves the steep part to the right).\n    #   Or, use a softer decay.\n\n    # Let's use a function that directly rewards small gaps and decays slowly.\n    # `score = exp(-k * gap)` is a starting point.\n    # To make the decay slower for larger gaps, we can use a form like:\n    # `score = exp(-k * gap) * (1 + alpha * gap)`\n    # Or `score = exp(-k * gap) / (1 + alpha * gap)` (inverse relationship for alpha)\n    # Or a piecewise approach.\n\n    # Let's try a combination of tight fit reward and loose fit \"grace\".\n    # `tight_priority = exp(-tight_fit_steepness * potential_remaining_cap_valid)`\n    # `loose_priority = exp(-loose_fit_decay_rate * potential_remaining_cap_valid)`\n    # We want to transition from `tight_priority` to `loose_priority`.\n\n    # A simpler sigmoid adjustment:\n    # Let `g = potential_remaining_cap_valid`.\n    # We want to compute a score `s(g)`.\n    # `s(0)` should be high. `s(large_g)` should be low but non-zero.\n    # A function like `exp(-k * g)` is good but decays quickly.\n    # `s(g) = 1.0 / (1.0 + exp(steepness * (g - shift)))`\n    # If we want looser fits to have higher scores, we need to reduce the exponent's\n    # positive values. This can be done by increasing `shift` or decreasing `steepness`.\n    # Let's try increasing `shift` to push the high-priority region further.\n\n    # `shift_value = 0.3` # A small positive gap might still be considered \"good enough\".\n    # `exponent_args = steepness * (potential_remaining_cap_valid - shift_value)`\n    # This would make a gap of `shift_value` have an exponent argument of 0, score 0.5.\n    # Gaps smaller than `shift_value` get scores > 0.5. Gaps larger get scores < 0.5.\n    # This is still best-fit like.\n\n    # To allow looser fits to have higher priority than just a decaying sigmoid score:\n    # we can consider the *inverse* of the remaining capacity as a factor,\n    # or give a baseline score.\n\n    # Let's try a formulation that rewards small gaps more strongly and then\n    # gradually rewards larger gaps with a slower decay.\n    # `score = exp(-k1 * g) * (1 + k2 * g)` seems promising.\n    # If `k1` is large, it's a tight fit preference. `k2` controls the boost for larger gaps.\n    # `k1 = tight_fit_steepness`\n    # `k2 = loose_fit_decay_rate` (inverse relationship, so `k2` should be small if `gap` is large)\n\n    # Let's use a function of the form: `score = exp(-k * g) * (1 + alpha * g)`.\n    # `k` controls initial decay, `alpha` controls the \"boost\" for larger gaps.\n    # For small `g`, `score ~ exp(-k * g)`.\n    # For large `g`, `score ~ (exp(-k * g) * alpha * g)`.\n    # This term `alpha * g * exp(-k * g)` has a maximum and then decays.\n    # We want `k` to be related to `tight_fit_steepness` and `alpha` to `loose_fit_decay_rate`.\n\n    # Let `k = steepness` and `alpha = decay_rate`.\n    # `score = np.exp(-steepness * potential_remaining_cap_valid) * (1 + decay_rate * potential_remaining_cap_valid)`\n\n    # Let's test this:\n    # If `g=0`: `score = exp(0) * (1 + 0) = 1` (perfect fit).\n    # If `g` is small positive: `score = exp(-steepness*g) * (1 + decay_rate*g)`.\n    # The `exp(-steepness*g)` term dominates and reduces the score.\n    # If `g` is larger: `exp(-steepness*g)` drops, but `(1 + decay_rate*g)` increases.\n    # We need to ensure the overall score is reasonable.\n\n    # Let's adjust the functional form to be more in line with sigmoid properties but with tuning.\n    # Consider a function that has a 'knee' rather than a smooth decay.\n    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`\n    # We can make the decay slower for larger gaps by making the exponent argument grow slower.\n    # Use `steepness * sqrt(gap)` or `steepness * log(gap + 1)`.\n\n    # Let's try a robust sigmoid modification that balances tight fits and allows looser fits.\n    # `score = 1.0 / (1.0 + exp(steepness * (g - g0)))` where `g0` is a target gap.\n    # For looser fits to be favored slightly:\n    # We can normalize the gap by item size: `g_norm = g / item`\n    # And use a sigmoid on `g_norm`: `1.0 / (1.0 + exp(steepness * (g_norm - shift_norm)))`\n\n    # Let's use the `exp(-k*g) * (1 + alpha*g)` form as it directly models\n    # strong initial decay and slower decay for larger values.\n    # `k` (steepness) should be large for tight fits.\n    # `alpha` (decay_rate) should be small if we want scores to drop fast for loose fits,\n    # and larger if we want loose fits to retain priority.\n    # So, `alpha` should be positively correlated with the preference for looser fits.\n    # If we want to prioritize tight fits but not ignore loose ones, `alpha` should be small.\n\n    # Let `steepness = tight_fit_steepness`\n    # Let `alpha = loose_fit_decay_rate` (This parameter name is a bit counter-intuitive, let's call it `loose_fit_boost_factor`).\n    # `boost_factor` controls how much larger gaps are \"boosted\" relative to strict exponential decay.\n    # Higher `boost_factor` means slower decay for loose fits.\n\n    boost_factor = loose_fit_decay_rate # Renaming for clarity\n    \n    # Calculate scores: exp(-steepness * g) * (1 + boost_factor * g)\n    # Need to handle potential overflows/underflows in exp.\n    # The term `steepness * g` can become very large.\n    # `exp(-large)` -> 0.\n    # `(1 + boost_factor * g)` can also grow.\n    # If `steepness` is large and `g` is small, `exp` dominates.\n    # If `boost_factor` is non-zero and `g` is large, `(1 + boost_factor * g)` grows.\n    # The product can still be problematic.\n\n    # Let's try to normalize the gaps or use a more stable function.\n    # Consider `score = exp(-k*g)`. This is stable.\n    # To make it decay slower for larger gaps, we can cap the exponent value or use a different function.\n\n    # Let's try a piecewise definition or a modified sigmoid that has varying steepness.\n    # `score = 1.0 / (1.0 + exp(steepness * (g - shift)))`\n    # To make it decay slower, increase `shift`. This moves the steep part to the right.\n    # `shift` can be thought of as a \"tolerance\" for gaps.\n\n    # Let's re-evaluate the reflection: \"Smoothness, non-linearity, and strategic parameter tuning improve heuristic performance.\"\n    # The sigmoid function is smooth and non-linear. Parameter tuning is key.\n    # The problem statement for `v1` suggests strong preference for tight fits.\n    # `v2` should potentially offer a balance.\n\n    # Let's try to create a scoring function that:\n    # 1. Gives high scores to tight fits (small `g`).\n    # 2. Gives moderate scores to slightly looser fits.\n    # 3. Gives a small but non-zero score to very loose fits, ensuring they are not entirely ignored.\n\n    # Consider `score = exp(-k1 * g)` for `g` up to a certain threshold `T`, and\n    # `score = C * exp(-k2 * g)` for `g > T` with `k2 < k1`.\n    # This is a piecewise approach.\n    # For a single function:\n    # `score = exp(-k * g)` where `k` itself is a function of `g`.\n    # e.g., `k = k_tight` for small `g`, `k = k_loose` for large `g`.\n\n    # Let's use a clamped exponential decay with an additive bonus for larger gaps.\n    # `score = exp(-steepness * potential_remaining_cap_valid) + bonus_factor * min(potential_remaining_cap_valid, bonus_cap)`\n    # The `bonus_factor` should be small, and `bonus_cap` defines the range for bonus.\n\n    # A simple modification to the sigmoid from v1:\n    # `score = 1.0 / (1.0 + exp(steepness * (gap - shift)))`\n    # Let `steepness` control the initial drop.\n    # Let `shift` control the point where scores become less than 0.5.\n    # To make scores decay slower: increase `shift`. This pushes the significant drop to larger gaps.\n\n    # Let's use a formulation inspired by logistic functions but with tunable rates.\n    # `score = 1.0 / (1.0 + exp(k * (g - g_mid)))`\n    # `k` = steepness\n    # `g_mid` = gap at which score is 0.5\n\n    # Let's try to shape the decay rate directly.\n    # For small `g`, we want `exp(-k_high * g)`.\n    # For large `g`, we want `exp(-k_low * g)`.\n    # A function like `exp(-k_high * g / (1 + alpha * g))` can transition between these.\n    # When `g` is small, `1 + alpha*g ~ 1`, so `exp(-k_high * g)`.\n    # When `g` is large, `1 + alpha*g` grows, making the exponent argument smaller (less negative),\n    # effectively slowing down the decay.\n\n    # `k_high = tight_fit_steepness`\n    # `k_low` (implicit) should be smaller.\n    # `alpha` = `loose_fit_decay_rate`\n    # `score = exp(-k_high * g / (1 + alpha * g))`\n\n    # This form seems promising. It's smooth, non-linear, and tunable.\n    # `tight_fit_steepness` controls how sharply the score drops for gaps close to zero.\n    # `loose_fit_decay_rate` controls how quickly the decay rate slows down for larger gaps.\n    # A higher `loose_fit_decay_rate` means slower decay for loose fits.\n\n    # Let's set parameters:\n    # `steepness` (k_high) = 10.0: Very sharp drop for small gaps.\n    # `decay_rate` (alpha) = 0.5: Makes the decay significantly slower for larger gaps.\n\n    # We need to clip the exponent argument to avoid `exp` overflow/underflow.\n    # The exponent is `-k_high * g / (1 + alpha * g)`.\n    # As `g` -> infinity, exponent -> `-k_high / alpha`.\n    # We need to ensure `-k_high / alpha` is not too large negative.\n    # If `k_high = 10`, `alpha = 0.5`, then `-k_high / alpha = -20`.\n    # This is fine for `exp`.\n    # If `g` is very close to 0, exponent is close to 0.\n    # If `g` is small positive, exponent is slightly negative.\n\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent term: -k_high * g / (1 + alpha * g)\n    # Avoid division by zero if g is negative (which shouldn't happen here, as g >= 0)\n    # Ensure denominator is at least 1 to avoid issues with very small alpha and g.\n    denominator = 1.0 + alpha * potential_remaining_cap_valid\n    # If denominator is extremely close to zero (e.g., alpha is tiny and g is tiny negative, though g is >=0), it could be an issue.\n    # For g >= 0, alpha >= 0, denominator is always >= 1.0.\n    \n    exponent_args = -k_high * potential_remaining_cap_valid / denominator\n\n    # Clip the exponent arguments for numerical stability.\n    # Values between -30 and 30 are generally safe.\n    # For `exponent_args = -k_high * g / (1 + alpha * g)`:\n    # Min value when g -> infinity: -k_high / alpha. If this is < -30, we should clip.\n    # Max value when g = 0: 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate priorities\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    # Normalization consideration: The scores are not guaranteed to sum to a specific value or be in a fixed range [0, 1].\n    # The current function `exp(-k*g / (1+a*g))` ranges from 1 (at g=0) down to `exp(-k/a)`.\n    # This is fine as we only care about relative priorities.\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response5.txt_stdout.txt",
    "code_path": "problem_iter4_code5.py",
    "code": "def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a balanced approach.\n\n    This heuristic aims to balance two objectives:\n    1. Prioritize bins that have just enough capacity (minimize waste).\n    2. Prioritize bins that have significantly more capacity to potentially accommodate future larger items.\n\n    It assigns higher priority to bins that are \"close\" to fitting the item, but also\n    gives a non-zero priority to bins with larger remaining capacities, preventing\n    a complete rejection of bins that are much larger.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value for bins that cannot fit the item.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a score.\n    # We use a combination of a negative exponential for \"close fits\" and a linear\n    # term for \"ample fits\".\n\n    # Score for bins that are \"close\" to fitting (minimal surplus).\n    # This term encourages filling bins as much as possible.\n    # We use exp(-x) where x is the surplus capacity.\n    surplus_capacity = bins_remain_cap[can_fit_mask] - item\n    close_fit_score = np.exp(-surplus_capacity / 1.0)  # Normalize surplus by a factor (e.g., 1.0)\n\n    # Score for bins that have ample capacity.\n    # This term assigns a small, increasing priority to larger bins,\n    # encouraging their use for potentially larger future items.\n    # A simple linear increase: `(bins_remain_cap[can_fit_mask] - item) * alpha`\n    # However, to avoid extremely high values and keep it relative, we can use\n    # a scaled version or a logarithmic scale.\n    # Let's use a simple linear scaling for now, but cap it or use a softer function.\n    # A simple linear factor that increases with remaining capacity.\n    # We can use a value that's not too dominant, e.g., 0.1 * remaining_capacity.\n    # To avoid very large values for very large bins, we can use a capped linear function\n    # or a function that saturates.\n    # Let's try a simple linear term that is scaled down, relative to the item size.\n    # We want this to be less important than the \"close fit\" score for small surpluses.\n    # Example: if item=5, bin_cap=10, surplus=5. Close fit score is exp(-5). Linear score might be 0.1 * 10 = 1.\n    # If item=5, bin_cap=6, surplus=1. Close fit score is exp(-1). Linear score might be 0.1 * 6 = 0.6.\n    # The close-fit score might dominate for small surpluses, which is good.\n    # For larger surpluses, the linear score could provide a boost.\n    # Let's consider the total capacity of the bin.\n    # Another approach: reward bins that leave a small remainder for the *next* item.\n    # `bins_remain_cap[can_fit_mask] - item` is the remaining space.\n    # Let's try to incorporate a measure of how much space is left *relative* to the item.\n    # Or, let's simply add a bonus for larger remaining capacities, but scale it.\n    # A simple linear bonus: `0.1 * bins_remain_cap[can_fit_mask]`\n    # This might lead to very large values for very large bins.\n    # Alternative: `(bins_remain_cap[can_fit_mask] - item) * scale_factor`.\n    # For example, scale factor = 0.01. If bin_cap=100, item=5, then surplus=95. Linear score = 0.95.\n    # This is small compared to exp(-0) = 1.\n    # Let's try a simpler additive bonus for remaining capacity itself, but capped or scaled.\n    # Let's use a function that is high for perfect fits and decreases, but then\n    # increases slowly for larger bins.\n    # This could be `exp(-surplus) + alpha * surplus`.\n    # Let's refine: `exp(-surplus / T) + alpha * surplus`\n    # If we want to encourage larger bins as a secondary goal, `alpha` should be positive.\n    # However, the current `priority_v1` (softmax) already implicitly handles this\n    # by assigning non-zero probabilities.\n    # The reflection suggests a \"balanced approach\". This might mean not going purely\n    # to softmax but having a more explicit way to favor large bins.\n\n    # Let's try to explicitly favor bins that have enough space for *another* similar item\n    # or a moderately sized item.\n    # Consider bins with remaining capacity `R`. If the current item is `I`,\n    # then `R - I` is the leftover.\n    # We want to prioritize `R - I` being small.\n    # We also want to consider `R` itself.\n    # A simple combination: `f(R - I) + g(R)`\n    # `f(x) = exp(-x)`\n    # `g(x)` could be linear, e.g., `alpha * x`.\n    # So, `priorities[can_fit_mask] = exp(-surplus_capacity) + alpha * bins_remain_cap[can_fit_mask]`\n    # The `alpha` needs to be small enough not to dominate the `exp` term for small surpluses.\n    # Let's set `alpha = 0.05` and a scaling factor for the `exp` term.\n    # `priorities[can_fit_mask] = exp(-surplus_capacity / 1.0) + 0.05 * bins_remain_cap[can_fit_mask]`\n\n    # This approach might still assign very high scores to very large bins.\n    # Let's revisit the \"balanced\" idea. Maybe it means not discarding bins too aggressively.\n    # The softmax approach (v1) already gives non-zero probabilities to all bins that can fit.\n    # The \"balanced\" aspect could be in how the score is composed.\n\n    # Let's try a score that is high for minimal waste, and then tapers off,\n    # but doesn't go to zero quickly for larger remaining capacities.\n    # Consider a function that is like `1 / (1 + waste)` (similar to v0 but with +1 in denom)\n    # and add a term that grows with remaining capacity, but not too fast.\n    # `score = (1.0 / (1.0 + surplus_capacity)) + 0.1 * (bins_remain_cap[can_fit_mask] / max_possible_capacity)`\n    # This still requires knowledge of max_possible_capacity, which might not be available.\n\n    # Let's try a modified exponential: `exp(-surplus / T)`.\n    # For \"balanced\", perhaps T should be larger, meaning the decay is slower.\n    # This would give more similar scores to bins with varying small to medium surpluses.\n    # Let's set T to a value that represents a \"typical\" bin capacity or a fraction of it.\n    # If we assume bin capacity is around 1.0 (normalized), maybe T=0.5 or T=1.0.\n    # `priorities[can_fit_mask] = np.exp(-surplus_capacity / 1.0)`\n\n    # Let's try to combine a \"best fit\" criterion with a \"largest fit\" criterion.\n    # Best fit: `-(bins_remain_cap[can_fit_mask] - item)` -> closer to 0 is better.\n    # Largest fit: `bins_remain_cap[can_fit_mask]` -> higher is better.\n    # A simple sum: `-(bins_remain_cap[can_fit_mask] - item) + alpha * bins_remain_cap[can_fit_mask]`\n    # This is `(alpha - 1) * bins_remain_cap[can_fit_mask] + item`\n    # If alpha < 1, this prioritizes smaller remaining capacities.\n    # If alpha > 1, this prioritizes larger remaining capacities.\n    # We want to prioritize smaller remaining capacities (minimal waste), but also\n    # not penalize larger bins too much.\n\n    # Let's use a score that is maximized when `bins_remain_cap` is slightly larger than `item`.\n    # And for `bins_remain_cap >> item`, the score should still be positive and non-decreasing,\n    # but at a slower rate.\n\n    # Consider a quadratic function that peaks: `-(x - ideal)^2`.\n    # Here, `x = bins_remain_cap[can_fit_mask]`. We want `ideal = item`.\n    # So, `-(bins_remain_cap[can_fit_mask] - item)^2` for perfect fit.\n    # This is `-(surplus_capacity)^2`.\n    # This would strongly penalize larger surpluses.\n\n    # Let's go back to the Softmax idea but with a different transformation.\n    # What if we transform the remaining capacity itself?\n    # Let `R = bins_remain_cap[can_fit_mask]`\n    # We want to favor `R` that are close to `item`.\n    # We also want to favor larger `R`.\n    # A compromise: Prioritize bins where `R` is \"sufficiently large\" but not excessively so.\n\n    # Let's consider a hybrid approach inspired by various heuristics:\n    # 1. Best Fit Decreasing (BFD) is an offline heuristic. For online, we adapt.\n    # 2. First Fit (FF) is simple.\n    # 3. Worst Fit (WF) puts into the bin with most space (opposite of best fit).\n    # Our goal is somewhere between Best Fit and a balanced approach.\n\n    # Let's define a score function `S(R, item)` for a bin with remaining capacity `R` for item `item`.\n    # We want `S` to be high when `R` is slightly larger than `item`.\n    # We want `S` to be positive but not necessarily peak for `R >> item`.\n\n    # Try a score that rewards \"just enough\" space AND \"plenty of space\":\n    # Score = f(surplus) + g(remaining_capacity)\n    # f(surplus) = exp(-surplus) -> high for small surplus\n    # g(remaining_capacity) = linear or capped linear increase\n\n    # Let's combine the idea of minimizing waste with a penalty for *very* large remaining capacities.\n    # Or, a bonus for moderate remaining capacities.\n    #\n    # If bin_cap < item: priority = 0\n    # If item <= bin_cap < item + threshold: priority is high and decreases with bin_cap.\n    # If bin_cap >= item + threshold: priority is moderate and increases with bin_cap.\n\n    # Let's use a score that is high for surplus `s = R - item`, and then decays.\n    # `score = exp(-s / scale)` where `scale` controls how fast it decays.\n    # A larger `scale` means slower decay, giving more similar priorities to bins with larger surpluses.\n    # This is essentially what `priority_v1` does. The \"balance\" might be in choosing `scale`.\n    # If we want to balance \"minimal waste\" and \"leaving space\", `scale` could be related to `item` or average bin size.\n\n    # Let's try a score that is explicitly composed of two parts:\n    # Part 1: Reward for fitting tightly (minimize surplus). `exp(-surplus)`.\n    # Part 2: Reward for having *some* significant remaining space, not too little, not too much.\n    # This could be a Gaussian-like function centered around some \"ideal\" remaining space.\n    # Or a capped linear function.\n\n    # Simple balanced approach:\n    # Prioritize bins that are close to fitting, and among those, pick the one with the smallest surplus.\n    # If no bins are close, then consider bins with larger capacity.\n\n    # Let's try:\n    # High priority for surplus = 0, decaying for positive surplus.\n    # A secondary boost for bins with significantly larger remaining capacity.\n\n    # Score = exp(-(bins_remain_cap - item) / T)  for bins that can fit.\n    # Let T be a parameter that controls the sensitivity to surplus.\n    # A smaller T means only bins very close to `item` get high scores.\n    # A larger T means bins with larger surpluses also get relatively high scores.\n    # To achieve \"balance\", we can set T to be, for example, the average remaining capacity of all bins,\n    # or a fraction of the bin capacity.\n\n    # Let's use T = 1.0 (or a normalized bin capacity) as a baseline.\n    # `priorities[can_fit_mask] = np.exp(-surplus_capacity / 1.0)`\n\n    # What if we want to explicitly give a bonus for bins that can fit *multiple* such items?\n    # Or a bonus for bins that have capacity > 2 * item?\n    # This can lead to complex scoring.\n\n    # Let's try a function that rewards being \"close enough\":\n    # For a bin with remaining capacity `R`, and item `I`.\n    # Score = 1 if `R < I`.\n    # Score = `exp(-(R-I))` if `I <= R < I + W`.\n    # Score = `exp(-W) + alpha * (R - (I+W))` if `R >= I + W`.\n    # Here, `W` is a threshold for \"close enough\". `alpha` controls the boost for larger bins.\n    # This can be simplified.\n\n    # Let's use a different approach:\n    # Prioritize bins by `-(remaining_capacity - item)`. This is the \"best fit\" score.\n    # If multiple bins have the same best fit score (e.g., identical remaining capacities),\n    # we might break ties by choosing the one with larger remaining capacity.\n    # This tie-breaking is a form of \"balanced\" approach.\n\n    # Let's try a combined score:\n    # `score = (bins_remain_cap[can_fit_mask] - item) - alpha * bins_remain_cap[can_fit_mask]`\n    # We want minimal waste, so `bins_remain_cap - item` should be small.\n    # So, `-(bins_remain_cap - item)` should be high.\n    # For \"balance\", let's penalize very large remaining capacities less severely.\n    #\n    # Consider: `score = -(bins_remain_cap[can_fit_mask] - item)` -> best fit.\n    # To balance, we can add a term that rewards larger bins but not excessively.\n    # `score = -(bins_remain_cap[can_fit_mask] - item) + alpha * log(bins_remain_cap[can_fit_mask])`\n    # `alpha` would need to be tuned. `log` scales down larger capacities.\n    # Let's pick `alpha = 0.1`.\n\n    # `priorities[can_fit_mask] = -(surplus_capacity) + 0.1 * np.log(bins_remain_cap[can_fit_mask])`\n    # `np.log(0)` is problematic, but `bins_remain_cap[can_fit_mask]` will be >= `item`,\n    # and `item` is usually positive. So `bins_remain_cap` will be positive.\n\n    # Let's re-evaluate the reflection: \"Softmax-like transformations can create more nuanced priorities than simple inverses.\"\n    # `priority_v1` uses `exp(-surplus)`. This is already a softmax-like transformation of the \"best fit\" scores.\n    # The \"nuance\" could come from modifying the input to `exp`.\n\n    # Let's modify the input to `exp` in `priority_v1`.\n    # `priority_v1` uses `score = -surplus_capacity`.\n    # To balance, let's add a term that favors larger bins.\n    # `score = -surplus_capacity + alpha * bins_remain_cap[can_fit_mask]`\n    # If `alpha` is small, this gives priority to minimal surplus bins.\n    # If `alpha` is larger, it starts favoring larger bins.\n    # Let `alpha = 0.05`. This means a bin with `item=10, R=11` has score `-1`. A bin with `item=10, R=21` has score `-11 + 0.05*21 = -11 + 1.05 = -9.95`.\n    # `exp(-1) \u2248 0.368`. `exp(-9.95) \u2248 0.000045`. This still heavily favors minimal surplus.\n\n    # What if we use a function that peaks at some `R > I`?\n    # For example, a Gaussian-like function centered at `I + delta`.\n    # `score = exp(-((R - (I + delta)) / sigma)^2)`\n    # This would heavily favor bins around `I + delta`.\n    # This is a form of \"balanced\" fit. `delta` and `sigma` would be tuning parameters.\n    # Let's set `delta = 0` and `sigma = 1.0`. This is `exp(-surplus^2)`.\n    # If `item=5`, `R=6`, surplus=1, score=exp(-1).\n    # If `item=5`, `R=10`, surplus=5, score=exp(-25).\n\n    # Let's try to explicitly reward bins that are \"large enough\" and also \"not too large\".\n    # Perhaps reward bins where `R / item` is in a sweet spot, say `[1.1, 1.5]`.\n    # And for `R` outside this, score decreases.\n\n    # Let's consider a simple modification to `priority_v1` to introduce balance:\n    # Instead of `-surplus_capacity`, use a function that decays slower for larger `R`.\n    # `score = -surplus_capacity / (1 + alpha * bins_remain_cap[can_fit_mask])`\n    # If `alpha = 0`, it's `priority_v1`.\n    # If `alpha` is small positive, the denominator grows, making the score less negative (higher `exp`).\n    # E.g., `alpha = 0.1`.\n    # `item=10, R=11` (surplus=1). Score = `-1 / (1 + 0.1*11) = -1 / 2.1 \u2248 -0.476`. `exp(-0.476) \u2248 0.62`.\n    # `item=10, R=21` (surplus=11). Score = `-11 / (1 + 0.1*21) = -11 / 3.1 \u2248 -3.54`. `exp(-3.54) \u2248 0.029`.\n    # This seems to give higher relative priority to larger bins compared to v1.\n\n    # Let's formalize this.\n    # We want to prioritize bins where `R` is close to `I`.\n    # Let `s = R - I` be the surplus.\n    # The base priority is related to `exp(-s)`.\n    # To add balance, we want to boost priority for larger `R`.\n    # Consider `score = -(s / (1 + alpha * R))`.\n    # We want `alpha` to be small, e.g., `0.05` or `0.1`.\n    # This function is more lenient on larger surpluses if the remaining capacity `R` is also large.\n\n    alpha = 0.1 # Tuning parameter. Smaller alpha gives more weight to minimal surplus.\n\n    # Initialize priorities to zero. Bins that cannot fit will retain this.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can accommodate the item.\n    valid_mask = bins_remain_cap >= item\n\n    # For valid bins, calculate the score.\n    # `surplus_capacity = bins_remain_cap[valid_mask] - item`\n    # `remaining_capacity = bins_remain_cap[valid_mask]`\n    #\n    # The score is designed to be higher for bins with smaller surplus.\n    # It also incorporates a term that slightly favors larger remaining capacities,\n    # providing a \"balanced\" approach.\n    # The term `alpha * remaining_capacity` in the denominator of the surplus division\n    # reduces the penalty for larger surpluses when the remaining capacity is large.\n    # This makes the scores for larger bins \"less negative\" (closer to zero) compared\n    # to a simple `-surplus` score.\n    surplus_capacity = bins_remain_cap[valid_mask] - item\n    remaining_capacity_valid = bins_remain_cap[valid_mask]\n\n    # Calculate the raw scores.\n    # We use `np.log1p` to handle potential `remaining_capacity_valid` being zero or very small,\n    # although given `valid_mask`, `remaining_capacity_valid >= item >= 0`.\n    # Using `log1p(x)` is `log(1+x)`. `log1p(0) = 0`.\n    # Let's stick to a simpler form for now.\n    # `score = -surplus_capacity / (1 + alpha * remaining_capacity_valid)`\n    # This ensures the denominator is always at least 1.\n\n    # We need to avoid division by zero if `1 + alpha * remaining_capacity_valid` is zero,\n    # which is not possible here as `alpha` and `remaining_capacity_valid` are non-negative.\n    # However, for very large `remaining_capacity_valid`, the score might become close to `-surplus_capacity / (alpha * remaining_capacity_valid)`.\n\n    # Let's use the `exp` function to transform these scores into probabilities/priorities.\n    # `exp(score)` will be higher for scores closer to zero.\n    # A score of 0 means perfect fit with no large bin bonus.\n    # A score closer to 0 means minimal surplus or large bin bonus.\n    scores_for_valid_bins = -surplus_capacity / (1.0 + alpha * remaining_capacity_valid)\n\n    # Assign the calculated scores to the priorities array for valid bins.\n    priorities[valid_mask] = scores_for_valid_bins\n\n    # Apply the exponential function.\n    return np.exp(priorities)",
    "response_id": 5,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 9, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/SLOC.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\nTraceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/cyclomatic_complexity.py\", line 7, in <module>\n    from gpt import priority_v2 as priority\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 1, in <module>\n    def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\nNameError: name 'np' is not defined\n"
  },
  {
    "stdout_filepath": "problem_iter4_response6.txt_stdout.txt",
    "code_path": "problem_iter4_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit\n    with an emphasis on tighter fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function where higher priority is given to bins that result in a\n    smaller remaining capacity after the item is placed.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).\n    This function ensures that smaller non-negative `remaining_capacity - item` values\n    (tighter fits) result in higher priority scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 10.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # The 'fit gap' is the remaining capacity *after* placing the item.\n    # We want to minimize this gap.\n    fit_gaps = bins_remain_cap[can_fit_mask] - item\n\n    # The sigmoid function is `1 / (1 + exp(x))`.\n    # To prioritize smaller `fit_gaps`, we want the score to be higher when `fit_gaps` is smaller.\n    # The function `1 / (1 + exp(x))` is a decreasing function of `x`.\n    # Therefore, we set `x = steepness * fit_gaps`.\n    # A smaller `fit_gap` leads to a smaller `x`, thus a larger score.\n    # A larger `fit_gap` leads to a larger `x`, thus a smaller score.\n    exponent_args = steepness * fit_gaps\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values around +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    # This ensures that perfect fits (gap=0) get a score close to 0.5, and tighter fits (negative gap, which shouldn't happen here ideally, but for robustness) get scores closer to 1.\n    # However, the intention is to penalize larger positive gaps.\n    # Let's adjust the formula to directly penalize the gap.\n    # We want a decreasing function of `fit_gaps`.\n    # The current form `1 / (1 + exp(steepness * fit_gaps))` works.\n    # For a perfect fit (fit_gaps = 0), score is 0.5.\n    # For a tight fit (fit_gaps slightly > 0), score is < 0.5.\n    # For a loose fit (fit_gaps much > 0), score is close to 0.\n    # The reflection wants to prioritize tighter fits. The current implementation does this.\n\n    # Let's reconsider the reflection: \"Prioritize tighter fits with a decreasing sigmoid of remaining capacity.\"\n    # The current code uses `remaining_capacity - item` (the gap) as the argument.\n    # A smaller gap is a tighter fit.\n    # `1 / (1 + exp(steepness * gap))` decreases as `gap` increases.\n    # So, smaller `gap` -> higher score. This matches the reflection.\n\n    # Example:\n    # Bin A: remaining=5, item=3 -> gap=2. Score = 1/(1+exp(10*2)) = 1/(1+e^20) ~ 0\n    # Bin B: remaining=4, item=3 -> gap=1. Score = 1/(1+exp(10*1)) = 1/(1+e^10) ~ 0\n    # Bin C: remaining=3.1, item=3 -> gap=0.1. Score = 1/(1+exp(10*0.1)) = 1/(1+e^1) ~ 0.27\n    # Bin D: remaining=3, item=3 -> gap=0. Score = 1/(1+exp(10*0)) = 1/(1+1) = 0.5\n\n    # If we want to prioritize tighter fits *more* aggressively, perhaps a different mapping or scaling of the gap is needed.\n    # The original v0 used `1.0 / (resulting_remaining_cap + epsilon)`. This is monotonically increasing with tightness.\n    # And then `1 / (1 + exp(-k * (1.0 / (resulting_remaining_cap + epsilon))))`.\n    # This is `1 / (1 + exp(-k / gap))`. This function increases as `gap` decreases.\n    # So v0 prioritized smaller gaps more directly.\n\n    # Let's try a modification based on v0's idea of inverse gap, but mapped to a decreasing sigmoid.\n    # We want higher score for smaller gaps.\n    # Let's use `1 / (1 + exp(steepness * gap))` as in v1, but adjust `steepness` or the `gap` calculation slightly if needed.\n    # The current v1 implementation already prioritizes tighter fits. The `steepness` parameter controls how *strongly* this preference is applied.\n    # A higher `steepness` means that even small differences in the gap will result in significant differences in priority.\n\n    # The reflection is essentially what v1 is doing. The code seems aligned with the reflection.\n    # The key is that `1 / (1 + exp(steepness * x))` is a decreasing function.\n    # So, if `x` is `fit_gaps`, then as `fit_gaps` (remaining capacity after packing) decreases (tighter fit), the score increases.\n\n    # Let's ensure the mapping is robust and clear.\n    # A \"perfect fit\" (remaining_capacity == item) should have a high score. `fit_gap = 0`. Score = 0.5.\n    # A \"very tight fit\" (remaining_capacity slightly less than item, which is not allowed by `can_fit_mask`, or just slightly more than item) should have an even higher score.\n    # If `bins_remain_cap` could be exactly `item`, `fit_gaps` is 0, score is 0.5.\n    # If we want a score approaching 1 for a perfect fit, we need to shift the sigmoid or invert the argument.\n\n    # Let's try to map `fit_gaps` to a scale where 0 is perfect, and positive values are deviations.\n    # Consider `exp(-steepness * fit_gaps)`.\n    # For `fit_gaps = 0` (perfect), this is `exp(0) = 1`.\n    # For `fit_gaps > 0` (loose), this decreases.\n    # We can use this as part of a score.\n    # Maybe normalize `fit_gaps`? `fit_gaps / bin_capacity`.\n\n    # The prompt asks for prioritization of *tighter* fits. This means smaller `fit_gaps`.\n    # The v1 sigmoid `1 / (1 + exp(steepness * fit_gaps))` gives higher scores for smaller `fit_gaps`.\n    # This *is* prioritizing tighter fits.\n    # The parameter `steepness` directly controls how strongly this prioritization happens.\n    # A `steepness` of 10.0 means that a difference of 0.1 in `fit_gaps` leads to a significant change in score.\n    # For `steepness=10`:\n    # gap = 0.0 -> exp(0) = 1 -> score = 0.5\n    # gap = 0.1 -> exp(1) = 2.718 -> score = 1/(1+2.718) ~ 0.27\n    # gap = 0.2 -> exp(2) = 7.389 -> score = 1/(1+7.389) ~ 0.11\n    # gap = 0.3 -> exp(3) = 20.08 -> score = 1/(1+20.08) ~ 0.047\n\n    # This seems to be working as intended by the reflection. The v1 implementation is already quite good for this.\n    # The only potential improvement could be the *range* of scores or the *shape* of the preference.\n    # If we want scores to be closer to 1 for tighter fits, we could modify the sigmoid.\n    # For example, map `fit_gaps` to `y` such that `y=0` is perfect fit, and then use `1 / (1 + exp(steepness * y))`.\n    # Or map `fit_gaps` to `-fit_gaps` and use `1 / (1 + exp(steepness * (-fit_gaps))) = 1 / (1 + exp(-steepness * fit_gaps))`.\n    # Let's test this alternative: `1 / (1 + exp(-steepness * fit_gaps))`\n    # For `steepness=10`:\n    # gap = 0.0 -> exp(0) = 1 -> score = 0.5\n    # gap = 0.1 -> exp(-1) = 0.367 -> score = 1/(1+0.367) ~ 0.73\n    # gap = 0.2 -> exp(-2) = 0.135 -> score = 1/(1+0.135) ~ 0.88\n    # gap = 0.3 -> exp(-3) = 0.049 -> score = 1/(1+0.049) ~ 0.95\n\n    # This alternative mapping `1 / (1 + exp(-steepness * fit_gaps))` gives higher scores for tighter fits,\n    # approaching 1, while the original v1 `1 / (1 + exp(steepness * fit_gaps))` approaches 0.5 for perfect fits.\n    # The reflection \"Prioritize tighter fits with a decreasing sigmoid of remaining capacity\" is a bit ambiguous.\n    # Does it mean the *score* is decreasing with remaining capacity? If so, v1 is correct.\n    # Or does it mean the *function itself* is a decreasing sigmoid applied to something related to capacity?\n\n    # Given the common usage of sigmoid in heuristics, mapping a preference (tighter fit) to a higher score is typical.\n    # The alternative `1 / (1 + exp(-steepness * fit_gaps))` achieves this more directly, assigning scores closer to 1 for better fits.\n    # Let's adopt this for `priority_v2` as it feels like a more direct implementation of \"higher priority for tighter fits\".\n\n    # Recalculate `exponent_args` for the new formula:\n    # We want higher scores for smaller `fit_gaps`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it increasing in `fit_gaps`, we need `x` to be decreasing in `fit_gaps`.\n    # So, `x = -steepness * fit_gaps`.\n\n    exponent_args = -steepness * fit_gaps\n\n    # Clipping is still important.\n    # For very small `fit_gaps` (e.g., item size very close to bin capacity), `exponent_args` can become very negative.\n    # e.g., `fit_gap = 0.001`, `steepness = 10` -> `exp(-0.01)` -> score close to 0.5.\n    # If `fit_gap = 0` -> `exp(0)` -> score = 0.5.\n    # If `fit_gap = -0.001` (hypothetical, if item > capacity but `can_fit_mask` allowed), `exp(0.01)` -> score slightly < 0.5.\n    # The critical part is preventing `exp` from going to infinity or zero.\n    # `exponent_args` range from `-steepness * max_fit_gap` to `-steepness * min_fit_gap`.\n    # If `fit_gaps` are always non-negative, the minimum is 0.\n    # The maximum `fit_gap` can be `bin_capacity - min_item_size`.\n    # Let's clip `exponent_args` to avoid extreme values.\n    # If `exponent_args` is very positive (large `fit_gap`), `exp` is large, score is near 0.\n    # If `exponent_args` is very negative (small `fit_gap`), `exp` is near 0, score is near 1.\n    # Clipping range needs to consider these effects.\n    # A safe range for `exp` is typically where `x` is between -20 and 20.\n    # If `steepness * fit_gaps` is between -20 and 20, then `exponent_args` should be between -20 and 20.\n    # So `x = -steepness * fit_gaps` should be between -20 and 20.\n    # This means `steepness * fit_gaps` should be between -20 and 20.\n    # Since `fit_gaps >= 0` and `steepness > 0`, `steepness * fit_gaps` is always >= 0.\n    # So the constraint is `steepness * fit_gaps <= 20`.\n    # `fit_gaps <= 20 / steepness`.\n    # This implies that `fit_gaps` larger than `20/steepness` will result in `exponent_args` being <= -20, and scores close to 1. This is not what we want.\n\n    # Let's go back to the original v1 formula's logic: `1 / (1 + exp(steepness * fit_gaps))`.\n    # This formula yields scores that *decrease* as `fit_gaps` increases.\n    # A smaller `fit_gap` (tighter fit) leads to a higher score.\n    # This aligns with prioritizing tighter fits. The scores naturally range from near 0 (very loose fits) up to 0.5 (perfect fits).\n    # If the goal is simply to *rank* bins by tightness, this is sufficient.\n    # If the goal is to have scores that *approach 1* for the best fits, the alternative formula is needed.\n\n    # Let's re-read reflection: \"Prioritize tighter fits with a decreasing sigmoid of remaining capacity.\"\n    # This phrasing is tricky.\n    # Option A: Prioritize bins where `remaining_capacity` is small. The score should be high for small `remaining_capacity`.\n    # Option B: The function *used* to calculate priority is a decreasing sigmoid, applied to `remaining_capacity`.\n\n    # If Option A is the intent:\n    # `fit_gaps = bins_remain_cap[can_fit_mask] - item`\n    # We want `score(bin)` to be high when `fit_gaps` is low.\n    # The function `f(x) = 1 / (1 + exp(steepness * x))` is decreasing in `x`.\n    # So, setting `x = fit_gaps` means score is high when `fit_gaps` is low. This matches Option A.\n\n    # If Option B is the intent:\n    # \"decreasing sigmoid of remaining capacity\": this could mean `1 / (1 + exp(steepness * bins_remain_cap[i]))`\n    # But we also need to consider the `item`. The priority should depend on *both* `item` and `bins_remain_cap`.\n    # This interpretation seems less likely as it doesn't directly use the `item` size in the sigmoid argument itself.\n\n    # Let's stick with the interpretation that we want a high score for small `fit_gaps`.\n    # The v1 code `1 / (1 + exp(steepness * fit_gaps))` already does this, giving scores up to 0.5.\n    # If we want scores up to 1, the `1 / (1 + exp(-steepness * fit_gaps))` formulation is better.\n    # Let's ensure the `steepness` and clipping are suitable for this version.\n\n    # New `exponent_args` = `-steepness * fit_gaps`.\n    # `steepness` = 10.0.\n    # If `fit_gaps` = 0 (perfect fit), `exponent_args` = 0. `exp(0)` = 1. Score = 0.5.\n    # If `fit_gaps` = 0.1, `exponent_args` = -1. `exp(-1)` = 0.367. Score = 0.73.\n    # If `fit_gaps` = 0.2, `exponent_args` = -2. `exp(-2)` = 0.135. Score = 0.88.\n    # If `fit_gaps` = 0.3, `exponent_args` = -3. `exp(-3)` = 0.049. Score = 0.95.\n    # If `fit_gaps` = 0.5, `exponent_args` = -5. `exp(-5)` = 0.0067. Score = 0.993.\n    # If `fit_gaps` = 1.0, `exponent_args` = -10. `exp(-10)` = 0.000045. Score = 0.99995.\n\n    # This mapping appears to strongly prioritize smaller gaps and assign scores approaching 1.\n    # The clipping needs to prevent `exp(-steepness * fit_gaps)` from becoming 0 or causing issues.\n    # If `exponent_args` becomes very small (e.g., -30), `exp` is near zero, score is near 1.\n    # If `exponent_args` becomes very large (e.g., +30), `exp` is very large, score is near 0.\n    # The `clipped_exponent_args` should be in a range that makes `exp` well-behaved.\n    # If `exponent_args = -steepness * fit_gaps`, and we want to prevent `exp` from being too small or too large:\n    # Let's clip `exponent_args` to [-30, 30].\n    # If `-steepness * fit_gaps` is less than -30 (i.e., `steepness * fit_gaps > 30`), then `exp` is very small, score is near 1.\n    # If `-steepness * fit_gaps` is greater than 30 (i.e., `steepness * fit_gaps < -30`), this won't happen as `fit_gaps >= 0`.\n    # So clipping `exponent_args` to [-30, 30] seems reasonable. `np.clip(-steepness * fit_gaps, -30.0, 30.0)`\n    # This ensures `exp` is between `exp(-30)` and `exp(30)`.\n\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 6,
    "obj": 7.778220981252493,
    "SLOC": 12.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response7.txt_stdout.txt",
    "code_path": "problem_iter4_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a Best Fit heuristic suitable for softmax selection.\n\n    This priority function prioritizes bins that have the least remaining capacity\n    while still being able to accommodate the item. This is a form of Best Fit.\n    The scores are calculated as the negative of the surplus capacity (remaining_capacity - item_size).\n    When these scores are exponentiated (e.g., for a softmax selection), bins with\n    smaller surplus capacities will receive higher probabilities, effectively\n    prioritizing the \"tightest fit\". Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to a very low value (negative infinity) for bins that cannot fit the item.\n    # When exponentiated, these will become 0, meaning they won't be selected.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    valid_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the surplus capacity.\n    # Surplus capacity = remaining_capacity - item_size.\n    # We want to minimize this surplus.\n    surplus_capacity = bins_remain_cap[valid_mask] - item\n\n    # To prioritize smaller surpluses for softmax-like selection, we use the negative surplus.\n    # A smaller surplus (e.g., 0) results in a higher score (e.g., 0), leading to a higher probability\n    # after exponentiation (e.g., exp(0) = 1). A larger surplus (e.g., 5) results in a lower score\n    # (e.g., -5), leading to a lower probability (e.g., exp(-5) \u2248 0.0067).\n    scores_for_valid_bins = -surplus_capacity\n\n    # Assign these calculated scores to the corresponding positions in the priorities array.\n    priorities[valid_mask] = scores_for_valid_bins\n\n    # Exponentiate the scores to get probabilities. This makes the selection process\n    # probabilistic and favors bins with smaller surplus capacities.\n    # A bin that cannot fit the item will have a priority of exp(-inf) = 0.\n    return np.exp(priorities)",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response8.txt_stdout.txt",
    "code_path": "problem_iter4_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version implements an improved \"Best Fit\" heuristic. It prioritizes bins\n    that, after placing the item, will have the smallest remaining capacity.\n    This aims to leave bins with larger remaining capacities for potentially larger\n    future items, thus optimizing space utilization more effectively.\n    Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority (i.e., a bin that results in\n        less remaining capacity after packing).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # The heuristic aims to minimize this remaining capacity.\n    # A smaller `bins_remain_cap[i] - item` value corresponds to a better fit.\n    # We want the highest priority for the smallest positive difference.\n    # Therefore, we can use the negative of this difference as the priority.\n    # A bin that results in a remaining capacity of 0 (perfect fit) will have a priority of 0.\n    # A bin that results in a remaining capacity of 1 will have a priority of -1.\n    # This naturally prioritizes bins that are closer to being full.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter4_response9.txt_stdout.txt",
    "code_path": "problem_iter4_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a refined Best Fit strategy.\n\n    This heuristic prioritizes bins that have the least remaining capacity *after*\n    packing the item (Best Fit). It further refines this by favoring bins where\n    the item occupies a larger proportion of the bin's *current* remaining capacity.\n    This encourages packing larger items into larger available spaces first,\n    potentially leaving smaller spaces for smaller items.\n\n    The score for a bin is calculated as:\n    score = (1 / (1 + remaining_capacity_after_packing)) * (item / (initial_remaining_capacity + epsilon))\n    where `remaining_capacity_after_packing` is minimized, and the second term\n    encourages fitting larger items into relatively larger bins.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority. Bins that cannot fit the item\n        receive a score of 0.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # To avoid division by zero for bins with 0 remaining capacity\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    remaining_after_packing = bins_remain_cap[can_fit_mask] - item\n    initial_remaining_cap_fitting_bins = bins_remain_cap[can_fit_mask]\n\n    # Primary criterion: Minimize remaining capacity after packing (Best Fit).\n    # We use the inverse of (1 + remaining_capacity) to assign higher scores to smaller remaining capacities.\n    # Adding 1 to remaining_after_packing ensures that if remaining_after_packing is 0, the score is 1.\n    best_fit_score = 1.0 / (1.0 + remaining_after_packing)\n\n    # Secondary criterion: Favor bins where the item represents a larger fraction of the *initial* remaining capacity.\n    # This encourages filling up bins more effectively with larger items relative to their current space.\n    # We add epsilon to the denominator to prevent division by zero and to ensure a small, non-zero score.\n    fit_proportion_score = item / (initial_remaining_cap_fitting_bins + epsilon)\n\n    # Combine the scores: Multiply the two scores.\n    # This way, bins that are a good fit (low remaining capacity) AND\n    # where the item fills a significant portion of the available space get higher priority.\n    priorities[can_fit_mask] = best_fit_score * fit_proportion_score\n\n    return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response0.txt_stdout.txt",
    "code_path": "problem_iter5_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit with emphasis on tight fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid function applied to the negative post-placement remaining capacity.\n    This formulation provides a smooth ranking, strongly favoring tighter fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as sigmoid(-steepness * (remaining_capacity - item)).\n    This function is monotonically increasing with respect to -(remaining_capacity - item),\n    meaning smaller non-negative remaining capacities (larger negative values) get higher scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 10.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # If a bin can fit, the remaining capacity after packing is: bins_remain_cap[i] - item\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the argument for the sigmoid function.\n    # We want to prioritize smaller `potential_remaining_cap_valid`.\n    # The sigmoid function `1 / (1 + exp(-x))` is increasing in `x`.\n    # To make it increase as `potential_remaining_cap_valid` decreases, we set `x = steepness * potential_remaining_cap_valid`.\n    # A small `potential_remaining_cap_valid` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `potential_remaining_cap_valid` (loose fit) results in a larger `x`, thus a lower score.\n    # To more strongly favor tight fits, we can negate the value: sigmoid(-steepness * potential_remaining_cap_valid).\n    # This makes the function increase as potential_remaining_cap_valid *decreases*.\n    # A perfect fit (potential_remaining_cap_valid = 0) yields exp(0) = 1, score = 0.5.\n    # A very tight fit (potential_remaining_cap_valid < 0, should not happen due to mask) would map to higher scores.\n    # A very loose fit (large potential_remaining_cap_valid) results in exp(-large_positive) -> 0, score -> 1. This is the opposite of what we want.\n\n    # Let's rethink the sigmoid argument for prioritizing small `potential_remaining_cap_valid`.\n    # We want higher scores for smaller `potential_remaining_cap_valid`.\n    # sigmoid(z) = 1 / (1 + exp(-z)) is increasing in z.\n    # So, we want z to be decreasing as `potential_remaining_cap_valid` increases.\n    # Thus, we want z = -steepness * potential_remaining_cap_valid.\n    # This way, a small `potential_remaining_cap_valid` (tight fit) leads to a larger negative number for -steepness * ...,\n    # which means exp(-large_negative) is large, leading to a score close to 1.\n    # A large `potential_remaining_cap_valid` (loose fit) leads to a smaller negative number,\n    # exp(-smaller_negative) is smaller, leading to a score closer to 0.5 or even less if the argument becomes positive.\n\n    # The original reflection suggests sigmoid/softmax on negative surplus.\n    # Surplus = remaining_capacity - item. Negative surplus is a good indicator of a tight fit.\n    # So, we want to apply sigmoid to a function that increases with negative surplus.\n    # A simple approach is sigmoid(steepness * (-surplus)) = sigmoid(-steepness * surplus).\n    # This matches the logic above.\n\n    # Let's use the negative of the surplus directly as the sigmoid argument.\n    # This ensures that smaller surpluses (tighter fits) get higher priority.\n    # A perfect fit (surplus = 0) will result in sigmoid(0) = 0.5.\n    # A tight fit (surplus < 0, though our mask should prevent this for *valid* bins) would push the score towards 1.\n    # A loose fit (surplus > 0) will push the score towards 0.5 or less.\n\n    sigmoid_args = -steepness * potential_remaining_cap_valid\n\n    # Clip the sigmoid arguments to prevent potential overflow/underflow in np.exp.\n    # A range like [-30, 30] is generally safe for np.exp.\n    # If sigmoid_args is very negative (tight fit), exp -> large, score -> 0. This is not what we want.\n    # If sigmoid_args is very positive (loose fit), exp -> 0, score -> 1. This is also not what we want.\n\n    # Let's reconsider the reflection: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # This means we want a function that is decreasing with `potential_remaining_cap_valid`.\n    # The sigmoid function `1 / (1 + exp(x))` is decreasing in `x`.\n    # So, we should set `x = steepness * potential_remaining_cap_valid`.\n    # This was the original `priority_v1`. The reflection might be slightly ambiguous or suggesting a different angle.\n\n    # Let's interpret \"smoothed, non-linear functions (like sigmoid/softmax on negative surplus) for tunable preference, favoring tighter fits\"\n    # as applying sigmoid to a value that *increases* as surplus *decreases*.\n    # Let `f(surplus) = -surplus`. Then `sigmoid(steepness * f(surplus)) = sigmoid(-steepness * surplus)`.\n    # A surplus of 0 (perfect fit) -> sigmoid(0) = 0.5.\n    # A surplus of 1 (loose fit) -> sigmoid(-steepness) -> close to 0.\n    # A surplus of -1 (tight fit) -> sigmoid(steepness) -> close to 1.\n    # This seems to be the interpretation that aligns with favoring tighter fits and achieving scores close to 1 for them.\n\n    # So, the sigmoid argument should be `steepness * (-potential_remaining_cap_valid)`.\n    # Let's use `adjusted_surplus = potential_remaining_cap_valid`.\n    # We want to prioritize small `adjusted_surplus`.\n    # Sigmoid(x) is increasing. We want the score to be high when `adjusted_surplus` is small.\n    # This means the input to sigmoid should be large when `adjusted_surplus` is small.\n    # A mapping like `M - adjusted_surplus` works, where M is a constant.\n    # If M is large, small `adjusted_surplus` leads to large `M - adjusted_surplus`.\n    # Let's try `M = 0`. So the argument is `-adjusted_surplus`.\n    # With `steepness`: `-steepness * adjusted_surplus`.\n\n    sigmoid_input = -steepness * potential_remaining_cap_valid\n\n    # Clip the sigmoid input for numerical stability.\n    # If `sigmoid_input` is very negative (tight fit, `potential_remaining_cap_valid` large positive), exp() is close to 0, score is close to 1.\n    # If `sigmoid_input` is very positive (loose fit, `potential_remaining_cap_valid` large negative), exp() is large, score is close to 0.\n\n    # This implies that \"favoring tighter fits\" means giving them *lower* scores if we interpret the reflection literally as \"sigmoid on negative surplus\".\n    # \"favoring tighter fits\" usually means giving them *higher* priority.\n\n    # Let's re-read carefully: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # This means bins with `potential_remaining_cap_valid` closer to 0 should have higher priority.\n    # The function should be *decreasing* with `potential_remaining_cap_valid`.\n    # Sigmoid `1 / (1 + exp(x))` is decreasing.\n    # So, `x = steepness * potential_remaining_cap_valid`.\n    # For `potential_remaining_cap_valid` = 0 (perfect fit), `x = 0`, sigmoid(0) = 0.5.\n    # For `potential_remaining_cap_valid` > 0 (loose fit), `x > 0`, sigmoid(x) < 0.5.\n    # For `potential_remaining_cap_valid` < 0 (tight fit, though not possible with mask), `x < 0`, sigmoid(x) > 0.5.\n\n    # The reflection also says \"smoothed, non-linear functions (like sigmoid/softmax on negative surplus)\".\n    # If we take negative surplus `S_neg = -potential_remaining_cap_valid`.\n    # Then we want a function that is increasing in `S_neg`.\n    # `sigmoid(steepness * S_neg) = sigmoid(-steepness * potential_remaining_cap_valid)`.\n    # For `potential_remaining_cap_valid` = 0, sigmoid(0) = 0.5.\n    # For `potential_remaining_cap_valid` > 0 (loose fit), `-steepness * potential_remaining_cap_valid` < 0, sigmoid(<0) < 0.5.\n    # For `potential_remaining_cap_valid` < 0 (tight fit, not possible with mask), `-steepness * potential_remaining_cap_valid` > 0, sigmoid(>0) > 0.5.\n\n    # The most common interpretation for \"favoring tighter fits\" with sigmoid is to map the \"tightness\" to a high value.\n    # Tightness can be represented by `1 / (1 + surplus)` or `exp(-surplus)`.\n    # Let's use `exp(-surplus)` as the value to be transformed by sigmoid to constrain it to [0, 1].\n    # `exp(-potential_remaining_cap_valid)` is high for small `potential_remaining_cap_valid`.\n    # Apply sigmoid to `steepness * (something that reflects tightness)`.\n    # Maybe the reflection implies using `1 - sigmoid(steepness * surplus)`?\n    # Or perhaps a transformation like `1 / (1 + exp(steepness * surplus))` is the intended interpretation of \"sigmoid on negative surplus\",\n    # where a higher surplus leads to a higher argument, thus a lower score. This seems to be what `priority_v1` does.\n\n    # Let's focus on \"favoring tighter fits\". This implies that if we have two bins that can fit an item,\n    # the one with less remaining capacity after packing should get a higher priority score.\n    # Bin A: remaining_cap = 5, item = 3. Post-placement remaining = 2.\n    # Bin B: remaining_cap = 8, item = 3. Post-placement remaining = 5.\n    # We want Bin A to have a higher score than Bin B.\n    # `potential_remaining_cap_valid`: Bin A = 2, Bin B = 5.\n    # Function should be decreasing in `potential_remaining_cap_valid`.\n\n    # `priority_v1` uses `1 / (1 + exp(steepness * (potential_remaining_cap_valid)))`.\n    # For Bin A: `1 / (1 + exp(steepness * 2))`.\n    # For Bin B: `1 / (1 + exp(steepness * 5))`.\n    # Since `steepness * 5 > steepness * 2`, the denominator for Bin B is larger, so Bin B gets a lower score.\n    # This aligns with the requirement of prioritizing minimal post-placement remaining capacity.\n\n    # The reflection also mentions \"smoothed, non-linear functions (like sigmoid/softmax on negative surplus)\".\n    # Let's consider `sigmoid(-steepness * potential_remaining_cap_valid)`.\n    # For Bin A: `sigmoid(-steepness * 2)`.\n    # For Bin B: `sigmoid(-steepness * 5)`.\n    # Since `-steepness * 2 > -steepness * 5`, the argument for Bin A is larger, and thus its sigmoid score will be higher.\n    # This also aligns with prioritizing minimal post-placement remaining capacity.\n\n    # The difference between the two approaches is the shape of the curve and where the steepness is applied.\n    # The `sigmoid(-steepness * x)` form is a common way to represent preference for smaller `x`.\n    # Let's implement this interpretation.\n\n    sigmoid_input_values = -steepness * potential_remaining_cap_valid\n\n    # Clip to prevent overflow/underflow.\n    # For `sigmoid(-steepness * x)`:\n    # If `x` is very small (tight fit), `sigmoid_input_values` is very large positive. exp(large_positive) -> inf, score -> 0. This is the opposite.\n    # If `x` is very large (loose fit), `sigmoid_input_values` is very large negative. exp(large_negative) -> 0, score -> 1. This is also the opposite.\n\n    # It seems my interpretation of the sigmoid function's behavior might be flipped, or the reflection's \"on negative surplus\" needs careful handling.\n    # Let's stick to the core requirement: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # This means `potential_remaining_cap_valid` should be minimized.\n    # A function `f(x)` that is decreasing for `x >= 0` is needed.\n    # `1 / (1 + exp(k*x))` is decreasing for `k > 0`. This is `priority_v1`.\n    # `exp(-k*x)` is decreasing for `k > 0`.\n\n    # Let's try a slightly different approach using exp for a more direct mapping to \"tightness\".\n    # A measure of tightness could be `1 / (1 + surplus)` or `exp(-surplus)`.\n    # Let's use `exp(-steepness * potential_remaining_cap_valid)`.\n    # For Bin A (surplus 2): `exp(-steepness * 2)`.\n    # For Bin B (surplus 5): `exp(-steepness * 5)`.\n    # Bin A gets a higher value, which can then be mapped to a priority score.\n    # To map this to a [0, 1] priority, we could use softmax or normalize.\n    # However, the prompt asks for a priority score for *each* bin, not relative probabilities yet.\n\n    # Let's go back to the sigmoid structure. The reflection might imply a transformation for the input to sigmoid.\n    # \"smoothed, non-linear functions (like sigmoid/softmax on negative surplus)\"\n    # Consider the \"negative surplus\": `neg_surplus = -potential_remaining_cap_valid`.\n    # We want higher scores for smaller `potential_remaining_cap_valid`, which means larger `neg_surplus`.\n    # `sigmoid(z)` increases with `z`.\n    # So, we need `z` to increase with `neg_surplus`.\n    # Thus, `z = steepness * neg_surplus = steepness * (-potential_remaining_cap_valid)`.\n    # This means `priority = sigmoid(steepness * (-potential_remaining_cap_valid))`.\n\n    # Let's re-verify the sigmoid behavior: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If `potential_remaining_cap_valid = 0` (perfect fit): `sigmoid(0) = 0.5`.\n    # If `potential_remaining_cap_valid = 1` (loose fit): `sigmoid(-steepness)`. If `steepness = 5`, `sigmoid(-5) approx 0.0067`. Score is low.\n    # If `potential_remaining_cap_valid = -1` (tight fit, not possible with mask): `sigmoid(steepness)`. If `steepness = 5`, `sigmoid(5) approx 0.9933`. Score is high.\n\n    # This mapping `sigmoid(steepness * (-potential_remaining_cap_valid))` correctly gives higher scores for tighter fits (more negative surplus).\n    # However, for the bins that *can* fit, `potential_remaining_cap_valid` will always be non-negative.\n    # So, `steepness * (-potential_remaining_cap_valid)` will always be non-positive.\n    # This means scores will always be <= 0.5. This is also not ideal, as a perfect fit should perhaps get 1.\n\n    # The initial reflection: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # The initial code `priority_v1` uses `1 / (1 + exp(steepness * (remaining_capacity - item)))`.\n    # This function gives higher scores to smaller positive values of `(remaining_capacity - item)`.\n    # A perfect fit `(remaining_capacity - item) = 0` gives `1 / (1 + exp(0)) = 0.5`.\n    # A slightly loose fit `(remaining_capacity - item) = 0.1` gives `1 / (1 + exp(steepness * 0.1))`, which is less than 0.5.\n    # A very loose fit `(remaining_capacity - item) = 10` gives `1 / (1 + exp(steepness * 10))`, which is close to 0.\n\n    # The reflection might be interpreted as wanting to *shift* the sigmoid curve so that perfect fits get higher scores.\n    # For example, mapping the surplus `s` to `max_s - s`.\n    # If max_s is large enough, then `max_s - s` will be large for small `s`.\n    # Let's try `sigmoid(steepness * (max_possible_surplus - potential_remaining_cap_valid))`.\n    # What is `max_possible_surplus`? It's related to the bin capacity minus the item size.\n    # This feels overly complex.\n\n    # Let's reconsider the interpretation of \"sigmoid on negative surplus\".\n    # If surplus `s = remaining_capacity - item`.\n    # Negative surplus is `-s`.\n    # We want a function that increases with `-s`.\n    # `sigmoid(k * (-s))` increases with `-s`.\n    # So, priority `P = sigmoid(k * (-s))`.\n    # For `s = 0` (perfect fit), `P = sigmoid(0) = 0.5`.\n    # For `s = 1` (loose fit), `P = sigmoid(-k)`. This is low.\n    # For `s = -1` (tight fit), `P = sigmoid(k)`. This is high.\n\n    # The reflection also says \"favoring tighter fits and improved bin utilization.\"\n    # Tighter fits implies smaller `potential_remaining_cap_valid`.\n    # Improved bin utilization is achieved by minimizing empty space.\n\n    # Let's refine the `priority_v1` logic slightly to better reflect the \"favoring tighter fits\" part using the negative surplus idea, but ensuring scores for good fits are high.\n    # If we use `sigmoid(-steepness * potential_remaining_cap_valid)`, scores are <= 0.5.\n    # If we want scores to be > 0.5 for good fits, the argument to sigmoid must be positive.\n    # This means `steepness * (-potential_remaining_cap_valid)` must be positive.\n    # This implies `-potential_remaining_cap_valid` must be positive.\n    # Which means `potential_remaining_cap_valid` must be negative. This is not possible with `can_fit_mask`.\n\n    # Alternative interpretation: Smooth ranking.\n    # Maybe a simpler transformation that is decreasing with surplus.\n    # E.g., `exp(-steepness * potential_remaining_cap_valid)`.\n    # For surplus 0: `exp(0) = 1`.\n    # For surplus 1: `exp(-steepness)`.\n    # For surplus 2: `exp(-2*steepness)`.\n    # This gives higher values for smaller surpluses.\n    # These values are already in a useful range, but not strictly [0, 1] for all possible inputs.\n    # If we want [0, 1], we can normalize this.\n\n    # Let's try to achieve the effect of sigmoid but centered at 0 for perfect fit, and increasing for negative surplus.\n    # `sigmoid(x)` centered at 0 with range [0, 1].\n    # We want to map `potential_remaining_cap_valid` to a value `y` such that `y` is minimized when `potential_remaining_cap_valid` is minimized.\n    # Let `y = potential_remaining_cap_valid`.\n    # We want `priority` to be decreasing in `y`.\n    # Consider the transformation: `1 - sigmoid(steepness * potential_remaining_cap_valid)`.\n    # For `potential_remaining_cap_valid = 0`: `1 - sigmoid(0) = 1 - 0.5 = 0.5`.\n    # For `potential_remaining_cap_valid = 1`: `1 - sigmoid(steepness)`. If `steepness = 5`, `1 - 0.9933 = 0.0067`. Low score.\n    # For `potential_remaining_cap_valid = -1`: `1 - sigmoid(-steepness)`. If `steepness = 5`, `1 - 0.0067 = 0.9933`. High score.\n\n    # This transformation `1 - sigmoid(steepness * potential_remaining_cap_valid)` seems to correctly prioritize smaller `potential_remaining_cap_valid` with scores starting from 0.5 and going down for larger surpluses, and potentially going up for negative surpluses.\n\n    # Let's stick to the reflection's phrasing more closely: \"sigmoid/softmax on negative surplus\".\n    # Let `x = potential_remaining_cap_valid`.\n    # Negative surplus is `-x`.\n    # We want a function that increases with `-x`.\n    # `sigmoid(k * (-x))` is such a function.\n    # To make \"tighter fits\" (smaller `x`) result in higher scores:\n    # The input to sigmoid must be larger for smaller `x`.\n    # `steepness * (-x)` makes the input larger for smaller `x`.\n\n    # Let's retry the `sigmoid(-steepness * potential_remaining_cap_valid)` logic but adjust the interpretation of the output.\n    # If `potential_remaining_cap_valid` is 0, score is 0.5.\n    # If `potential_remaining_cap_valid` is slightly positive (loose), score is < 0.5.\n    # If `potential_remaining_cap_valid` is slightly negative (tight), score is > 0.5.\n\n    # The prompt asks for a priority score. The interpretation that gives scores for good fits above 0.5 is likely desired.\n    # This means we need the argument to sigmoid to be positive for good fits.\n    # The \"negative surplus\" should be thought of as a measure of \"goodness of fit\".\n    # Let `goodness = -potential_remaining_cap_valid`.\n    # We want higher scores for higher `goodness`.\n    # `sigmoid(steepness * goodness)` does this.\n    # So, `sigmoid(steepness * (-potential_remaining_cap_valid))`.\n    # BUT this results in scores <= 0.5 for valid bins (where `potential_remaining_cap_valid >= 0`).\n\n    # Consider the function: `f(s) = exp(-steepness * s)` where `s` is surplus.\n    # This maps surplus 0 to 1, surplus 1 to exp(-steepness), surplus 2 to exp(-2*steepness).\n    # These values are decreasing and are already in a good range.\n    # Let's normalize this to ensure it's in [0, 1] and maybe scale it.\n    # `normalized_exp = exp(-steepness * s) / max_exp_value`?\n    # This seems to be getting complicated.\n\n    # Let's go with a common interpretation of \"smooth preference for tighter fits\" using sigmoid.\n    # The goal is to make bins with smaller `potential_remaining_cap_valid` have higher scores.\n    # The function `1 / (1 + exp(steepness * x))` is decreasing in `x`.\n    # So, `priority = 1 / (1 + exp(steepness * potential_remaining_cap_valid))`.\n    # This is exactly what `priority_v1` does.\n\n    # The \"Improved code\" might need to change the steepness parameter or the way it's applied.\n    # The reflection: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # \"Use smoothed, non-linear functions (like sigmoid/softmax on negative surplus) for tunable preference, favoring tighter fits and improved bin utilization.\"\n\n    # \"favoring tighter fits\" is the key.\n    # If we use `sigmoid(-steepness * surplus)`, a perfect fit (surplus 0) gives 0.5.\n    # A very tight fit (surplus -0.01) gives `sigmoid(steepness * 0.01)`, which is slightly > 0.5.\n    # A loose fit (surplus 1) gives `sigmoid(-steepness)`.\n    # This implies that \"favoring tighter fits\" means scores can be > 0.5.\n\n    # Let's adjust `priority_v1` slightly to use the `sigmoid(-steepness * surplus)` form, which is common for this type of problem.\n    # This will center the scores around 0.5.\n    # For bins that can fit:\n    # `potential_remaining_cap_valid` is the surplus.\n    # We want to prioritize smaller surplus.\n    # Using `sigmoid(-steepness * surplus)` achieves this.\n    # A surplus of 0 gives 0.5. A surplus of 1 gives sigmoid(-steepness). A surplus of -1 gives sigmoid(steepness).\n    # This means that bins where the item fits perfectly get a score of 0.5. Bins where it fits loosely get scores less than 0.5.\n    # Bins where it fits \"tightly\" (if that were possible with the mask) would get scores greater than 0.5.\n\n    # The key might be in tuning `steepness` and understanding the desired score range.\n    # If we want scores to be more pronounced, we can increase `steepness`.\n    # The original code `priority_v1` uses `steepness=5.0`.\n    # The reflection doesn't specify a *new* function shape, but rather hints at using sigmoid on negative surplus.\n\n    # Let's adjust the logic to use `sigmoid(-steepness * surplus)` and see if that aligns better with \"favoring tighter fits\".\n    # The scores will be in the range (0, 0.5] for valid bins. This seems counter-intuitive for \"favoring\".\n\n    # Let's reconsider `1 - sigmoid(steepness * surplus)`.\n    # Surplus 0 -> 0.5\n    # Surplus 1 -> 1 - sigmoid(steepness) (low score)\n    # Surplus -1 -> 1 - sigmoid(-steepness) (high score)\n    # This means that if an item is *smaller* than the remaining capacity, it's a good fit.\n    # If `potential_remaining_cap_valid` is positive, it means loose fit.\n    # If `potential_remaining_cap_valid` is zero, it's a perfect fit.\n    # We want higher scores for smaller `potential_remaining_cap_valid`.\n\n    # Let's use `sigmoid(steepness * (max_surplus - potential_remaining_cap_valid))`.\n    # This feels hacky if `max_surplus` isn't well-defined.\n\n    # Let's go back to the reflection: \"smoothed, non-linear functions (like sigmoid/softmax on negative surplus)\".\n    # If we think of \"negative surplus\" as a measure of \"how much better than perfect fit we are\", i.e., `item - remaining_capacity`.\n    # This is negative for loose fits, and positive for tight fits.\n    # So, we want a function that increases with `item - remaining_capacity`.\n    # `sigmoid(steepness * (item - remaining_capacity))` would work.\n    # For `remaining_capacity - item = 0` (perfect fit): `sigmoid(0) = 0.5`.\n    # For `remaining_capacity - item = 1` (loose fit): `sigmoid(-steepness)`. Low score.\n    # For `remaining_capacity - item = -1` (tight fit): `sigmoid(steepness)`. High score.\n\n    # The issue is that for valid bins, `remaining_capacity - item >= 0`.\n    # So `item - remaining_capacity <= 0`.\n    # This means `steepness * (item - remaining_capacity) <= 0`.\n    # The sigmoid will always be <= 0.5 for valid bins.\n\n    # This implies that the \"improved\" heuristic might be using a different base function, or transforming the output of `priority_v1`.\n    # The reflection also mentions \"improved bin utilization\".\n\n    # Let's re-read the reflection for `priority_v1`: \"Prioritize bins with minimal *post-placement* remaining capacity. Use smoothed, non-linear functions (like sigmoid/softmax on negative surplus) for tunable preference, favoring tighter fits and improved bin utilization.\"\n\n    # The provided `priority_v1` uses `1 / (1 + exp(steepness * (remaining_capacity - item)))`.\n    # This function is decreasing with `(remaining_capacity - item)`.\n    # This *does* prioritize bins with minimal post-placement remaining capacity.\n    # A score of 0.5 for a perfect fit, decreasing for looser fits.\n\n    # Perhaps the \"improved\" aspect is in how `steepness` is used or what it represents.\n    # The reflection might be suggesting to emphasize the \"favoring tighter fits\" aspect more strongly.\n    # This could mean increasing `steepness`.\n    # Or it could mean a different formulation.\n\n    # Let's consider the phrasing \"sigmoid/softmax on negative surplus\".\n    # If surplus `s = remaining_capacity - item`. Negative surplus `-s`.\n    # Using sigmoid on `-s`: `sigmoid(k * (-s))`.\n    # This function is increasing with `-s`.\n    # As `-s` increases (meaning `s` decreases, i.e., tighter fit), the score increases.\n    # For `s = 0` (perfect fit), score is 0.5.\n    # For `s = 1` (loose fit), score is `sigmoid(-k)`.\n    # For `s = -1` (tight fit), score is `sigmoid(k)`.\n\n    # The issue remains that for bins where the item fits, `s >= 0`.\n    # This means `sigmoid(k * (-s))` will always be <= 0.5.\n\n    # The reflection also says \"favoring tighter fits\".\n    # What if we use the \"complementary\" sigmoid? `1 - sigmoid(x) = sigmoid(-x)`.\n    # So, `1 - sigmoid(steepness * s)`.\n    # For `s = 0`: `1 - sigmoid(0) = 1 - 0.5 = 0.5`.\n    # For `s = 1`: `1 - sigmoid(steepness)`. This is close to 0.\n    # For `s = -1`: `1 - sigmoid(-steepness)`. This is close to 1.\n    # This form `1 - sigmoid(steepness * s)` seems to correctly prioritize smaller `s`, giving scores > 0.5 for `s < 0` and < 0.5 for `s > 0`.\n\n    # Let's implement this `1 - sigmoid(steepness * surplus)` as `priority_v2`.\n    # `surplus = bins_remain_cap[can_fit_mask] - item`\n    # `steepness = 10.0` (increased to emphasize preference for tight fits)\n    # `sigmoid_input = steepness * surplus`\n    # `priority = 1.0 - (1.0 / (1.0 + np.exp(-sigmoid_input)))`\n    # `priority = 1.0 - sigmoid(steepness * surplus)`\n\n    # Let's use a higher steepness to reflect \"favoring tighter fits\" more strongly.\n    steepness = 10.0\n\n    # Calculate the surplus for bins where the item can fit.\n    surplus_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the argument for the sigmoid function.\n    # We want smaller surplus to result in higher priority.\n    # The function `1 - sigmoid(steepness * surplus)` does this.\n    # A surplus of 0 (perfect fit) gives 0.5.\n    # A surplus of +1 (loose fit) gives `1 - sigmoid(steepness)`, which is a low score.\n    # A surplus of -1 (tight fit) gives `1 - sigmoid(-steepness)`, which is a high score.\n    # Since we only consider `surplus >= 0` due to the mask, the scores will be <= 0.5.\n    # This still seems slightly off if we want perfect fits to get high scores like 1.\n\n    # Perhaps the reflection is suggesting `softmax` on some measure of \"goodness\".\n    # \"favoring tighter fits\": a simple measure of \"tightness\" is `1 / (1 + surplus)`.\n    # Or `exp(-steepness * surplus)`.\n    # Let's use `exp(-steepness * surplus)` and then apply softmax across bins if we were picking one.\n    # For a priority score, we need a value per bin.\n\n    # Let's consider `priority_v1` again: `1 / (1 + exp(steepness * surplus))`.\n    # This gives decreasing scores for increasing surplus.\n    # Perfect fit (surplus 0) gives 0.5.\n    # Loose fit (surplus 1) gives `1 / (1 + exp(steepness))`.\n    # This seems to be the standard interpretation for Best Fit variants using sigmoid.\n\n    # The reflection might be interpreted as: instead of `sigmoid(x)`, use `sigmoid(-x)` or `1 - sigmoid(x)`.\n    # If we use `sigmoid(-steepness * surplus)`:\n    # Surplus 0 -> 0.5\n    # Surplus 1 -> sigmoid(-steepness) (low)\n    # Surplus -1 -> sigmoid(steepness) (high)\n    # For valid bins, surplus >= 0, so scores are <= 0.5.\n\n    # If we use `1 - sigmoid(steepness * surplus)`:\n    # Surplus 0 -> 0.5\n    # Surplus 1 -> 1 - sigmoid(steepness) (low)\n    # Surplus -1 -> 1 - sigmoid(-steepness) (high)\n    # For valid bins, surplus >= 0, so scores are <= 0.5.\n\n    # Both interpretations lead to scores <= 0.5 for valid bins.\n    # This suggests that perhaps the \"improved\" function should map perfect fits to values closer to 1.\n\n    # Let's reconsider the phrasing: \"sigmoid/softmax on negative surplus\".\n    # Let `negative_surplus = -surplus`.\n    # We want to map `negative_surplus` to a score.\n    # A higher `negative_surplus` (meaning a smaller positive surplus, or a negative surplus) should give a higher score.\n    # Consider the function `f(x) = exp(steepness * x)`. This increases with `x`.\n    # So `exp(steepness * negative_surplus) = exp(steepness * (-surplus))`.\n    # This value is 1 for surplus 0, < 1 for surplus > 0, and > 1 for surplus < 0.\n    # We need to map this to [0, 1] and ensure it's decreasing with surplus.\n\n    # Let's combine the \"minimal post-placement remaining capacity\" with the \"sigmoid on negative surplus\".\n    # `potential_remaining_cap_valid` is the post-placement remaining capacity.\n    # Minimal capacity is good.\n    # Negative surplus is `item - remaining_capacity`. We want to favor smaller `item - remaining_capacity`.\n\n    # The reflection is suggesting a function that emphasizes tighter fits.\n    # A simpler interpretation: make the `steepness` parameter higher.\n    # However, the phrasing \"sigmoid/softmax on negative surplus\" suggests a structural change.\n\n    # Let's try mapping the surplus to `1 / (1 + exp(k * (surplus - offset)))`.\n    # If offset = 0, this is `priority_v1`.\n\n    # Let's try a different approach: transform the surplus.\n    # Consider `transformed_surplus = -potential_remaining_cap_valid`.\n    # We want higher scores for smaller `potential_remaining_cap_valid`, meaning larger `transformed_surplus`.\n    # Let's use `sigmoid(steepness * transformed_surplus)` and shift it.\n    # `sigmoid(steepness * (-potential_remaining_cap_valid))` yields scores <= 0.5 for valid bins.\n    # To make scores higher for better fits, we could do `0.5 + 0.5 * sigmoid(steepness * (-potential_remaining_cap_valid))`\n    # This maps surplus 0 to 0.5 + 0.5 * sigmoid(0) = 0.5 + 0.5 * 0.5 = 0.75.\n    # Surplus 1 -> 0.5 + 0.5 * sigmoid(-steepness) (low)\n    # Surplus -1 -> 0.5 + 0.5 * sigmoid(steepness) (high)\n    # This seems to align better with \"favoring tighter fits\" and getting higher scores.\n\n    # Let's implement `0.5 + 0.5 * sigmoid(steepness * (-potential_remaining_cap_valid))`\n\n    sigmoid_argument = steepness * (-surplus_valid)\n\n    # Clip the sigmoid argument for stability.\n    # If `sigmoid_argument` is very large positive (tight fit): exp -> inf, sigmoid -> 0. Score -> 0.5 + 0.5 * 0 = 0.5.\n    # If `sigmoid_argument` is very large negative (loose fit): exp -> 0, sigmoid -> 1. Score -> 0.5 + 0.5 * 1 = 1.0.\n    # This is still not prioritizing tight fits with high scores. The logic is flipped.\n\n    # Let's try `0.5 + 0.5 * sigmoid(steepness * potential_remaining_cap_valid)`.\n    # Surplus 0 -> 0.5 + 0.5 * sigmoid(0) = 0.75.\n    # Surplus 1 -> 0.5 + 0.5 * sigmoid(steepness) (high score, > 0.75).\n    # This prioritizes looser fits.\n\n    # The key must be the *exact* form of the sigmoid transformation mentioned in the reflection.\n    # \"sigmoid/softmax on negative surplus\".\n    # Let negative surplus be `ns = -surplus_valid`.\n    # We want higher scores for higher `ns`.\n    # The function `sigmoid(k * ns)` increases with `ns`.\n    # So, `priority = sigmoid(steepness * ns)`.\n    # For surplus 0, ns = 0, priority = 0.5.\n    # For surplus 1, ns = -1, priority = sigmoid(-steepness). Low.\n    # For surplus -1, ns = 1, priority = sigmoid(steepness). High.\n\n    # The problem is that `surplus_valid` is always >= 0.\n    # So `ns` is always <= 0.\n    # This means `sigmoid(steepness * ns)` will always be <= 0.5.\n\n    # Let's consider a different function for preference:\n    # A function that is high for small `surplus`.\n    # `1 / (1 + surplus^2)` ?\n    # `exp(-surplus)` ?\n\n    # The reflection implies using a sigmoid function.\n    # The way to get scores > 0.5 for good fits is to ensure the sigmoid argument is positive for good fits.\n    # Good fit means small `surplus`.\n    # If `surplus = 0`, we want a positive argument.\n    # If `surplus = 1`, we want a smaller positive argument or a negative argument.\n\n    # The reflection: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # This means the function should be *decreasing* in `potential_remaining_cap_valid`.\n    # `priority_v1` does this.\n    # \"Use smoothed, non-linear functions (like sigmoid/softmax on negative surplus) for tunable preference, favoring tighter fits and improved bin utilization.\"\n\n    # Let's focus on the \"favoring tighter fits\" part.\n    # This means that if two bins can fit an item, the one with less remaining capacity should have a higher score.\n    # `priority_v1` achieves this by having decreasing scores for increasing surplus.\n\n    # Perhaps the 'improved' aspect is simply to increase steepness or to use a different scaling.\n    # The reflection is quite suggestive. Let's try to use \"negative surplus\" directly in a way that makes sense.\n    # Negative surplus is `item - remaining_capacity`.\n    # This is <= 0 for valid bins.\n    # Let's use `sigmoid(steepness * (item - remaining_capacity))` and add a bias to make scores higher for good fits.\n    # `bias + sigmoid(steepness * (item - remaining_capacity))`\n    # If bias = 0.5, we get `0.5 + sigmoid(steepness * (item - remaining_capacity))`.\n    # For surplus 0: `0.5 + sigmoid(0) = 0.75`.\n    # For surplus 1: `0.5 + sigmoid(-steepness)`. Low.\n    # For surplus -1: `0.5 + sigmoid(steepness)`. High.\n\n    # This seems to be the most consistent interpretation of \"favoring tighter fits\" with a sigmoid-like function where good fits get high scores.\n\n    # Let's use this form: `0.5 + 0.5 * sigmoid(steepness * (item - remaining_capacity))`\n    # Which is equivalent to `0.5 + 0.5 * sigmoid(-steepness * (remaining_capacity - item))`\n    # This form maps perfect fits to 0.75, loose fits to values between 0.5 and 0.75, and very loose fits towards 0.5.\n    # Wait, if surplus is positive, `-steepness * surplus` is negative. `sigmoid(-ve)` is < 0.5.\n    # So `0.5 + 0.5 * (<0.5)` results in a score < 0.75. This is correct.\n    # Loose fit (surplus 1): `0.5 + 0.5 * sigmoid(-steepness)`. Low score.\n    # Perfect fit (surplus 0): `0.5 + 0.5 * sigmoid(0) = 0.75`.\n    # This provides a higher baseline for good fits and penalizes loose fits more.\n\n    # `item - remaining_capacity` is the negative of the surplus.\n    # Let's calculate `neg_surplus_valid = item - bins_remain_cap[can_fit_mask]`\n\n    neg_surplus_valid = item - bins_remain_cap[can_fit_mask]\n\n    # Use a higher steepness to emphasize tighter fits.\n    steepness = 10.0\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher scores for smaller surpluses (i.e., larger negative surpluses).\n    # The function `sigmoid(steepness * neg_surplus)` increases with `neg_surplus`.\n    sigmoid_argument = steepness * neg_surplus_valid\n\n    # Clip the sigmoid argument for stability.\n    clipped_sigmoid_argument = np.clip(sigmoid_argument, -30.0, 30.0)\n\n    # Calculate the sigmoid values. Scores will be <= 0.5 for valid bins (since neg_surplus_valid <= 0).\n    sigmoid_values = 1.0 / (1.0 + np.exp(-clipped_sigmoid_argument))\n\n    # To favor tighter fits more, and get scores higher than 0.5 for good fits:\n    # We can scale and shift the sigmoid.\n    # A common approach is `0.5 + 0.5 * sigmoid(steepness * (-surplus))`.\n    # This maps surplus 0 to 0.75.\n    # Let's use this transformation.\n    # The `sigmoid_values` calculated above are `sigmoid(steepness * neg_surplus_valid)`.\n    # So the priority score will be `0.5 + 0.5 * sigmoid_values`.\n\n    # For valid bins:\n    # If surplus is 0 (neg_surplus is 0), sigmoid_values is 0.5. Priority = 0.5 + 0.5 * 0.5 = 0.75.\n    # If surplus is 1 (neg_surplus is -1), sigmoid_values is sigmoid(-steepness). Priority = 0.5 + 0.5 * sigmoid(-steepness) (low score).\n    # If surplus is -1 (neg_surplus is 1, not possible with mask), sigmoid_values is sigmoid(steepness). Priority = 0.5 + 0.5 * sigmoid(steepness) (high score).\n\n    priorities[can_fit_mask] = 0.5 + 0.5 * sigmoid_values\n\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 18.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response1.txt_stdout.txt",
    "code_path": "problem_iter5_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Smoothed Best Fit.\n\n    This heuristic prioritizes bins that can accommodate the item. It uses a smoothed,\n    non-linear function to rank bins based on their post-placement remaining capacity,\n    favoring tighter fits. The preference for tighter fits is tunable via a 'steepness'\n    parameter.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated using a sigmoid-like transformation of the *negative* post-placement\n    remaining capacity: `1 / (1 + exp(-steepness * (bins_remain_cap - item)))`.\n    This function is monotonically increasing with respect to `bins_remain_cap - item`,\n    meaning smaller non-negative remaining capacities (tighter fits) get higher scores,\n    approaching 1.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the post-placement remaining capacity for bins that can fit the item.\n    post_placement_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # To favor smaller remaining capacities (tighter fits), we use a sigmoid-like function\n    # that increases as the remaining capacity decreases.\n    # The function `1 / (1 + exp(-x))` increases as `x` increases.\n    # We want it to increase as `post_placement_remaining_cap` decreases.\n    # So, we set `x = -steepness * post_placement_remaining_cap`.\n    # A small `post_placement_remaining_cap` (tight fit) results in a large negative `x`,\n    # leading to `exp(-x)` being large, and thus the score `1/(1+large)` being small.\n    # This is the opposite of what we want.\n\n    # Let's re-evaluate the reflection: \"Prioritize bins with minimal *post-placement* remaining capacity.\"\n    # This means smaller (bins_remain_cap - item) should yield higher priority.\n    # A sigmoid function like `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it decrease as `(bins_remain_cap - item)` increases, we use `x = steepness * (bins_remain_cap - item)`.\n    # This was the logic in v1.\n\n    # The reflection also says: \"Use smoothed, non-linear functions (like sigmoid/softmax on negative surplus)\".\n    # Let's interpret \"negative surplus\" as `item - bins_remain_cap` for bins that *don't* fit,\n    # and perhaps `bins_remain_cap - item` (the remaining capacity) for bins that *do* fit.\n    #\n    # If we want to prioritize bins with minimal *post-placement* remaining capacity,\n    # it means we want to maximize `-(bins_remain_cap - item)` (or minimize `bins_remain_cap - item`).\n    #\n    # Consider the function `f(x) = sigmoid(k * (-x))`, where `x` is the post-placement remaining capacity.\n    # `f(x) = 1 / (1 + exp(-k * x))`\n    # This function increases as `x` decreases. So, smaller remaining capacity gives higher score.\n    # This aligns with the reflection and the goal.\n\n    # Calculate the argument for the sigmoid function: -steepness * post_placement_remaining_cap\n    # A smaller remaining capacity (tighter fit) leads to a more negative argument,\n    # which when negated by `-steepness` becomes a larger positive value,\n    # resulting in a score closer to 1.\n    # A larger remaining capacity (looser fit) leads to a less negative argument,\n    # which when negated by `-steepness` becomes a smaller positive value (or negative),\n    # resulting in a score closer to 0.5 or lower.\n    exponent_args = -steepness * post_placement_remaining_cap\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    # Here, if `post_placement_remaining_cap` is very small (tight fit), `exponent_args` is very negative,\n    # `np.exp` approaches 0, and the score approaches 1.\n    # If `post_placement_remaining_cap` is very large (loose fit), `exponent_args` is very positive,\n    # `np.exp` approaches infinity, and the score approaches 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # Scores range from ~0 (very loose fit) to ~1 (very tight fit).\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 1,
    "obj": 11.188671719186283,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response2.txt_stdout.txt",
    "code_path": "problem_iter5_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed\n    non-linear function favoring tighter fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a softened version of the \"smallest surplus\" idea. Specifically, for bins\n    that can fit the item, the priority is determined by `sigmoid(-steepness * (remaining_capacity - item))`.\n    This function is monotonically decreasing with respect to `(remaining_capacity - item)`,\n    meaning smaller non-negative remaining capacities (tighter fits) get higher scores.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated as 1 / (1 + exp(steepness * (remaining_capacity - item))).\n    This formulation ensures that a perfect fit (remaining_capacity - item = 0) results\n    in a score of 0.5, tighter fits (negative surplus) result in scores > 0.5, and\n    looser fits (positive surplus) result in scores < 0.5.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) up to 1 (perfect or very tight fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the post-placement remaining capacity for bins that can fit the item.\n    post_placement_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # We want to prioritize smaller `post_placement_remain_cap`.\n    # The function `1 / (1 + exp(x))` is decreasing in `x`.\n    # To make it decrease as `post_placement_remain_cap` increases, we set `x = steepness * post_placement_remain_cap`.\n    # A small `post_placement_remain_cap` (tight fit) results in a smaller `x`, thus a higher score.\n    # A large `post_placement_remain_cap` (loose fit) results in a larger `x`, thus a lower score.\n    # For a perfect fit (post_placement_remain_cap = 0), x=0, score=0.5.\n    # For negative post_placement_remain_cap (item is smaller than remaining cap), x is negative, exp(x) is small, score > 0.5.\n    # For positive post_placement_remain_cap (item is larger than remaining cap, which should not happen due to can_fit_mask,\n    # but conceptually if it did, x would be positive, score < 0.5).\n    exponent_args = steepness * post_placement_remain_cap\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins using the sigmoid function.\n    # This results in scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative).\n    # Scores for bins that cannot fit remain 0.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 2,
    "obj": 3.9888312724371757,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response3.txt_stdout.txt",
    "code_path": "problem_iter5_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed,\n    non-linear function prioritizing minimal post-placement remaining capacity.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a transformed function of the remaining capacity to favor tighter fits more\n    aggressively than a simple sigmoid, allowing for tunable preference.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated based on the negative of the post-placement remaining capacity,\n    smoothed by a softplus-like function or a scaled inverse, aiming for a more\n    pronounced preference for near-perfect fits.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to a higher value\n        (for near-perfect fits).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Tunable parameter for controlling the \"steepness\" or emphasis on tighter fits.\n    # A higher value means stronger preference for very small remaining capacities.\n    smoothness_factor = 2.0 \n    \n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # We want to prioritize smaller (remaining_capacity - item).\n    # A small post-placement remaining capacity is desirable.\n    post_placement_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Use a function that maps smaller post-placement remaining capacities to higher scores.\n    # A common approach for prioritizing smaller values non-linearly is using\n    # a transformation like `1 / (1 + x)` or `exp(-x)`, or a smoothed version.\n    # Let's use a smoothed inverse function, `1 / (1 + smoothness_factor * x)`\n    # for `x >= 0`. This gives a score between 0 and 1.\n    # For perfect fits (post_placement_remain_cap = 0), score is 1.\n    # For very loose fits, score approaches 0.\n    \n    # To make it more aggressive, we can consider `exp(-smoothness_factor * x)`\n    # which goes from 1 to 0. Or a quadratic-like behavior for small values.\n    # Let's try a combination: prioritize very tight fits more.\n    # A function like `max(0, 1 - (post_placement_remain_cap / bin_capacity_if_available)**2)`\n    # could be considered, but it requires knowing the original bin capacity.\n    # Sticking to remaining capacity: `exp(-smoothness_factor * post_placement_remain_cap)`\n    # is a good candidate for a smooth non-linear preference.\n    \n    # Let's refine the sigmoid idea from v1, making it more focused on small surpluses.\n    # Instead of `1/(1+exp(steepness*(rem-item)))`, consider a function that\n    # amplifies small `rem-item` values.\n    # `exp(-steepness * (rem-item))` maps small positive surpluses to values near 1,\n    # and larger surpluses to values closer to 0.\n    \n    # Using exp(-x) for the valid bins:\n    # A small `post_placement_remain_cap` (tight fit) results in a large negative argument\n    # for `exp`, so `exp` is large (close to 1).\n    # A large `post_placement_remain_cap` (loose fit) results in a smaller negative argument\n    # for `exp`, so `exp` is smaller (closer to 0).\n    \n    # Apply the exponential function. The argument is `-smoothness_factor * post_placement_remain_cap`.\n    # We want to ensure that the argument to exp is not excessively negative to avoid underflow,\n    # though standard exp handles small negative numbers well.\n    # Clipping to avoid potential extreme values if needed, but for `exp(-x)` with `x>=0` and\n    # reasonable `smoothness_factor`, it's usually fine.\n    \n    exponent_args = -smoothness_factor * post_placement_remain_cap\n    \n    # Calculate priority scores for bins that can fit.\n    # The scores will be between 0 (for very loose fits) and approximately 1 (for perfect fits).\n    priorities[can_fit_mask] = np.exp(exponent_args)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter5_response4.txt_stdout.txt",
    "code_path": "problem_iter5_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed\n    non-linear function prioritizing minimal post-placement remaining capacity.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a sigmoid-like function (specifically, 1 / (1 + exp(x))), where 'x' is\n    tuned to strongly favor tighter fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated based on the negative surplus (item - remaining_capacity) which\n    we want to minimize. To use a function that rewards smaller values, we transform\n    the remaining capacity. A bin with remaining capacity `r` after packing will have\n    a score based on `steepness * (r - item)`. Smaller `r` leads to smaller `r-item`,\n    which results in a higher priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect or near-perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 10.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this value is smallest (tightest fit).\n    post_placement_remain_cap = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument for the sigmoid function.\n    # The function 1 / (1 + exp(x)) is monotonically decreasing in x.\n    # To make it favor smaller `post_placement_remain_cap`, we use `steepness * post_placement_remain_cap`.\n    # A small `post_placement_remain_cap` (tight fit) results in a smaller argument, thus a higher score.\n    # A large `post_placement_remain_cap` (loose fit) results in a larger argument, thus a lower score.\n    exponent_args = steepness * post_placement_remain_cap\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args, exp -> 0, score -> 1. For very positive args, exp -> inf, score -> 0.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins.\n    # The scores will range from close to 1 (for perfect fits, post_placement_remain_cap=0)\n    # down to values close to 0 (for very loose fits).\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 4,
    "obj": 4.108496210610296,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]