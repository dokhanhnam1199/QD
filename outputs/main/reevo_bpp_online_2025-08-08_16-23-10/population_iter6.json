[
  {
    "stdout_filepath": "problem_iter6_response0.txt_stdout.txt",
    "code_path": "problem_iter6_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Sigmoid Best Fit\n    with emphasis on favoring tighter fits by shifting the sigmoid.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a modified sigmoid function to provide a smooth ranking, strongly favoring\n    tighter fits by mapping perfect fits to scores higher than 0.5.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated as 0.5 + 0.5 * sigmoid(steepness * (item - remaining_capacity)).\n    This formulation means that a perfect fit (remaining_capacity - item = 0) results\n    in a score of 0.75, while looser fits get scores between 0.5 and 0.75, and very\n    loose fits approach 0.5. Tighter fits (if possible, i.e., remaining_capacity < item)\n    would result in scores greater than 0.75.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit) upwards, with tighter fits receiving higher scores.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    steepness = 10.0  # Tunable parameter: higher values mean stronger preference for tight fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the negative surplus for bins where the item can fit.\n    # Negative surplus = item - remaining_capacity\n    # A smaller remaining capacity (tighter fit) leads to a larger negative surplus.\n    neg_surplus_valid = item - bins_remain_cap[can_fit_mask]\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher scores for smaller `remaining_capacity` (i.e., larger `neg_surplus`).\n    # The function `sigmoid(steepness * x)` increases with `x`.\n    # Thus, `sigmoid(steepness * neg_surplus)` increases as `neg_surplus` increases.\n    sigmoid_argument = steepness * neg_surplus_valid\n\n    # Clip the sigmoid argument for numerical stability.\n    # The range [-30, 30] is generally safe for np.exp.\n    clipped_sigmoid_argument = np.clip(sigmoid_argument, -30.0, 30.0)\n\n    # Calculate the sigmoid values.\n    # For valid bins, neg_surplus_valid <= 0, so sigmoid_argument <= 0.\n    # This means sigmoid_values will be <= 0.5 for valid bins.\n    sigmoid_values = 1.0 / (1.0 + np.exp(-clipped_sigmoid_argument))\n\n    # Transform sigmoid values to prioritize tighter fits more strongly.\n    # We shift and scale the sigmoid output: 0.5 + 0.5 * sigmoid_values.\n    # This maps:\n    # - Perfect fit (neg_surplus=0): sigmoid_values=0.5 -> priority = 0.5 + 0.5*0.5 = 0.75\n    # - Loose fit (neg_surplus<0): sigmoid_values<0.5 -> priority < 0.75 (closer to 0.5 for very loose fits)\n    # - Tighter fit (neg_surplus>0, not applicable with mask): sigmoid_values>0.5 -> priority > 0.75\n    priorities[can_fit_mask] = 0.5 + 0.5 * sigmoid_values\n\n    return priorities",
    "response_id": 0,
    "obj": 4.108496210610296,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response1.txt_stdout.txt",
    "code_path": "problem_iter6_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a modified Best Fit.\n\n    This version implements a modified \"Best Fit\" heuristic. It prioritizes bins\n    that have the least remaining capacity *after* the item is placed,\n    but with a twist: it adds a small penalty for bins that would be *exactly* full.\n    This encourages leaving a tiny bit of space if possible, which can sometimes\n    be beneficial for subsequent items. Bins that cannot fit the item are given\n    a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate the remaining capacity after placing the item.\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # Prioritize bins with the smallest remaining capacity.\n    # We use the negative of the remaining capacity.\n    # Add a small epsilon to the priority for bins that would be exactly full,\n    # making them slightly less preferable than bins with a tiny bit of leftover space.\n    # This encourages leaving a small gap (e.g., 0.001) rather than filling completely.\n    epsilon = 1e-6\n    priorities[can_fit_mask] = -remaining_after_fit - (remaining_after_fit == 0) * epsilon\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response2.txt_stdout.txt",
    "code_path": "problem_iter6_code2.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed,\n    non-linear function that strongly favors tighter fits.\n\n    This heuristic prioritizes bins that can accommodate the item and have the smallest\n    remaining capacity after packing (Best Fit strategy). The priority is calculated\n    using a transformation of the negative post-placement remaining capacity,\n    emphasizing preference for near-perfect fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit, the score\n    is calculated using `0.5 + 0.5 * sigmoid(steepness * (item - remaining_capacity))`.\n    This formulation maps a perfect fit (surplus 0) to a score of 0.75, a loose fit\n    to scores between 0.5 and 0.75, and a tight fit (if possible) to scores greater than 0.75,\n    thus strongly favoring tighter fits and better bin utilization.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) to values approaching 1\n        (for very tight fits).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Tunable parameter for controlling the \"steepness\" or emphasis on tighter fits.\n    # A higher value means stronger preference for very small remaining capacities.\n    steepness = 10.0  \n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the \"negative surplus\" for bins that can fit.\n    # Negative surplus = item - remaining_capacity.\n    # A larger negative surplus (i.e., smaller positive surplus or negative surplus) indicates a tighter fit.\n    neg_surplus_valid = item - bins_remain_cap[can_fit_mask]\n\n    # Calculate the argument for the sigmoid function.\n    # We want higher scores for larger `neg_surplus_valid`.\n    # `sigmoid(x)` increases with `x`. So, we use `steepness * neg_surplus_valid`.\n    sigmoid_argument = steepness * neg_surplus_valid\n\n    # Clip the sigmoid argument for numerical stability.\n    # Values outside [-30, 30] can cause overflow/underflow in exp.\n    clipped_sigmoid_argument = np.clip(sigmoid_argument, -30.0, 30.0)\n\n    # Calculate the sigmoid values.\n    # For valid bins, `neg_surplus_valid` is <= 0, so `clipped_sigmoid_argument` is <= 0.\n    # Thus, `sigmoid_values` will be <= 0.5.\n    sigmoid_values = 1.0 / (1.0 + np.exp(-clipped_sigmoid_argument))\n\n    # Transform sigmoid values to prioritize tighter fits more strongly.\n    # The transformation `0.5 + 0.5 * sigmoid_values` maps:\n    # - A perfect fit (surplus=0, neg_surplus=0) -> sigmoid_values=0.5 -> score = 0.5 + 0.5*0.5 = 0.75.\n    # - A loose fit (surplus > 0, neg_surplus < 0) -> sigmoid_values < 0.5 -> score < 0.75.\n    # - A tight fit (surplus < 0, neg_surplus > 0) -> sigmoid_values > 0.5 -> score > 0.75.\n    # This ensures that tighter fits receive higher priority scores.\n    priorities[can_fit_mask] = 0.5 + 0.5 * sigmoid_values\n\n    return priorities",
    "response_id": 2,
    "obj": 4.108496210610296,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response3.txt_stdout.txt",
    "code_path": "problem_iter6_code3.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a tunable\n    exponential decay function that favors tighter fits but allows looser fits\n    to retain some priority.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it strongly favors bins with minimal remaining capacity after packing (tight fits).\n    However, it allows the priority score to decay more slowly for bins with\n    larger remaining capacities (looser fits), ensuring that these bins are not\n    completely disregarded and can still be considered.\n\n    The priority score is calculated using a function of the form:\n    `score = exp(-k_high * gap / (1 + alpha * gap))`,\n    where `gap` is the remaining capacity after packing (`bins_remain_cap - item`).\n\n    - `k_high` (controlled by `tight_fit_steepness`): Determines how sharply the\n      priority drops for small gaps (tight fits). A higher value means a stronger\n      preference for very tight fits.\n    - `alpha` (controlled by `loose_fit_decay_rate`): Influences how the decay\n      rate slows down for larger gaps (looser fits). A higher value means the\n      priority decays more slowly for looser fits, giving them more consideration.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated as described above. The score is 1 for a perfect fit\n    (gap=0) and decreases as the gap increases. The `alpha` parameter ensures\n    this decrease is slower for larger gaps.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from approximately 0 to 1.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Tunable parameters:\n    # `tight_fit_steepness`: Controls how rapidly priority drops for small gaps.\n    # Higher value means stronger preference for very tight fits.\n    tight_fit_steepness = 10.0  \n    \n    # `loose_fit_decay_rate`: Controls how much the decay slows down for larger gaps.\n    # Higher value means looser fits retain priority for longer.\n    loose_fit_decay_rate = 0.5  \n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity (gap) for bins that can fit the item.\n    # gap = remaining_capacity - item\n    gaps_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the score using the tunable exponential decay function.\n    # Score = exp(-k_high * gap / (1 + alpha * gap))\n    \n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the denominator: 1 + alpha * gap.\n    # For gap >= 0 and alpha >= 0, denominator is always >= 1.0.\n    denominator = 1.0 + alpha * gaps_valid\n\n    # Calculate the exponent argument: -k_high * gap / denominator\n    # This argument will be 0 for gap=0 and become increasingly negative as gap increases.\n    exponent_args = -k_high * gaps_valid / denominator\n\n    # Clip the exponent arguments for numerical stability to avoid exp overflow/underflow.\n    # Values between -30 and 30 are generally safe for np.exp.\n    # The minimum value of exponent_args occurs as gap -> infinity, which is -k_high / alpha.\n    # Clipping ensures that exp() operates on values within a stable range.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores.\n    # The score is exp(clipped_exponent_args).\n    # For gap=0, score=exp(0)=1.0.\n    # For gap>0, score < 1.0 and decays. The decay rate is influenced by k_high and alpha.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 3,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response4.txt_stdout.txt",
    "code_path": "problem_iter6_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a refined\n    Sigmoid Best Fit approach that also considers the \"Worst Fit\" aspect for\n    bins that are not a tight fit.\n\n    This heuristic prioritizes bins that can accommodate the item. Among those,\n    it favors bins that result in a smaller remaining capacity (Best Fit).\n    However, to avoid creating too many nearly-full bins prematurely, it also\n    gives a moderate score to bins that are a significantly looser fit,\n    preventing them from being completely ignored but still prioritizing\n    tighter fits.\n\n    The priority is calculated using a modified sigmoid-like function that has a\n    steeper increase for near-perfect fits and a slower decrease for looser fits.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit,\n    the score is calculated based on the remaining capacity `g = bins_remain_cap[i] - item`.\n    The function used is `exp(-k_high * g / (1 + alpha * g))`:\n    - `k_high` (tight_fit_steepness): Controls how sharply the priority drops for gaps close to zero.\n    - `alpha` (loose_fit_decay_rate): Controls how quickly the decay rate slows down for larger gaps.\n      A higher `alpha` means slower decay for loose fits, giving them more priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are designed to be non-negative, with higher scores indicating\n        higher priority. Scores range from 0 (cannot fit) up to 1 (perfect fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Parameters to tune the behavior:\n    # `tight_fit_steepness`: Controls how quickly the priority drops for near-perfect fits.\n    # `loose_fit_decay_rate`: Controls how quickly the priority decays for looser fits.\n    #                         Higher values give more preference to looser fits.\n    tight_fit_steepness = 10.0  # Higher value means stronger preference for very tight fits\n    loose_fit_decay_rate = 0.5  # Lower value means slower decay for looser fits (higher priority for loose fits)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # g = remaining_capacity_after_packing\n    gaps = bins_remain_cap[can_fit_mask] - item\n\n    # Use the modified sigmoid-like function: exp(-k_high * g / (1 + alpha * g))\n    # k_high = tight_fit_steepness\n    # alpha = loose_fit_decay_rate\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent term.\n    # Denominator is (1 + alpha * g). For g>=0 and alpha>=0, this is always >= 1.\n    denominator = 1.0 + alpha * gaps\n    exponent_args = -k_high * gaps / denominator\n\n    # Clip the exponent arguments for numerical stability.\n    # Values between -30 and 30 are generally safe for np.exp.\n    # As g -> infinity, exponent_args -> -k_high / alpha.\n    # For k_high=10, alpha=0.5, this is -20, which is safe.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate priorities for the bins that can fit the item.\n    # At g=0 (perfect fit), exponent_args=0, score=exp(0)=1.\n    # As g increases, the exponent becomes more negative, and the score decreases.\n    # The `alpha` parameter slows down this decrease for larger `g`.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response5.txt_stdout.txt",
    "code_path": "problem_iter6_code5.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a balanced\n    non-linear function that favors tight fits but allows looser fits to be\n    considered moderately.\n\n    This heuristic aims to combine the \"Best Fit\" tendency (prioritizing bins\n    with minimal remaining capacity after packing) with a \"Worst Fit\" aspect\n    (not completely discarding bins with large remaining capacity).\n\n    The priority is calculated using a function that decays smoothly but\n    less aggressively for larger remaining capacities. The proposed function is\n    `exp(-k_high * g / (1 + alpha * g))`, where `g` is the remaining capacity\n    after packing the item.\n\n    - `k_high`: Controls the steepness of the priority drop for small remaining capacities (tight fits).\n                A higher value strongly favors very tight fits.\n    - `alpha`: Controls how much the decay rate slows down for larger remaining capacities (looser fits).\n               A higher value means looser fits retain more priority.\n\n    The score for a bin is 0 if the item cannot fit. For bins that can fit:\n    - If `g` is close to 0 (tight fit), the score is close to `exp(0) = 1`.\n    - As `g` increases, the term `g / (1 + alpha * g)` increases slower than `g` itself,\n      making the exponent less negative and the score decay more slowly than a simple exponential.\n    - The effective decay rate transitions from `k_high` to `k_high / (1 + alpha * g)`. As `g` becomes large,\n      this approaches `k_high / (alpha * g)`, effectively slowing down the decay.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores are non-negative, with higher scores indicating higher priority.\n        Scores range from near 0 for very loose fits up to 1 for perfect fits.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Tunable parameters:\n    # `tight_fit_steepness`: Controls initial decay rate for tight fits. Higher means stronger preference.\n    # `loose_fit_decay_rate`: Controls how much the decay rate slows for looser fits. Higher means looser fits are more considered.\n    tight_fit_steepness = 10.0  # Corresponds to k_high\n    loose_fit_decay_rate = 0.5  # Corresponds to alpha\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # This represents the \"gap\" or \"slack\" after packing.\n    remaining_capacity_after_packing = bins_remain_cap[can_fit_mask] - item\n\n    # Use the proposed function: exp(-k_high * g / (1 + alpha * g))\n    k_high = tight_fit_steepness\n    alpha = loose_fit_decay_rate\n\n    # Calculate the exponent argument. Ensure it's numerically stable.\n    # The denominator (1 + alpha * g) is always >= 1 for non-negative g and alpha.\n    denominator = 1.0 + alpha * remaining_capacity_after_packing\n    exponent_args = -k_high * remaining_capacity_after_packing / denominator\n\n    # Clip exponent arguments to prevent overflow/underflow in np.exp.\n    # Values between -30 and 30 are generally safe for exp.\n    # The minimum value of the exponent is -k_high / alpha (as remaining_capacity_after_packing -> infinity).\n    # For k_high=10, alpha=0.5, this is -20, which is well within safe limits.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for the valid bins.\n    priorities[can_fit_mask] = np.exp(clipped_exponent_args)\n\n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response6.txt_stdout.txt",
    "code_path": "problem_iter6_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using a smoothed\n    non-linear function favoring tighter fits, with a stronger emphasis on near-perfect fits.\n\n    This heuristic prioritizes bins that can accommodate the item. The priority is\n    calculated using a modified sigmoid function that gives higher scores to bins\n    with minimal remaining capacity after packing (Best Fit strategy).\n\n    The priority for a bin is calculated as:\n    1 / (1 + exp(steepness * (remaining_capacity - item)))\n\n    This function has the property that:\n    - If remaining_capacity - item is large positive (loose fit), the score approaches 0.\n    - If remaining_capacity - item is 0 (perfect fit), the score is 0.5.\n    - If remaining_capacity - item is negative (item is smaller than remaining cap), the score approaches 1.\n\n    To further emphasize near-perfect fits and distinguish them from very loose fits,\n    we can introduce a penalty for overly loose fits, effectively pushing their scores\n    closer to zero more aggressively. This can be achieved by scaling the exponent\n    by a factor that grows with the surplus.\n\n    A potential approach: `priority = sigmoid(steepness * (item - remaining_capacity))`\n    This way, smaller `remaining_capacity` (tighter fit) leads to larger `item - remaining_capacity`,\n    resulting in a higher priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit or very loose fit) up to 1 (very tight fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    # Steepness controls how quickly the priority drops as the fit becomes looser.\n    # A higher steepness means a stronger preference for tighter fits.\n    steepness = 8.0  # Tunable parameter, increased steepness from v1\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the 'tightness' factor for bins that can fit the item.\n    # A smaller positive value indicates a tighter fit.\n    # A larger positive value indicates a looser fit.\n    # For a perfect fit, this value is 0.\n    tightness_factor = bins_remain_cap[can_fit_mask] - item\n\n    # We want higher priority for smaller tightness_factor.\n    # Using `np.exp(-steepness * tightness_factor)` will give high values for small tightness_factor.\n    # However, we want scores to be bounded. The sigmoid function is a good candidate.\n    # Let's define the sigmoid argument `x` such that `sigmoid(x)` is high when `tightness_factor` is small.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # If we set `x = steepness * (item - bins_remain_cap[can_fit_mask])`\n    # then `x` is large and positive for tight fits, leading to score ~ 1.\n    # `x` is large and negative for loose fits, leading to score ~ 0.\n    # This is opposite of what we want for Best Fit with higher priority for *smaller* remaining capacity.\n    #\n    # Let's re-evaluate v1's logic: `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # This indeed gives 0.5 for perfect fit, and approaches 1 for negative `remaining_capacity - item`\n    # and approaches 0 for positive `remaining_capacity - item`. This favors *very tight* fits over perfect fits.\n    #\n    # To create a \"smoother non-linear function favoring tighter fits\" and distinguish\n    # better between near-perfect fits and moderately loose fits, we can consider\n    # a function that is steeper around the perfect fit point, or penalizes looseness more.\n    #\n    # A simple modification could be to amplify the exponent for loose fits.\n    # Consider `sigmoid(steepness * (remaining_capacity - item) * penalty_factor)`\n    # where `penalty_factor` is 1 for tight fits and increases for loose fits.\n    #\n    # Alternative: Use a function like `1 - tanh(steepness * (remaining_capacity - item))`\n    # `tanh(x)` ranges from -1 to 1.\n    # If `remaining_capacity - item` is very negative (tight fit), `tanh` is ~-1, score ~ 1 - (-1) = 2 (problematic, need scaling)\n    # If `remaining_capacity - item` is 0 (perfect fit), `tanh` is 0, score ~ 1 - 0 = 1.\n    # If `remaining_capacity - item` is very positive (loose fit), `tanh` is ~1, score ~ 1 - 1 = 0.\n    #\n    # Let's stick with sigmoid but adjust the argument to prioritize smaller `tightness_factor`.\n    # We want score to be high when `tightness_factor` is small.\n    # `sigmoid(S * (C - x))` where `C` is capacity, `x` is item size.\n    # `sigmoid(S * (target - actual))`\n    # Let's use `sigmoid(steepness * (1.0 - (bins_remain_cap[can_fit_mask] / item)))` if item is not zero?\n    # This might be unstable if item is small.\n    #\n    # Back to `1 / (1 + exp(X))`. We want X to be small (negative) for good fits.\n    # `X = steepness * (bins_remain_cap[can_fit_mask] - item)`. This is what v1 does.\n    # Let's try to modify the `steepness` based on the surplus.\n    # If surplus `s = bins_remain_cap[can_fit_mask] - item`, we want the exponent to be `steepness * s`.\n    # We want to penalize large `s`.\n    # Maybe `steepness * (s + s^2)`? Or `steepness * s * (1 + s)`?\n    #\n    # Let's try scaling the `steepness` by the `tightness_factor` itself.\n    # If `tightness_factor` is negative (item larger than capacity, should not happen), `steepness` becomes very negative, exp -> large.\n    # If `tightness_factor` is 0 (perfect fit), exponent is 0, score is 0.5.\n    # If `tightness_factor` is positive (loose fit), `steepness` becomes positive, and it increases with `tightness_factor`.\n    # This means the exponent grows faster for looser fits.\n    #\n    # Consider exponent: `steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This amplifies the exponent for positive surpluses.\n\n    # Let's try a more direct approach that emphasizes smaller non-negative surpluses.\n    # We want a function that is high for `tightness_factor` close to 0.\n    # Consider `1 / (1 + (tightness_factor / some_scale)^2)` or `exp(-(tightness_factor / some_scale)^2)`.\n    # The latter gives 1 for perfect fit, and decreases towards 0 for loose fits.\n    # This might be simpler and more direct for favoring near-perfect fits.\n\n    # Let's refine the v1 idea: `sigmoid(steepness * (item - remaining_capacity))`\n    # `item - bins_remain_cap[can_fit_mask]`\n    # For tight fits: `item` is close to `bins_remain_cap`, difference is small positive. `sigmoid(steepness * small_positive)` -> score near 0.5.\n    # For perfect fits: `item == bins_remain_cap`, difference is 0. `sigmoid(0)` -> 0.5.\n    # For very tight fits (item < remaining): `item - remaining` is negative. `sigmoid(steepness * negative)` -> score near 1.\n    # For loose fits: `item` much smaller than `bins_remain_cap`, difference is large positive. `sigmoid(steepness * large_positive)` -> score near 0.\n\n    # This interpretation seems to favor *very tight* fits over perfect fits.\n    # The prompt says \"favoring tighter fits\". This implies that a bin that has *just enough* space\n    # should get a higher priority than a bin with *lots of extra* space.\n    #\n    # Let's go back to the prompt's \"Better code\" and its interpretation:\n    # `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # Perfect fit: `rem - item = 0`, score = 0.5\n    # Tighter fit (rem < item conceptually, but masked out): `rem - item` negative, score > 0.5\n    # Looser fit (rem > item): `rem - item` positive, score < 0.5.\n    # This function *does* favor tighter fits, with perfect fit being exactly in the middle.\n    # The issue might be that the drop-off for looser fits isn't steep enough.\n    #\n    # To emphasize *near-perfect* fits more, we can increase the steepness.\n    # Let's try a function that is very sensitive to small positive surpluses.\n    #\n    # Consider `exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This gives 1 for perfect fit, and decays exponentially for loose fits.\n    # For tight fits (negative surplus), this would give `exp(0)` which is 1.\n    # This would score perfect and very tight fits equally high.\n    #\n    # Let's try to combine the best of both. Prioritize bins that fit,\n    # and among those, prefer those with minimal *non-negative* remaining capacity.\n    #\n    # `priority = exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`\n    # This gives 1.0 for perfect fits and tight fits.\n    # For loose fits, it drops off.\n    #\n    # How to differentiate between perfect and \"very tight but still fits\"?\n    # The previous interpretation of v1 favoring negative `rem - item` is interesting.\n    # \"scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative).\"\n    # This means the prompt implies `rem < item` is possible and gets score near 1. This is confusing if `rem` is remaining capacity.\n    # The mask `bins_remain_cap >= item` ensures `remaining_capacity - item >= 0`.\n    # So `post_placement_remain_cap` is always non-negative.\n    # The scores from v1 would thus be between 0 and 0.5, with 0.5 for perfect fits.\n    # This implies v1 actually favors *looser* fits if interpreted strictly.\n    #\n    # Let's assume the intention of v1 was to favor bins where `remaining_capacity - item` is smallest (closest to 0).\n    # In that case, `1 / (1 + exp(steepness * (remaining_capacity - item)))` where `remaining_capacity - item >= 0`.\n    # This function *decreases* as `remaining_capacity - item` increases.\n    # So, smaller `remaining_capacity - item` (tighter fit) gives *higher* scores.\n    #\n    # The \"Better code\" description: \"scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative)\"\n    # This contradicts the code `remaining_capacity - item` which is always non-negative due to the mask.\n    # If `post_placement_remain_cap` is always `>= 0`, then `steepness * post_placement_remain_cap` is always `>= 0`.\n    # `exp(...)` is always `>= 1`. `1 / (1 + exp(...))` is always `< 0.5`.\n    # So, v1 actually gives scores between 0 and 0.5, with 0.5 for perfect fits, and scores approach 0 for loose fits.\n    # This means v1 *does* favor tighter fits.\n    #\n    # To make it \"improved\", we need to make the drop-off for loose fits more pronounced.\n    # We can use a higher `steepness`, or modify the exponent.\n    #\n    # Let's consider a quadratic penalty for surplus:\n    # `priority = 1 / (1 + exp(steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + bins_remain_cap[can_fit_mask] - item)))`\n    # This amplifies the exponent for larger surpluses.\n\n    # Let's use a simpler approach: exponential decay for positive surplus, and a constant high value for perfect/negative surplus.\n    # Since we are masked to `bins_remain_cap >= item`, the `tightness_factor` is always >= 0.\n    # We want scores to be high for small `tightness_factor`.\n    #\n    # How about: `exp(-steepness * tightness_factor)`\n    # `tightness_factor = bins_remain_cap[can_fit_mask] - item`\n    # This gives 1 for perfect fit.\n    # For loose fit, it drops off.\n    #\n    # To differentiate better, let's make it more sensitive to small values.\n    # `exp(-steepness * tightness_factor)` where `steepness` is high.\n    #\n    # Let's try `exp(-steepness * tightness_factor^2)`?\n    # This would peak at 1 for perfect fit, and decay quickly.\n    #\n    # Consider this approach:\n    # For bins that can fit, calculate `surplus = bins_remain_cap[can_fit_mask] - item`.\n    # Priority score = `exp(-steepness * surplus)`\n    # This gives a score of 1 for a perfect fit (surplus=0), and scores decrease towards 0 as surplus increases.\n    # This directly favors tighter fits.\n\n    steepness = 10.0  # Increased steepness to emphasize tighter fits more.\n\n    # Calculate the surplus for bins that can fit the item.\n    # surplus = remaining_capacity - item\n    # We want smaller surplus to have higher priority.\n    surplus = bins_remain_cap[can_fit_mask] - item\n\n    # Use an exponential decay function. A perfect fit (surplus=0) gets a score of 1.\n    # Looser fits (positive surplus) get scores less than 1, decaying exponentially.\n    # We want to use a scale that makes the decay noticeable but not too rapid.\n    # The `steepness` parameter controls this decay rate.\n    # A higher `steepness` means faster decay for surplus.\n    #\n    # To make it more \"smoothed non-linear\" and perhaps distinguish better than pure exponential:\n    # Use a logistic function again, but let's ensure it favors small *non-negative* surpluses.\n    # The v1 function `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # actually works as intended if we interpret `remaining_capacity - item` as the \"slack\".\n    # Smaller slack -> higher priority.\n    #\n    # The \"reflection\" stated \"Smooth non-linear functions, like sigmoid, can yield better results than linear ones.\"\n    # v1 already uses sigmoid. The improvement might come from tuning `steepness` or changing the input to sigmoid.\n    #\n    # Let's try to make the function more sensitive to small surpluses by squaring the term inside the exponent.\n    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(steepness * (surplus**2)))`\n    # This would make perfect fit give 0.5.\n    # Small surplus: `surplus^2` is very small positive. `exp` is slightly > 1. Score is slightly < 0.5.\n    # Larger surplus: `surplus^2` is larger positive. `exp` is larger. Score is smaller.\n    # This actually favors looser fits or perfect fits!\n    #\n    # Let's reverse the input to sigmoid: `1 / (1 + exp(-steepness * (surplus)))`\n    # This gives score of 0.5 for surplus=0.\n    # Small positive surplus: exp is slightly > 1. Score slightly < 0.5.\n    # Large positive surplus: exp is large. Score is small.\n    # This *still* favors tighter fits, with perfect fit at 0.5.\n    #\n    # What if we want a score of 1 for perfect fit and decreasing from there?\n    # Consider `exp(-steepness * surplus)`.\n    # Perfect fit (surplus=0): score = 1.\n    # Small surplus: score < 1.\n    # Large surplus: score approaches 0.\n    # This function directly favors the tightest possible fit.\n\n    # Let's use this exponential decay as the improved heuristic.\n    # The steepness parameter controls how quickly the priority drops off for larger surpluses.\n    # A higher steepness means that only bins with very small surpluses will receive high scores.\n\n    # Calculate priorities using exponential decay of surplus.\n    # Clipping the argument to exp to prevent overflow/underflow.\n    # If surplus is very large, exp(-steepness * surplus) can underflow to 0.\n    # If surplus is negative (should not happen here), exp could grow large.\n    # We are interested in surplus >= 0.\n    # Max surplus can be bin_capacity - min_item_size.\n    # If bin_capacity=100 and min_item=1, max surplus ~ 99.\n    # With steepness=10, exp(-10 * 99) is very close to 0.\n    # A safe range for exp argument is [-30, 30].\n    # -steepness * surplus. We want this to be between -30 and 0 ideally.\n    # So, surplus should be between 0 and 3.\n    # This implies the steepness needs tuning or the function needs adjustment for larger surpluses.\n\n    # Let's rescale the surplus to a more manageable range.\n    # Max possible surplus could be the bin capacity itself.\n    # Let's normalize surplus by a reasonable upper bound, e.g., the bin capacity or average item size.\n    # For simplicity, let's use the `steepness` to control the decay.\n    # We want the score to drop significantly if surplus is, say, 10% of bin capacity.\n    # If bin capacity is 100, a surplus of 10.\n    # exp(-steepness * 10). If steepness=1, exp(-10) ~ 0.\n    # If steepness=0.1, exp(-1) ~ 0.36.\n    #\n    # Let's reconsider the sigmoid function from v1 and boost the steepness.\n    # The problem description implies v1 is good, just needs improvement.\n    # The description of v1's score range was confusing, but the *logic* favors tighter fits.\n    # Higher steepness in `1 / (1 + exp(steepness * (remaining_capacity - item)))`\n    # means the score drops more sharply as `remaining_capacity - item` increases.\n    # This directly improves the preference for tighter fits.\n\n    steepness = 15.0  # Significantly increased steepness\n\n    # Calculate the post-placement remaining capacity (slack) for bins that can fit the item.\n    slack = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the exponent argument. Larger slack results in a larger exponent.\n    exponent_args = steepness * slack\n\n    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.\n    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.\n    # For very negative args (not possible here due to mask), exp -> 0, score -> 1.\n    # For very positive args (loose fits), exp -> inf, score -> 0.\n    # Perfect fit (slack=0) gives exponent=0, score=0.5.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores. Scores range from 0 (loose fit) to 0.5 (perfect fit).\n    # Higher steepness makes the transition from 0.5 to 0 much faster for any positive slack.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 6,
    "obj": 4.198244914240141,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response7.txt_stdout.txt",
    "code_path": "problem_iter6_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a tailored inverse mapping with a scaling factor.\n\n    This heuristic prioritizes bins where placing the item leaves the least amount\n    of remaining capacity. It calculates the \"resulting remaining capacity\" for each\n    bin that can fit the item and then applies a transformation that maps smaller\n    resulting remaining capacities to higher priority scores. The transformation\n    involves taking the inverse of the resulting remaining capacity, adding a\n    small epsilon to prevent division by zero, and then scaling this value.\n    A sensitivity parameter `k` controls how strongly tighter fits are favored.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Higher scores indicate higher priority.\n    \"\"\"\n    # Initialize priorities to zero for all bins. Bins that cannot fit the item will retain this zero priority.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that have enough remaining capacity to fit the item.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the remaining capacity *after* placing the item.\n    # We want to prioritize bins where this resulting remaining capacity is minimized.\n    resulting_remaining_cap = bins_remain_cap[can_fit_mask] - item\n\n    # To prioritize smaller resulting remaining capacities, we use a transformation.\n    # A common approach is to use the inverse: 1 / (resulting_remaining_cap).\n    # This naturally gives higher values for smaller resulting capacities.\n    # Add a small epsilon to avoid division by zero or extremely large values when resulting_remaining_cap is very close to zero.\n    epsilon = 1e-9\n    \n    # Scale the inverse by a factor 'k'. A larger 'k' amplifies the difference between bins,\n    # making the priority more sensitive to smaller resulting remaining capacities.\n    # This is an alternative to the sigmoid in v1, aiming for a more direct mapping of tightness.\n    k = 20.0  # Sensitivity parameter: Controls how strongly tighter fits are favored.\n    \n    # The transformed score directly reflects how \"tight\" the fit is.\n    # A resulting_remaining_cap of 0 gives k / epsilon, which is a very high score.\n    # As resulting_remaining_cap increases, the score decreases.\n    transformed_scores = k / (resulting_remaining_cap + epsilon)\n\n    # Assign the calculated transformed scores to the bins that can fit the item.\n    priorities[can_fit_mask] = transformed_scores\n\n    return priorities",
    "response_id": 7,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response8.txt_stdout.txt",
    "code_path": "problem_iter6_code8.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version implements a \"Best Fit\" heuristic by prioritizing bins that\n    minimize the remaining capacity *after* packing the item (i.e., minimize slack).\n    Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority (i.e., a bin that results in\n        less remaining capacity after packing).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, the priority is the negative of the slack\n    # (remaining capacity after placing the item). This way, smaller slack\n    # (better fit) results in a higher priority score.\n    priorities[can_fit_mask] = -(bins_remain_cap[can_fit_mask] - item)\n\n    return priorities",
    "response_id": 8,
    "obj": 4.048663741523748,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter6_response9.txt_stdout.txt",
    "code_path": "problem_iter6_code9.py",
    "code": "import numpy as np\nimport random\nimport math\nimport scipy\nimport torch\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, prioritizing tight fits and item/bin proportion.\n\n    This heuristic prioritizes bins that can accommodate the item and result in a\n    small remaining capacity after packing (tight fit). It also incorporates a\n    secondary factor that favors packing the item into bins where it represents\n    a larger proportion of the *current* remaining capacity. This dual objective\n    aims to minimize wasted space by finding tight fits and by encouraging\n    larger items to occupy relatively larger available slots.\n\n    The score for a bin is calculated using a sigmoid function applied to the\n    \"fit gap\" (remaining capacity after packing). A smaller fit gap (tighter fit)\n    results in a higher score. The sigmoid function `1 / (1 + exp(-k * gap))`\n    maps smaller gaps to scores closer to 1.\n\n    The secondary factor is `item / (initial_remaining_capacity + epsilon)`,\n    which is high when the item is large relative to the bin's current capacity.\n\n    The final priority is the product of these two scores.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        Scores range from 0 (cannot fit) to approximately 1 (excellent fit).\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    epsilon = 1e-9  # Small value to avoid division by zero.\n    steepness = 15.0 # Tunable parameter: higher values mean stronger preference for tighter fits.\n\n    # Identify bins where the item can fit.\n    can_fit_mask = bins_remain_cap >= item\n\n    if not np.any(can_fit_mask):\n        return priorities\n\n    # Calculate the remaining capacity for bins that can fit the item.\n    # The 'fit gap' is the remaining capacity *after* placing the item.\n    # We want to minimize this gap for tight fits.\n    fit_gaps = bins_remain_cap[can_fit_mask] - item\n    initial_remaining_caps = bins_remain_cap[can_fit_mask]\n\n    # Primary criterion: Tight fit using a sigmoid function.\n    # We use `1 / (1 + exp(-steepness * fit_gap))` which results in scores\n    # approaching 1 for smaller `fit_gaps` (tighter fits).\n    # A perfect fit (fit_gap = 0) gives a score of 0.5.\n    # Scores increase as `fit_gap` becomes smaller (approaching 0).\n    exponent_args_tight_fit = -steepness * fit_gaps\n    # Clip to prevent overflow/underflow in np.exp.\n    clipped_exponent_args = np.clip(exponent_args_tight_fit, -30.0, 30.0)\n    tight_fit_score = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    # Secondary criterion: Item proportion of current bin capacity.\n    # This encourages packing larger items into relatively larger available bins.\n    # A higher score is given if the item is a larger fraction of the current bin capacity.\n    proportion_score = item / (initial_remaining_caps + epsilon)\n\n    # Combine scores by multiplication.\n    # This prioritizes bins that are both a tight fit AND where the item\n    # represents a significant portion of the available space.\n    priorities[can_fit_mask] = tight_fit_score * proportion_score\n\n    return priorities",
    "response_id": 9,
    "obj": 5.045871559633042,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  }
]