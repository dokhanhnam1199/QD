```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using an
    enhanced Sigmoid Best Fit.

    This heuristic prioritizes bins that can accommodate the item.
    It favors bins that result in a smaller remaining capacity after packing
    (Best Fit strategy). To achieve this, it uses a sigmoid function applied
    to the negative of the excess capacity (capacity - item). This means
    bins with exactly the item's size (zero excess capacity) get the highest
    priority (close to 1), and bins with a slight excess get progressively
    lower priorities. Bins that cannot fit the item receive a priority of 0.

    The score for a bin is 0 if the item cannot fit. For bins that can fit,
    the score is calculated as 1 / (1 + exp(-steepness * (bins_remain_cap - item))).
    This formulation emphasizes exact fits and penalizes larger remaining
    capacities more strongly.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores range from 0 (cannot fit or very loose fit) to 1 (perfect fit).
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    steepness = 5.0  # Tunable parameter: higher values mean stronger preference for tight fits.

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # Calculate the "closeness" score for bins that can fit.
    # We want to reward bins where (bins_remain_cap - item) is small.
    # The sigmoid function 1 / (1 + exp(-x)) increases with x.
    # So, we want x to be large when (bins_remain_cap - item) is small.
    # Setting x = steepness * (bins_remain_cap - item) doesn't quite do this directly.
    # Instead, let's consider the "waste" or "excess capacity" if we fit the item.
    # If we fit the item into a bin with capacity C, the remaining capacity is C - item.
    # We want to prioritize smaller values of (C - item).
    # A good way to do this with sigmoid is to use the negative of the difference
    # as the argument to exp: `exp(-steepness * (C - item))`.
    # When (C - item) is 0 (perfect fit), the argument is 0, exp(0)=1, score = 1/(1+1) = 0.5.
    # Wait, the prior reflection suggested prioritizing exact matches, which should be score 1.
    # Let's re-evaluate:
    # Sigmoid: `S(x) = 1 / (1 + exp(-x))`
    # If we want score 1 for exact fit (remaining_cap - item = 0), then exp(-x) should be 0.
    # This means -x should be large negative, so x should be large positive.
    # Let's use `x = steepness * (ideal_capacity - current_capacity)`.
    # Here, `ideal_capacity` would be the `item` size.
    # So, `x = steepness * (item - (bins_remain_cap - item))` is not right.
    # Let's go back to the goal: prioritize bins with smallest `bins_remain_cap - item`.
    # If `remaining_cap_after_fit = bins_remain_cap - item`, we want to maximize
    # a function that is high for small `remaining_cap_after_fit`.
    # Consider `f(x) = 1 / (1 + exp(steepness * x))`. This is decreasing in x.
    # We want it to be high for small `remaining_cap_after_fit`.
    # So, let `x = remaining_cap_after_fit`.
    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(steepness * (bins_remain_cap[can_fit_mask] - item)))`
    # This matches priority_v1. The reflection mentioned "prioritize exact matches" and "smooth preference curves".
    # `priority_v1` does this. For `bins_remain_cap - item = 0`, the score is 1/(1+exp(0)) = 0.5.
    # For `bins_remain_cap - item < 0` (impossible due to mask), this would be >0.5.
    # For `bins_remain_cap - item > 0`, the score decreases.
    # The reflection also says "penalizing large unused space". This is achieved by the sigmoid.
    #
    # Let's try to interpret the reflection more literally:
    # "prioritize exact matches" -> score 1 for `bins_remain_cap == item`.
    # "then bins minimizing remaining capacity" -> decreasing score as `bins_remain_cap - item` increases.
    #
    # How about a function that is 1 at 0 and decreases?
    # `max(0, 1 - steepness * (bins_remain_cap - item))`? Linear. Not smooth.
    #
    # The sigmoid `1 / (1 + exp(-x))` *increases* with x.
    # If we set `x = steepness * (item - (bins_remain_cap - item))`, this doesn't quite make sense.
    #
    # Let's use `bins_remain_cap - item` directly. We want this value to be small.
    # The function `1 / (1 + exp(k * x))` is a decreasing function of `x`.
    # If we let `x = bins_remain_cap - item`, we get smaller scores for smaller `x`. This is the opposite.
    #
    # Let's redefine the sigmoid for our purpose.
    # We want a function `f(diff)` where `diff = bins_remain_cap - item`.
    # `f(0) = 1` (exact match)
    # `f(positive_small) = high` (close fit)
    # `f(positive_large) = low` (loose fit)
    # `f(negative)` = 0 (cannot fit - already handled by mask)
    #
    # Consider `1 - (1 / (1 + exp(-steepness * diff))) = exp(-steepness * diff) / (1 + exp(-steepness * diff))`
    # This is 0 at diff=0. Not what we want.
    #
    # Consider `1 / (1 + exp(steepness * diff))`.
    # At diff=0, this is 0.5.
    # As diff increases, it goes to 0.
    # As diff decreases (towards negative), it goes to 1.
    #
    # The original `priority_v1` uses `1 / (1 + exp(steepness * (remaining_capacity - item)))`.
    # This gives 0.5 for perfect fit, and decreases for larger remaining capacities.
    # This prioritizes bins with smaller remaining capacity *after* packing.
    #
    # The prompt reflection: "prioritize exact matches, then bins minimizing remaining capacity."
    # This implies a higher score for exact matches than for just "minimizing remaining capacity".
    # The current sigmoid approach gives 0.5 for an exact match.
    #
    # Let's try a different approach:
    # If `bins_remain_cap == item`, priority is 1.
    # Otherwise, for `bins_remain_cap > item`, use a decreasing function.
    #
    # How about: `priority = exp(-steepness * (bins_remain_cap - item))` for `bins_remain_cap >= item`.
    # At `bins_remain_cap == item`, priority is `exp(0) = 1`.
    # As `bins_remain_cap - item` increases, priority decreases towards 0.
    # This fits the reflection better.
    #
    # Let's adjust the steepness parameter's interpretation.
    #
    # The function `exp(-steepness * x)` is a good candidate.
    # We want smaller `bins_remain_cap - item` to yield higher scores.
    #
    # Let's use the formulation `exp(-steepness * diff)` where `diff = bins_remain_cap - item`.
    # If `bins_remain_cap - item` is 0, exp(0) = 1.
    # If `bins_remain_cap - item` is small positive, exp is slightly less than 1.
    # If `bins_remain_cap - item` is large positive, exp is close to 0.
    # This fits "prioritize exact matches, then bins minimizing remaining capacity."

    # Calculate the 'fit difference' for bins that can accommodate the item.
    # This is the remaining capacity after packing the item.
    fit_difference = bins_remain_cap[can_fit_mask] - item

    # Calculate priority using an exponential decay function.
    # The score is `exp(-steepness * fit_difference)`.
    # This ensures a score of 1.0 for a perfect fit (fit_difference = 0)
    # and scores approaching 0 for larger fit_differences (looser fits).
    # This directly addresses the reflection's requirement to prioritize exact matches.

    # Clip the exponent argument to prevent potential overflow/underflow.
    # For very large negative `fit_difference` (though not possible with the mask, theoretically),
    # `steepness * fit_difference` would be large negative, exp -> 0.
    # For very large positive `fit_difference`, `steepness * fit_difference` would be large positive, exp -> inf.
    # Let's cap the argument to be safe. A range of [-30, 30] is usually fine for exp.
    # Since `fit_difference` is always non-negative here, we only need to worry about
    # `steepness * fit_difference` becoming very large positive.
    # If `fit_difference` is very large, the priority should be close to 0.
    # `exp(-steepness * large_positive)` will naturally go to 0.
    # We might want to avoid `exp(very_large_negative)` if we were using `-fit_difference`.
    # Here, `exp(-steepness * fit_difference)` is safe.
    # Let's still consider clipping for robustness, though `fit_difference` is >= 0.
    # If `fit_difference` is very large, `-steepness * fit_difference` becomes very negative.
    # `np.exp` handles large negative inputs gracefully by returning 0.
    # We mainly need to worry about the intermediate `steepness * fit_difference` calculation
    # if `fit_difference` is enormous, or `steepness` is enormous.
    # Clipping the result of `steepness * fit_difference` might be sufficient.
    # `clipped_exponent_arg = np.clip(steepness * fit_difference, -np.inf, 30.0)` ensures `exp` doesn't overflow if `-steepness * fit_difference` is very positive.
    # But `fit_difference >= 0`, so `steepness * fit_difference >= 0`.
    # Thus, `-steepness * fit_difference <= 0`.
    # So, `np.exp` will be between 0 and 1. No overflow risk here.
    # We can just compute directly.

    scores_for_valid_bins = np.exp(-steepness * fit_difference)

    # Assign the calculated scores to the corresponding bins.
    priorities[can_fit_mask] = scores_for_valid_bins

    return priorities
```
