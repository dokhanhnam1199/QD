```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """
    Returns priority with which we want to add item to each bin using a smoothed
    non-linear function favoring tighter fits, with a stronger emphasis on near-perfect fits.

    This heuristic prioritizes bins that can accommodate the item. The priority is
    calculated using a modified sigmoid function that gives higher scores to bins
    with minimal remaining capacity after packing (Best Fit strategy).

    The priority for a bin is calculated as:
    1 / (1 + exp(steepness * (remaining_capacity - item)))

    This function has the property that:
    - If remaining_capacity - item is large positive (loose fit), the score approaches 0.
    - If remaining_capacity - item is 0 (perfect fit), the score is 0.5.
    - If remaining_capacity - item is negative (item is smaller than remaining cap), the score approaches 1.

    To further emphasize near-perfect fits and distinguish them from very loose fits,
    we can introduce a penalty for overly loose fits, effectively pushing their scores
    closer to zero more aggressively. This can be achieved by scaling the exponent
    by a factor that grows with the surplus.

    A potential approach: `priority = sigmoid(steepness * (item - remaining_capacity))`
    This way, smaller `remaining_capacity` (tighter fit) leads to larger `item - remaining_capacity`,
    resulting in a higher priority.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of remaining capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
        Scores range from 0 (cannot fit or very loose fit) up to 1 (very tight fit).
    """
    priorities = np.zeros_like(bins_remain_cap, dtype=float)
    # Steepness controls how quickly the priority drops as the fit becomes looser.
    # A higher steepness means a stronger preference for tighter fits.
    steepness = 8.0  # Tunable parameter, increased steepness from v1

    # Identify bins where the item can fit.
    can_fit_mask = bins_remain_cap >= item

    # Calculate the 'tightness' factor for bins that can fit the item.
    # A smaller positive value indicates a tighter fit.
    # A larger positive value indicates a looser fit.
    # For a perfect fit, this value is 0.
    tightness_factor = bins_remain_cap[can_fit_mask] - item

    # We want higher priority for smaller tightness_factor.
    # Using `np.exp(-steepness * tightness_factor)` will give high values for small tightness_factor.
    # However, we want scores to be bounded. The sigmoid function is a good candidate.
    # Let's define the sigmoid argument `x` such that `sigmoid(x)` is high when `tightness_factor` is small.
    # `sigmoid(x) = 1 / (1 + exp(-x))`
    # If we set `x = steepness * (item - bins_remain_cap[can_fit_mask])`
    # then `x` is large and positive for tight fits, leading to score ~ 1.
    # `x` is large and negative for loose fits, leading to score ~ 0.
    # This is opposite of what we want for Best Fit with higher priority for *smaller* remaining capacity.
    #
    # Let's re-evaluate v1's logic: `1 / (1 + exp(steepness * (remaining_capacity - item)))`
    # This indeed gives 0.5 for perfect fit, and approaches 1 for negative `remaining_capacity - item`
    # and approaches 0 for positive `remaining_capacity - item`. This favors *very tight* fits over perfect fits.
    #
    # To create a "smoother non-linear function favoring tighter fits" and distinguish
    # better between near-perfect fits and moderately loose fits, we can consider
    # a function that is steeper around the perfect fit point, or penalizes looseness more.
    #
    # A simple modification could be to amplify the exponent for loose fits.
    # Consider `sigmoid(steepness * (remaining_capacity - item) * penalty_factor)`
    # where `penalty_factor` is 1 for tight fits and increases for loose fits.
    #
    # Alternative: Use a function like `1 - tanh(steepness * (remaining_capacity - item))`
    # `tanh(x)` ranges from -1 to 1.
    # If `remaining_capacity - item` is very negative (tight fit), `tanh` is ~-1, score ~ 1 - (-1) = 2 (problematic, need scaling)
    # If `remaining_capacity - item` is 0 (perfect fit), `tanh` is 0, score ~ 1 - 0 = 1.
    # If `remaining_capacity - item` is very positive (loose fit), `tanh` is ~1, score ~ 1 - 1 = 0.
    #
    # Let's stick with sigmoid but adjust the argument to prioritize smaller `tightness_factor`.
    # We want score to be high when `tightness_factor` is small.
    # `sigmoid(S * (C - x))` where `C` is capacity, `x` is item size.
    # `sigmoid(S * (target - actual))`
    # Let's use `sigmoid(steepness * (1.0 - (bins_remain_cap[can_fit_mask] / item)))` if item is not zero?
    # This might be unstable if item is small.
    #
    # Back to `1 / (1 + exp(X))`. We want X to be small (negative) for good fits.
    # `X = steepness * (bins_remain_cap[can_fit_mask] - item)`. This is what v1 does.
    # Let's try to modify the `steepness` based on the surplus.
    # If surplus `s = bins_remain_cap[can_fit_mask] - item`, we want the exponent to be `steepness * s`.
    # We want to penalize large `s`.
    # Maybe `steepness * (s + s^2)`? Or `steepness * s * (1 + s)`?
    #
    # Let's try scaling the `steepness` by the `tightness_factor` itself.
    # If `tightness_factor` is negative (item larger than capacity, should not happen), `steepness` becomes very negative, exp -> large.
    # If `tightness_factor` is 0 (perfect fit), exponent is 0, score is 0.5.
    # If `tightness_factor` is positive (loose fit), `steepness` becomes positive, and it increases with `tightness_factor`.
    # This means the exponent grows faster for looser fits.
    #
    # Consider exponent: `steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + max(0, bins_remain_cap[can_fit_mask] - item))`
    # This amplifies the exponent for positive surpluses.

    # Let's try a more direct approach that emphasizes smaller non-negative surpluses.
    # We want a function that is high for `tightness_factor` close to 0.
    # Consider `1 / (1 + (tightness_factor / some_scale)^2)` or `exp(-(tightness_factor / some_scale)^2)`.
    # The latter gives 1 for perfect fit, and decreases towards 0 for loose fits.
    # This might be simpler and more direct for favoring near-perfect fits.

    # Let's refine the v1 idea: `sigmoid(steepness * (item - remaining_capacity))`
    # `item - bins_remain_cap[can_fit_mask]`
    # For tight fits: `item` is close to `bins_remain_cap`, difference is small positive. `sigmoid(steepness * small_positive)` -> score near 0.5.
    # For perfect fits: `item == bins_remain_cap`, difference is 0. `sigmoid(0)` -> 0.5.
    # For very tight fits (item < remaining): `item - remaining` is negative. `sigmoid(steepness * negative)` -> score near 1.
    # For loose fits: `item` much smaller than `bins_remain_cap`, difference is large positive. `sigmoid(steepness * large_positive)` -> score near 0.

    # This interpretation seems to favor *very tight* fits over perfect fits.
    # The prompt says "favoring tighter fits". This implies that a bin that has *just enough* space
    # should get a higher priority than a bin with *lots of extra* space.
    #
    # Let's go back to the prompt's "Better code" and its interpretation:
    # `1 / (1 + exp(steepness * (remaining_capacity - item)))`
    # Perfect fit: `rem - item = 0`, score = 0.5
    # Tighter fit (rem < item conceptually, but masked out): `rem - item` negative, score > 0.5
    # Looser fit (rem > item): `rem - item` positive, score < 0.5.
    # This function *does* favor tighter fits, with perfect fit being exactly in the middle.
    # The issue might be that the drop-off for looser fits isn't steep enough.
    #
    # To emphasize *near-perfect* fits more, we can increase the steepness.
    # Let's try a function that is very sensitive to small positive surpluses.
    #
    # Consider `exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`
    # This gives 1 for perfect fit, and decays exponentially for loose fits.
    # For tight fits (negative surplus), this would give `exp(0)` which is 1.
    # This would score perfect and very tight fits equally high.
    #
    # Let's try to combine the best of both. Prioritize bins that fit,
    # and among those, prefer those with minimal *non-negative* remaining capacity.
    #
    # `priority = exp(-steepness * max(0, bins_remain_cap[can_fit_mask] - item))`
    # This gives 1.0 for perfect fits and tight fits.
    # For loose fits, it drops off.
    #
    # How to differentiate between perfect and "very tight but still fits"?
    # The previous interpretation of v1 favoring negative `rem - item` is interesting.
    # "scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative)."
    # This means the prompt implies `rem < item` is possible and gets score near 1. This is confusing if `rem` is remaining capacity.
    # The mask `bins_remain_cap >= item` ensures `remaining_capacity - item >= 0`.
    # So `post_placement_remain_cap` is always non-negative.
    # The scores from v1 would thus be between 0 and 0.5, with 0.5 for perfect fits.
    # This implies v1 actually favors *looser* fits if interpreted strictly.
    #
    # Let's assume the intention of v1 was to favor bins where `remaining_capacity - item` is smallest (closest to 0).
    # In that case, `1 / (1 + exp(steepness * (remaining_capacity - item)))` where `remaining_capacity - item >= 0`.
    # This function *decreases* as `remaining_capacity - item` increases.
    # So, smaller `remaining_capacity - item` (tighter fit) gives *higher* scores.
    #
    # The "Better code" description: "scores between ~0.5 (for perfect fit) and ~1 (for very tight fits where post_placement_remain_cap is negative)"
    # This contradicts the code `remaining_capacity - item` which is always non-negative due to the mask.
    # If `post_placement_remain_cap` is always `>= 0`, then `steepness * post_placement_remain_cap` is always `>= 0`.
    # `exp(...)` is always `>= 1`. `1 / (1 + exp(...))` is always `< 0.5`.
    # So, v1 actually gives scores between 0 and 0.5, with 0.5 for perfect fits, and scores approach 0 for loose fits.
    # This means v1 *does* favor tighter fits.
    #
    # To make it "improved", we need to make the drop-off for loose fits more pronounced.
    # We can use a higher `steepness`, or modify the exponent.
    #
    # Let's consider a quadratic penalty for surplus:
    # `priority = 1 / (1 + exp(steepness * (bins_remain_cap[can_fit_mask] - item) * (1 + bins_remain_cap[can_fit_mask] - item)))`
    # This amplifies the exponent for larger surpluses.

    # Let's use a simpler approach: exponential decay for positive surplus, and a constant high value for perfect/negative surplus.
    # Since we are masked to `bins_remain_cap >= item`, the `tightness_factor` is always >= 0.
    # We want scores to be high for small `tightness_factor`.
    #
    # How about: `exp(-steepness * tightness_factor)`
    # `tightness_factor = bins_remain_cap[can_fit_mask] - item`
    # This gives 1 for perfect fit.
    # For loose fit, it drops off.
    #
    # To differentiate better, let's make it more sensitive to small values.
    # `exp(-steepness * tightness_factor)` where `steepness` is high.
    #
    # Let's try `exp(-steepness * tightness_factor^2)`?
    # This would peak at 1 for perfect fit, and decay quickly.
    #
    # Consider this approach:
    # For bins that can fit, calculate `surplus = bins_remain_cap[can_fit_mask] - item`.
    # Priority score = `exp(-steepness * surplus)`
    # This gives a score of 1 for a perfect fit (surplus=0), and scores decrease towards 0 as surplus increases.
    # This directly favors tighter fits.

    steepness = 10.0  # Increased steepness to emphasize tighter fits more.

    # Calculate the surplus for bins that can fit the item.
    # surplus = remaining_capacity - item
    # We want smaller surplus to have higher priority.
    surplus = bins_remain_cap[can_fit_mask] - item

    # Use an exponential decay function. A perfect fit (surplus=0) gets a score of 1.
    # Looser fits (positive surplus) get scores less than 1, decaying exponentially.
    # We want to use a scale that makes the decay noticeable but not too rapid.
    # The `steepness` parameter controls this decay rate.
    # A higher `steepness` means faster decay for surplus.
    #
    # To make it more "smoothed non-linear" and perhaps distinguish better than pure exponential:
    # Use a logistic function again, but let's ensure it favors small *non-negative* surpluses.
    # The v1 function `1 / (1 + exp(steepness * (remaining_capacity - item)))`
    # actually works as intended if we interpret `remaining_capacity - item` as the "slack".
    # Smaller slack -> higher priority.
    #
    # The "reflection" stated "Smooth non-linear functions, like sigmoid, can yield better results than linear ones."
    # v1 already uses sigmoid. The improvement might come from tuning `steepness` or changing the input to sigmoid.
    #
    # Let's try to make the function more sensitive to small surpluses by squaring the term inside the exponent.
    # `priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(steepness * (surplus**2)))`
    # This would make perfect fit give 0.5.
    # Small surplus: `surplus^2` is very small positive. `exp` is slightly > 1. Score is slightly < 0.5.
    # Larger surplus: `surplus^2` is larger positive. `exp` is larger. Score is smaller.
    # This actually favors looser fits or perfect fits!
    #
    # Let's reverse the input to sigmoid: `1 / (1 + exp(-steepness * (surplus)))`
    # This gives score of 0.5 for surplus=0.
    # Small positive surplus: exp is slightly > 1. Score slightly < 0.5.
    # Large positive surplus: exp is large. Score is small.
    # This *still* favors tighter fits, with perfect fit at 0.5.
    #
    # What if we want a score of 1 for perfect fit and decreasing from there?
    # Consider `exp(-steepness * surplus)`.
    # Perfect fit (surplus=0): score = 1.
    # Small surplus: score < 1.
    # Large surplus: score approaches 0.
    # This function directly favors the tightest possible fit.

    # Let's use this exponential decay as the improved heuristic.
    # The steepness parameter controls how quickly the priority drops off for larger surpluses.
    # A higher steepness means that only bins with very small surpluses will receive high scores.

    # Calculate priorities using exponential decay of surplus.
    # Clipping the argument to exp to prevent overflow/underflow.
    # If surplus is very large, exp(-steepness * surplus) can underflow to 0.
    # If surplus is negative (should not happen here), exp could grow large.
    # We are interested in surplus >= 0.
    # Max surplus can be bin_capacity - min_item_size.
    # If bin_capacity=100 and min_item=1, max surplus ~ 99.
    # With steepness=10, exp(-10 * 99) is very close to 0.
    # A safe range for exp argument is [-30, 30].
    # -steepness * surplus. We want this to be between -30 and 0 ideally.
    # So, surplus should be between 0 and 3.
    # This implies the steepness needs tuning or the function needs adjustment for larger surpluses.

    # Let's rescale the surplus to a more manageable range.
    # Max possible surplus could be the bin capacity itself.
    # Let's normalize surplus by a reasonable upper bound, e.g., the bin capacity or average item size.
    # For simplicity, let's use the `steepness` to control the decay.
    # We want the score to drop significantly if surplus is, say, 10% of bin capacity.
    # If bin capacity is 100, a surplus of 10.
    # exp(-steepness * 10). If steepness=1, exp(-10) ~ 0.
    # If steepness=0.1, exp(-1) ~ 0.36.
    #
    # Let's reconsider the sigmoid function from v1 and boost the steepness.
    # The problem description implies v1 is good, just needs improvement.
    # The description of v1's score range was confusing, but the *logic* favors tighter fits.
    # Higher steepness in `1 / (1 + exp(steepness * (remaining_capacity - item)))`
    # means the score drops more sharply as `remaining_capacity - item` increases.
    # This directly improves the preference for tighter fits.

    steepness = 15.0  # Significantly increased steepness

    # Calculate the post-placement remaining capacity (slack) for bins that can fit the item.
    slack = bins_remain_cap[can_fit_mask] - item

    # Calculate the exponent argument. Larger slack results in a larger exponent.
    exponent_args = steepness * slack

    # Clip the exponent arguments to prevent potential overflow/underflow in np.exp.
    # Values like +/- 700 can cause issues. A range like [-30, 30] is generally safe.
    # For very negative args (not possible here due to mask), exp -> 0, score -> 1.
    # For very positive args (loose fits), exp -> inf, score -> 0.
    # Perfect fit (slack=0) gives exponent=0, score=0.5.
    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)

    # Calculate the priority scores. Scores range from 0 (loose fit) to 0.5 (perfect fit).
    # Higher steepness makes the transition from 0.5 to 0 much faster for any positive slack.
    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))

    return priorities
```
