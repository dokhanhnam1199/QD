{"system": "You are an expert in the domain of optimization heuristics. Your task is to design heuristics that can effectively solve optimization problems.\nYour response outputs Python code and nothing else. Format your code as a Python code string: \"```python ... ```\".\n", "user": "Write a priority function for Solving online Bin Packing Problem (BPP). BPP requires packing a set of items of various sizes into the smallest number of fixed-sized bins. Online BPP requires packing an item as soon as it is received.\nThe priority function takes as input an item and an array of bins_remain_cap (containing the remaining capacity of each bin) and returns a priority score for each bin. The bin with the highest priority score will be selected for the item.\n\n\n[Worse code]\ndef priority_v0(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    In this version, we explore and exploit bins based on a probability.\n    The core idea is to balance choosing the seemingly \"best\" bin (greedy approach)\n    with occasionally trying other bins to potentially find a better overall packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration (trying random bins)\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify feasible bins for the current item\n    feasible_bins_mask = bins_remain_cap >= item\n    feasible_bins_indices = np.where(feasible_bins_mask)[0]\n\n    if not feasible_bins_indices.size:\n        # No bin can accommodate the item, return all zeros\n        return priorities\n\n    # Greedy part: assign higher priority to bins that leave less remaining capacity\n    # This is a variation of the First Fit Decreasing or Best Fit heuristic\n    # where we prioritize bins that are \"tightest\" fits.\n    # We can normalize remaining capacity to get a 'goodness' score,\n    # or simply use the remaining capacity itself as an indicator for greedy choice.\n    # Here, we'll favor bins with less remaining capacity among the feasible ones.\n    remaining_capacities_feasible = bins_remain_cap[feasible_bins_mask]\n    # A higher score indicates a better greedy choice (less remaining space)\n    # So we can invert the remaining capacity or use a function like 1 / (remaining_capacity - item + 1)\n    # to give higher scores to tighter fits. Adding 1 avoids division by zero.\n    greedy_scores = 1.0 / (remaining_capacities_feasible - item + 1e-9)\n\n    # Apply greedy scores to feasible bins\n    priorities[feasible_bins_mask] = greedy_scores\n\n    # Exploration part: with probability epsilon, choose a random feasible bin\n    if np.random.rand() < epsilon and feasible_bins_indices.size > 0:\n        random_feasible_index = np.random.choice(feasible_bins_indices)\n        # Assign a uniformly high priority to the randomly chosen bin\n        # to ensure it gets a chance. The value is arbitrary but should\n        # be high enough to potentially override greedy scores.\n        priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities for exploration\n        priorities[random_feasible_index] = 1.0 # Give maximum priority to the random bin\n    elif feasible_bins_indices.size > 0:\n        # Normalize greedy scores to a common range if needed, or just ensure they are positive\n        # and that feasible bins have higher priority than infeasible ones.\n        # We already set feasible bins' priorities > 0. If we want to emphasize\n        # the greedy choice over random exploration for non-random picks:\n        # we can amplify the greedy scores slightly.\n        # Let's scale them to be between 0 and 1, for instance.\n        min_greedy_score = np.min(greedy_scores)\n        max_greedy_score = np.max(greedy_scores)\n        if max_greedy_score - min_greedy_score > 1e-9:\n            priorities[feasible_bins_mask] = (greedy_scores - min_greedy_score) / (max_greedy_score - min_greedy_score)\n        else:\n            # All feasible bins have the same greedy score\n            priorities[feasible_bins_mask] = 0.5 # Assign a neutral high score\n\n\n    return priorities\n\n[Better code]\ndef priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated based on how well an item fits into a bin.\n    A higher priority is given to bins where the remaining capacity is just enough\n    or slightly more than the item size. A large surplus capacity is penalized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can actually fit the item\n    # For bins that cannot fit, their priority will be effectively zero after softmax.\n    # However, to avoid issues with log(0) or division by zero, we can set a very small capacity\n    # or filter them out conceptually. For simplicity in softmax, we will give them a very low score.\n\n    # Calculate the \"fitness score\" for each bin.\n    # We want to penalize bins that have significantly more space than needed.\n    # A simple approach is to use the ratio of remaining capacity to item size.\n    # We are looking for a ratio close to 1.\n    # A large ratio (bin_remain_cap >> item) is bad.\n    # A ratio slightly > 1 is good.\n    # A ratio < 1 is impossible.\n\n    # To make it work with softmax, we want higher values for better fits.\n    # Let's define a score that is high when bin_remain_cap is close to item,\n    # and decreases as bin_remain_cap grows larger than item.\n\n    # Consider bins that can accommodate the item.\n    valid_bins_mask = bins_remain_cap >= item\n    \n    # Initialize priorities to a very low value for invalid bins.\n    priorities = np.full_like(bins_remain_cap, -np.inf) \n\n    # For valid bins, calculate a score.\n    # A score of 1 / (bin_remain_cap - item + 1) would give higher scores to smaller remaining capacities.\n    # Adding 1 to the denominator to avoid division by zero if remaining capacity == item.\n    # This score is high when remaining capacity is close to item size, and decreases as it increases.\n    # We want to avoid very large remaining capacities.\n    # Let's scale this score. We can use a term like `1 / (gap + epsilon)` where gap is `bins_remain_cap - item`.\n    # A small gap is good. A large gap is bad.\n    # We can use `exp(-(bins_remain_cap - item) / scale)` or similar for a decaying score.\n    # For softmax, a simpler approach might be to aim for values that differentiate.\n    # Let's try `-(bins_remain_cap - item)`. This gives higher values to smaller gaps.\n    # We need to ensure the output of this calculation is not too sensitive or too uniform.\n    \n    # Let's consider the gap: `bins_remain_cap - item`.\n    # A small gap is good. A large gap is bad.\n    # We want to maximize the value when gap is small.\n    # Let's try a score that is `-(gap)^2` or similar.\n    # Or even simpler: `1.0 / (gap + 1.0)` which maps smaller gaps to higher values.\n    \n    # Using `1.0 / (bins_remain_cap - item + 1e-6)` might be problematic if `item` is very large relative to `bins_remain_cap`.\n    # Let's use a score based on the \"waste\": `bins_remain_cap - item`.\n    # We want to minimize waste.\n    # Let's use an exponential decay on the waste, but inverted.\n    # A high score for low waste. `exp(-waste / temperature)`.\n    # For softmax, `exp(score)` is used. So we need `score` to be high for low waste.\n    # `score = -(bins_remain_cap - item)` or `score = - (bins_remain_cap - item)**2`\n    # Let's try `score = -(bins_remain_cap - item)`. This prioritizes bins with minimal remaining capacity.\n\n    # Softmax requires exp(score). We want scores to be positive for the exponential.\n    # So let's shift the scores to be positive, or use a transformation that results in positive values.\n    # Consider the inverse of the \"waste\": `1 / (waste + epsilon)`.\n    # Or simply use the negative waste directly, and `np.exp` will handle it.\n\n    # Let's use `score = -(bins_remain_cap - item)`. This means smaller gaps get higher scores.\n    # If we want to prioritize bins with *just* enough space, this is reasonable.\n    # A larger gap results in a more negative score.\n    # When softmax is applied, `-inf` scores will be zero.\n    \n    # Let's refine the scoring. We want the ratio `item / bins_remain_cap` to be as close to 1 as possible, but only when `bins_remain_cap >= item`.\n    # Consider the inverse of the remaining capacity: `1.0 / bins_remain_cap`. This prioritizes fuller bins (smaller remaining capacity), which is good if `item` fits.\n    # Let's combine this with the condition.\n    # A potential score for valid bins: `item / bins_remain_cap`. This is > 1 for perfect fit if `bins_remain_cap == item`, and approaches 0 for very large `bins_remain_cap`.\n    # Let's use `1.0 / (bins_remain_cap - item + 1e-6)` to prioritize bins with minimal surplus.\n    # This value is large when `bins_remain_cap - item` is small.\n\n    # Let's use the ratio of item size to bin capacity, but adjusted.\n    # We are interested in bins where `bins_remain_cap` is close to `item`.\n    # Let's assign a score that is high when `bins_remain_cap - item` is small.\n    # Consider `score = - (bins_remain_cap - item)**2`. This gives a parabolic penalty for larger gaps.\n    # The peak is at `bins_remain_cap == item`.\n\n    # A simple way to make it suitable for softmax is to use `exp(score)`\n    # So, we need `score` to be higher for preferred bins.\n    # Preferred bins are those with `bins_remain_cap` closest to `item`.\n    # `score = - (bins_remain_cap - item)`.\n    # The maximum score for `bins_remain_cap = item` would be 0.\n    # If `bins_remain_cap > item`, score becomes negative.\n    \n    # Let's use `bins_remain_cap - item` and invert it.\n    # A bin with `bins_remain_cap = 10` and `item = 5` has a gap of 5.\n    # A bin with `bins_remain_cap = 6` and `item = 5` has a gap of 1.\n    # We want to prioritize the second bin.\n    # So, a smaller gap should result in a higher score.\n    # Let's try `score = 1.0 / (bins_remain_cap - item + epsilon)`.\n    # Or `score = - (bins_remain_cap - item)`.\n\n    # Let's try a simple heuristic that prioritizes bins with the smallest *remaining* capacity that can still fit the item.\n    # This is often called \"Best Fit\".\n    # For Softmax-Based Fit, we can transform this preference into scores.\n    # We want bins where `bins_remain_cap` is small (but >= item) to have high scores.\n    # Consider the transformed scores `scores = -bins_remain_cap` for valid bins.\n    # This will give higher scores to bins with less remaining capacity.\n\n    scores = np.zeros_like(bins_remain_cap)\n    valid_indices = np.where(bins_remain_cap >= item)[0]\n    \n    # Assign a score that prioritizes bins with smaller remaining capacity.\n    # We want `bins_remain_cap` to be small. So, `scores = -bins_remain_cap` would achieve this.\n    # However, the actual values of `bins_remain_cap` might vary widely.\n    # Let's consider the \"waste\" again: `waste = bins_remain_cap - item`.\n    # We want to minimize waste.\n    # `score = -waste` or `score = -waste**2`.\n    # Let's use a function that is high for small waste.\n    # `score = 1.0 / (waste + 1.0)` - this works well when `waste` is small and non-negative.\n    # Example:\n    # waste = 0, score = 1.0\n    # waste = 1, score = 0.5\n    # waste = 5, score = 0.16\n    # waste = 10, score = 0.09\n\n    # Apply this scoring to valid bins.\n    waste = bins_remain_cap[valid_indices] - item\n    scores[valid_indices] = 1.0 / (waste + 1e-6) # Adding epsilon for numerical stability\n\n    # Now, apply softmax to convert these scores into probabilities/priorities.\n    # Softmax function: `exp(score) / sum(exp(scores))`\n    # For selection, we just need the `exp(score)` part, as the denominator is constant for all bins.\n    # `np.exp(scores)` will give us the relative priorities.\n    \n    # If we want to directly use the scores that are higher for better fits,\n    # then the scores themselves can be used, and softmax will normalize them.\n    # Let's use a score that reflects \"how close\" the remaining capacity is to the item size.\n    # A good fit has `bins_remain_cap` slightly larger than `item`.\n    # Let's consider `-(bins_remain_cap - item)`.\n    \n    # A different approach: penalize large remaining capacities.\n    # If `bins_remain_cap < item`, it's not a valid bin, score is effectively 0.\n    # For `bins_remain_cap >= item`:\n    # We want a high score when `bins_remain_cap` is close to `item`.\n    # Let's use `score = -(bins_remain_cap - item)`.\n    \n    # Example: item = 5\n    # Bin 1: remain_cap = 5, score = -(5-5) = 0\n    # Bin 2: remain_cap = 7, score = -(7-5) = -2\n    # Bin 3: remain_cap = 10, score = -(10-5) = -5\n    # Bin 4: remain_cap = 3, score = -inf (or very low)\n    \n    # `np.exp([0, -2, -5, -inf])` would be approximately `[1.0, 0.135, 0.0067, 0.0]`\n    # This gives higher probability to bin 1 (exact fit), then bin 2, then bin 3.\n    # This aligns with a \"Best Fit\" strategy transformed for softmax.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with negative infinity for softmax to yield 0 priority\n    valid_mask = bins_remain_cap >= item\n    \n    # For valid bins, calculate the score. We want bins with smaller `bins_remain_cap - item` to have higher scores.\n    # Let the score be the negative of the excess capacity.\n    excess_capacity = bins_remain_cap[valid_mask] - item\n    scores_for_valid_bins = -excess_capacity\n    \n    priorities[valid_mask] = scores_for_valid_bins\n\n    # The `np.exp(priorities)` will produce values where higher is better.\n    # Softmax is implicitly applied when selecting the bin with the maximum *logit* (score).\n    # If we need the probabilities, we'd divide by sum(exp(priorities)).\n    # For a priority function that returns scores for `np.argmax`, simply returning `np.exp(priorities)` is common.\n    # Or, even simpler, just return the scores themselves if the selection mechanism uses `np.argmax` on raw scores.\n    # The request implies returning scores for each bin, and the bin with highest priority score is selected.\n    # So, `np.exp(scores)` directly gives the desired output.\n\n    return np.exp(priorities)\n\n[Reflection]\nPrioritize bins with minimal surplus. Use softmax for smooth preference transitions.\n\n[Improved code]\nPlease write an improved function `priority_v2`, according to the reflection. Output code only and enclose your code with Python code block: ```python ... ```."}