```python
import numpy as np

def priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin using Softmax-Based Fit.

    This heuristic prioritizes bins that can fit the item, with a preference for
    bins that leave less remaining capacity (tighter fit) after placing the item.
    The Softmax function is used to convert these fit scores into probabilities,
    where a better fit (smaller remaining capacity) gets a higher probability.

    Args:
        item: Size of item to be added to the bin.
        bins_remain_cap: Array of capacities for each bin.

    Return:
        Array of same size as bins_remain_cap with priority score of each bin.
    """
    # Calculate how much space would be left if the item is placed in each bin
    # Only consider bins that can actually fit the item
    remaining_after_fit = bins_remain_cap - item
    
    # Create an initial score array, initialized to negative infinity for bins
    # that cannot fit the item. This ensures they will have zero probability after softmax.
    scores = np.full_like(bins_remain_cap, -np.inf)

    # For bins that can fit the item, calculate a "fitness score".
    # A smaller remaining capacity (tighter fit) should have a higher score.
    # We can use the negative of the remaining capacity, or a transformation
    # that favors smaller positive values. A common approach is to use
    # 1 / (remaining_capacity + epsilon) to avoid division by zero and
    # emphasize smaller remaining capacities.
    # Alternatively, and perhaps simpler for softmax, is to use the negative
    # remaining capacity directly. The softmax will then make bins with
    # less remaining capacity have higher probabilities.
    
    # Let's use the negative remaining capacity as the "fit score".
    # Bins that can fit have remaining_after_fit >= 0
    can_fit_mask = remaining_after_fit >= 0
    
    # For bins that can fit, assign a score that represents how "good" the fit is.
    # A good fit is a small positive remaining capacity.
    # To make softmax work well, we want larger positive values for better fits.
    # So, we can use -remaining_after_fit. A remaining_after_fit of 0 (perfect fit)
    # gives a score of 0. A remaining_after_fit of 1 gives -1, etc.
    # So a larger score means a better fit (smaller remaining_after_fit).
    
    # However, we need to be careful if remaining_after_fit can be zero.
    # If remaining_after_fit is 0, -remaining_after_fit is 0.
    # If remaining_after_fit is > 0, -remaining_after_fit is < 0.
    # This aligns with the intuition that a perfect fit is the best.
    
    # A common strategy in these types of heuristics is to consider the "waste"
    # created. We want to minimize waste for a "tight fit".
    # Let's define a value that represents desirability of a bin:
    # Higher value means more desirable.
    # Desirability could be inversely proportional to the remaining capacity *after* fitting.
    # We want to favor bins with small positive remaining capacity after fitting.
    
    # A simple mapping that works well with softmax:
    # -1 / (remaining_capacity_after_fit + 1)
    # For remaining_after_fit = 0, score = -1/1 = -1
    # For remaining_after_fit = 1, score = -1/2 = -0.5
    # For remaining_after_fit = 2, score = -1/3 = -0.33
    # This assigns higher scores to smaller remaining capacities (more desirable bins).
    # If a bin cannot fit, it gets -inf.
    
    # Let's refine this: we want to maximize the desirability of fitting.
    # If we fit, the remaining capacity is bins_remain_cap - item.
    # We want this value to be small.
    # Let's transform this remaining capacity into a score where smaller positive
    # values are better.
    # A common strategy: exp( - k * remaining_capacity )
    # or simply, use the negative of remaining capacity, and softmax handles it.
    
    # Let's use a score where a perfect fit (remaining = 0) is best,
    # and progressively worse.
    # If a bin can fit, calculate its "attractiveness".
    # A bin that leaves less space is more attractive.
    # So, let `attractiveness` = -(bins_remain_cap - item)
    # Or, if the remaining capacity is `r`, we want to maximize something like `exp(-r)`.
    
    # Let's consider a simplified approach for clarity that works with softmax:
    # Calculate `fitness_value` for bins that can fit the item.
    # We want bins where `bins_remain_cap - item` is small.
    # Let `fitness_value` = 1.0 / (bins_remain_cap - item + epsilon)
    # where epsilon is a small constant to avoid division by zero.
    # Then apply softmax. Higher fitness_value leads to higher probability.
    
    # Alternative interpretation of "Softmax-Based Fit":
    # The priority is derived from how well the item fits.
    # Let's define a score `s_i` for bin `i`.
    # If item `j` fits in bin `i` (capacity `c_i`):
    #   `s_i = exp(-alpha * (c_i - item_j))` where `alpha > 0`.
    #   This makes bins with smaller remaining capacity (`c_i - item_j`) have higher scores.
    # If item `j` does not fit in bin `i`:
    #   `s_i = 0` (or a very small number like -inf for softmax).
    
    # Let's implement this with alpha=1 for simplicity, and handle the "cannot fit" case.
    
    alpha = 1.0 # Tuning parameter for sensitivity to remaining capacity
    
    # Calculate potential remaining capacity for all bins
    potential_remaining = bins_remain_cap - item
    
    # Initialize scores for all bins to a very low value (effectively zero probability after softmax)
    # We use a large negative number instead of -np.inf to avoid issues with softmax if all are -inf.
    # However, for typical softmax implementations, -np.inf is appropriate.
    # Let's stick with a very low number or use conditional logic.
    
    # Let's build the scores array:
    # For bins that can fit (potential_remaining >= 0), assign score exp(-alpha * potential_remaining).
    # For bins that cannot fit, assign 0 (or a value that results in 0 probability).
    
    # Compute the exponent term for bins that can fit.
    # A perfect fit (remaining = 0) gives exp(0) = 1.
    # A fit leaving 1 unit of space gives exp(-alpha).
    # A fit leaving more space gives smaller values.
    
    # We can directly compute exp(-alpha * potential_remaining) and then mask out invalid ones.
    
    # Calculate the exponent terms.
    exponent_terms = -alpha * potential_remaining
    
    # Apply exponential to get scores.
    scores_exp = np.exp(exponent_terms)
    
    # Where the item does not fit, the score should be effectively zero.
    # Set scores to 0 for bins where potential_remaining < 0.
    scores = np.where(potential_remaining >= 0, scores_exp, 0)
    
    # Note: If `bins_remain_cap` is empty or `item` is invalid, this might fail.
    # Assuming valid inputs as per problem description.

    # If all scores are 0 (e.g., item too large for all bins),
    # softmax might behave unexpectedly or return NaNs if the sum is zero.
    # In a real scenario, you might want to add a fallback to create a new bin.
    # For this function, we assume at least one bin *could* potentially fit or the output
    # will simply be all zeros if no bins can.

    # Apply softmax to get probabilities (which are our priorities)
    # Softmax function: exp(z_i) / sum(exp(z_j))
    # Here, `scores` are already `exp(z_i)` terms.
    
    sum_of_scores = np.sum(scores)
    
    if sum_of_scores == 0:
        # This happens if the item cannot fit into any bin.
        # In this scenario, all bins have a priority of 0.
        # Or, we could return an array of 1/N to indicate no preference,
        # but given the context of "priority of adding to each bin",
        # 0 priority makes sense if it cannot be added.
        # However, usually the problem implies the item WILL be placed.
        # If it cannot fit, it might imply a new bin creation strategy which this
        # function doesn't handle. For THIS function, we return 0 for all bins.
        return np.zeros_like(bins_remain_cap)
    else:
        priorities = scores / sum_of_scores
        return priorities

```
