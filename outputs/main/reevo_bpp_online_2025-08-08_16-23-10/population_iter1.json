[
  {
    "stdout_filepath": "problem_iter1_response0.txt_stdout.txt",
    "code_path": "problem_iter1_code0.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using First Fit strategy.\n\n    This priority function implements the First Fit strategy for the online Bin Packing Problem.\n    It assigns a high priority (1) to bins that have enough remaining capacity to fit the item,\n    and a low priority (0) to bins that do not. This ensures that the first available bin\n    with sufficient capacity is chosen.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assign priority 1 if the bin has enough capacity, otherwise 0\n    priorities = (bins_remain_cap >= item).astype(float)\n    return priorities",
    "response_id": 0,
    "obj": 4.487435181491823,
    "SLOC": 3.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response1.txt_stdout.txt",
    "code_path": "problem_iter1_code1.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This version implements the \"Best Fit\" heuristic for the online Bin Packing Problem.\n    The priority is higher for bins that have just enough capacity to fit the item,\n    minimizing wasted space. Bins that cannot fit the item are given a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity *after* placing the item\n    # We want to prioritize bins where this value is minimized (i.e., the \"best fit\")\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n\n    # The priority score is inversely related to the remaining capacity after fitting.\n    # A smaller remaining capacity (closer to 0) means a higher priority.\n    # To avoid division by zero and to give higher scores to bins that fit snugly,\n    # we can use a large number minus the remaining capacity.\n    # Add a small epsilon to the remaining capacity to avoid division by zero if an item\n    # perfectly fills a bin and to ensure that bins with less remaining space get higher scores.\n    # A common approach is to use 1 / (remaining_capacity + epsilon).\n    # However, for best fit, we want to MINIMIZE remaining_capacity.\n    # So, a good strategy is to assign a high priority to the bin with the smallest\n    # `remaining_after_fit`.\n    # We can transform the `remaining_after_fit` values such that smaller values\n    # result in larger priorities. A simple way is to invert them or use their negative.\n    # Let's use 1 / (remaining_after_fit + a_small_constant) where the small constant\n    # helps in cases where remaining_after_fit is 0.\n    # A more direct way for \"best fit\" is to simply rank them: the best fit has the minimum\n    # remaining space. So, we can assign a score based on the rank or a transformed value.\n\n    # For best fit, the goal is to find the bin that has the smallest remaining capacity\n    # *after* the item is placed in it, provided it fits.\n    # So, we want to maximize `-(bins_remain_cap - item)` among valid bins.\n    # Or equivalently, minimize `bins_remain_cap - item`.\n\n    # Let's assign priority based on how \"tightly\" the item fits.\n    # The difference `bins_remain_cap - item` represents the slack.\n    # We want to minimize this slack.\n    # A higher priority score should be given to a smaller slack.\n    # So, we can use something like `1 / (slack + epsilon)` or `max_slack - slack`.\n    # Using `max_slack - slack` makes larger values of slack have lower priority.\n    # Let's calculate the slack for bins that can fit the item.\n\n    slacks = bins_remain_cap[can_fit_mask] - item\n\n    # If there are no bins that can fit the item, return all zeros.\n    if not slacks.size:\n        return priorities\n\n    # We want to assign higher priority to smaller slacks.\n    # We can achieve this by taking the inverse of (slack + a small number)\n    # to avoid division by zero, or by taking the negative of slack.\n    # Let's use `1 / (slack + 1e-6)` which means smaller slacks get larger values.\n    # Alternatively, we can directly use the negative slack as priority if we\n    # are going to select the maximum, or if we are going to select the minimum of `-priority`.\n\n    # Let's use a scoring system where a perfect fit (slack = 0) gets the highest priority.\n    # A slightly larger slack gets a slightly lower priority.\n    # `1.0 / (slack + 1e-9)` works well.\n    # Or, `(max_slack_possible - slack)` where `max_slack_possible` is greater than any slack.\n    # For simplicity and to directly implement \"smallest remaining capacity\",\n    # we can assign priority as `-slack`. Then the bin with the highest priority\n    # will be the one with the smallest slack.\n\n    priorities[can_fit_mask] = -slacks\n\n    return priorities",
    "response_id": 1,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response2.txt_stdout.txt",
    "code_path": "problem_iter1_code2.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin, using Worst Fit.\n\n    The Worst Fit strategy aims to place the current item into the bin that has\n    the most remaining capacity. This leaves larger remaining capacities in other\n    bins, potentially allowing for larger future items to be packed.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Returns:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a more preferred bin.\n    \"\"\"\n    # Initialize priorities to a very small negative number (or negative infinity)\n    # to ensure that bins that cannot fit the item get a lower priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Find bins that have enough capacity to fit the current item.\n    # For these bins, the priority is simply their remaining capacity.\n    # The higher the remaining capacity, the higher the priority (Worst Fit).\n    can_fit_mask = bins_remain_cap >= item\n    priorities[can_fit_mask] = bins_remain_cap[can_fit_mask]\n\n    return priorities",
    "response_id": 2,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response3.txt_stdout.txt",
    "code_path": "problem_iter1_code3.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The Almost Full Fit strategy prioritizes bins that are almost full,\n    meaning they have very little remaining capacity. This aims to leave\n    more space in other bins for potentially larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # We want to prioritize bins with less remaining capacity.\n    # A simple inverse relationship can be used.\n    # We also need to ensure the item fits.\n    for i in range(len(bins_remain_cap)):\n        if bins_remain_cap[i] >= item:\n            # Higher priority for bins with less remaining capacity.\n            # Adding a small constant to avoid division by zero or very small numbers\n            # can be considered, but for simplicity, we can use a large value for\n            # smaller remaining capacity.\n            # A large positive number for bins with very little space.\n            # A small positive number for bins with more space.\n            # A priority of 0 for bins that cannot fit the item.\n            \n            # Strategy: prioritize bins that are 'almost full'.\n            # This can be interpreted as bins where bins_remain_cap[i] - item is small.\n            # So, the priority should be inversely proportional to (bins_remain_cap[i] - item).\n            # To avoid division by zero if bins_remain_cap[i] == item, we can add a small epsilon.\n            \n            # Let's define 'almost full' by a threshold. If remaining capacity is below a certain percentage\n            # of the bin's original capacity (we don't have original capacity here, but we can infer\n            # relative 'fullness' from remaining capacity itself).\n            \n            # A simple heuristic: higher priority for smaller remaining capacity.\n            # Inverse of remaining capacity, but scaled to avoid extreme values.\n            # Consider the 'gap' created: bins_remain_cap[i] - item. Smaller gap is better.\n            \n            # Let's try a priority that is higher when the remaining capacity is *just enough*\n            # or slightly more than the item.\n            # For example, priority = 1 / (bins_remain_cap[i] - item + epsilon)\n            # Where epsilon is a small positive number.\n            # This gives higher priority to bins where the remaining capacity is just slightly\n            # larger than the item.\n\n            epsilon = 1e-9  # Small epsilon to prevent division by zero\n            priorities[i] = 1.0 / (bins_remain_cap[i] - item + epsilon)\n        else:\n            priorities[i] = 0.0 # Item does not fit\n\n    # Normalize priorities if needed, but for selection, relative order is sufficient.\n    # The higher the value, the more preferred the bin.\n\n    return priorities",
    "response_id": 3,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Command '['python3', '-u', '/home/dokhanhnam1199/QD/problems/bpp_online/eval.py', '5000', '/home/dokhanhnam1199/QD', 'train']' timed out after 49.99996850900061 seconds"
  },
  {
    "stdout_filepath": "problem_iter1_response4.txt_stdout.txt",
    "code_path": "problem_iter1_code4.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    The Exact Fit First strategy prioritizes bins that can exactly fit the item.\n    If no bin can exactly fit the item, it prioritizes bins that leave the smallest\n    remaining capacity after placing the item (Best Fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can exactly fit the item\n    exact_fit_bins = (bins_remain_cap == item)\n\n    # Assign a very high priority to exact fit bins\n    # We use a large number (e.g., 1e9) to ensure these are picked first.\n    # The specific value can be tuned, but it should be significantly larger\n    # than any possible \"best fit\" priority.\n    priorities[exact_fit_bins] = 1e9\n\n    # For bins that don't exactly fit, calculate the \"best fit\" priority.\n    # We want to minimize the remaining capacity after placing the item.\n    # Therefore, a smaller (bins_remain_cap - item) is better.\n    # To translate this into a higher priority score, we can use the negative\n    # of the remaining capacity or a transformation like 1 / (remaining_capacity + epsilon).\n    # For simplicity and to ensure non-zero denominators, we'll use a negative score.\n    # We only consider bins that can actually fit the item (bins_remain_cap >= item).\n    \n    # Create a mask for bins that can fit the item but are not an exact fit\n    can_fit_and_not_exact = (bins_remain_cap >= item) & ~exact_fit_bins\n\n    # Calculate the remaining capacity for these bins\n    remaining_after_fit = bins_remain_cap[can_fit_and_not_exact] - item\n\n    # Assign priority based on remaining capacity.\n    # We want smaller remaining capacity to have higher priority.\n    # So, we assign a score that is inversely related to the remaining capacity.\n    # A simple way is to use a large value minus the remaining capacity.\n    # Here, we'll use a large base score minus the remaining capacity.\n    # The 'large base' ensures that even the best \"not exact fit\" is still lower\n    # priority than an \"exact fit\".\n    \n    # Calculate a 'goodness' score for non-exact fits: higher score for smaller remaining capacity\n    # We want to rank these from smallest remaining capacity to largest.\n    # A simple inverse relation or subtracting from a large number works.\n    # Let's subtract the remaining capacity from a large number.\n    # This ensures that a bin with remaining_after_fit=1 has a higher priority than\n    # remaining_after_fit=2, given they are both not exact fits.\n    \n    # Assign priorities for bins that can fit but not exactly\n    # We use a base value that is lower than the exact fit priority,\n    # and then subtract the remaining capacity.\n    # The smaller the remaining_after_fit, the higher the priority for these bins.\n    # For example, if remaining_after_fit = 0.1, priority is 100 - 0.1 = 99.9\n    # If remaining_after_fit = 0.5, priority is 100 - 0.5 = 99.5\n    \n    # To make it consistent with the \"exact fit first\" and then \"best fit\"\n    # the priority score itself should reflect this hierarchy.\n    # Priority for exact fit is `high_priority_value`.\n    # Priority for best fit should be lower, but ordered by smallest `remaining_capacity`.\n    # We can use `score = high_priority_value - epsilon - remaining_capacity`.\n    # Let's use a tiered system:\n    # Tier 1: Exact Fit (priority = 2)\n    # Tier 2: Best Fit (priority = 1 - (remaining_capacity / MAX_CAPACITY)) to keep it positive and ordered.\n    # A simpler approach:\n    # Exact fit bins get priority 1.\n    # Non-exact fit bins that can fit get priority proportional to the inverse of remaining capacity.\n    # Or even simpler: exact fit gets a very high score, others get scores based on their goodness.\n\n    # Let's refine the priority:\n    # Exact fit: priority = 1 (highest category)\n    # Can fit, but not exact fit: priority = 1 - (remaining_capacity / MAX_BIN_CAPACITY)\n    # This way, a smaller remaining capacity gives a higher priority (closer to 1).\n    # Bins that cannot fit should have a priority of 0 or negative.\n\n    # Let's assign 1.0 to exact fits.\n    # For other bins that can fit, let's assign a priority that's a function of\n    # minimizing the remaining space. We want smaller remaining space to be better.\n    # A common approach for \"best fit\" is to maximize `-(remaining_space)`.\n    # So, priority is `- (bins_remain_cap - item)`.\n    # We need to ensure that exact fits are always preferred over non-exact fits.\n\n    # Let's try this scoring:\n    # Exact fit bins: score = infinity (or a very large number)\n    # Non-exact fit bins that can fit: score = 1 / (remaining_capacity + epsilon) - this is prone to large numbers and requires careful scaling.\n    # A more robust way:\n    # Exact fit bins get priority = 2\n    # Bins that can fit, but not exactly: priority = 1 - (bins_remain_cap - item) / MAX_POSSIBLE_REMAINING_CAPACITY.\n    # Bins that cannot fit: priority = 0.\n\n    # A common heuristic approach is to assign discrete levels of priority.\n    # Level 1: Exact Fit\n    # Level 2: Best Fit among those that can fit\n    # Level 0: Cannot fit\n\n    # Let's assign scores directly.\n    # Assign a score to bins that exactly fit.\n    # For bins that can fit but not exactly, assign a score based on how \"close\" they are.\n    # The closer the fit (smaller remaining capacity), the higher the score.\n\n    # Initialize priorities to 0 (representing bins that cannot fit the item)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item (remaining capacity >= item size)\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, we want to find the \"exact fit\" first.\n    # If there are exact fits, give them the highest priority.\n    # Otherwise, give priority based on \"best fit\".\n\n    # Identify exact fit bins\n    exact_fit_mask = (bins_remain_cap == item)\n\n    # Assign highest priority to exact fit bins.\n    # Let's use a scale where higher number is better.\n    # Assigning a value like 1000 for exact fit.\n    priorities[exact_fit_mask] = 1000.0\n\n    # For bins that can fit but are not exact fits, assign a secondary priority.\n    # The priority should be higher for bins with smaller remaining capacity.\n    # We can use a score like: max_possible_remaining - actual_remaining.\n    # To ensure these secondary priorities are lower than exact fit, we can subtract\n    # from a value lower than 1000.\n\n    # Consider only bins that can fit but are not exact fits.\n    non_exact_can_fit_mask = can_fit_mask & ~exact_fit_mask\n\n    # Calculate the remaining capacity for these bins\n    remaining_after_fit_values = bins_remain_cap[non_exact_can_fit_mask] - item\n\n    # To give higher priority to smaller remaining capacities, we can invert it.\n    # For example, if max capacity is 1.0, and item is 0.3:\n    # Remaining capacities: 0.7, 0.5, 0.2\n    # We want priorities: high for 0.2, then 0.5, then 0.7.\n    # A simple inversion like 1 / (remaining_capacity + epsilon) can work,\n    # but might lead to very large numbers if remaining_capacity is small.\n    # A better approach is to use a decreasing function of remaining capacity.\n    # We can map remaining capacities to priorities that are always less than the exact fit priority.\n    # Let's use a large base value (e.g., 900) and subtract a scaled version of remaining capacity.\n    # The maximum possible remaining capacity for a bin that can fit item `i` is `MAX_BIN_CAPACITY - i`.\n    # So, `remaining_after_fit_values` will be in the range `[0, MAX_BIN_CAPACITY - item]`.\n    # We want the highest priority for `remaining_after_fit_values = 0` (which would be exact fit, already handled).\n    # So we focus on `remaining_after_fit_values > 0`.\n    # We want the smallest `remaining_after_fit_values` to have the highest priority within this group.\n    # Let's define a score: `base_score_for_non_exact - remaining_after_fit`.\n    # The `base_score_for_non_exact` should be less than 1000. Let's use 999.\n    # Then the priority for a bin with remaining capacity `R` is `999 - R`.\n    # This prioritizes smaller `R` (closer fits).\n\n    if np.any(non_exact_can_fit_mask):\n        # Assuming bins_remain_cap are always positive and item is positive.\n        # Max possible remaining capacity for a bin that fits item `i` can be bounded by the max capacity of any bin.\n        # A robust approach might use a normalizing factor if bin capacities vary wildly,\n        # but for a fixed capacity bin (as implied by BPP), we can use a value related to that.\n        # Let's just use a score that decreases with remaining capacity.\n        # Example: `999 - remaining_after_fit_values`.\n        priorities[non_exact_can_fit_mask] = 999.0 - remaining_after_fit_values\n\n    return priorities",
    "response_id": 4,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response5.txt_stdout.txt",
    "code_path": "problem_iter1_code5.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit).\n\n    This heuristic prioritizes bins that have a remaining capacity closest to the item's size.\n    The \"distance\" is defined as the absolute difference between the bin's remaining capacity and the item's size.\n    The priority is the inverse of this distance. Bins with remaining capacity exactly equal to the item size\n    (i.e., zero distance) get the highest priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate the \"distance\" to each bin. We only consider bins that can fit the item.\n    # For bins that cannot fit the item, the distance is effectively infinity,\n    # so their priority will be 0.\n    fit_mask = bins_remain_cap >= item\n    \n    # Calculate the difference between remaining capacity and item size for fitting bins\n    diffs = bins_remain_cap[fit_mask] - item\n    \n    # Calculate inverse distance. Add a small epsilon to avoid division by zero.\n    # A diff of 0 means perfect fit, so priority should be very high.\n    epsilon = 1e-9\n    priorities_fit = 1.0 / (diffs + epsilon)\n    \n    # Initialize priorities array with zeros\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Assign calculated priorities to fitting bins\n    priorities[fit_mask] = priorities_fit\n    \n    return priorities",
    "response_id": 5,
    "obj": 4.048663741523748,
    "SLOC": 8.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response6.txt_stdout.txt",
    "code_path": "problem_iter1_code6.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score strategy.\n\n    The Sigmoid Fit Score prioritizes bins that leave a smaller remaining capacity\n    after the item is placed, aiming to fill bins more efficiently. It uses a\n    sigmoid function to map the \"tightness\" of the fit (inverse of remaining capacity)\n    to a priority score.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We can only place the item in bins that have enough capacity.\n    # For bins with insufficient capacity, the priority is 0.\n    valid_bins_mask = bins_remain_cap >= item\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Calculate the remaining capacity if the item were placed in each valid bin.\n    remaining_capacities = bins_remain_cap[valid_bins_mask] - item\n\n    # We want to prioritize bins where the remaining capacity is *small*.\n    # This means a higher priority for a tighter fit.\n    # A simple way to achieve this is to use the inverse of the remaining capacity.\n    # To avoid division by zero or very large numbers for near-zero remaining capacities,\n    # we can add a small epsilon.\n    epsilon = 1e-6\n    # The \"fit score\" is higher for smaller remaining capacities.\n    # Let's use the inverse of remaining capacity as a proxy for tightness.\n    # Higher inverse means tighter fit.\n    fit_scores = 1.0 / (remaining_capacities + epsilon)\n\n    # Now, we apply a sigmoid function. The sigmoid function squashes values\n    # between 0 and 1. This can help normalize the scores and make them\n    # less sensitive to extreme values.\n    # A common sigmoid is 1 / (1 + exp(-x)).\n    # If fit_score is large (tight fit), exp(-fit_score) will be close to 0, so sigmoid ~ 1.\n    # If fit_score is small (loose fit), exp(-fit_score) will be large, so sigmoid ~ 0.\n    # This maps \"tightness\" (high fit_score) to high priority.\n    # We can also scale and shift the fit_score before applying sigmoid to\n    # control the steepness of the priority change. For simplicity, we'll\n    # directly use the fit_score as input to the sigmoid.\n\n    # Using np.clip to ensure we don't get NaNs or infinities from exp if\n    # fit_scores become extremely large or small (though less likely here).\n    # For demonstration, a simple sigmoid `1 / (1 + exp(-x))` is applied.\n    # To make it sensitive to *small* remaining capacity, we want a high score\n    # for small remaining capacity. So, `fit_scores` directly represent this.\n    # Applying sigmoid directly to `fit_scores`:\n    # A higher `fit_score` (meaning smaller `remaining_capacities`) should yield a higher priority.\n    # `sigmoid(x) = 1 / (1 + exp(-x))` where higher x -> higher sigmoid output.\n    # We can scale `fit_scores` to influence sensitivity. Let's scale by a factor `k`.\n    # `k * fit_scores`. A larger `k` means higher priority for tighter fits.\n    k = 1.0 # Scaling factor - adjust this for desired sensitivity\n    scaled_fit_scores = k * fit_scores\n    sigmoid_priorities = 1 / (1 + np.exp(-scaled_fit_scores))\n\n    # Assign these calculated priorities back to the valid bins\n    priorities[valid_bins_mask] = sigmoid_priorities\n\n    return priorities",
    "response_id": 6,
    "obj": 4.048663741523748,
    "SLOC": 11.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response7.txt_stdout.txt",
    "code_path": "problem_iter1_code7.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we randomly select a bin that can accommodate the item.\n    The priority function here aims to simulate this by assigning higher priority\n    to bins that can fit the item, and then introducing a random element to break ties\n    and achieve the \"random\" aspect.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can accommodate the item\n    fit_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(fit_indices) == 0:\n        # If no bin can fit the item, return all zeros (or signal failure)\n        # For this heuristic, we'll return zeros, implying no good fit.\n        return priorities\n\n    # Assign a base priority (e.g., 1) to bins that can fit the item.\n    priorities[fit_indices] = 1.0\n\n    # Introduce randomness to the selection among fitting bins.\n    # We can add a small random value to the priorities of fitting bins.\n    # This ensures that when multiple bins have the same base priority (i.e., they all fit),\n    # the selection becomes random.\n    random_component = np.random.rand(len(bins_remain_cap)) * 0.1  # Small random values between 0 and 0.1\n    priorities += random_component\n\n    return priorities",
    "response_id": 7,
    "obj": 73.15516553649782,
    "SLOC": 9.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response8.txt_stdout.txt",
    "code_path": "problem_iter1_code8.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The strategy is to greedily choose the bin with the most remaining capacity that can fit the item,\n    but with a probability of epsilon, choose a random bin that can fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Exploration rate\n\n    # Identify bins that can fit the item\n    valid_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(valid_bins_indices) == 0:\n        # No bin can fit the item, return all zeros (or handle as an error/new bin creation)\n        return np.zeros_like(bins_remain_cap)\n\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Epsilon-Greedy logic\n    if np.random.rand() < epsilon:\n        # Explore: choose a random valid bin\n        chosen_bin_index = np.random.choice(valid_bins_indices)\n        priorities[chosen_bin_index] = 1.0  # High priority for the chosen random bin\n    else:\n        # Exploit: choose the best valid bin (most remaining capacity)\n        best_bin_index = valid_bins_indices[np.argmax(bins_remain_cap[valid_bins_indices])]\n        priorities[best_bin_index] = 1.0  # High priority for the best bin\n\n    return priorities",
    "response_id": 8,
    "obj": 137.96370163542085,
    "SLOC": 13.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response9.txt_stdout.txt",
    "code_path": "problem_iter1_code9.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    This heuristic prioritizes bins that can fit the item, with a preference for\n    bins that leave less remaining capacity (tighter fit) after placing the item.\n    The Softmax function is used to convert these fit scores into probabilities,\n    where a better fit (smaller remaining capacity) gets a higher probability.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Calculate how much space would be left if the item is placed in each bin\n    # Only consider bins that can actually fit the item\n    remaining_after_fit = bins_remain_cap - item\n    \n    # Create an initial score array, initialized to negative infinity for bins\n    # that cannot fit the item. This ensures they will have zero probability after softmax.\n    scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # For bins that can fit the item, calculate a \"fitness score\".\n    # A smaller remaining capacity (tighter fit) should have a higher score.\n    # We can use the negative of the remaining capacity, or a transformation\n    # that favors smaller positive values. A common approach is to use\n    # 1 / (remaining_capacity + epsilon) to avoid division by zero and\n    # emphasize smaller remaining capacities.\n    # Alternatively, and perhaps simpler for softmax, is to use the negative\n    # remaining capacity directly. The softmax will then make bins with\n    # less remaining capacity have higher probabilities.\n    \n    # Let's use the negative remaining capacity as the \"fit score\".\n    # Bins that can fit have remaining_after_fit >= 0\n    can_fit_mask = remaining_after_fit >= 0\n    \n    # For bins that can fit, assign a score that represents how \"good\" the fit is.\n    # A good fit is a small positive remaining capacity.\n    # To make softmax work well, we want larger positive values for better fits.\n    # So, we can use -remaining_after_fit. A remaining_after_fit of 0 (perfect fit)\n    # gives a score of 0. A remaining_after_fit of 1 gives -1, etc.\n    # So a larger score means a better fit (smaller remaining_after_fit).\n    \n    # However, we need to be careful if remaining_after_fit can be zero.\n    # If remaining_after_fit is 0, -remaining_after_fit is 0.\n    # If remaining_after_fit is > 0, -remaining_after_fit is < 0.\n    # This aligns with the intuition that a perfect fit is the best.\n    \n    # A common strategy in these types of heuristics is to consider the \"waste\"\n    # created. We want to minimize waste for a \"tight fit\".\n    # Let's define a value that represents desirability of a bin:\n    # Higher value means more desirable.\n    # Desirability could be inversely proportional to the remaining capacity *after* fitting.\n    # We want to favor bins with small positive remaining capacity after fitting.\n    \n    # A simple mapping that works well with softmax:\n    # -1 / (remaining_capacity_after_fit + 1)\n    # For remaining_after_fit = 0, score = -1/1 = -1\n    # For remaining_after_fit = 1, score = -1/2 = -0.5\n    # For remaining_after_fit = 2, score = -1/3 = -0.33\n    # This assigns higher scores to smaller remaining capacities (more desirable bins).\n    # If a bin cannot fit, it gets -inf.\n    \n    # Let's refine this: we want to maximize the desirability of fitting.\n    # If we fit, the remaining capacity is bins_remain_cap - item.\n    # We want this value to be small.\n    # Let's transform this remaining capacity into a score where smaller positive\n    # values are better.\n    # A common strategy: exp( - k * remaining_capacity )\n    # or simply, use the negative of remaining capacity, and softmax handles it.\n    \n    # Let's use a score where a perfect fit (remaining = 0) is best,\n    # and progressively worse.\n    # If a bin can fit, calculate its \"attractiveness\".\n    # A bin that leaves less space is more attractive.\n    # So, let `attractiveness` = -(bins_remain_cap - item)\n    # Or, if the remaining capacity is `r`, we want to maximize something like `exp(-r)`.\n    \n    # Let's consider a simplified approach for clarity that works with softmax:\n    # Calculate `fitness_value` for bins that can fit the item.\n    # We want bins where `bins_remain_cap - item` is small.\n    # Let `fitness_value` = 1.0 / (bins_remain_cap - item + epsilon)\n    # where epsilon is a small constant to avoid division by zero.\n    # Then apply softmax. Higher fitness_value leads to higher probability.\n    \n    # Alternative interpretation of \"Softmax-Based Fit\":\n    # The priority is derived from how well the item fits.\n    # Let's define a score `s_i` for bin `i`.\n    # If item `j` fits in bin `i` (capacity `c_i`):\n    #   `s_i = exp(-alpha * (c_i - item_j))` where `alpha > 0`.\n    #   This makes bins with smaller remaining capacity (`c_i - item_j`) have higher scores.\n    # If item `j` does not fit in bin `i`:\n    #   `s_i = 0` (or a very small number like -inf for softmax).\n    \n    # Let's implement this with alpha=1 for simplicity, and handle the \"cannot fit\" case.\n    \n    alpha = 1.0 # Tuning parameter for sensitivity to remaining capacity\n    \n    # Calculate potential remaining capacity for all bins\n    potential_remaining = bins_remain_cap - item\n    \n    # Initialize scores for all bins to a very low value (effectively zero probability after softmax)\n    # We use a large negative number instead of -np.inf to avoid issues with softmax if all are -inf.\n    # However, for typical softmax implementations, -np.inf is appropriate.\n    # Let's stick with a very low number or use conditional logic.\n    \n    # Let's build the scores array:\n    # For bins that can fit (potential_remaining >= 0), assign score exp(-alpha * potential_remaining).\n    # For bins that cannot fit, assign 0 (or a value that results in 0 probability).\n    \n    # Compute the exponent term for bins that can fit.\n    # A perfect fit (remaining = 0) gives exp(0) = 1.\n    # A fit leaving 1 unit of space gives exp(-alpha).\n    # A fit leaving more space gives smaller values.\n    \n    # We can directly compute exp(-alpha * potential_remaining) and then mask out invalid ones.\n    \n    # Calculate the exponent terms.\n    exponent_terms = -alpha * potential_remaining\n    \n    # Apply exponential to get scores.\n    scores_exp = np.exp(exponent_terms)\n    \n    # Where the item does not fit, the score should be effectively zero.\n    # Set scores to 0 for bins where potential_remaining < 0.\n    scores = np.where(potential_remaining >= 0, scores_exp, 0)\n    \n    # Note: If `bins_remain_cap` is empty or `item` is invalid, this might fail.\n    # Assuming valid inputs as per problem description.\n\n    # If all scores are 0 (e.g., item too large for all bins),\n    # softmax might behave unexpectedly or return NaNs if the sum is zero.\n    # In a real scenario, you might want to add a fallback to create a new bin.\n    # For this function, we assume at least one bin *could* potentially fit or the output\n    # will simply be all zeros if no bins can.\n\n    # Apply softmax to get probabilities (which are our priorities)\n    # Softmax function: exp(z_i) / sum(exp(z_j))\n    # Here, `scores` are already `exp(z_i)` terms.\n    \n    sum_of_scores = np.sum(scores)\n    \n    if sum_of_scores == 0:\n        # This happens if the item cannot fit into any bin.\n        # In this scenario, all bins have a priority of 0.\n        # Or, we could return an array of 1/N to indicate no preference,\n        # but given the context of \"priority of adding to each bin\",\n        # 0 priority makes sense if it cannot be added.\n        # However, usually the problem implies the item WILL be placed.\n        # If it cannot fit, it might imply a new bin creation strategy which this\n        # function doesn't handle. For THIS function, we return 0 for all bins.\n        return np.zeros_like(bins_remain_cap)\n    else:\n        priorities = scores / sum_of_scores\n        return priorities",
    "response_id": 9,
    "obj": 4.048663741523748,
    "SLOC": 15.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response10.txt_stdout.txt",
    "code_path": "problem_iter1_code10.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin.\n\n    This heuristic implements a modified First Fit strategy.\n    It prioritizes bins that can fit the item. Among those that can fit,\n    it gives a higher priority to bins that have just enough remaining capacity\n    to fit the item (minimizing wasted space in that specific bin).\n    If no bin has *exactly* enough space, it falls back to the bin that\n    leaves the least amount of remaining space after packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item is placed in a bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # Prioritize bins that have exactly enough space for the item\n    # Assign a high priority (e.g., 2) to these bins.\n    exact_fit_mask = (bins_remain_cap == item) & can_fit_mask\n    priorities[exact_fit_mask] = 2.0\n\n    # For bins that can fit but not perfectly, assign a priority based on\n    # minimizing remaining space.\n    # Assign a priority of 1 to these bins.\n    # The actual sorting will ensure that bins with less remaining space get picked first.\n    can_fit_but_not_exact_mask = can_fit_mask & ~exact_fit_mask\n    priorities[can_fit_but_not_exact_mask] = 1.0\n\n    # For bins that can fit, the secondary sorting criteria (after priority score)\n    # will be the potential remaining capacity. Lower remaining capacity is better.\n    # We can achieve this by making the priority score inversely related to the\n    # remaining capacity for bins that are not an exact fit.\n    # Since we want *least* remaining capacity to be best, and numpy sorts in\n    # ascending order for positive values, we can multiply the remaining capacity\n    # by -1 to give a higher (less negative) score to bins with less space.\n    # However, our current priority scheme already assigns 2 for exact fits and 1 for others.\n    # The subsequent selection logic typically picks the highest priority, and\n    # if priorities are tied, it looks at other criteria.\n    # For simplicity and to align with \"highest priority score means best\",\n    # let's make the priority proportional to how *tightly* it fits.\n    # Higher priority for less remaining space.\n\n    # For bins that can fit, let's assign a priority that favors less remaining space.\n    # A higher priority means we want to pick it.\n    # The value `bins_remain_cap - item` represents the *wasted* space in that bin.\n    # We want to minimize wasted space. So, we want smaller positive values of `bins_remain_cap - item`.\n    # If we want higher priority for smaller wasted space, we can use `1 / (wasted_space + epsilon)`\n    # or subtract the wasted space from a large constant.\n    # Let's try a simpler approach: give a priority that's higher for tighter fits.\n    # Bins that can fit but aren't exact fits:\n    # Priority = 1 + (maximum possible remaining capacity - actual remaining capacity) / maximum possible remaining capacity\n    # This is equivalent to: priority = 1 + (bin_cap - item) / bin_cap IF bin_cap is the same.\n    # With variable bin capacities, we can assign priority based on the remaining space *after* placing the item.\n    # Lower `bins_remain_cap - item` is better.\n    # To make higher priority better, we can use something like:\n    # `max_possible_waste - (bins_remain_cap[i] - item)` if the item fits.\n    # A more direct approach for the 'least remaining space' among fitting bins:\n    # Assign a priority that is inversely related to the remaining space.\n    # For example, `1 / (bins_remain_cap[i] - item + 1e-9)` if it fits.\n    # This makes bins with very little remaining space have very high priorities.\n\n    # Let's refine:\n    # Priority = 0 if cannot fit.\n    # Priority = 1 if fits but not exactly.\n    # Priority = 2 if fits exactly.\n    # For bins with priority 1, we need a tie-breaker: prefer bins with less remaining capacity.\n    # To incorporate this directly into the priority score for easier sorting:\n    # For bins where `can_fit_but_not_exact_mask` is true:\n    # Let's assign a priority that reflects the \"tightness\" of the fit.\n    # A higher score means a tighter fit.\n    # The potential remaining capacity after packing is `bins_remain_cap[i] - item`.\n    # We want to maximize `-(bins_remain_cap[i] - item)` (i.e., minimize `bins_remain_cap[i] - item`).\n    # So, a score like `-(bins_remain_cap[i] - item)` can work for tie-breaking.\n    # Combined priority: 2 for exact, 1 - epsilon * (remaining_after_fit) for others.\n\n    # Let's try a simpler structure for the priority score itself to directly favor less remaining space.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # For bins that can fit the item\n    fitting_bins_indices = np.where(bins_remain_cap >= item)[0]\n\n    if len(fitting_bins_indices) > 0:\n        # Calculate remaining capacity for fitting bins\n        remaining_after_fit = bins_remain_cap[fitting_bins_indices] - item\n\n        # Assign higher priority to bins with less remaining space.\n        # A large negative number for bins with lots of remaining space,\n        # and values closer to zero for bins with very little remaining space.\n        # This way, the maximum priority will come from bins with minimal remaining space.\n        # We can use `np.max(remaining_after_fit) - remaining_after_fit`\n        # to get values where smaller remaining_after_fit results in larger priorities.\n        priorities[fitting_bins_indices] = np.max(remaining_after_fit) - remaining_after_fit\n\n        # Add a bonus for exact fits to ensure they are prioritized even if other bins have very little remaining space.\n        # For example, an exact fit (remaining_after_fit = 0) should have a higher priority than any\n        # non-exact fit where remaining_after_fit > 0.\n        # We can add a large constant to exact fits.\n        exact_fit_indices = fitting_bins_indices[remaining_after_fit == 0]\n        if len(exact_fit_indices) > 0:\n            # Ensure exact fits get a priority higher than any possible non-exact fit score.\n            # The current `remaining_after_fit` values are non-negative.\n            # The priorities we assigned are `max_waste - waste`. Max possible priority is `max_waste`.\n            # So, `max_waste + 1` for exact fits ensures they are always picked over non-exact fits if using argmax.\n            priorities[exact_fit_indices] = np.max(remaining_after_fit) + 1\n\n    return priorities",
    "response_id": 10,
    "obj": 4.048663741523748,
    "SLOC": 17.0,
    "cyclomatic_complexity": 3.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response11.txt_stdout.txt",
    "code_path": "problem_iter1_code11.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Best Fit strategy: prioritize bins that have just enough space for the item.\n    # A higher priority score means the bin is a better fit.\n\n    # Initialize priorities to a low value (e.g., negative infinity) to indicate\n    # that bins that cannot fit the item have no priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate the \"remaining space after packing\"\n    # This is the key to Best Fit: we want the smallest remaining space.\n    # We'll use this as a primary factor for priority.\n    remaining_space = bins_remain_cap[can_fit_mask] - item\n\n    # Assign priorities:\n    # A higher priority means a better fit.\n    # We want to minimize the remaining space, so we can invert it.\n    # A common way to do this is to use the negative of the remaining space,\n    # or 1 / (remaining_space + epsilon) to avoid division by zero if remaining_space is 0.\n    # Let's use the negative of remaining_space. The smallest (most negative)\n    # remaining space will result in the highest (least negative) priority score.\n    # For instance, if remaining spaces are [0.1, 0.05, 0.2], the priorities would be [-0.1, -0.05, -0.2].\n    # -0.05 is the highest priority.\n\n    # To make it more intuitive that higher is better, we can transform\n    # the priority so that the \"best fit\" (smallest remaining space) yields\n    # the highest score. We can achieve this by subtracting the remaining space\n    # from a large number (e.g., the maximum possible remaining space, which is the bin capacity itself,\n    # or a large constant, or just its inverse if we handle zero remaining space carefully).\n\n    # A simple way to prioritize smallest remaining space is to assign a score\n    # inversely proportional to the remaining space. However, directly using\n    # 1 / remaining_space can cause issues if remaining_space is 0.\n    # A more robust approach for Best Fit is to assign a penalty based on how much\n    # \"worse\" the fit is. The best fit has zero penalty.\n\n    # Let's assign priority based on the remaining capacity after placing the item.\n    # The best fit is the bin with the minimum remaining capacity after placing the item.\n    # We can score this by taking the negative of the remaining space.\n    # So, remaining_space = bins_remain_cap[can_fit_mask] - item\n    # priority = -remaining_space\n    # The highest priority will correspond to the smallest (least negative) value.\n\n    # Example: item = 0.4, bins_remain_cap = [0.6, 0.5, 1.0, 0.3]\n    # can_fit_mask = [True, True, True, False]\n    # bins_remain_cap[can_fit_mask] = [0.6, 0.5, 1.0]\n    # remaining_space = [0.2, 0.1, 0.6]\n    # For Best Fit, we want the bin with remaining space 0.1 (the second bin).\n\n    # To make the highest priority score represent the best fit:\n    # We want the smallest `remaining_space`. So, `1 / (remaining_space + epsilon)` works.\n    # Or, a simpler and more stable approach: Assign priority `bin_cap - item` if it fits.\n    # The higher this value, the less \"tight\" the fit.\n    # For Best Fit, we want the *smallest* `bin_cap - item`.\n    # So, we can assign priority as `- (bin_cap - item)`. The largest negative value is best.\n    # Or, a common heuristic is to assign priority based on the negative of the difference,\n    # or `1 / (difference + small_epsilon)`.\n    # Let's use the negative of the remaining space directly. Higher (less negative) is better.\n\n    # Let's refine: Higher priority means better fit.\n    # Best Fit aims to minimize `bins_remain_cap - item`.\n    # So, bins with smaller `bins_remain_cap - item` should get higher priority.\n    # We can map the difference `d = bins_remain_cap[can_fit_mask] - item` to a priority.\n    # A simple mapping: `priority = -d`. This means smallest `d` gives least negative priority.\n    # To ensure the highest priority is clearly the \"best\" fit, we can transform this.\n    # For example, `priority = 1.0 / (d + epsilon)` where epsilon is a small positive number.\n    # This makes `d=0` give the highest priority.\n\n    # A common and effective way for Best Fit is to use the reciprocal of the\n    # remaining space *after* fitting the item. This rewards bins that are almost full.\n    # We need to avoid division by zero.\n    epsilon = 1e-9\n    priorities[can_fit_mask] = 1.0 / (remaining_space + epsilon)\n\n    # Another common strategy for Best Fit: Prioritize bins that leave the *least* space.\n    # This means we want to minimize `bins_remain_cap - item`.\n    # So, we can give a priority that is inversely proportional to `bins_remain_cap - item`.\n    # Let's try assigning a high score to bins that have the smallest remaining capacity after fitting.\n    #\n    # We want to maximize `priority`.\n    # Best Fit wants to minimize `bins_remain_cap - item`.\n    #\n    # So, a priority function `P(bin) = 1 / (bins_remain_cap[bin] - item)` would work if `bins_remain_cap[bin] - item > 0`.\n    # If `bins_remain_cap[bin] - item == 0`, this is the perfect fit.\n    #\n    # Let's consider the differences:\n    # differences = bins_remain_cap[can_fit_mask] - item\n    # The bin with the minimum difference is the best fit.\n    # To make the smallest difference result in the highest priority:\n    # `priorities[can_fit_mask] = -differences`  (Smallest diff becomes largest negative, which is still not intuitive for \"highest priority\")\n    # `priorities[can_fit_mask] = 1.0 / (differences + epsilon)` (Smallest diff gives largest positive priority)\n    #\n    # Let's stick with the reciprocal of the remaining space after packing, as it's a strong indicator of \"best fit\".\n\n    return priorities",
    "response_id": 11,
    "obj": 4.198244914240141,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response12.txt_stdout.txt",
    "code_path": "problem_iter1_code12.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    In the Worst Fit strategy for the online Bin Packing Problem, we aim to place\n    the current item into the bin that has the largest remaining capacity. This\n    strategy is \"greedy\" in the sense that it tries to leave as much space as\n    possible in the bins, hoping to accommodate larger items later. For an online\n    problem, the priority is assigned based on how \"bad\" a fit would be in terms\n    of minimizing the remaining capacity after the item is placed. Thus, bins\n    with more remaining capacity are considered \"better\" candidates (higher priority)\n    for an item in a Worst Fit approach because they represent the worst possible\n    fit at that moment.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n        A higher score indicates a higher priority (i.e., a bin with more remaining\n        capacity).\n    \"\"\"\n    # Initialize priorities to a very small negative number to handle cases\n    # where an item cannot fit into any bin.\n    priorities = np.full_like(bins_remain_cap, -np.inf, dtype=float)\n\n    # Calculate the remaining capacity if the item were placed in each bin.\n    # This is only possible if the bin has enough capacity for the item.\n    possible_fits = bins_remain_cap >= item\n\n    # For bins where the item fits, the priority is simply the remaining capacity\n    # of that bin. This aligns with the Worst Fit strategy where we prefer bins\n    # with the most remaining space.\n    priorities[possible_fits] = bins_remain_cap[possible_fits]\n\n    return priorities",
    "response_id": 12,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response13.txt_stdout.txt",
    "code_path": "problem_iter1_code13.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit strategy.\n\n    The strategy prioritizes bins that are almost full but can still accommodate the item.\n    A higher priority is given to bins with a smaller remaining capacity that can fit the item.\n    If a bin cannot fit the item, its priority is 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate priority\n    # The priority is inversely proportional to the remaining capacity,\n    # effectively prioritizing bins that will be \"almost full\" after packing.\n    # We add a small epsilon to avoid division by zero if a bin has exactly 'item' remaining capacity.\n    fitting_bins_remain_cap = bins_remain_cap[can_fit_mask]\n    \n    if fitting_bins_remain_cap.size > 0:\n        # Calculate priorities for fitting bins.\n        # Higher priority for smaller remaining capacity (closer to being full).\n        # Using 1 / (remaining_capacity - item + epsilon) ensures smaller remaining_capacity gives higher priority.\n        # Adding a small value to denominator to avoid division by zero if remaining_capacity == item\n        epsilon = 1e-9\n        priorities[can_fit_mask] = 1 / (fitting_bins_remain_cap - item + epsilon)\n\n        # Normalize priorities to have a common scale, for example, a maximum of 1.\n        # This can help in scenarios where the absolute values of remaining capacities vary greatly.\n        max_priority = np.max(priorities[can_fit_mask])\n        if max_priority > 0:\n            priorities[can_fit_mask] /= max_priority\n\n    return priorities",
    "response_id": 13,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 40, in priority_v2\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n11\n3\n"
  },
  {
    "stdout_filepath": "problem_iter1_response14.txt_stdout.txt",
    "code_path": "problem_iter1_code14.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First strategy prioritizes bins where the item fits exactly, meaning\n    the remaining capacity of the bin is exactly equal to the item's size.\n    Among bins that offer an exact fit, we might further refine the priority.\n    In this implementation, we simply assign a higher priority to exact fits.\n    For bins that do not offer an exact fit, we assign a priority of 0.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Find bins where the item fits exactly\n    exact_fit_indices = np.where(bins_remain_cap == item)[0]\n\n    # Assign a high priority to bins that offer an exact fit.\n    # We can use a value greater than any possible difference or capacity.\n    # For simplicity, a large constant is used here.\n    high_priority_value = 1000.0\n    priorities[exact_fit_indices] = high_priority_value\n\n    # For all other bins (where item doesn't fit exactly), the priority remains 0.\n    # This strategy aims to use bins that are perfectly filled by the current item.\n    # If multiple bins offer an exact fit, any one of them can be chosen\n    # based on the inherent ordering of numpy arrays or could be further refined\n    # by a secondary criterion (e.g., lowest original bin index for stability).\n    # This implementation gives equal high priority to all exact fits.\n\n    return priorities",
    "response_id": 14,
    "obj": 4.198244914240141,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response15.txt_stdout.txt",
    "code_path": "problem_iter1_code15.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit).\n\n    The Inverse Distance strategy prioritizes bins that are a close fit for the item.\n    A smaller remaining capacity that is still sufficient for the item will receive\n    a higher priority. This aims to utilize bin space more efficiently.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Identify bins that can actually accommodate the item\n    suitable_bins_mask = bins_remain_cap >= item\n\n    # Calculate the \"gap\" or \"waste\" if the item is placed in a suitable bin\n    # A smaller gap means a better fit.\n    gaps = bins_remain_cap[suitable_bins_mask] - item\n\n    # To use inverse distance, we want a higher score for smaller gaps.\n    # If a gap is 0 (perfect fit), the inverse of gap would be infinity.\n    # To avoid division by zero and handle the perfect fit case, we can add a small epsilon\n    # or simply assign a very high score for a perfect fit.\n    # For this implementation, we'll use (1 / (gap + epsilon)) for small gaps\n    # and a large constant for perfect fits (gap == 0).\n    epsilon = 1e-9  # A small value to prevent division by zero\n\n    # Initialize priorities for suitable bins\n    priorities[suitable_bins_mask] = 1.0 / (gaps + epsilon)\n\n    # Optional: Boost the priority for perfect fits to ensure they are considered first.\n    # This is not strictly part of the inverse distance but can be a useful enhancement.\n    perfect_fit_mask = (gaps < epsilon) & suitable_bins_mask\n    priorities[perfect_fit_mask] = np.max(priorities) * 100  # Assign a very high score\n\n    # Ensure that bins that cannot fit the item have a priority of 0\n    priorities[~suitable_bins_mask] = 0\n\n    return priorities",
    "response_id": 15,
    "obj": 4.198244914240141,
    "SLOC": 10.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response16.txt_stdout.txt",
    "code_path": "problem_iter1_code16.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score strategy prioritizes bins that leave a remaining capacity\n    closest to half of the bin's capacity after packing the item. This aims to\n    balance the usage of bins and leave room for future items.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # A very small epsilon to avoid division by zero or issues with sigmoid at large values\n    epsilon = 1e-9\n\n    # Calculate the remaining capacity if the item is placed in each bin\n    potential_remaining_cap = bins_remain_cap - item\n\n    # We want to find bins where potential_remaining_cap is close to 0 (perfect fit)\n    # and also bins where potential_remaining_cap is significantly larger than the item\n    # but not too large that it wastes space.\n    # The sigmoid function can help us model this.\n    # Let's map the \"desirability\" to a range.\n    # A perfect fit (potential_remaining_cap = 0) should be highly desirable.\n    # A very small remaining capacity (potential_remaining_cap < 0) is not possible.\n    # A very large remaining capacity (potential_remaining_cap >> item) might be less desirable than a tight fit.\n\n    # We'll use a sigmoid function. The input to the sigmoid will be related to\n    # how \"close\" the remaining capacity is to some ideal value.\n    # Let's aim for the ideal remaining capacity to be 0 (perfect fit).\n    # The sigmoid function `1 / (1 + exp(-x))` has an output between 0 and 1.\n    # A larger input `x` results in a value closer to 1.\n    # We want a high score when potential_remaining_cap is small and positive.\n    # So, we can use `1 / (1 + exp(-k * (ideal - x)))` where `ideal=0`.\n    # This means `1 / (1 + exp(k * x))` which is `1 / (1 + exp(-k * (-x)))`.\n    # If `x` is the potential_remaining_cap, then `k*x` can be our sigmoid input.\n    # A positive `x` means there's still space. We want smaller `x` to be better.\n    # So, we want the sigmoid argument to be larger when `x` is small.\n    # This suggests using `exp(-k * x)`.\n\n    # Let's normalize the potential remaining capacity to be in a range where\n    # sigmoid can effectively distinguish between good and bad fits.\n    # The maximum possible remaining capacity is the original bin capacity (assuming\n    # bins_remain_cap reflects this when no item is present).\n    # However, we don't have the original bin capacity. We only have remaining.\n    # A simpler approach might be to consider the 'waste' or 'tightness'.\n\n    # Strategy: Use a sigmoid where the input is a scaled version of the\n    # potential remaining capacity. We want bins with smaller potential_remaining_cap\n    # to have higher scores (closer to 1), indicating a tighter fit.\n    # However, we also need to consider that the item must fit.\n    # Bins where potential_remaining_cap < 0 are invalid.\n\n    valid_bins_mask = potential_remaining_cap >= 0\n\n    # For valid bins, we want to prioritize those with smaller remaining capacity.\n    # A simple approach is to invert the remaining capacity and scale it.\n    # Or, use a sigmoid function that squashes larger positive values towards 0.\n    # Sigmoid: 1 / (1 + exp(-x))\n    # We want higher score for smaller positive remaining capacity.\n    # Let's transform potential_remaining_cap to something like `-k * potential_remaining_cap`.\n    # A higher score (closer to 1) means we want to pack into that bin.\n\n    # Let's try to map the remaining capacity `r` to a score `s`.\n    # We want:\n    # - `s` close to 1 for small positive `r` (tight fit)\n    # - `s` close to 0 for large positive `r` (wasteful fit)\n    # - `s` = 0 for `r` < 0 (cannot fit)\n\n    # The function `exp(-k*r)` would give higher values for smaller `r`.\n    # Let's use `sigmoid(k * (max_waste - r))`.\n    # Where `max_waste` is some target or a value that shifts the sigmoid.\n    # If we don't know the original bin size, we can't establish a clear \"waste\".\n\n    # Let's redefine the score: We want bins with remaining capacity `r` such that\n    # `item` fits and `r` is minimized. This is the \"Best Fit\" heuristic.\n    # The Sigmoid Fit Score could be an extension.\n\n    # Consider a sigmoid centered around a \"good\" remaining capacity, perhaps near 0.\n    # The sigmoid `1 / (1 + exp(k * (x - c)))` where `x` is potential_remaining_cap.\n    # If `c` is the ideal remaining capacity (e.g., 0), and `k` controls steepness.\n    # For `x < c`, the score is high. For `x > c`, the score is low.\n\n    # Let's use a sigmoid on the *negative* of the potential remaining capacity.\n    # `f(x) = 1 / (1 + exp(-x))`\n    # We want `f(some_func(potential_remaining_cap))`\n    # If `some_func(r) = -r`, then for small `r` (good), `-r` is large negative, sigmoid is near 0. This is wrong.\n    # If `some_func(r) = r`, then for small `r` (good), sigmoid is near 0.5 or less. This is wrong.\n\n    # Let's consider the complementary sigmoid: `1 - (1 / (1 + exp(-x))) = exp(-x) / (1 + exp(-x))` which is `1 / (1 + exp(x))`.\n    # This function gives values near 1 for large negative `x`, and near 0 for large positive `x`.\n    # If we use `1 / (1 + exp(k * potential_remaining_cap))`:\n    # - Small `potential_remaining_cap` (good) -> large negative `k * potential_remaining_cap` -> score near 1.\n    # - Large `potential_remaining_cap` (bad) -> large positive `k * potential_remaining_cap` -> score near 0.\n    # This seems like a good candidate for \"Best Fit\"-like behavior.\n\n    # We need to choose a scaling factor `k`. A larger `k` makes the transition steeper.\n    # A reasonable `k` could be related to the inverse of the typical item size or bin capacity,\n    # but without that information, we can pick a general value.\n    # Let's try k=1 for now.\n\n    # Handle cases where item cannot fit. For these, priority is 0.\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Calculate scores for bins where the item fits\n    # We want to prioritize bins with smaller remaining capacity *after* packing.\n    # Let's map the remaining capacity `r` to a priority score.\n    # We want higher priority for smaller `r` (where `r >= 0`).\n    # Consider the function `g(r) = 1 / (1 + exp(k * r))`\n    # For `r=0` (perfect fit), `g(0) = 1 / (1 + 1) = 0.5`.\n    # For `r > 0` and small, `g(r)` is slightly less than 0.5.\n    # For `r > 0` and large, `g(r)` approaches 0.\n\n    # This is good if we want to differentiate between tight fits and loose fits,\n    # but it doesn't give a strong preference for the tightest fit if there are multiple\n    # tight fits.\n\n    # Let's reconsider the \"ideal\" remaining capacity.\n    # The goal of bin packing is to minimize the number of bins.\n    # For an online algorithm, a good strategy is to try and make fits as tight as possible\n    # to leave larger spaces available for larger items.\n    # So, the \"best fit\" strategy minimizes `bins_remain_cap[i] - item`.\n\n    # How to use Sigmoid to implement Best Fit?\n    # Sigmoid is monotonic. We want a function that is decreasing with `bins_remain_cap[i] - item`.\n    # `f(x) = 1 / (1 + exp(k * x))` where `x` is `bins_remain_cap[i] - item`.\n    # `k` controls the steepness. A larger `k` means the preference for smaller `x` is stronger.\n\n    # We need to choose a suitable value for `k`.\n    # If the item size `item` is very small relative to `bins_remain_cap`, then `bins_remain_cap[i] - item`\n    # is close to `bins_remain_cap[i]`. If `k` is too large, the scores might saturate too quickly.\n    # If `k` is too small, the sigmoid is too flat, and it's almost a random choice.\n\n    # Let's consider the \"waste\" which is `bins_remain_cap[i] - item`.\n    # We want to minimize waste.\n    # We can normalize the waste relative to something. If we knew the original bin capacity `C`,\n    # we could normalize by `C`. Without `C`, we can normalize by the sum of remaining capacities\n    # or the maximum remaining capacity, but that might be unstable.\n\n    # A simple approach: Use sigmoid directly on the remaining capacity.\n    # Let `r = bins_remain_cap[i] - item`.\n    # If `r < 0`, it's invalid (priority 0).\n    # If `r >= 0`, we want higher scores for smaller `r`.\n    # Let's use `score = exp(-k * r)`. This gives higher scores for smaller `r`.\n    # However, this score can be arbitrarily large if `r` is very negative.\n    # We need to clip or use a bounded function like sigmoid.\n\n    # Let's use `score = 1 / (1 + exp(k * r))` where `r = bins_remain_cap[i] - item`.\n    # A higher `k` means a stronger preference for smaller `r`.\n    # A suitable `k` could be `1.0 / mean(bins_remain_cap)` or `1.0 / max(bins_remain_cap)` or similar,\n    # but this requires knowing these values or estimating them.\n    # For a general-purpose heuristic, a fixed moderate `k` is often used. Let's try `k=1`.\n    # We need to be careful about potential_remaining_cap being zero or very close to zero.\n    # If `potential_remaining_cap[i] = 0`, then `exp(k * 0) = 1`, score is 0.5. This is fine.\n\n    # Consider the inverse of remaining capacity as a score for positive values:\n    # For `r = bins_remain_cap[i] - item`:\n    # If `r < 0`, priority is 0.\n    # If `r == 0`, priority is high (perfect fit).\n    # If `r > 0` and small, priority is high.\n    # If `r > 0` and large, priority is low.\n\n    # Let's use the sigmoid function `f(x) = 1 / (1 + exp(-x))`\n    # We want to map `r` such that small `r` maps to large `x`.\n    # So, let `x = -k * r`.\n    # `score = 1 / (1 + exp(k * r))`.\n    # Where `r` is `potential_remaining_cap`.\n\n    # We need to apply this only to valid bins.\n    # Let's scale `k` perhaps by a small factor to avoid extreme values.\n    # A factor related to item size might be useful if item sizes vary greatly.\n    # E.g., `k = 1.0 / item` or `k = 1.0 / np.mean(bins_remain_cap)` etc.\n    # Without specific context on typical capacities or item sizes,\n    # let's use a tunable parameter `steepness`.\n    steepness = 5.0  # Adjust this to control how strongly small remaining capacity is favored\n\n    # For valid bins, calculate the sigmoid score.\n    # We want smaller `potential_remaining_cap` to result in higher scores.\n    # The function `1 / (1 + exp(steepness * r))` does this.\n    # Where `r = potential_remaining_cap`.\n\n    # To prevent potential numerical issues with large exponents,\n    # we can add a small epsilon if needed, or clip values.\n    # However, `np.exp` usually handles large inputs by returning inf,\n    # which leads to division by inf (0) or division of 1 by (1+inf) which is 0.\n    # And for very negative inputs, exp(very_negative) -> 0, so 1/(1+0) = 1.\n\n    # Let's ensure `potential_remaining_cap` doesn't cause overflow in `exp`.\n    # The exponent is `steepness * potential_remaining_cap`.\n    # If `potential_remaining_cap` is large and positive, exponent is large positive, exp is inf, score is 0.\n    # If `potential_remaining_cap` is large and negative, exponent is large negative, exp is 0, score is 1.\n    # But `potential_remaining_cap` cannot be arbitrarily negative, it's `bins_remain_cap - item`.\n    # The minimum value is when `bins_remain_cap` is small.\n\n    # Let's cap the `potential_remaining_cap` for the sigmoid calculation to avoid issues\n    # if `bins_remain_cap` values are extremely small or `item` is very large.\n    # A reasonable cap could be related to the item size itself. If remaining capacity\n    # is much larger than the item, it's likely a loose fit.\n    # Let's clip `potential_remaining_cap` to a reasonable range, e.g., [-item, max_bin_capacity].\n    # Without `max_bin_capacity`, let's use the maximum value of `bins_remain_cap` in the current set\n    # as a proxy for scale.\n\n    # A simpler approach: calculate directly for valid bins.\n    # For valid bins (potential_remaining_cap >= 0):\n    # Score = 1 / (1 + exp(steepness * potential_remaining_cap))\n    # This will give scores between ~0 (for large remaining) and 0.5 (for zero remaining).\n    # This is a bit counter-intuitive if we want \"high score = high priority\".\n\n    # Let's try a function that maps small positive `r` to high scores.\n    # `score = exp(-k * r)` could work if we bound the input or scale it.\n\n    # Alternative Sigmoid strategy:\n    # Prioritize bins where the remaining capacity is \"just enough\" or slightly more than the item.\n    # This is \"Best Fit\" heuristic.\n    # We want to maximize a function that is high when `bins_remain_cap[i] - item` is small and non-negative.\n    # The function `sigmoid(alpha - beta * (bins_remain_cap[i] - item))` can do this.\n    # If `bins_remain_cap[i] - item` is small, then `alpha - beta * small_positive` is large, sigmoid is near 1.\n    # If `bins_remain_cap[i] - item` is large, then `alpha - beta * large_positive` is small, sigmoid is near 0.\n    # `alpha` shifts the sigmoid center. `beta` controls steepness.\n\n    # Let's try to simplify this. We are essentially ranking based on `bins_remain_cap[i] - item`.\n    # The heuristic is \"prioritize bins with smallest non-negative remaining capacity\".\n\n    # A Sigmoid Fit Score can be formulated as prioritizing fits that leave a remainder close to 0.\n    # Let `r = bins_remain_cap[i] - item`.\n    # We want to map `r` to a priority `p` such that:\n    # p = 0 if r < 0\n    # p is high for r close to 0\n    # p is low for r large\n\n    # The sigmoid function `1 / (1 + exp(x))` is good for this if `x` is `k * r`.\n    # Higher `k` means stronger preference for smaller `r`.\n    # We need to ensure `x` doesn't cause overflow for `exp`.\n    # If `r` is large positive, `x` is large positive, `exp(x)` is inf, `1/(1+inf)` is 0.\n    # If `r` is zero, `x` is zero, `exp(x)` is 1, `1/(1+1)` is 0.5.\n    # If `r` is negative, `x` is negative, `exp(x)` is small, `1/(1+small)` is close to 1.\n\n    # This means `1 / (1 + exp(k * r))` gives higher priority to bins that cannot fit the item\n    # if we don't handle the `r < 0` case.\n\n    # Let's make the score represent how \"good\" the fit is, from 0 (worst) to 1 (best).\n    # A perfect fit (`r=0`) should have the highest score.\n    # A large `r` should have a low score.\n    # `r < 0` is invalid, score 0.\n\n    # Consider `score = exp(-k * r)`. This is maximized at `r=0`.\n    # But it's not bounded by 1 and can grow large.\n    # To bound it, we can use `tanh`. `tanh(x)` goes from -1 to 1.\n    # `tanh(k * r)`:\n    #   - large negative r -> -1\n    #   - r=0 -> 0\n    #   - large positive r -> 1\n    # We want high score for small positive `r`.\n\n    # Let's try mapping the difference `bins_remain_cap[i] - item` to a \"fit quality\".\n    # `fit_quality = - (bins_remain_cap[i] - item)`. Higher is better.\n    # Then use sigmoid on a scaled version.\n    # `score = 1 / (1 + exp(-k * fit_quality))`\n    # `score = 1 / (1 + exp(k * (bins_remain_cap[i] - item)))`\n    # This is the same as `1 / (1 + exp(k * r))`.\n\n    # The range of `bins_remain_cap` and `item` is important for choosing `k`.\n    # If the range of `bins_remain_cap[i] - item` is large, `k` might need to be smaller.\n    # If the range is small, `k` can be larger.\n\n    # Let's assume a typical scenario where item sizes and bin capacities are positive floats.\n    # We want to favor bins where `bins_remain_cap[i] - item` is small and non-negative.\n\n    # Using `np.clip` on the exponent argument to `np.exp` can prevent overflow.\n    # The exponent is `steepness * potential_remaining_cap`.\n    # Max positive value for `potential_remaining_cap` could be, say, `max(bins_remain_cap)`.\n    # Min value could be `-item`.\n    # Let's cap the exponent to a range like [-10, 10].\n\n    # Calculate the exponent values for valid bins\n    exponent_values = steepness * potential_remaining_cap[valid_bins_mask]\n\n    # Clip exponent values to avoid overflow/underflow issues in exp\n    # A range of [-20, 20] for the exponent is usually safe for standard float types.\n    # exp(20) is ~4.8e8, exp(-20) is ~2e-9.\n    clipped_exponent_values = np.clip(exponent_values, -20, 20)\n\n    # Calculate the sigmoid scores for valid bins using the clipped exponents\n    # The function `1 / (1 + exp(x))` gives higher values for smaller `x`.\n    # We want higher values for smaller `potential_remaining_cap`.\n    # So, we want `x` to be `-k * potential_remaining_cap`.\n    # Let's rewrite: `score = 1 / (1 + exp(steepness * potential_remaining_cap))`\n    # For small `potential_remaining_cap` (e.g., 0.1), `exp(steepness * 0.1)` is large, score is small. This is not what we want.\n\n    # We need a function that gives higher scores for smaller positive `r`.\n    # The function `1 / (1 + exp(-steepness * r))` increases as `r` decreases.\n    # If `r` is small positive, `-steepness * r` is small negative, exp is close to 0, score is ~1.\n    # If `r` is large positive, `-steepness * r` is large negative, exp is close to 0, score is ~1. This is not right.\n\n    # Let's go back to the `1 / (1 + exp(k * r))` formulation.\n    # This function `f(r) = 1 / (1 + exp(k * r))` is DECREASING in `r`.\n    # So, smaller `r` (good) leads to higher `f(r)`.\n    # If `r = 0` (perfect fit), `f(0) = 0.5`.\n    # If `r` is slightly positive, `f(r)` is slightly less than 0.5.\n    # If `r` is large positive, `f(r)` approaches 0.\n    # If `r` is slightly negative, `f(r)` is slightly more than 0.5.\n    # If `r` is very negative, `f(r)` approaches 1.\n\n    # This implies that bins that CANNOT fit the item (`r < 0`) would get the HIGHEST priority if we use this directly.\n    # This is exactly the opposite of what we want.\n\n    # How to achieve:\n    # Priority = 0 for r < 0\n    # Priority = high for r = 0\n    # Priority = medium for small positive r\n    # Priority = low for large positive r\n\n    # Consider the \"gap\" heuristic: Prioritize bins with `bins_remain_cap[i] >= item`.\n    # Among these, select the one with minimum `bins_remain_cap[i] - item`.\n    # This is \"Best Fit\".\n\n    # Let's try a sigmoid centered at 0 with a negative slope.\n    # `sigmoid(center - steepness * value)`\n    # We want value = `bins_remain_cap[i] - item`.\n    # We want high scores when `value` is small and positive.\n    # Let `value = bins_remain_cap[i] - item`.\n    # Let `score = sigmoid(C - k * value)` where `k > 0`.\n    # Example: `sigmoid(x) = 1 / (1 + exp(-x))`\n    # `score = 1 / (1 + exp(-(C - k * value))) = 1 / (1 + exp(-C + k * value))`\n    # If `value` is small positive, `k*value` is small positive. `-C + k*value` is large negative (if C is large). exp->0, score->1.\n    # If `value` is large positive, `k*value` is large positive. `-C + k*value` is large positive. exp->inf, score->0.\n\n    # This seems correct. We need to choose `C` and `k`.\n    # `C` can be thought of as a threshold for how \"loose\" a fit is acceptable.\n    # If we want to prioritize fits that are \"just enough\", `C` could be related to 0.\n    # `k` determines the sensitivity to the difference.\n\n    # Let's set `C = 0` and `k = steepness`.\n    # `score = 1 / (1 + exp(steepness * (bins_remain_cap[i] - item)))`\n    # This is the function we analyzed that was decreasing.\n    # It gives:\n    #   - r < 0 => score > 0.5 (approaching 1)\n    #   - r = 0 => score = 0.5\n    #   - r > 0 => score < 0.5 (approaching 0)\n\n    # This means it strongly prefers bins that CANNOT fit the item, which is WRONG.\n\n    # The problem is applying sigmoid directly to remaining capacity.\n    # A true Sigmoid Fit Score for Best Fit might involve fitting a sigmoid to\n    # the points `(r, priority_for_r)`.\n\n    # Let's simplify the objective: Among valid bins, we want the one with minimal `bins_remain_cap[i] - item`.\n    # This is like finding the minimum.\n    # We can use sigmoid to create a \"soft\" minimum.\n\n    # A different approach: The sigmoid could represent the probability of selecting a bin.\n    # We want higher probability for bins that are \"tight\".\n    # Let's use `1 / (1 + exp(-k * (target_rem_cap - actual_rem_cap)))`\n    # Where `target_rem_cap` is the ideal remaining capacity. Ideally 0.\n    # `actual_rem_cap` is `bins_remain_cap[i] - item`.\n\n    # Score = `1 / (1 + exp(-k * (0 - (bins_remain_cap[i] - item))))`\n    # Score = `1 / (1 + exp(k * (bins_remain_cap[i] - item)))`\n\n    # This is the same decreasing function. The core issue is that negative remainders\n    # are treated as \"very good fits\" by this function's structure.\n\n    # How about creating a piecewise score?\n    # For `r < 0`: score = 0\n    # For `r >= 0`: score = `sigmoid(k * (MaxPossibleDiff - r))` where MaxPossibleDiff is some large value.\n    # Or `sigmoid(k * (-r))`?\n    # `1 / (1 + exp(-k * (-r))) = 1 / (1 + exp(k * r))`. Still the decreasing function.\n\n    # Let's use a sigmoid to rank the \"goodness\" of the remaining space.\n    # The more remaining space `r`, the less desirable the bin might be for a tight packing strategy.\n    # Consider mapping `r` to `score = sigmoid(gain - steepness * r)`\n    # Let `gain` be some offset.\n    # If we set `gain = 0`, we have `sigmoid(-steepness * r) = 1 / (1 + exp(steepness * r))`.\n\n    # What if the sigmoid is applied to the inverse of the remaining capacity?\n    # `score = 1 / (1 + exp(-k * (1.0 / (potential_remaining_cap + epsilon))))`\n    # For `r` near 0, `1/r` is large positive. `score` approaches 1.\n    # For `r` large, `1/r` is near 0. `score` approaches 0.5.\n    # This looks promising!\n\n    # Let's define the priority:\n    # For bins where `potential_remaining_cap >= 0`:\n    # Let `value = potential_remaining_cap`.\n    # We want high score for small `value`.\n    # Consider `score = 1 / (1 + exp(-k * (C - value)))` where `k > 0`.\n    # To make it prefer small `value`, we need `k` and `C` to be tuned.\n    # If `C = 0`, score is `1 / (1 + exp(k * value))`. This is decreasing.\n\n    # Let's use a simpler sigmoid transformation for \"best fit\" behavior:\n    # For bins where `potential_remaining_cap >= 0`, we want to score them based on how small `potential_remaining_cap` is.\n    # This is equivalent to ranking them from smallest `potential_remaining_cap` to largest.\n\n    # The Sigmoid Fit Score strategy aims to find a sweet spot in remaining capacity,\n    # or to strongly favor tight fits.\n\n    # Let's try prioritizing based on `1 - sigmoid(k * remaining_capacity)`.\n    # If `remaining_capacity` is small positive, `k*r` is small positive, sigmoid is >0.5. `1-sigmoid` is <0.5. Bad.\n    # If `remaining_capacity` is large positive, `k*r` is large positive, sigmoid is near 1. `1-sigmoid` is near 0. Good. This is favoring loose fits.\n\n    # It seems the interpretation of \"Sigmoid Fit Score\" needs to be clear.\n    # If it's about \"how close to a perfect fit (remainder=0)\", then `1/(1+exp(k*(r-0)))` is decreasing, so smallest `r` gives highest score.\n\n    # Let's define the score more concretely:\n    # The priority score for a bin `i` should be high if `bins_remain_cap[i] >= item` AND `bins_remain_cap[i] - item` is minimized.\n    # Let `r = bins_remain_cap[i] - item`.\n    # We want a function `f(r)` such that:\n    # f(r) = 0 for r < 0\n    # f(r) is decreasing for r >= 0.\n\n    # Let's try a sigmoid on `k * (large_value - r)`\n    # Consider the score to be a transformed version of `potential_remaining_cap`.\n    # We want small non-negative `potential_remaining_cap` to get high scores.\n    # Let `score_raw = -potential_remaining_cap`. Higher `score_raw` is better.\n    # Then apply sigmoid: `score = 1 / (1 + exp(-k * score_raw))`\n    # `score = 1 / (1 + exp(k * potential_remaining_cap))`\n    # Again, this favors negative `potential_remaining_cap` which means item doesn't fit.\n\n    # Let's adjust the target. The goal is to have a small, but potentially non-zero, remaining capacity.\n    # This can leave room for items of slightly different sizes, or perhaps encourage future fits.\n    # However, for \"Best Fit\", we want remainder exactly 0.\n\n    # Consider the objective: maximize `1 / (1 + exp(k * (r - C)))` for `r >= 0`.\n    # This function is maximized when `r - C` is minimized (most negative).\n    # If `C = 0`, then maximized when `r` is most negative.\n\n    # Let's try mapping `potential_remaining_cap` to a priority.\n    # The closer `potential_remaining_cap` is to `item`'s original size, the better. No, this is First Fit Decreasing.\n    # The closer `potential_remaining_cap` is to 0, the better.\n\n    # A reasonable \"Sigmoid Fit Score\" could be based on how well the item fills the bin relative to its remaining capacity.\n    # For valid bins: `bins_remain_cap[i] >= item`\n    # Score based on `bins_remain_cap[i] - item`.\n    # We want to give high scores to bins with `bins_remain_cap[i] - item` close to 0.\n\n    # Let's use `sigmoid(gain - k * difference)` where `difference = bins_remain_cap[i] - item`.\n    # `sigmoid(x) = 1 / (1 + exp(-x))`\n    # `score = 1 / (1 + exp(-(gain - k * difference)))`\n    # `score = 1 / (1 + exp(k * difference - gain))`\n\n    # Let `k = steepness`.\n    # If we want a \"perfect fit\" to be ideal, we want `difference = 0` to have the highest score.\n    # `score(0) = 1 / (1 + exp(-gain))`\n    # If `difference > 0`, `k*difference` is larger positive. `score` will be less than `score(0)`.\n    # If `difference < 0` (item does not fit), we want score = 0.\n    # The function `1 / (1 + exp(k * difference - gain))` needs to be 0 for `difference < 0`.\n    # This means `k * difference - gain` should be very large positive for `difference < 0`.\n    # Let's set `gain = k * target_difference`. If target is 0, `gain = 0`.\n    # `score = 1 / (1 + exp(k * difference))`\n    # This function is decreasing, but `difference < 0` gives higher scores than `difference = 0`.\n\n    # The core issue is that sigmoid is monotonic. We need to apply it to a quantity that is ranked correctly.\n    # The ranking we want is: (small `r` >= 0) > (large `r` >= 0) > ( `r` < 0)\n\n    # Let's modify the sigmoid's input:\n    # For valid bins, calculate `priorities[i] = sigmoid(-k * potential_remaining_cap[i])`\n    # `sigmoid(-x) = 1 - sigmoid(x)`.\n    # So `priorities[i] = 1 - sigmoid(k * potential_remaining_cap[i])`\n    # `priorities[i] = 1 - 1 / (1 + exp(k * potential_remaining_cap[i]))`\n    # `priorities[i] = exp(k * potential_remaining_cap[i]) / (1 + exp(k * potential_remaining_cap[i]))`\n    # This is also `sigmoid(-k * potential_remaining_cap[i])`.\n\n    # Let `r = potential_remaining_cap`.\n    # `sigmoid(-k * r)`:\n    #   - If `r` is small positive (good fit), `-k*r` is small negative. sigmoid is < 0.5.\n    #   - If `r` is large positive (bad fit), `-k*r` is large negative. sigmoid is close to 0.\n    #   - If `r` is zero, `-k*r` is zero. sigmoid is 0.5.\n    #   - If `r` is negative (cannot fit), `-k*r` is positive. sigmoid is > 0.5.\n\n    # This still ranks negative remaining capacities as \"better\" than 0 remaining capacity.\n\n    # The \"Sigmoid Fit Score\" could be a way to smooth the ranking.\n    # Let's use a function that prioritizes minimum `potential_remaining_cap` among valid bins.\n    # The score for a bin should be `f(potential_remaining_cap)`.\n    # `f` should be decreasing for `potential_remaining_cap >= 0`.\n\n    # A simple approach is to use a large sigmoid value for small `r`.\n    # What if we map `potential_remaining_cap` to `r_mapped = C - k * potential_remaining_cap`?\n    # Then apply sigmoid: `score = sigmoid(r_mapped)`.\n    # For `potential_remaining_cap` close to 0, we want `r_mapped` to be large positive.\n    # This implies `C` should be large, or `-k` should be large positive. So `k` is negative.\n    # But we used `k>0` for steepness.\n\n    # Let's invert the interpretation. Let's say `score = sigmoid(k * (C - potential_remaining_cap))`\n    # If `potential_remaining_cap` is small, `C - potential_remaining_cap` is large, sigmoid -> 1.\n    # If `potential_remaining_cap` is large, `C - potential_remaining_cap` is small, sigmoid -> low.\n    # This seems correct for valid bins!\n\n    # We need to handle invalid bins (score = 0).\n    # And we need to choose `C` and `k`.\n    # Let `k = steepness`.\n    # A good choice for `C` would be something that represents a \"neutral\" point.\n    # If `C=0`, then `score = sigmoid(k * (-potential_remaining_cap)) = 1 / (1 + exp(k * potential_remaining_cap))`\n    # This is the decreasing function we already analyzed.\n\n    # Let's adjust the scaling and shift.\n    # The goal is that for `potential_remaining_cap >= 0`:\n    # small values get scores close to 1.\n    # large values get scores close to 0.\n\n    # Let `y = potential_remaining_cap`.\n    # Consider a function like `exp(-k * y)`. This is decreasing.\n    # Let's scale and shift it within sigmoid:\n    # `score = sigmoid(alpha + beta * y)` where `beta` is negative.\n    # `score = sigmoid(alpha - beta_abs * y)` where `beta_abs > 0`.\n    # `score = 1 / (1 + exp(-(alpha - beta_abs * y)))`\n    # `score = 1 / (1 + exp(beta_abs * y - alpha))`\n    # This is `1 / (1 + exp(k * (y - alpha/k)))`.\n    # So it's `1 / (1 + exp(k * (y - C)))`. Where `C = alpha/k`.\n\n    # This means `1 / (1 + exp(k * (potential_remaining_cap - C)))` is a decreasing function.\n    # It will give high scores for small `potential_remaining_cap`.\n    # We need to set `C` and `k`.\n    # Let `k = steepness`.\n    # If `C` is the target \"best fit\" remaining capacity, which is 0.\n    # Then `score = 1 / (1 + exp(steepness * potential_remaining_cap))`\n    # This function gives:\n    # r = 0 -> score = 0.5\n    # r > 0 -> score < 0.5\n    # r < 0 -> score > 0.5\n\n    # So, to make it work, we must explicitly set scores to 0 for invalid bins.\n    # For valid bins, the score `1 / (1 + exp(steepness * potential_remaining_cap))`\n    # will rank bins with smaller `potential_remaining_cap` higher.\n\n    # Let's implement this.\n    # The \"strength\" of preference for tighter fits will be controlled by `steepness`.\n\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Find bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n    potential_remaining_cap = bins_remain_cap - item\n\n    # For bins that can fit the item, calculate the sigmoid priority score.\n    # The function 1 / (1 + exp(steepness * remaining_capacity)) gives higher scores for smaller remaining_capacity.\n    # A steepness of, say, 5.0 means the score will drop significantly as remaining capacity increases beyond a small value.\n\n    # Calculate exponent argument. Use potential_remaining_cap for valid bins.\n    exponent_args = steepness * potential_remaining_cap[can_fit_mask]\n\n    # Clip exponent arguments to prevent overflow/underflow.\n    # Values too large positive (e.g., > 700 for double precision exp) result in 0.\n    # Values too large negative (e.g., < -700) result in 1.\n    # A range of [-20, 20] for the argument of `exp` usually gives reasonable output.\n    clipped_exponent_args = np.clip(exponent_args, -30, 30) # Slightly larger range for safety\n\n    # Calculate the priority scores for valid bins.\n    # Using the formulation that gives higher priority to smaller remaining capacity:\n    # score = 1 / (1 + exp(steepness * r))\n    # Where r = potential_remaining_cap.\n    # For r = 0, score = 0.5. For r > 0, score < 0.5. For r << 0, score >> 0.5.\n    # This is NOT the desired behavior for \"best fit\".\n\n    # Let's try the opposite sigmoid's input.\n    # `score = 1 / (1 + exp(-steepness * r))`\n    # This function `g(r) = 1 / (1 + exp(-k*r))` is INCREASING in `r`.\n    #   - r = 0 -> score = 0.5\n    #   - r > 0 -> score > 0.5\n    #   - r < 0 -> score < 0.5\n\n    # This means it favors bins with *larger* remaining capacity. This is \"Worst Fit\".\n\n    # The \"Sigmoid Fit Score\" strategy needs to assign higher scores to bins\n    # where `bins_remain_cap[i] - item` is small and non-negative.\n\n    # Let's define the score based on the \"tightness\" of the fit.\n    # Tightness: `1.0 / (potential_remaining_cap + epsilon)` for `potential_remaining_cap >= 0`.\n    # This gives a large score for `r` near 0.\n    # Then apply sigmoid to this mapped value.\n    # Let `scaled_tightness = k * (1.0 / (potential_remaining_cap + epsilon))`\n    # Score = `sigmoid(scaled_tightness)` = `1 / (1 + exp(-scaled_tightness))`\n    # Score = `1 / (1 + exp(-k / (potential_remaining_cap + epsilon)))`\n\n    # Let's test this.\n    # `r` is `potential_remaining_cap`.\n    # If `r` is very small positive (e.g., 1e-5):\n    #   `1.0 / (r + epsilon)` is large positive.\n    #   `-k / (r + epsilon)` is large negative.\n    #   `exp(...)` is close to 0.\n    #   `score` is close to 1. (GOOD)\n    # If `r` is large positive (e.g., 100):\n    #   `1.0 / (r + epsilon)` is close to 0.\n    #   `-k / (r + epsilon)` is close to 0.\n    #   `exp(...)` is close to 1.\n    #   `score` is close to 0.5. (OK, but we want lower score for larger `r`).\n\n    # This seems to prioritize smaller remaining capacities well.\n    # We need to ensure `epsilon` is small enough and chosen wisely.\n\n    # Let's refine: `steepness` controls the slope.\n    # `priorities = 1 / (1 + exp(-steepness / (potential_remaining_cap + epsilon)))`\n\n    # Handling invalid bins:\n    # For `can_fit_mask[i] == False`, priority is 0.\n\n    priorities = np.zeros_like(bins_remain_cap)\n    epsilon = 1e-9  # Small value to prevent division by zero\n\n    # Calculate remaining capacity only for bins where item can fit\n    valid_bins_mask = bins_remain_cap >= item\n    potential_remaining_cap_valid = bins_remain_cap[valid_bins_mask] - item\n\n    # For valid bins, calculate a measure of \"tightness\"\n    # `tightness_measure = 1.0 / (potential_remaining_cap_valid + epsilon)`\n    # This value is large when `potential_remaining_cap_valid` is small.\n\n    # Apply sigmoid to a scaled version of this tightness measure.\n    # The sigmoid `1 / (1 + exp(-x))` increases with `x`.\n    # We want score to increase with tightness.\n    # Let `x = steepness * tightness_measure`.\n    # `score = 1 / (1 + exp(-steepness * (1.0 / (potential_remaining_cap_valid + epsilon))))`\n\n    # Let's test `steepness = 5.0`.\n    # `r = 0.01` (small positive remaining capacity)\n    #   `tightness = 1.0 / (0.01 + 1e-9) approx 100`\n    #   `x = 5.0 * 100 = 500`\n    #   `exp(-500)` is very close to 0.\n    #   `score = 1 / (1 + 0) = 1`. (Excellent score)\n\n    # `r = 10.0` (larger remaining capacity)\n    #   `tightness = 1.0 / (10.0 + 1e-9) approx 0.1`\n    #   `x = 5.0 * 0.1 = 0.5`\n    #   `exp(-0.5) approx 0.606`\n    #   `score = 1 / (1 + 0.606) approx 0.623`. (Decent score, lower than 1)\n\n    # `r = 0.0` (perfect fit)\n    #   `tightness = 1.0 / (0.0 + 1e-9) = 1e9` (very large)\n    #   `x = 5.0 * 1e9` (very large)\n    #   `exp(-x)` is extremely close to 0.\n    #   `score = 1`.\n\n    # This formulation seems robust for prioritizing bins with the smallest non-negative remaining capacity.\n    # `steepness` controls how aggressively we penalize larger remaining capacities.\n\n    # Handle potential overflow in `exp` if `steepness / (potential_remaining_cap + epsilon)` is very large negative.\n    # This happens if `potential_remaining_cap` is large positive.\n    # If `steepness / (potential_remaining_cap + epsilon)` becomes very small negative, exp is ~1, score is 0.5.\n    # If it becomes very large negative, exp is ~0, score is 1. This is the issue.\n\n    # Let's ensure the argument to `exp` is within a safe range.\n    # The argument is `-steepness / (potential_remaining_cap + epsilon)`.\n    # If `potential_remaining_cap` is small positive, the argument is large negative.\n    # If `potential_remaining_cap` is large positive, the argument is small negative.\n\n    # Let `arg = -steepness / (potential_remaining_cap_valid + epsilon)`\n    # If `arg` is very negative (e.g., < -30), `exp(arg)` -> 0, score -> 1.\n    # If `arg` is very positive (e.g., > 30), `exp(arg)` -> inf, score -> 0.\n\n    # We want scores to be high for small `potential_remaining_cap`.\n    # This means `arg` should be large negative. This gives score near 1.\n    # For larger `potential_remaining_cap`, `arg` becomes less negative or even positive.\n    # If `arg` is close to 0, score is 0.5.\n    # If `arg` is positive, score is < 0.5.\n\n    # So, `score = 1 / (1 + exp(arg))` where `arg = -steepness * (1.0 / (potential_remaining_cap_valid + epsilon))`.\n    # Let's clip `arg` to avoid overflow/underflow for `exp`.\n    # We want to avoid `arg` becoming extremely large negative or extremely large positive.\n    # `arg` can be very large negative if `potential_remaining_cap_valid` is close to 0.\n    # `arg` is close to 0 if `potential_remaining_cap_valid` is very large.\n\n    # If `potential_remaining_cap_valid` is close to zero, `1.0 / (potential_remaining_cap_valid + epsilon)` is large.\n    # Then `arg` is large negative. `exp(arg)` is near 0. `score` is near 1. (Good)\n    # If `potential_remaining_cap_valid` is very large, `1.0 / (potential_remaining_cap_valid + epsilon)` is close to zero.\n    # Then `arg` is close to 0. `exp(arg)` is near 1. `score` is near 0.5. (This means larger remaining capacity gets score 0.5, which is not strictly worse than optimal 0.5 for perfect fit).\n\n    # To make larger remaining capacities get lower scores, we might need to modify this.\n    # Perhaps `score = 1 / (1 + exp(k * potential_remaining_cap))` but inverted or shifted.\n\n    # Let's use the previous formulation that seemed to work:\n    # `score = 1 / (1 + exp(-k * (C - value)))`\n    # where `value = potential_remaining_cap`.\n    # `score = 1 / (1 + exp(k * value - C))`\n\n    # We want higher scores for smaller `value`.\n    # This function is decreasing if `k > 0`.\n\n    # Let `k = steepness`.\n    # We need `C` to position the sigmoid.\n    # If we set `C` to a value that makes `k * C` equal to the \"median\" difference, or related to typical differences.\n\n    # Let's try to use `sigmoid(k * (max_diff - r))` where `max_diff` is a large constant or max possible diff.\n    # If `r=0`, `sigmoid(k*max_diff)` -> 1.\n    # If `r=max_diff`, `sigmoid(0)` -> 0.5.\n    # If `r > max_diff`, `sigmoid(-ve)` -> <0.5.\n\n    # The function should prioritize smallest `r >= 0`.\n    # Let `f(r)` be the priority score for `r`.\n    # `f(0)` highest, `f(large_r)` lower.\n\n    # Let's consider the function `1 / (1 + exp(-k * r))` but on a transformed `r`.\n    # If we transform `r` such that it's large positive when `r` is small, and small negative when `r` is large.\n    # This is what `1.0 / (r + epsilon)` does, but the range might not be ideal for sigmoid.\n\n    # Let's cap `potential_remaining_cap` for the \"good\" range.\n    # Say, anything greater than `item * 2` is considered \"very loose\".\n\n    # Let's retry the formulation that promotes small positive remainders.\n    # `score = 1 / (1 + exp(k * (r - C)))` where `k>0`.\n    # This is a decreasing function. To get high scores for small `r`, we need `r-C` to be small (most negative).\n    # This means `C` should be the target value, and if `r` is below `C`, score is high.\n    # The ideal target is `r = 0`. So `C = 0`.\n    # `score = 1 / (1 + exp(k * r))`.\n\n    # Re-evaluation of `1 / (1 + exp(k * r))` for `r = bins_remain_cap - item`.\n    # `k > 0`.\n    # `r < 0` (invalid fit): `k*r` is negative. `exp(k*r)` is < 1. `1 / (1 + exp)` is > 0.5. (high priority)\n    # `r = 0` (perfect fit): `k*r` is 0. `exp(0) = 1`. `1 / (1 + 1) = 0.5`. (medium priority)\n    # `r > 0` (loose fit): `k*r` is positive. `exp(k*r)` is > 1. `1 / (1 + exp)` is < 0.5. (low priority)\n\n    # This function, `1 / (1 + exp(k * r))`, strongly prefers bins that CANNOT fit the item.\n    # This is the opposite of \"Best Fit\".\n\n    # We need a score that increases as `r` decreases for `r >= 0`.\n    # And is 0 for `r < 0`.\n\n    # Let's define a transformation:\n    # `transformed_r = -r` for `r >= 0`\n    # `transformed_r = very_small_number` for `r < 0`\n    # Then apply `sigmoid(k * transformed_r)`.\n\n    # `transformed_r` for valid bins: `-potential_remaining_cap_valid`\n    # For invalid bins, `potential_remaining_cap_valid` is not applicable.\n    # For valid bins `r >= 0`: We want `f(r)` to be decreasing.\n    # `f(r) = 1 / (1 + exp(k * r))` works.\n    # BUT we need to ensure that bins where `r < 0` get score 0.\n\n    # Let's combine:\n    # For `i` where `bins_remain_cap[i] < item`: `priorities[i] = 0.0`\n    # For `i` where `bins_remain_cap[i] >= item`:\n    #   `r = bins_remain_cap[i] - item`\n    #   `priorities[i] = 1 / (1 + exp(steepness * r))`\n\n    # This seems to be the most sensible application of the sigmoid to achieve \"Best Fit\" ranking.\n    # `steepness` controls how sharply the priority drops as remaining capacity increases.\n    # A higher `steepness` means a stronger preference for tight fits.\n\n    priorities = np.zeros_like(bins_remain_cap)\n    steepness = 5.0  # Tunable parameter for sigmoid steepness\n\n    # Identify bins where the item can fit\n    can_fit_mask = bins_remain_cap >= item\n\n    # Calculate remaining capacity for these valid bins\n    potential_remaining_cap_valid = bins_remain_cap[can_fit_mask] - item\n\n    # Calculate the sigmoid score for valid bins.\n    # The function `1 / (1 + exp(x))` decreases as `x` increases.\n    # We want higher scores for smaller `potential_remaining_cap_valid`.\n    # So, we use `x = steepness * potential_remaining_cap_valid`.\n    # This means the scores are higher for smaller `potential_remaining_cap_valid`.\n\n    exponent_args = steepness * potential_remaining_cap_valid\n\n    # Clip arguments to exp to prevent overflow/underflow.\n    # If `exponent_args` is large positive (>30), exp -> inf, score -> 0.\n    # If `exponent_args` is large negative (<-30), exp -> 0, score -> 1.\n    clipped_exponent_args = np.clip(exponent_args, -30.0, 30.0)\n\n    # Calculate the priority scores for valid bins.\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(clipped_exponent_args))\n\n    return priorities",
    "response_id": 16,
    "obj": 4.487435181491823,
    "SLOC": 25.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response17.txt_stdout.txt",
    "code_path": "problem_iter1_code17.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit strategy.\n\n    In Random Fit, we consider bins that can accommodate the item. Among these,\n    we randomly select one. To implement this as a priority function where\n    the highest priority bin is chosen, we can assign a high priority to\n    eligible bins and a low priority to ineligible bins. To introduce\n    randomness within eligible bins, we can add a random perturbation to\n    their priority.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins that can accommodate the item\n    eligible_bins_mask = bins_remain_cap >= item\n    \n    # Assign a base priority to eligible bins\n    # We can use the remaining capacity as a base, aiming to keep larger bins\n    # available for potentially larger future items, though Random Fit doesn't\n    # explicitly optimize for this. A simple uniform high value also works.\n    # Here, we'll use the remaining capacity itself, as it's a common heuristic\n    # for other strategies, but here it's just to provide a non-zero value.\n    priorities[eligible_bins_mask] = bins_remain_cap[eligible_bins_mask] \n    \n    # Add a random perturbation to introduce randomness among eligible bins\n    # This ensures that if multiple bins are equally \"good\" (e.g., same remaining capacity),\n    # the choice is still somewhat random.\n    random_perturbation = np.random.rand(len(bins_remain_cap)) * 0.1  # Small random value\n    priorities[eligible_bins_mask] += random_perturbation[eligible_bins_mask]\n    \n    # Ineligible bins will have a priority of 0 (as initialized),\n    # ensuring they are never chosen if eligible bins exist.\n    \n    return priorities",
    "response_id": 17,
    "obj": 149.30195452732352,
    "SLOC": 7.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response18.txt_stdout.txt",
    "code_path": "problem_iter1_code18.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    The Epsilon-Greedy strategy balances exploration (trying less optimal bins)\n    and exploitation (choosing the best bin).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.1  # Probability of exploration\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # If no bin can fit, return zero priorities (this shouldn't happen in a well-defined scenario)\n    if not np.any(can_fit_mask):\n        return np.zeros_like(bins_remain_cap)\n\n    # Exploitation: Calculate the \"goodness\" of fitting the item into each capable bin.\n    # A good heuristic is to prioritize bins that leave minimal remaining space\n    # after packing the item (i.e., minimize waste).\n    # We want to maximize this score, so we use (bins_remain_cap - item).\n    # Adding a small constant to avoid zero scores for perfect fits might be considered,\n    # but for simplicity, we'll keep it as is.\n    exploitation_scores = np.zeros_like(bins_remain_cap)\n    exploitation_scores[can_fit_mask] = bins_remain_cap[can_fit_mask] - item\n\n    # Exploration: Randomly select bins to explore.\n    # With probability epsilon, we choose a random bin that can fit the item.\n    # We assign a random positive score to encourage trying it.\n    exploration_scores = np.zeros_like(bins_remain_cap)\n    num_capable_bins = np.sum(can_fit_mask)\n    if num_capable_bins > 0:\n        random_indices = np.random.choice(np.where(can_fit_mask)[0], size=max(1, int(epsilon * num_capable_bins)), replace=False)\n        # Assign a score to encourage exploration. A small positive value is sufficient.\n        # We can also add a small random noise to differentiate exploration choices.\n        exploration_scores[random_indices] = np.random.rand(len(random_indices)) * 0.1 # Small random value\n\n    # Combine exploitation and exploration\n    # The idea is that with probability (1-epsilon), we use the exploitation score.\n    # With probability epsilon, we might pick a random bin (exploration_scores).\n    # A simple way to combine is to use a weighted sum, but for a priority function\n    # where we select the max, we can think of it as:\n    # if random() < epsilon: use exploration score; else: use exploitation score.\n    # For generating a priority array, we can modify exploitation scores based on epsilon.\n    priorities = exploitation_scores.copy()\n\n    # Introduce some randomness to the best bins based on epsilon\n    # This is a common way to implement epsilon-greedy for selection\n    # For priority scores, we can add noise to exploitation scores with probability epsilon\n    noise_mask = np.random.rand(len(bins_remain_cap)) < epsilon\n    noise_mask = np.logical_and(noise_mask, can_fit_mask) # Only add noise to bins that can fit\n\n    # Add small random noise to some of the capable bins to encourage exploration\n    # The magnitude of the noise should be small enough not to overpower good exploitation choices\n    noise = np.random.rand(len(bins_remain_cap)) * 0.05 # Small noise\n    priorities[noise_mask] += noise[noise_mask]\n\n    # Ensure that bins that cannot fit have zero priority.\n    priorities[~can_fit_mask] = 0\n\n    return priorities",
    "response_id": 18,
    "exec_success": false,
    "obj": Infinity,
    "traceback_msg": "Traceback (most recent call last):\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 112, in <module>\n    avg_num_bins = -evaluate(dataset)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 55, in evaluate\n    _, bins_packed = online_binpack(items.astype(float), bins)\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/eval.py\", line 28, in online_binpack\n    priorities = priority(item, bins[valid_bin_indices])\n  File \"/home/dokhanhnam1199/QD/problems/bpp_online/gpt.py\", line 63, in priority_v2\n    # Here, we'll return near-zero probabilities.\nnumpy._core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n19\n3\n"
  },
  {
    "stdout_filepath": "problem_iter1_response19.txt_stdout.txt",
    "code_path": "problem_iter1_code19.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated based on how well the item fits into the bin.\n    Bins with remaining capacity equal to or slightly larger than the item\n    will have higher probabilities. The softmax function is used to convert\n    these fit scores into probabilities (priorities).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a small negative number to avoid log(0) or zero division issues\n    # if no bin can fit the item. This ensures that bins that cannot fit the item\n    # will have very low probabilities after softmax.\n    fit_scores = np.full_like(bins_remain_cap, -np.inf)\n\n    # Consider only bins that have enough remaining capacity for the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a 'fit score'.\n    # A good heuristic is to favor bins where the remaining capacity is\n    # just enough for the item (minimizing wasted space).\n    # We can use the negative of the difference as a score,\n    # so smaller difference means a larger (better) score.\n    # Alternatively, we can use 1 / (remaining_capacity - item + epsilon)\n    # to reward smaller remaining capacities.\n    # Let's use a score that emphasizes bins with very little remaining space\n    # after placing the item.\n    if np.any(can_fit_mask):\n        # Calculate the remaining capacity after placing the item\n        remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n        # A higher score for smaller remaining space. Adding a small epsilon\n        # to the denominator to avoid division by zero if remaining_after_fit is 0.\n        # The '-1' exponent is because we want to maximize the score for smaller gaps.\n        # The magnitude of the score can be scaled to influence the softmax behavior.\n        # Here, a larger negative exponent might amplify differences,\n        # but for simplicity, we'll use a direct relationship with inverse remaining space.\n        # A score inversely proportional to the wasted space: 1 / (wasted_space + epsilon)\n        # to prioritize tighter fits.\n        epsilon = 1e-6\n        fit_scores[can_fit_mask] = 1.0 / (remaining_after_fit + epsilon)\n\n\n    # Apply softmax to convert fit scores into priorities (probabilities)\n    # Softmax: exp(score) / sum(exp(scores))\n    # Adding a small constant to the scores can help stabilize softmax if all scores are -inf.\n    # However, if some bins *can* fit, those will have finite positive scores,\n    # and the -inf will correctly result in near-zero probabilities.\n    exp_scores = np.exp(fit_scores)\n    sum_exp_scores = np.sum(exp_scores)\n\n    if sum_exp_scores == 0:\n        # If no bin can fit the item, or if all fit scores resulted in exp(score) being 0 (unlikely with positive values)\n        # return a uniform distribution, or a distribution favoring the first available bin.\n        # For this problem, if nothing fits, no bin should be chosen. A uniform low probability is reasonable.\n        # A truly optimal strategy would signal that no bin is suitable.\n        # Here, we'll return near-zero probabilities.\n        return np.ones_like(bins_remain_cap) / len(bins_remain_cap) if len(bins_remain_cap) > 0 else np.array([])\n    else:\n        priorities = exp_scores / sum_exp_scores\n        return priorities",
    "response_id": 19,
    "obj": 4.198244914240141,
    "SLOC": 14.0,
    "cyclomatic_complexity": 4.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response20.txt_stdout.txt",
    "code_path": "problem_iter1_code20.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using a First Fit Decreasing-like approach.\n\n    This heuristic prioritizes bins that can accommodate the item and\n    gives higher priority to bins that have a remaining capacity closer to the item's size.\n    This is an adaptation of the First Fit Decreasing strategy for the online setting.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Initialize priorities to a very low value, so bins that cannot fit the item get no priority.\n    priorities = np.full_like(bins_remain_cap, -np.inf)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit the item, calculate a priority score\n    # The score is based on the \"tightness\" of the fit.\n    # A higher score means a tighter fit (remaining capacity closer to item size).\n    # We use the inverse of the difference to give higher priority to tighter fits.\n    # Adding 1 to the denominator prevents division by zero if remaining capacity is exactly the item size.\n    priorities[can_fit_mask] = 1.0 / (bins_remain_cap[can_fit_mask] - item + 1e-9) # Adding a small epsilon for numerical stability\n\n    # Invert the priorities so that the highest value represents the best bin to fit into.\n    # Since we want to minimize bins, we are looking for the tightest fit.\n    # The current calculation already assigns higher values to tighter fits.\n    # So, we can directly return. However, if we wanted to prioritize bins with MORE\n    # remaining capacity for a different strategy, we might need inversion.\n    # For the First Fit concept aiming for tightest fit, the current `priorities` are good.\n    # If we want to use np.argmax, higher values are better.\n\n    # A common First Fit variation might also consider the overall remaining capacity\n    # to avoid \"wasted\" space too early. However, the prompt emphasizes a \"First Fit\"\n    # strategy, which typically implies trying to fit the item into the first available bin.\n    # Our priority here is to define \"first available\" in a way that's beneficial.\n    # The current approach prioritizes bins that are almost full and can still fit the item.\n\n    return priorities",
    "response_id": 20,
    "obj": 4.198244914240141,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response21.txt_stdout.txt",
    "code_path": "problem_iter1_code21.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Best Fit.\n\n    The Best Fit strategy prioritizes bins that have the smallest remaining capacity\n    after placing the item, as long as the item fits. A larger priority score indicates\n    a more preferred bin. If an item does not fit in a bin, its priority is set to 0.\n    The priority is inversely proportional to the remaining capacity after placement,\n    with an adjustment to favor bins that leave minimal waste.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n    \n    # Identify bins where the item can fit\n    possible_bins_mask = bins_remain_cap >= item\n    \n    # Calculate the remaining capacity for bins where the item fits\n    remaining_capacities_after_fit = bins_remain_cap[possible_bins_mask] - item\n    \n    # The priority is higher for bins that have less remaining capacity after fitting\n    # We can use the inverse of remaining capacity or a similar measure.\n    # To avoid division by zero and to ensure higher values are better,\n    # we can use a term like 1 / (1 + remaining_capacity) or a large constant minus remaining_capacity.\n    # Let's use a formulation that rewards smaller remaining capacities more strongly,\n    # but also ensures a baseline preference for bins that are nearly full.\n    \n    # A common approach for \"best fit\" is to prioritize bins that have the smallest\n    # capacity that is still greater than or equal to the item.\n    # This means we want to find bins with `bins_remain_cap[i]` that are closest to `item`.\n    \n    # Let's calculate a priority score that is high when `bins_remain_cap[i] - item` is small.\n    # A simple way is `1 / (1 + (bins_remain_cap[i] - item))` for fitting bins.\n    # However, for best fit, we want the smallest leftover space.\n    # So, the \"best\" bin will have the smallest positive `bins_remain_cap[i] - item`.\n    \n    # We can assign a high priority to bins where `bins_remain_cap[i] - item` is minimized.\n    # Let's try to make the priority a measure of how \"tight\" the fit is.\n    # A small positive value for `bins_remain_cap[i] - item` indicates a good fit.\n    \n    # We want to maximize the priority. The best fit leaves the minimum positive `remaining_capacity`.\n    # So, a larger priority should be given to bins with smaller positive `remaining_capacity`.\n    # We can invert this. Let's use `-(bins_remain_cap - item)` for fitting bins.\n    # The smallest `bins_remain_cap - item` will result in the largest negative number,\n    # which is not what we want if higher means better.\n    \n    # Let's refine: Higher priority means \"more likely to be chosen\".\n    # For Best Fit, the bin with the smallest *positive* difference\n    # (`bins_remain_cap - item`) is the best.\n    # We can achieve this by setting priority proportional to `1 / (1 + (bins_remain_cap - item))`.\n    # However, this might give very high priority to bins that have a lot of remaining capacity\n    # if the `item` is very small.\n    \n    # A common way to implement \"best fit\" priority:\n    # For each bin `j` where `bins_remain_cap[j] >= item`, calculate a \"fit score\".\n    # The best fit is the one with the minimum `bins_remain_cap[j] - item`.\n    # We want to map this to a priority where higher is better.\n    # We can set the priority to be `(max_possible_capacity + 1) - (bins_remain_cap[j] - item)`.\n    # Or, simply use the negative difference, and then take the maximum among fitting bins.\n    \n    # Let's try a priority that is proportional to `1 / (1 + (remaining_capacity - item))` for bins that fit.\n    # To ensure \"best fit\" (smallest `remaining_capacity - item`), we can\n    # assign higher priorities to smaller positive differences.\n    \n    # Option 1: Direct inverse of remaining space (higher priority for smaller leftover)\n    # This can be sensitive to the scale of capacities.\n    # priorities[possible_bins_mask] = 1.0 / (1.0 + remaining_capacities_after_fit)\n\n    # Option 2: A large value minus the leftover space. This rewards tighter fits.\n    # We need a reference for \"large value\". The maximum possible remaining capacity\n    # of a bin before adding an item could be a baseline, but bins vary.\n    # Let's consider the maximum *initial* capacity of any bin (though not explicitly given,\n    # we can assume a hypothetical maximum or a large constant).\n    # For simplicity, let's assign a higher priority to bins with smaller `remaining_capacities_after_fit`.\n    \n    # To make \"best fit\" mean highest priority, we want the smallest `bins_remain_cap - item`.\n    # So, if `bins_remain_cap[j] - item = d`, we want a priority `P(d)` that is increasing with `d`.\n    # Wait, no, we want highest priority for the *smallest positive d*.\n    # So, `P(d)` should be *decreasing* as `d` increases for `d >= 0`.\n    # However, we also want to ensure that bins that *can* fit are prioritized over those that *cannot*.\n    \n    # Let's use a scoring system where:\n    # - Bins that don't fit get a score of 0.\n    # - Bins that fit get a score related to how little space is left.\n    #   The smaller the `remaining_capacity - item`, the higher the score.\n    #   This implies an inverse relationship with `remaining_capacity - item`.\n    \n    # To make the highest priority correspond to the \"best fit\" (smallest positive residual),\n    # we can assign a score that is large when `bins_remain_cap[i] - item` is small and positive.\n    # A score of `1 / (1 + (bins_remain_cap[i] - item))` for fitting bins would give higher scores to smaller residuals.\n    # However, `np.argmax` on `1 / (1 + residuals)` would pick the smallest residual.\n    \n    # Consider the values of `bins_remain_cap - item` for fitting bins. We want the minimum of these.\n    # If we set `priorities[possible_bins_mask] = -(bins_remain_cap[possible_bins_mask] - item)`,\n    # then `np.argmax` will select the bin with the smallest residual (most negative number).\n    # This is effectively Best Fit.\n    \n    priorities[possible_bins_mask] = -(bins_remain_cap[possible_bins_mask] - item)\n    \n    return priorities",
    "response_id": 21,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response22.txt_stdout.txt",
    "code_path": "problem_iter1_code22.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Worst Fit strategy.\n\n    The Worst Fit strategy prioritizes bins that have the largest remaining capacity.\n    This strategy aims to leave smaller remaining capacities in more bins,\n    hoping to accommodate larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # For bins where the item can fit (remaining capacity >= item size)\n    # assign a priority equal to the remaining capacity.\n    # Bins with larger remaining capacity get higher priority (Worst Fit).\n    # Bins where the item cannot fit are assigned a priority of 0.\n    fit_mask = bins_remain_cap >= item\n    priorities[fit_mask] = bins_remain_cap[fit_mask]\n\n    return priorities",
    "response_id": 22,
    "obj": 149.30195452732352,
    "SLOC": 5.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response23.txt_stdout.txt",
    "code_path": "problem_iter1_code23.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Almost Full Fit.\n\n    The Almost Full Fit strategy prioritizes bins that are \"almost full\" but can\n    still accommodate the item. This heuristic aims to leave more space in\n    less full bins for potentially larger items later.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Bins that cannot fit the item have a priority of 0 (will not be selected).\n    # For bins that can fit the item, we want to prioritize those that are\n    # \"almost full\". A simple way to define \"almost full\" is by considering\n    # the remaining capacity relative to the item's size.\n    #\n    # The logic is:\n    # 1. Identify bins where the item can fit.\n    # 2. For these bins, calculate a score that is higher for bins with less\n    #    remaining capacity (i.e., more full).\n    # 3. A good scoring mechanism could be the inverse of the remaining capacity,\n    #    or something that increases as remaining capacity decreases.\n    #    However, to be \"almost full\", we don't want the bins that are *exactly*\n    #    full (remaining capacity = item size) but rather those where adding\n    #    the item leaves a small remainder.\n    #\n    # A scoring mechanism could be:\n    # - (bin_remain_cap - item) : This gives a negative score if it's not a good fit.\n    #   We are interested in bins that fit, so `bin_remain_cap >= item`.\n    # - The \"tightness\" of the fit can be measured by how much space is left *after*\n    #   fitting the item. We want this left-over space to be small.\n    #   So, `bin_remain_cap - item` should be small and non-negative.\n    #\n    # Let's define the priority as a value that is high when `bin_remain_cap - item`\n    # is small and positive.\n    # A suitable function could be:\n    # priority = 1 / (bin_remain_cap - item + epsilon) where epsilon is a small\n    # positive number to avoid division by zero if bin_remain_cap == item.\n    # However, this gives higher priority to bins that are *barely* fitting the item,\n    # which is what \"almost full\" implies.\n\n    # Create a boolean mask for bins that can accommodate the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # Initialize priorities to zero\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # For bins that can fit the item, calculate the priority\n    # The priority is inversely proportional to the remaining capacity after placing the item.\n    # We add a small epsilon to the denominator to prevent division by zero when\n    # remaining capacity exactly equals item size.\n    epsilon = 1e-6\n    remaining_after_fit = bins_remain_cap[can_fit_mask] - item\n    priorities[can_fit_mask] = 1.0 / (remaining_after_fit + epsilon)\n\n    # Alternatively, we can express \"almost full\" as maximizing the remaining space\n    # but not too much. A simple approach: we want the remaining capacity to be\n    # as small as possible, but greater than or equal to the item size.\n    # So, we can sort by `bins_remain_cap` in ascending order for those that fit.\n    # A way to convert ascending sort order into higher priority is to take the\n    # difference from the maximum possible remaining capacity or a derived value.\n\n    # Let's refine the \"almost full\" idea. We want bins that leave a small positive\n    # remainder. So, `bins_remain_cap - item` should be small and non-negative.\n    # We can map this small positive remainder to a high priority.\n    #\n    # Consider the gap: `gap = bins_remain_cap - item`.\n    # We want to maximize `-(gap)` for `gap >= 0`.\n    #\n    # A more direct \"almost full\" interpretation might be related to the\n    # percentage of capacity used *after* fitting the item.\n    # `percentage_used_after_fit = (bin_capacity - (bins_remain_cap - item)) / bin_capacity`\n    # This is getting complicated without knowing the original bin capacity.\n    #\n    # Let's stick to the simpler interpretation: prioritize bins where `bins_remain_cap - item` is minimized.\n    # So, the priority is inversely proportional to `bins_remain_cap - item`.\n    # This makes bins where `bins_remain_cap` is just slightly larger than `item`\n    # have the highest priority.\n\n    # Let's retry with a clear definition: \"Almost Full Fit\" prioritizes bins that are\n    # closest to being full, meaning their remaining capacity is small.\n    # When we consider placing 'item', we look at `bins_remain_cap`.\n    # We only consider bins where `bins_remain_cap >= item`.\n    # Among these, we want the one with the *smallest* `bins_remain_cap`.\n    # So, we want to sort by `bins_remain_cap` in ascending order.\n    #\n    # To convert this to a priority score (higher is better):\n    # 1. Find the minimum remaining capacity among bins that can fit the item.\n    # 2. Calculate priority based on how close a bin's remaining capacity is to this minimum.\n    #\n    # Let `min_fitting_remain_cap` be the smallest `bins_remain_cap` such that `bins_remain_cap >= item`.\n    # Priority for bin `i` that fits: `some_function(bins_remain_cap[i] - min_fitting_remain_cap)`.\n    # A simple decreasing function of this difference will work.\n    # Example: `priority = 1 / (bins_remain_cap[i] - min_fitting_remain_cap + epsilon)`\n    # This means bins with the smallest fitting capacity (i.e., closest to `item` but >= `item`)\n    # will get the highest priority.\n\n    if not np.any(can_fit_mask):\n        # No bin can fit the item\n        return priorities\n\n    fitting_capacities = bins_remain_cap[can_fit_mask]\n    # Find the minimum remaining capacity among bins that can fit the item.\n    min_fitting_capacity = np.min(fitting_capacities)\n\n    # Calculate priorities for bins that can fit the item.\n    # We want to prioritize bins with `bins_remain_cap` closest to `min_fitting_capacity`.\n    # A higher score for smaller difference `bins_remain_cap[i] - min_fitting_capacity`.\n    # Use the inverse of the difference (plus epsilon to avoid division by zero if multiple bins have `min_fitting_capacity`).\n    differences = bins_remain_cap[can_fit_mask] - min_fitting_capacity\n    priorities[can_fit_mask] = 1.0 / (differences + epsilon)\n\n    return priorities",
    "response_id": 23,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 2.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response24.txt_stdout.txt",
    "code_path": "problem_iter1_code24.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Exact Fit First.\n\n    Exact Fit First prioritizes bins that can accommodate the item with the least\n    remaining capacity after placement. This aims to fill bins as much as possible.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    priorities = np.zeros_like(bins_remain_cap)\n\n    # Only consider bins that have enough capacity for the item\n    possible_bins_mask = bins_remain_cap >= item\n\n    # Calculate the remaining capacity if the item were placed in the bin\n    # We want to minimize this remaining capacity, so higher priority means lower remaining capacity.\n    # Therefore, we use the negative of the remaining capacity as the priority.\n    remaining_capacities_after_placement = bins_remain_cap[possible_bins_mask] - item\n\n    # Assign a priority score: a higher score means a better fit.\n    # We use the negative of the remaining capacity, so the smallest remaining capacity\n    # gets the highest (least negative) score.\n    priorities[possible_bins_mask] = -remaining_capacities_after_placement\n\n    return priorities",
    "response_id": 24,
    "obj": 4.048663741523748,
    "SLOC": 6.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response25.txt_stdout.txt",
    "code_path": "problem_iter1_code25.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Inverse Distance (Proximity Fit).\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We want to prioritize bins that have just enough space for the item.\n    # A bin is a good candidate if its remaining capacity is close to the item size.\n    # The \"inverse distance\" here refers to how close the remaining capacity is to the item size.\n    # A smaller difference means a higher priority.\n    \n    # Calculate the difference between bin remaining capacity and the item size.\n    # We are only interested in bins where the item can actually fit.\n    differences = bins_remain_cap - item\n    \n    # For bins where the item doesn't fit, assign a very low priority (or 0)\n    # to effectively exclude them from consideration.\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n    \n    # Identify bins where the item fits\n    can_fit_mask = differences >= 0\n    \n    # For bins where the item fits, calculate the inverse of the difference + a small epsilon \n    # to avoid division by zero if a bin has exactly the required capacity.\n    # A smaller difference means a larger inverse, hence higher priority.\n    # Adding 1 to the difference and taking the reciprocal is a common way to implement inverse distance.\n    # Alternatively, we can use the difference directly and then take the negative to invert the ranking\n    # or use a sigmoid-like function to map differences to priorities.\n    \n    # Let's try a simple inverse: 1 / (difference + 1)\n    # This prioritizes bins with smaller remaining capacity, pushing towards a tighter fit.\n    priorities[can_fit_mask] = 1.0 / (differences[can_fit_mask] + 1.0)\n    \n    # Alternative: Prioritize bins that are \"almost full\" for this item,\n    # meaning the remaining capacity is just enough.\n    # Let's consider a strategy that favors bins that, after placing the item,\n    # have the *least* remaining capacity among those that can fit the item.\n    # This is akin to \"Best Fit\" which is a common heuristic for BPP.\n    # If we want to implement \"Inverse Distance (Proximity Fit)\" in a way that\n    # prioritizes bins *closest* to fitting the item, then the difference\n    # (bins_remain_cap - item) should be minimized.\n    # The reciprocal of this difference (plus a small value) would give higher scores\n    # to smaller differences.\n    \n    # Let's refine this: prioritize bins where `bins_remain_cap - item` is minimal and non-negative.\n    # We want higher priority for smaller positive differences.\n    # So, a good priority score could be `1 / (bins_remain_cap - item + epsilon)` for fitting bins.\n    # Or, if we want to avoid division and just rank them, we could use `- (bins_remain_cap - item)`.\n    # The higher the difference (meaning more remaining capacity), the lower the priority.\n    \n    # Let's use the inverse difference idea, making sure smaller differences get higher scores.\n    # We can transform `bins_remain_cap - item` into a priority.\n    # A direct mapping: if diff is 0, priority is high. If diff is large, priority is low.\n    # A simple way is to use `np.exp(-differences)` for fitting bins, or something similar.\n    \n    # Let's go with the \"Best Fit\" intuition adapted for inverse proximity:\n    # Prioritize bins that have the *least* remaining capacity but can still fit the item.\n    # This means minimizing `bins_remain_cap` among valid bins.\n    # A higher priority should be given to a bin if `bins_remain_cap - item` is small and positive.\n    \n    # Calculate a score based on how \"close\" the remaining capacity is to the item size.\n    # We want to minimize `bins_remain_cap - item` for fitting bins.\n    # Let's use the negative of the difference. The smaller the non-negative difference, the larger the negative value will be (closer to zero).\n    \n    # For bins that can fit the item, the \"proximity\" is `bins_remain_cap - item`.\n    # We want to maximize the priority, so we want to give higher priority to smaller non-negative differences.\n    # Consider the values of `bins_remain_cap[can_fit_mask] - item`.\n    # If these values are 0, 1, 2, 3... we want priorities that are high, slightly lower, even lower, etc.\n    \n    # Option 1: Higher priority for smaller non-negative differences.\n    # We can use `np.reciprocal(differences[can_fit_mask] + 1.0)` as before, but let's reconsider the scaling.\n    # The direct reciprocal might be too sensitive to small differences.\n    \n    # Option 2: A decreasing function of the difference.\n    # Let's try `np.exp(-(bins_remain_cap[can_fit_mask] - item))`.\n    # This gives a high priority when the difference is 0 (exponential of 0 is 1), and it decreases as the difference increases.\n    # This fits the \"proximity\" idea well.\n    \n    # Ensure we don't have extremely large negative exponents if differences are large.\n    # However, for BPP, differences usually aren't astronomically large compared to item sizes.\n    \n    priorities[can_fit_mask] = np.exp(-(differences[can_fit_mask]))\n\n    # To ensure we don't get numerical issues with very large differences,\n    # we could cap the negative exponent if needed, but for typical BPP, this is likely fine.\n    # For instance, if `bins_remain_cap - item` can be very large (e.g., > 100), `exp(-100)` is close to zero.\n    # The problem statement implies we are using the \"Inverse Distance\" strategy for proximity.\n    # A direct interpretation of inverse distance would be `1 / distance`.\n    # Here, \"distance\" is the difference `bins_remain_cap - item`.\n    # So, `1 / (bins_remain_cap - item + epsilon)` where epsilon avoids division by zero.\n    # Let's refine this: a smaller difference means higher priority.\n    \n    # Let's stick to the principle of minimizing the gap after placement.\n    # If a bin has capacity 10 and item size is 7, gap is 3.\n    # If another bin has capacity 12 and item size is 7, gap is 5.\n    # We want to prioritize the bin with gap 3 over the bin with gap 5.\n    \n    # Using `1 / (gap + 1)` means:\n    # For gap 0: 1 / (0 + 1) = 1.0 (Highest priority)\n    # For gap 1: 1 / (1 + 1) = 0.5\n    # For gap 2: 1 / (2 + 1) = 0.333...\n    # For gap 5: 1 / (5 + 1) = 0.166...\n    # This ranking is correct.\n\n    # Let's reconsider the prompt's suggestion for \"Inverse Distance (Proximity Fit)\".\n    # If we consider \"distance\" as how far the bin's remaining capacity is from a \"perfect fit\" (i.e., equal to item size),\n    # then this distance is `abs(bins_remain_cap - item)`.\n    # However, in online BPP, we *must* fit the item, so `bins_remain_cap >= item`.\n    # Thus, the distance is simply `bins_remain_cap - item`.\n    # Inverse distance implies higher values for smaller distances.\n    \n    # So, for fitting bins: priority = 1 / (bins_remain_cap - item + epsilon)\n    # Using epsilon to avoid division by zero.\n    epsilon = 1e-9  # A small value to avoid division by zero\n    \n    # Update priorities using the inverse of the non-negative difference\n    priorities[can_fit_mask] = 1.0 / (differences[can_fit_mask] + epsilon)\n    \n    # This heuristic encourages packing into bins that are almost full, minimizing wasted space.\n    # It's a form of \"Best Fit\" heuristic.\n    \n    return priorities",
    "response_id": 25,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response26.txt_stdout.txt",
    "code_path": "problem_iter1_code26.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Sigmoid Fit Score.\n\n    The Sigmoid Fit Score prioritizes bins that have a remaining capacity\n    close to the item's size. This is achieved by applying a sigmoid function\n    to the difference between the bin's remaining capacity and the item's size.\n    A positive difference (bin has more capacity than item) is mapped to a\n    higher priority, while a negative difference (bin has less capacity than item)\n    is mapped to a lower priority. The steepness of the sigmoid can be adjusted.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Avoid division by zero or very small numbers if an item exactly fits\n    # or if there are no bins with capacity.\n    # We add a small epsilon to the denominator to prevent this.\n    epsilon = 1e-6\n\n    # Calculate the difference between bin remaining capacity and item size.\n    # We are interested in bins where remaining_capacity >= item.\n    # For bins where remaining_capacity < item, the score should be very low.\n    # To achieve this, we can consider (remaining_capacity - item).\n    # For bins that can fit the item, we want a higher priority if the remaining capacity\n    # is closer to the item size.\n    # Let's transform this to a sigmoid function. A common approach for \"best fit\"\n    # is to penalize excessive remaining space.\n\n    # Consider a score that is higher when remaining_capacity is just enough.\n    # A sigmoid function works well for this.\n    # Let's define the sigmoid argument as a transformation of (remaining_capacity - item).\n    # We want a score close to 1 when remaining_capacity is slightly larger than item,\n    # and a score close to 0 when remaining_capacity is much larger or smaller.\n\n    # Option 1: Directly use a sigmoid on the capacity-item difference.\n    # The argument of the sigmoid, x, will be (bins_remain_cap - item).\n    # We want a peak around x=0.\n    # A standard sigmoid is 1 / (1 + exp(-x)). This peaks at x=0.\n    # We might want to scale x or adjust the sigmoid.\n\n    # Let's try a sigmoid that is centered around the item size.\n    # We want to maximize priority when bins_remain_cap is slightly larger than item.\n    # If bins_remain_cap < item, the priority should be 0.\n\n    # Calculate the 'fitness' for each bin: remaining capacity minus item size.\n    # Only consider bins that can actually fit the item.\n    fits = bins_remain_cap >= item\n    non_fits_score = -100  # Assign a very low score to bins that cannot fit the item.\n\n    # For bins that can fit, we want to prioritize those with the smallest remaining capacity\n    # that still fits the item. This is the \"best fit\" strategy.\n    # A sigmoid function can model this \"sweet spot\".\n\n    # Let's transform the difference: (bins_remain_cap - item)\n    # We want higher scores when this difference is small and non-negative.\n    # A sigmoid function, like 1 / (1 + exp(-k * (value - center))), can create this effect.\n\n    # Let's use the difference (item - bins_remain_cap) for the sigmoid argument.\n    # This way, when bins_remain_cap is close to item (difference is small and positive),\n    # exp(-k * diff) will be small, and the sigmoid will be close to 1.\n    # When bins_remain_cap is much larger than item (difference is large and negative),\n    # exp(-k * diff) will be large, and the sigmoid will be close to 0.\n    # When bins_remain_cap < item (difference is positive and large),\n    # exp(-k * diff) will be very large, and the sigmoid will be close to 0.\n\n    # Let's define the sigmoid argument as -(bins_remain_cap - item) to align with typical \"best fit\" logic.\n    # Argument: -(bins_remain_cap - item) = item - bins_remain_cap\n    # We want to maximize the sigmoid when item - bins_remain_cap is close to 0 and non-negative.\n    # This means bins_remain_cap should be close to item.\n\n    # Let's use k=1 for simplicity. The center can be adjusted if needed.\n    # The sigmoid function: S(x) = 1 / (1 + exp(-x))\n    # Our input to sigmoid will be (item - bins_remain_cap)\n    # So, priority = 1 / (1 + exp(-(item - bins_remain_cap)))\n\n    # To handle bins that don't fit (bins_remain_cap < item), their argument (item - bins_remain_cap)\n    # will be positive and potentially large. exp(-(positive)) will be small, leading to a priority near 1.\n    # This is not what we want for bins that don't fit.\n\n    # Let's refine the sigmoid argument and the condition.\n    # We are only interested in bins where bins_remain_cap >= item.\n    # For these bins, let's prioritize those where (bins_remain_cap - item) is minimal.\n\n    # Consider the inverse of the difference: 1 / (bins_remain_cap - item + epsilon).\n    # This gives higher scores for smaller positive differences.\n    # However, we need a bounded score, and a sigmoid is requested.\n\n    # Let's use a sigmoid centered at 0.\n    # argument = -(bins_remain_cap - item) / scale\n    # We want to map bins_remain_cap = item to a high priority.\n    # At bins_remain_cap = item, argument = 0, sigmoid is 0.5. This is not ideal.\n\n    # Let's rethink the \"Sigmoid Fit Score\" in the context of BPP.\n    # A common heuristic is \"Best Fit\": pick the bin with the smallest remaining capacity\n    # that can accommodate the item.\n    # The sigmoid fit score can approximate this by giving a high score to bins\n    # where remaining_capacity is close to item size.\n\n    # Let's create a score where:\n    # 1. If bins_remain_cap < item, priority is 0.\n    # 2. If bins_remain_cap >= item, priority is based on (bins_remain_cap - item).\n    # We want to maximize priority as (bins_remain_cap - item) approaches 0 from the positive side.\n\n    # Let's map (bins_remain_cap - item) to a sigmoid-like curve.\n    # Consider a transformation where the \"ideal\" value for remaining_capacity is 'item'.\n    # We want a function that peaks at item.\n\n    # Sigmoid function: S(x) = 1 / (1 + exp(-k*x))\n    # If we use x = -(bins_remain_cap - item) = item - bins_remain_cap:\n    # If item - bins_remain_cap = 0 (i.e., bins_remain_cap == item), S(0) = 0.5.\n    # If item - bins_remain_cap is positive (i.e., bins_remain_cap < item), S is large (approaching 1).\n    # If item - bins_remain_cap is negative (i.e., bins_remain_cap > item), S is small (approaching 0).\n    # This is inverted from what we want.\n\n    # Let's invert the sigmoid or adjust the argument.\n    # Using `1 - S(x)` or `S(-x)` can invert it.\n    # `1 - S(x) = 1 - 1 / (1 + exp(-x)) = exp(-x) / (1 + exp(-x)) = 1 / (1 + exp(x))`\n    # Let x = item - bins_remain_cap.\n    # Priority = 1 / (1 + exp(item - bins_remain_cap))\n    # If bins_remain_cap == item: Priority = 1 / (1 + exp(0)) = 1 / 2 = 0.5. Still not ideal.\n\n    # Let's use the difference directly: (bins_remain_cap - item)\n    # We want a function that is high when this difference is small and positive.\n    # And low when the difference is large positive, or negative.\n\n    # Consider a sigmoid argument that becomes more negative as `bins_remain_cap - item` increases.\n    # argument = -k * (bins_remain_cap - item)\n    # Let k = 1.\n    # argument = -(bins_remain_cap - item) = item - bins_remain_cap\n    # Priority = 1 / (1 + exp(-(item - bins_remain_cap))) = 1 / (1 + exp(bins_remain_cap - item))\n\n    # Now, if bins_remain_cap == item: priority = 1 / (1 + exp(0)) = 0.5.\n    # If bins_remain_cap > item (positive diff): exp(positive) is large, priority approaches 0.\n    # If bins_remain_cap < item (negative diff): exp(negative) is small, priority approaches 1.\n    # This is still not capturing \"best fit\" well.\n\n    # The core idea of a sigmoid fit is to map a \"fitting metric\" to a priority.\n    # The \"fitting metric\" for best fit is `bins_remain_cap - item`. We want this to be small.\n    # We need a function `f(diff)` such that `f` is maximized when `diff` is minimal (and non-negative).\n\n    # Let's try a different sigmoid form.\n    # Consider `f(x) = exp(-k*x)` for x >= 0. This is a decreasing function.\n    # We need it bounded.\n\n    # Let's consider the normalized difference, but that requires the bin capacity as well.\n    # For online BPP, we typically don't have the original bin capacity readily available,\n    # only the remaining capacity.\n\n    # Let's use the `item / bins_remain_cap` ratio, but it has issues when `bins_remain_cap` is small.\n    # A sigmoid applied to `(item / bins_remain_cap)` could work if we scale it.\n\n    # Revisit the goal: prioritize bins with `bins_remain_cap` close to `item`, where `bins_remain_cap >= item`.\n    # This means `bins_remain_cap - item` should be minimized and non-negative.\n\n    # Let's define a scaled difference `d = (bins_remain_cap - item) / scale`.\n    # We want to map `d=0` to a high value, and `d` large positive or negative to low values.\n    # A Gaussian-like shape can achieve this: `exp(-k * d^2)`.\n    # But the request is for a Sigmoid Fit Score.\n\n    # Let's assume the \"Sigmoid Fit Score\" implies using a sigmoid to map the \"fit quality\" to a [0, 1] range,\n    # and we want to prioritize bins where the remaining capacity is just enough for the item.\n\n    # Let's use the transformation: `sigmoid(k * (item - bins_remain_cap))`\n    # k: controls the steepness. Higher k means the transition from low to high priority is sharper around the \"ideal\" fit.\n    # `item - bins_remain_cap` : this term is positive when `bins_remain_cap < item`, negative when `bins_remain_cap > item`.\n    # We want to give high priority when `bins_remain_cap >= item`, and `bins_remain_cap` is close to `item`.\n    # This means `item - bins_remain_cap` should be close to 0 or a small negative number.\n\n    # If `bins_remain_cap < item` (item - bins_remain_cap is positive):\n    #   `k * (item - bins_remain_cap)` is positive. Sigmoid is close to 1. High priority. INCORRECT.\n\n    # Let's use `sigmoid(-k * (bins_remain_cap - item))`\n    # `bins_remain_cap - item`: this term is positive when `bins_remain_cap > item`, negative when `bins_remain_cap < item`.\n    # We want high priority when `bins_remain_cap` is slightly larger than `item`, so `bins_remain_cap - item` is small and positive.\n    # `-k * (bins_remain_cap - item)`:\n    #   If `bins_remain_cap - item` is small and positive: `-k * small_positive` is small negative. Sigmoid is close to 0.5.\n    #   If `bins_remain_cap - item` is large positive: `-k * large_positive` is large negative. Sigmoid is close to 0.\n    #   If `bins_remain_cap - item` is negative: `-k * negative` is positive. Sigmoid is close to 1.\n\n    # This is also not directly mapping \"best fit\" to high priority.\n\n    # Let's consider the \"fit percentage\" conceptually.\n    # A bin with `bins_remain_cap = 5` and item = `4` has `4/5 = 80%` fit.\n    # A bin with `bins_remain_cap = 10` and item = `4` has `4/10 = 40%` fit.\n    # A bin with `bins_remain_cap = 100` and item = `4` has `4/100 = 4%` fit.\n\n    # This ratio approach needs to be bounded and use sigmoid.\n    # We are interested in bins where `bins_remain_cap >= item`.\n    # For these, we want `bins_remain_cap` to be as close to `item` as possible.\n\n    # Let's consider the reciprocal of the remaining capacity, normalized.\n    # Or the inverse of the \"waste\": `item / bins_remain_cap`.\n    # We want to maximize `item / bins_remain_cap` when `bins_remain_cap >= item`.\n    # The issue is when `bins_remain_cap` is large, this ratio is small.\n    # When `bins_remain_cap` is just slightly larger than `item`, the ratio is close to 1.\n\n    # Let's try mapping `bins_remain_cap` to a sigmoid such that it's high when `bins_remain_cap` is close to `item`.\n    # We need to restrict to bins that can fit.\n\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify bins that can fit the item\n    can_fit_mask = bins_remain_cap >= item\n\n    # For bins that can fit, calculate a \"fit score\" related to how close the remaining capacity is to the item size.\n    # We want to prioritize bins where `bins_remain_cap - item` is minimal.\n    # Let's use `bins_remain_cap - item` as the basis for our sigmoid.\n    # A sigmoid function `1 / (1 + exp(-x))` peaks at x=0.\n    # We want to center our sigmoid around `bins_remain_cap - item = 0`.\n    # If we use `sigmoid(k * (bins_remain_cap - item))`:\n    #   - When `bins_remain_cap - item` is large positive (large waste): Sigmoid is close to 1. High priority. BAD.\n    #   - When `bins_remain_cap - item` is small positive (small waste): Sigmoid is close to 0.5.\n    #   - When `bins_remain_cap - item` is negative (doesn't fit): Sigmoid is close to 0. BAD.\n\n    # We need a sigmoid that *decreases* as `bins_remain_cap - item` increases.\n    # So, let the sigmoid argument be `-k * (bins_remain_cap - item)`.\n    # Let's choose k=1 for simplicity.\n    # Argument: `-(bins_remain_cap - item) = item - bins_remain_cap`.\n\n    # `sigmoid(item - bins_remain_cap)`:\n    #   - `bins_remain_cap < item`: `item - bins_remain_cap > 0`. Sigmoid is > 0.5. (Incorrect for non-fits)\n    #   - `bins_remain_cap = item`: `item - bins_remain_cap = 0`. Sigmoid is 0.5.\n    #   - `bins_remain_cap > item`: `item - bins_remain_cap < 0`. Sigmoid is < 0.5.\n\n    # This is still problematic. Let's constrain it to the `can_fit_mask`.\n\n    # For the bins that can fit: `bins_remain_cap >= item`\n    # We want to give higher scores when `bins_remain_cap - item` is small.\n    # Let's define a transformed value `x = bins_remain_cap - item`.\n    # We want a function `f(x)` that decreases from a peak as `x` increases from 0.\n\n    # Sigmoid form: `1 / (1 + exp(-k * transformed_value))`\n    # Let's try transforming `bins_remain_cap` itself.\n    # If `bins_remain_cap` is very large, its priority should be low.\n    # If `bins_remain_cap` is just `item`, its priority should be high.\n\n    # Consider a sigmoid function that maps `bins_remain_cap` to priority.\n    # The \"ideal\" `bins_remain_cap` is `item`.\n    # We can use a sigmoid like `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    # Let `k` be a steepness parameter. Let's start with `k=1`.\n\n    # `priority = 1 / (1 + exp(bins_remain_cap - item))`\n    #   - `bins_remain_cap < item`: `bins_remain_cap - item` is negative. `exp(negative)` is small. Priority > 0.5.\n    #   - `bins_remain_cap = item`: `bins_remain_cap - item = 0`. `exp(0) = 1`. Priority = 1/2 = 0.5.\n    #   - `bins_remain_cap > item`: `bins_remain_cap - item` is positive. `exp(positive)` is large. Priority < 0.5.\n\n    # This seems to invert the desired behavior for best fit. We want higher priority for smaller waste.\n\n    # Let's use the reciprocal of the 'waste' within a sigmoid.\n    # Waste = `bins_remain_cap - item`.\n    # We want to maximize priority when waste is close to 0.\n    # Consider `1 / (waste + epsilon)`. This increases as waste decreases.\n    # To bound it and use sigmoid, we can map this to sigmoid.\n\n    # Alternative interpretation of \"Sigmoid Fit Score\":\n    # The sigmoid function itself is used to score how well an item fits.\n    # It maps a range of \"fit qualities\" to a [0, 1] priority.\n    # The typical \"best fit\" logic aims to minimize `bins_remain_cap - item`.\n\n    # Let's try `priorities = sigmoid(k * (1 - (bins_remain_cap / (item + epsilon))))`\n    # This uses ratio of item to remaining capacity.\n    # This makes sense if we consider the percentage of capacity utilized by the item.\n    # But we want to prioritize smaller *remaining capacity* when it fits.\n\n    # Let's define a quantity that is maximized when `bins_remain_cap` is just enough for `item`.\n    # `f(bins_remain_cap) = - (bins_remain_cap - item)` for `bins_remain_cap >= item`.\n    # This is maximized when `bins_remain_cap - item` is minimal.\n    # So, the \"score\" is `- (bins_remain_cap - item)` for bins that can fit.\n\n    # We want to map this score to a sigmoid output.\n    # Sigmoid: `1 / (1 + exp(-x))`\n    # If we use `x = k * (-(bins_remain_cap - item)) = k * (item - bins_remain_cap)`\n    # `k` controls steepness. Let's set `k = 1`.\n    # `priorities[can_fit_mask] = 1 / (1 + np.exp(-(item - bins_remain_cap[can_fit_mask])))`\n    # `priorities[can_fit_mask] = 1 / (1 + np.exp(bins_remain_cap[can_fit_mask] - item))`\n\n    # Let's analyze this for `can_fit_mask = True`:\n    #   - `bins_remain_cap = item`: exp(0) = 1. Priority = 1/2 = 0.5.\n    #   - `bins_remain_cap` slightly > `item`: exp(small positive). Priority < 0.5.\n    #   - `bins_remain_cap` much > `item`: exp(large positive). Priority near 0.\n    # This assigns lower priority to bins that are \"better fits\" (less waste).\n\n    # The Sigmoid Fit Score should prioritize bins that are \"nearly full\" after packing.\n    # This implies that the remaining capacity is small but sufficient.\n\n    # Let's try to map `bins_remain_cap` to a desirable value.\n    # The desirable range for `bins_remain_cap` is `[item, item + small_delta]`.\n    # We want to give a high score for `bins_remain_cap` close to `item`.\n\n    # Let's scale the difference `bins_remain_cap - item`.\n    # Consider the inverse of the difference for available bins: `1 / (bins_remain_cap - item + epsilon)`\n    # This score is higher for smaller positive differences.\n    # We can then apply a sigmoid to this transformed score to bound it.\n\n    # Let `score_value = 1.0 / (bins_remain_cap[can_fit_mask] - item + epsilon)`\n    # The maximum `score_value` occurs when `bins_remain_cap - item` is smallest.\n    # This `score_value` can be very large.\n\n    # Let's apply sigmoid to this to get a bounded priority.\n    # We want `sigmoid(transformed_score)` to be high when `score_value` is high.\n    # Let `transformed_score = k * score_value`.\n    # If `k` is large, `sigmoid(large_value)` is close to 1.\n\n    # This approach might be overly complex if there's a standard interpretation.\n\n    # Let's reconsider the prompt's description: \"prioritizes bins that have a remaining capacity close to the item's size\".\n    # This aligns with \"Best Fit\".\n    # A common sigmoid application in optimization is to represent a probability or a \"degree of fitness\".\n\n    # Consider a score that is maximized at `bins_remain_cap = item`.\n    # A function like `exp(-(bins_remain_cap - item)^2)` is Gaussian-like.\n    # A sigmoid alternative: `1 / (1 + exp(-k * (item - bins_remain_cap)))`\n    # Let's analyze `sigmoid(k * (item - bins_remain_cap))`.\n    # Argument: `item - bins_remain_cap`.\n    # - `bins_remain_cap < item`: Arg is positive. Sigmoid > 0.5.\n    # - `bins_remain_cap = item`: Arg is 0. Sigmoid = 0.5.\n    # - `bins_remain_cap > item`: Arg is negative. Sigmoid < 0.5.\n\n    # This seems to give priority to bins that are *underfilled* with respect to the item.\n    # This is the opposite of best fit.\n\n    # Let's try `sigmoid(k * (bins_remain_cap - item))`\n    # Argument: `bins_remain_cap - item`.\n    # - `bins_remain_cap < item`: Arg is negative. Sigmoid < 0.5.\n    # - `bins_remain_cap = item`: Arg is 0. Sigmoid = 0.5.\n    # - `bins_remain_cap > item`: Arg is positive. Sigmoid > 0.5.\n\n    # This gives higher priority to bins with *more* remaining capacity than needed.\n    # Again, not ideal for best fit.\n\n    # Let's invert the mapping. We want high priority for small positive `bins_remain_cap - item`.\n    # Consider the function: `g(x) = 1 / (1 + exp(k * x))` which is `1 - sigmoid(k*x)`.\n    # `g(x)` is decreasing with `x`.\n    # Let `x = bins_remain_cap - item`.\n    # Priority = `1 / (1 + exp(k * (bins_remain_cap - item)))`\n    #   - `bins_remain_cap = item`: Priority = 0.5.\n    #   - `bins_remain_cap` slightly > `item`: Priority < 0.5.\n    #   - `bins_remain_cap` much > `item`: Priority near 0.\n    # This assigns low priority to bins that are better fits.\n\n    # What if we normalize `bins_remain_cap` relative to `item`?\n    # Or consider how much of the *item* fits into the *remaining capacity*.\n    # Ratio `bins_remain_cap / item`. We want this ratio to be close to 1.\n\n    # Let's use a sigmoid centered at `bins_remain_cap = item`.\n    # Consider mapping `bins_remain_cap` to `bins_remain_cap - item`.\n    # We want a function `f(diff)` where `diff = bins_remain_cap - item`.\n    # `f` should be maximized for `diff` near 0 (from the positive side).\n    # Let's define a logistic function (sigmoid) centered such that it peaks at the desired value.\n\n    # A standard sigmoid `1 / (1 + exp(-x))` peaks at `x=0`.\n    # If we transform our variable `bins_remain_cap` so that `item` maps to `0`.\n    # Let `transformed_val = k * (bins_remain_cap - item)`.\n    # We want to map `transformed_val` to a priority.\n    # Priority should be high when `bins_remain_cap` is slightly larger than `item`.\n\n    # Let's define a \"fitness\" score that is good when `bins_remain_cap` is slightly above `item`.\n    # Fitness = `(bins_remain_cap - item)`. We want this to be small and positive.\n    # Let's define a sigmoid for this \"fitness\".\n\n    # Consider the function `sigmoid_centered(x, center, steepness)` which maps `center` to 0.5.\n    # This is `1 / (1 + exp(-steepness * (x - center)))`.\n\n    # We want to map `bins_remain_cap` to a priority score.\n    # Let's use a logistic function that is \"inverted\" such that it decreases.\n    # `priorities = 1 / (1 + exp(k * (bins_remain_cap - item)))` for `bins_remain_cap >= item`.\n    # If `bins_remain_cap` is much larger than `item`, `bins_remain_cap - item` is large positive, priority is near 0.\n    # If `bins_remain_cap` is just `item`, `bins_remain_cap - item` is 0, priority is 0.5.\n    # This is still not quite right.\n\n    # Let's consider what \"Sigmoid Fit Score\" might practically mean in an online setting.\n    # It likely means assigning a score based on how \"tightly\" an item fits.\n    # If a bin has `remaining_capacity = 5` and item size is `4`, then `waste = 1`.\n    # If another bin has `remaining_capacity = 10` and item size is `4`, then `waste = 6`.\n    # We want to favor the bin with `waste = 1`.\n\n    # Let's use a sigmoid to map the `waste` to a priority.\n    # We want high priority for small waste (positive).\n    # A sigmoid mapping `x` to `1 / (1 + exp(-k*x))` maps small `x` to near 0, and large `x` to near 1.\n    # If we use `x = - (bins_remain_cap - item) = item - bins_remain_cap`:\n    # `priorities[can_fit_mask] = 1 / (1 + np.exp(-k * (item - bins_remain_cap[can_fit_mask])))`\n    # `priorities[can_fit_mask] = 1 / (1 + np.exp(k * (bins_remain_cap[can_fit_mask] - item)))`\n\n    # Let's set k=1 for simplicity, and add a slight bias if needed.\n    # With k=1:\n    # `bins_remain_cap = item`  => exp(0) = 1  => priority = 0.5\n    # `bins_remain_cap` slightly > `item` => exp(small_pos) => priority < 0.5\n    # `bins_remain_cap` much > `item` => exp(large_pos) => priority near 0.\n    # This means bins with *more* space remaining get *lower* priority. This is good for best fit!\n\n    # Let's check the behavior of `bins_remain_cap < item`.\n    # Our `can_fit_mask` handles this by setting priorities to 0.\n\n    # So, the strategy is:\n    # 1. Assign 0 priority to bins that cannot fit the item.\n    # 2. For bins that can fit, calculate priority using `1 / (1 + exp(k * (bins_remain_cap - item)))`.\n    #    This assigns higher priority to bins with less remaining capacity (closer to item size).\n\n    # We need to choose a `k` (steepness). A higher `k` makes the decision sharper.\n    # A value of `k=1` might be too gentle. Let's try `k=5`.\n    steepness = 5.0\n\n    # Apply the sigmoid formula only to bins that can fit the item.\n    # For bins that can fit, their priority is calculated as:\n    # P = 1 / (1 + exp(steepness * (remaining_capacity - item)))\n    # This function decreases as remaining_capacity increases relative to item.\n    # Thus, it favors bins with less remaining capacity (best fit).\n\n    # Add epsilon to the denominator of exp for numerical stability if needed, though typically not for exp.\n    # The `bins_remain_cap - item` could be zero. `exp(0) = 1`. No division by zero.\n    # However, to avoid potential overflow if `bins_remain_cap - item` is very large negative (e.g., if item is huge\n    # and capacity is tiny, which is filtered by `can_fit_mask` anyway), we can cap the argument.\n    # Or more relevantly, if `bins_remain_cap - item` is very large positive, exp becomes very large, priority goes to 0.\n\n    # Let's cap the argument to prevent very small exponent values (close to exp(0)).\n    # If `bins_remain_cap - item` is large, `exp(...)` will be large and priority will be close to 0.\n    # This is the desired behavior for large remaining capacity.\n\n    priorities[can_fit_mask] = 1.0 / (1.0 + np.exp(steepness * (bins_remain_cap[can_fit_mask] - item)))\n\n    return priorities",
    "response_id": 26,
    "obj": 4.048663741523748,
    "SLOC": 9.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response27.txt_stdout.txt",
    "code_path": "problem_iter1_code27.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Random Fit.\n\n    In Random Fit, we randomly select a bin that has enough capacity for the item.\n    This priority function assigns a higher priority to bins that have enough capacity\n    for the item, effectively making it more likely for those bins to be chosen.\n    A truly random selection among valid bins would be handled by the selection\n    mechanism outside this function. This function aims to 'filter' bins that\n    cannot fit the item.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # Assign a high priority (e.g., 1) to bins that can accommodate the item,\n    # and a low priority (e.g., 0) to bins that cannot.\n    # This allows the selection logic to pick from valid bins.\n    priorities = np.where(bins_remain_cap >= item, 1, 0)\n    return priorities",
    "response_id": 27,
    "obj": 4.487435181491823,
    "SLOC": 3.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response28.txt_stdout.txt",
    "code_path": "problem_iter1_code28.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Epsilon-Greedy.\n\n    In this version, we explore and exploit bins based on a probability.\n    The core idea is to balance choosing the seemingly \"best\" bin (greedy approach)\n    with occasionally trying other bins to potentially find a better overall packing.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    epsilon = 0.2  # Probability of exploration (trying random bins)\n    num_bins = len(bins_remain_cap)\n    priorities = np.zeros_like(bins_remain_cap, dtype=float)\n\n    # Identify feasible bins for the current item\n    feasible_bins_mask = bins_remain_cap >= item\n    feasible_bins_indices = np.where(feasible_bins_mask)[0]\n\n    if not feasible_bins_indices.size:\n        # No bin can accommodate the item, return all zeros\n        return priorities\n\n    # Greedy part: assign higher priority to bins that leave less remaining capacity\n    # This is a variation of the First Fit Decreasing or Best Fit heuristic\n    # where we prioritize bins that are \"tightest\" fits.\n    # We can normalize remaining capacity to get a 'goodness' score,\n    # or simply use the remaining capacity itself as an indicator for greedy choice.\n    # Here, we'll favor bins with less remaining capacity among the feasible ones.\n    remaining_capacities_feasible = bins_remain_cap[feasible_bins_mask]\n    # A higher score indicates a better greedy choice (less remaining space)\n    # So we can invert the remaining capacity or use a function like 1 / (remaining_capacity - item + 1)\n    # to give higher scores to tighter fits. Adding 1 avoids division by zero.\n    greedy_scores = 1.0 / (remaining_capacities_feasible - item + 1e-9)\n\n    # Apply greedy scores to feasible bins\n    priorities[feasible_bins_mask] = greedy_scores\n\n    # Exploration part: with probability epsilon, choose a random feasible bin\n    if np.random.rand() < epsilon and feasible_bins_indices.size > 0:\n        random_feasible_index = np.random.choice(feasible_bins_indices)\n        # Assign a uniformly high priority to the randomly chosen bin\n        # to ensure it gets a chance. The value is arbitrary but should\n        # be high enough to potentially override greedy scores.\n        priorities = np.zeros_like(bins_remain_cap, dtype=float) # Reset priorities for exploration\n        priorities[random_feasible_index] = 1.0 # Give maximum priority to the random bin\n    elif feasible_bins_indices.size > 0:\n        # Normalize greedy scores to a common range if needed, or just ensure they are positive\n        # and that feasible bins have higher priority than infeasible ones.\n        # We already set feasible bins' priorities > 0. If we want to emphasize\n        # the greedy choice over random exploration for non-random picks:\n        # we can amplify the greedy scores slightly.\n        # Let's scale them to be between 0 and 1, for instance.\n        min_greedy_score = np.min(greedy_scores)\n        max_greedy_score = np.max(greedy_scores)\n        if max_greedy_score - min_greedy_score > 1e-9:\n            priorities[feasible_bins_mask] = (greedy_scores - min_greedy_score) / (max_greedy_score - min_greedy_score)\n        else:\n            # All feasible bins have the same greedy score\n            priorities[feasible_bins_mask] = 0.5 # Assign a neutral high score\n\n\n    return priorities",
    "response_id": 28,
    "obj": 4.3777423214998095,
    "SLOC": 23.0,
    "cyclomatic_complexity": 6.0,
    "exec_success": true
  },
  {
    "stdout_filepath": "problem_iter1_response29.txt_stdout.txt",
    "code_path": "problem_iter1_code29.py",
    "code": "import numpy as np\n\ndef priority_v2(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:\n    \"\"\"Returns priority with which we want to add item to each bin using Softmax-Based Fit.\n\n    The priority is calculated based on how well an item fits into a bin.\n    A higher priority is given to bins where the remaining capacity is just enough\n    or slightly more than the item size. A large surplus capacity is penalized.\n\n    Args:\n        item: Size of item to be added to the bin.\n        bins_remain_cap: Array of remaining capacities for each bin.\n\n    Return:\n        Array of same size as bins_remain_cap with priority score of each bin.\n    \"\"\"\n    # We only consider bins that can actually fit the item\n    # For bins that cannot fit, their priority will be effectively zero after softmax.\n    # However, to avoid issues with log(0) or division by zero, we can set a very small capacity\n    # or filter them out conceptually. For simplicity in softmax, we will give them a very low score.\n\n    # Calculate the \"fitness score\" for each bin.\n    # We want to penalize bins that have significantly more space than needed.\n    # A simple approach is to use the ratio of remaining capacity to item size.\n    # We are looking for a ratio close to 1.\n    # A large ratio (bin_remain_cap >> item) is bad.\n    # A ratio slightly > 1 is good.\n    # A ratio < 1 is impossible.\n\n    # To make it work with softmax, we want higher values for better fits.\n    # Let's define a score that is high when bin_remain_cap is close to item,\n    # and decreases as bin_remain_cap grows larger than item.\n\n    # Consider bins that can accommodate the item.\n    valid_bins_mask = bins_remain_cap >= item\n    \n    # Initialize priorities to a very low value for invalid bins.\n    priorities = np.full_like(bins_remain_cap, -np.inf) \n\n    # For valid bins, calculate a score.\n    # A score of 1 / (bin_remain_cap - item + 1) would give higher scores to smaller remaining capacities.\n    # Adding 1 to the denominator to avoid division by zero if remaining capacity == item.\n    # This score is high when remaining capacity is close to item size, and decreases as it increases.\n    # We want to avoid very large remaining capacities.\n    # Let's scale this score. We can use a term like `1 / (gap + epsilon)` where gap is `bins_remain_cap - item`.\n    # A small gap is good. A large gap is bad.\n    # We can use `exp(-(bins_remain_cap - item) / scale)` or similar for a decaying score.\n    # For softmax, a simpler approach might be to aim for values that differentiate.\n    # Let's try `-(bins_remain_cap - item)`. This gives higher values to smaller gaps.\n    # We need to ensure the output of this calculation is not too sensitive or too uniform.\n    \n    # Let's consider the gap: `bins_remain_cap - item`.\n    # A small gap is good. A large gap is bad.\n    # We want to maximize the value when gap is small.\n    # Let's try a score that is `-(gap)^2` or similar.\n    # Or even simpler: `1.0 / (gap + 1.0)` which maps smaller gaps to higher values.\n    \n    # Using `1.0 / (bins_remain_cap - item + 1e-6)` might be problematic if `item` is very large relative to `bins_remain_cap`.\n    # Let's use a score based on the \"waste\": `bins_remain_cap - item`.\n    # We want to minimize waste.\n    # Let's use an exponential decay on the waste, but inverted.\n    # A high score for low waste. `exp(-waste / temperature)`.\n    # For softmax, `exp(score)` is used. So we need `score` to be high for low waste.\n    # `score = -(bins_remain_cap - item)` or `score = - (bins_remain_cap - item)**2`\n    # Let's try `score = -(bins_remain_cap - item)`. This prioritizes bins with minimal remaining capacity.\n\n    # Softmax requires exp(score). We want scores to be positive for the exponential.\n    # So let's shift the scores to be positive, or use a transformation that results in positive values.\n    # Consider the inverse of the \"waste\": `1 / (waste + epsilon)`.\n    # Or simply use the negative waste directly, and `np.exp` will handle it.\n\n    # Let's use `score = -(bins_remain_cap - item)`. This means smaller gaps get higher scores.\n    # If we want to prioritize bins with *just* enough space, this is reasonable.\n    # A larger gap results in a more negative score.\n    # When softmax is applied, `-inf` scores will be zero.\n    \n    # Let's refine the scoring. We want the ratio `item / bins_remain_cap` to be as close to 1 as possible, but only when `bins_remain_cap >= item`.\n    # Consider the inverse of the remaining capacity: `1.0 / bins_remain_cap`. This prioritizes fuller bins (smaller remaining capacity), which is good if `item` fits.\n    # Let's combine this with the condition.\n    # A potential score for valid bins: `item / bins_remain_cap`. This is > 1 for perfect fit if `bins_remain_cap == item`, and approaches 0 for very large `bins_remain_cap`.\n    # Let's use `1.0 / (bins_remain_cap - item + 1e-6)` to prioritize bins with minimal surplus.\n    # This value is large when `bins_remain_cap - item` is small.\n\n    # Let's use the ratio of item size to bin capacity, but adjusted.\n    # We are interested in bins where `bins_remain_cap` is close to `item`.\n    # Let's assign a score that is high when `bins_remain_cap - item` is small.\n    # Consider `score = - (bins_remain_cap - item)**2`. This gives a parabolic penalty for larger gaps.\n    # The peak is at `bins_remain_cap == item`.\n\n    # A simple way to make it suitable for softmax is to use `exp(score)`\n    # So, we need `score` to be higher for preferred bins.\n    # Preferred bins are those with `bins_remain_cap` closest to `item`.\n    # `score = - (bins_remain_cap - item)`.\n    # The maximum score for `bins_remain_cap = item` would be 0.\n    # If `bins_remain_cap > item`, score becomes negative.\n    \n    # Let's use `bins_remain_cap - item` and invert it.\n    # A bin with `bins_remain_cap = 10` and `item = 5` has a gap of 5.\n    # A bin with `bins_remain_cap = 6` and `item = 5` has a gap of 1.\n    # We want to prioritize the second bin.\n    # So, a smaller gap should result in a higher score.\n    # Let's try `score = 1.0 / (bins_remain_cap - item + epsilon)`.\n    # Or `score = - (bins_remain_cap - item)`.\n\n    # Let's try a simple heuristic that prioritizes bins with the smallest *remaining* capacity that can still fit the item.\n    # This is often called \"Best Fit\".\n    # For Softmax-Based Fit, we can transform this preference into scores.\n    # We want bins where `bins_remain_cap` is small (but >= item) to have high scores.\n    # Consider the transformed scores `scores = -bins_remain_cap` for valid bins.\n    # This will give higher scores to bins with less remaining capacity.\n\n    scores = np.zeros_like(bins_remain_cap)\n    valid_indices = np.where(bins_remain_cap >= item)[0]\n    \n    # Assign a score that prioritizes bins with smaller remaining capacity.\n    # We want `bins_remain_cap` to be small. So, `scores = -bins_remain_cap` would achieve this.\n    # However, the actual values of `bins_remain_cap` might vary widely.\n    # Let's consider the \"waste\" again: `waste = bins_remain_cap - item`.\n    # We want to minimize waste.\n    # `score = -waste` or `score = -waste**2`.\n    # Let's use a function that is high for small waste.\n    # `score = 1.0 / (waste + 1.0)` - this works well when `waste` is small and non-negative.\n    # Example:\n    # waste = 0, score = 1.0\n    # waste = 1, score = 0.5\n    # waste = 5, score = 0.16\n    # waste = 10, score = 0.09\n\n    # Apply this scoring to valid bins.\n    waste = bins_remain_cap[valid_indices] - item\n    scores[valid_indices] = 1.0 / (waste + 1e-6) # Adding epsilon for numerical stability\n\n    # Now, apply softmax to convert these scores into probabilities/priorities.\n    # Softmax function: `exp(score) / sum(exp(scores))`\n    # For selection, we just need the `exp(score)` part, as the denominator is constant for all bins.\n    # `np.exp(scores)` will give us the relative priorities.\n    \n    # If we want to directly use the scores that are higher for better fits,\n    # then the scores themselves can be used, and softmax will normalize them.\n    # Let's use a score that reflects \"how close\" the remaining capacity is to the item size.\n    # A good fit has `bins_remain_cap` slightly larger than `item`.\n    # Let's consider `-(bins_remain_cap - item)`.\n    \n    # A different approach: penalize large remaining capacities.\n    # If `bins_remain_cap < item`, it's not a valid bin, score is effectively 0.\n    # For `bins_remain_cap >= item`:\n    # We want a high score when `bins_remain_cap` is close to `item`.\n    # Let's use `score = -(bins_remain_cap - item)`.\n    \n    # Example: item = 5\n    # Bin 1: remain_cap = 5, score = -(5-5) = 0\n    # Bin 2: remain_cap = 7, score = -(7-5) = -2\n    # Bin 3: remain_cap = 10, score = -(10-5) = -5\n    # Bin 4: remain_cap = 3, score = -inf (or very low)\n    \n    # `np.exp([0, -2, -5, -inf])` would be approximately `[1.0, 0.135, 0.0067, 0.0]`\n    # This gives higher probability to bin 1 (exact fit), then bin 2, then bin 3.\n    # This aligns with a \"Best Fit\" strategy transformed for softmax.\n\n    priorities = np.full_like(bins_remain_cap, -np.inf) # Initialize with negative infinity for softmax to yield 0 priority\n    valid_mask = bins_remain_cap >= item\n    \n    # For valid bins, calculate the score. We want bins with smaller `bins_remain_cap - item` to have higher scores.\n    # Let the score be the negative of the excess capacity.\n    excess_capacity = bins_remain_cap[valid_mask] - item\n    scores_for_valid_bins = -excess_capacity\n    \n    priorities[valid_mask] = scores_for_valid_bins\n\n    # The `np.exp(priorities)` will produce values where higher is better.\n    # Softmax is implicitly applied when selecting the bin with the maximum *logit* (score).\n    # If we need the probabilities, we'd divide by sum(exp(priorities)).\n    # For a priority function that returns scores for `np.argmax`, simply returning `np.exp(priorities)` is common.\n    # Or, even simpler, just return the scores themselves if the selection mechanism uses `np.argmax` on raw scores.\n    # The request implies returning scores for each bin, and the bin with highest priority score is selected.\n    # So, `np.exp(scores)` directly gives the desired output.\n\n    return np.exp(priorities)",
    "response_id": 29,
    "obj": 4.048663741523748,
    "SLOC": 13.0,
    "cyclomatic_complexity": 1.0,
    "exec_success": true
  }
]